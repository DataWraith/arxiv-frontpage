<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Frontpage</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .paper {
            margin-bottom: 30px;
            margin-top: 30px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .paper-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .paper-meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        .paper-abstract {
            margin-bottom: 10px;
        }
        .abstract-short {
            display: inline;
        }
        .abstract-full {
            display: none;
        }
        .more-link {
            color: blue;
            cursor: pointer;
            text-decoration: underline;
        }
        .tag-badge {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 5px;
            margin-bottom: 5px;
            border-radius: 3px;
            font-size: 0.8em;
            color: white;
        }
        .tag-badge.high-confidence {
            opacity: 1;
        }
        .tag-badge.low-confidence {
            opacity: 0.6;
            display: none;
        }
        .interestingness-score {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 10px;
            color: white;
            border-radius: 3px;
            font-weight: bold;
        }
        .interestingness-positive {
            background-color: #4CAF50;
        }
        .interestingness-negative {
            background-color: #f44336;
        }
        .interestingness-neutral {
            background-color: #9e9e9e;
        }
        .last-updated {
            text-align: right;
            color: #666;
            font-size: 0.9em;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .intro {
            text-align: center;
            max-width: 60em;
            margin: 0 auto;
            color: #888;
        }
        .copy-icon {
            display: inline-block;
            width: 16px;
            height: 16px;
            cursor: pointer;
            margin-left: 5px;
            opacity: 0.5;
        }
        .copy-icon:hover {
            opacity: 1;
        }
        .json-popup {
            display: none;
            position: fixed;
            top: 20px;
            right: 20px;
            background: white;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            max-width: 500px;
            max-height: 300px;
            overflow: auto;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        a {
            color: inherit;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h1 {
            text-align: center;
        }
        h1 a {
            text-decoration: underline;
        }
        .date-section {
            margin-bottom: 40px;
        }
        .date-header {
            color: #666;
            font-size: 1.5em;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #eee;
        }
    </style>
</head>
<body>
    <h1>
        <a href="https://github.com/DataWraith/arxiv-frontpage">DataWraith's</a> ArXiv Frontpage
    </h1>

    <div class="last-updated">
        Last updated: 2025-05-29
    </div>

    <p class="intro">
        This frontpage is made by scraping ArXiv's computer science RSS feed and tagging papers with a classifier.
    </p>

    <p class="intro">
        Each tag is weighted according to my preferences to compute a paper's <i>interestingness</i> score.
    </p>
    
    
    <div class="date-section">
        <h2 class="date-header">2025-05-29</h2>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.9872
                </span>
                <a href="https://arxiv.org/abs/2505.22011" target="_blank" rel="noopener noreferrer">Prototype Embedding Optimization for Human-Object Interaction Detection in Livestreaming</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Menghui Zhang, Jing Zhang, Lin Chen, Li Zhuo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Livestreaming often involves interactions between streamers and objects, which is critical for understanding and regulating web content. While human-object interaction (HOI) detection has made some progress in general-purpose video downstream tasks, when applied to recognize the interaction behavior</span>
                
                <span class="abstract-full" style="display: none;">Livestreaming often involves interactions between streamers and objects, which is critical for understanding and regulating web content. While human-object interaction (HOI) detection has made some progress in general-purpose video downstream tasks, when applied to recognize the interaction behaviors between a streamer and different objects in livestreaming, it tends to focuses too much on the objects and neglects their interactions with the streamer, which leads to object bias. To solve this issue, we propose a prototype embedding optimization for human-object interaction detection (PeO-HOI). First, the livestreaming is preprocessed using object detection and tracking techniques to extract features of the human-object (HO) pairs. Then, prototype embedding optimization is adopted to mitigate the effect of object bias on HOI. Finally, after modelling the spatio-temporal context between HO pairs, the HOI detection results are obtained by the prediction head. The experimental results show that the detection accuracy of the proposed PeO-HOI method has detection accuracies of 37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset VidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset BJUT-HOI, which effectively improves the HOI detection performance in livestreaming.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 15.5%">
                            Computer Vision
                        </span>
                <!-- Federated Learning: 3.6 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- LLMs: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.775
                </span>
                <a href="https://arxiv.org/abs/2505.21852" target="_blank" rel="noopener noreferrer">A Provable Approach for End-to-End Safe Reinforcement Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Akifumi Wachi, Kohei Miyaguchi, Takumi Tanabe, Rei Sato, Youhei Akimoto
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A longstanding goal in safe reinforcement learning (RL) is a method to ensure the safety of a policy throughout the entire process, from learning to operation. However, existing safe RL paradigms inherently struggle to achieve this objective. We propose a method, called Provably Lifetime Safe RL (PL</span>
                
                <span class="abstract-full" style="display: none;">A longstanding goal in safe reinforcement learning (RL) is a method to ensure the safety of a policy throughout the entire process, from learning to operation. However, existing safe RL paradigms inherently struggle to achieve this objective. We propose a method, called Provably Lifetime Safe RL (PLS), that integrates offline safe RL with safe policy deployment to address this challenge. Our proposed method learns a policy offline using return-conditioned supervised learning and then deploys the resulting policy while cautiously optimizing a limited set of parameters, known as target returns, using Gaussian processes (GPs). Theoretically, we justify the use of GPs by analyzing the mathematical relationship between target and actual returns. We then prove that PLS finds near-optimal target returns while guaranteeing safety with high probability. Empirically, we demonstrate that PLS outperforms baselines both in safety and reward performance, thereby achieving the longstanding goal to obtain high rewards while ensuring the safety of a policy throughout the lifetime from learning to operation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #44f899" title="Confidence: 7.2%">
                            Reinforcement Learning
                        </span>
                <!-- Federated Learning: 3.6 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Medicine: 1.9 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Computer Vision: 1.0 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.7256
                </span>
                <a href="https://arxiv.org/abs/2503.06226" target="_blank" rel="noopener noreferrer">Optimal Output Feedback Learning Control for Discrete-Time Linear Quadratic Regulation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kedi Xie, Martin Guay, Shimin Wang, Fang Deng, Maobin Lu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper studies the linear quadratic regulation (LQR) problem of unknown discrete-time systems via dynamic output feedback learning control. In contrast to the state feedback, the optimality of the dynamic output feedback control for solving the LQR problem requires an implicit condition on the c</span>
                
                <span class="abstract-full" style="display: none;">This paper studies the linear quadratic regulation (LQR) problem of unknown discrete-time systems via dynamic output feedback learning control. In contrast to the state feedback, the optimality of the dynamic output feedback control for solving the LQR problem requires an implicit condition on the convergence of the state observer. Moreover, due to unknown system matrices and the existence of observer error, it is difficult to analyze the convergence and stability of most existing output feedback learning-based control methods. To tackle these issues, we propose a generalized dynamic output feedback learning control approach with guaranteed convergence, stability, and optimality performance for solving the LQR problem of unknown discrete-time linear systems. In particular, a dynamic output feedback controller is designed to be equivalent to a state feedback controller. This equivalence relationship is an inherent property without requiring convergence of the estimated state by the state observer, which plays a key role in establishing the off-policy learning control approaches. By value iteration and policy iteration schemes, the adaptive dynamic programming based learning control approaches are developed to estimate the optimal feedback control gain. In addition, a model-free stability criterion is provided by finding a nonsingular parameterization matrix, which contributes to establishing a switched iteration scheme. Furthermore, the convergence, stability, and optimality analyses of the proposed output feedback learning control approaches are given. Finally, the theoretical results are validated by two numerical examples.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #44f899" title="Confidence: 7.3%">
                            Reinforcement Learning
                        </span>
                <!-- Federated Learning: 4.4 -->
                    
                <!-- Evolutionary Algorithms: 2.6 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Cryptography: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Medicine: 1.3 -->
                    
                <!-- Finance: 1.2 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.6836
                </span>
                <a href="https://arxiv.org/abs/2407.19696" target="_blank" rel="noopener noreferrer">Cross-Layer Feature Pyramid Transformer for Small Object Detection in Aerial Images</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zewen Du, Zhenjiang Hu, Guiyu Zhao, Ying Jin, Hongbin Ma
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Object detection in aerial images has always been a challenging task due to the generally small size of the objects. Most current detectors prioritize the development of new detection frameworks, often overlooking research on fundamental components such as feature pyramid networks. In this paper, we</span>
                
                <span class="abstract-full" style="display: none;">Object detection in aerial images has always been a challenging task due to the generally small size of the objects. Most current detectors prioritize the development of new detection frameworks, often overlooking research on fundamental components such as feature pyramid networks. In this paper, we introduce the Cross-Layer Feature Pyramid Transformer (CFPT), a novel upsampler-free feature pyramid network designed specifically for small object detection in aerial images. CFPT incorporates two meticulously designed attention blocks with linear computational complexity: Cross-Layer Channel-Wise Attention (CCA) and Cross-Layer Spatial-Wise Attention (CSA). CCA achieves cross-layer interaction by dividing channel-wise token groups to perceive cross-layer global information along the spatial dimension, while CSA enables cross-layer interaction by dividing spatial-wise token groups to perceive cross-layer global information along the channel dimension. By integrating these modules, CFPT enables efficient cross-layer interaction in a single step, thereby avoiding the semantic gap and information loss associated with element-wise summation and layer-by-layer transmission. In addition, CFPT incorporates global contextual information, which improves detection performance for small objects. To further enhance location awareness during cross-layer interaction, we propose the Cross-Layer Consistent Relative Positional Encoding (CCPE) based on inter-layer mutual receptive fields. We evaluate the effectiveness of CFPT on three challenging object detection datasets in aerial images: VisDrone2019-DET, TinyPerson, and xView. Extensive experiments demonstrate that CFPT outperforms state-of-the-art feature pyramid networks while incurring lower computational costs. The code is available at https://github.com/duzw9311/CFPT.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 12.7%">
                            Computer Vision
                        </span>
                <!-- GNN: 3.0 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- LLMs: 2.4 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.6088
                </span>
                <a href="https://arxiv.org/abs/2505.21952" target="_blank" rel="noopener noreferrer">Properties of zero-determinant strategies in multichannel games</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Masahiko Ueda
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Controlling payoffs in repeated games is one of the important topics in control theory of multi-agent systems. Recently proposed zero-determinant strategies enable players to unilaterally enforce linear relations between payoffs. Furthermore, based on the mathematics of zero-determinant strategies, </span>
                
                <span class="abstract-full" style="display: none;">Controlling payoffs in repeated games is one of the important topics in control theory of multi-agent systems. Recently proposed zero-determinant strategies enable players to unilaterally enforce linear relations between payoffs. Furthermore, based on the mathematics of zero-determinant strategies, regional payoff control, in which payoffs are enforced into some feasible regions, has been discovered in social dilemma situations. More recently, theory of payoff control was extended to multichannel games, where players parallelly interact with each other in multiple channels. However, properties of zero-determinant strategies specific to multichannel games are still not clear. In this paper, we elucidate properties of zero-determinant strategies in multichannel games. First, we relate the existence condition of zero-determinant strategies in multichannel games to that of zero-determinant strategies in each channel. We then show that the existence of zero-determinant strategies in multichannel games requires the existence of zero-determinant strategies in some channels. This result implies that the existence of zero-determinant strategies in multichannel games is tightly restricted by structure of games played in each channel.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 11.1%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #546bc5" title="Confidence: 10.2%">
                            Game Theory
                        </span>
                <!-- Federated Learning: 3.4 -->
                    
                <!-- Bayesian Optimization: 2.7 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Cryptography: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Networks: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.5454
                </span>
                <a href="https://arxiv.org/abs/2505.22492" target="_blank" rel="noopener noreferrer">Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hongyi Zhou, Josiah P. Hanna, Jin Zhu, Ying Yang, Chengchun Shi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper studies off-policy evaluation (OPE) in reinforcement learning with a focus on behavior policy estimation for importance sampling. Prior work has shown empirically that estimating a history-dependent behavior policy can lead to lower mean squared error (MSE) even when the true behavior pol</span>
                
                <span class="abstract-full" style="display: none;">This paper studies off-policy evaluation (OPE) in reinforcement learning with a focus on behavior policy estimation for importance sampling. Prior work has shown empirically that estimating a history-dependent behavior policy can lead to lower mean squared error (MSE) even when the true behavior policy is Markovian. However, the question of why the use of history should lower MSE remains open. In this paper, we theoretically demystify this paradox by deriving a bias-variance decomposition of the MSE of ordinary importance sampling (IS) estimators, demonstrating that history-dependent behavior policy estimation decreases their asymptotic variances while increasing their finite-sample biases. Additionally, as the estimated behavior policy conditions on a longer history, we show a consistent decrease in variance. We extend these findings to a range of other OPE estimators, including the sequential IS estimator, the doubly robust estimator and the marginalized IS estimator, with the behavior policy estimated either parametrically or non-parametrically.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #44f899" title="Confidence: 5.1%">
                            Reinforcement Learning
                        </span>
                <!-- Federated Learning: 3.2 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Medicine: 2.2 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Game Theory: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.473
                </span>
                <a href="https://arxiv.org/abs/2505.22353" target="_blank" rel="noopener noreferrer">VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Noora Al-Emadi, Ingmar Weber, Yin Yang, Ferda Ofli
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Detecting vehicles in satellite images is crucial for traffic management, urban planning, and disaster response. However, current models struggle with real-world diversity, particularly across different regions. This challenge is amplified by geographic bias in existing datasets, which often focus o</span>
                
                <span class="abstract-full" style="display: none;">Detecting vehicles in satellite images is crucial for traffic management, urban planning, and disaster response. However, current models struggle with real-world diversity, particularly across different regions. This challenge is amplified by geographic bias in existing datasets, which often focus on specific areas and overlook regions like the Middle East. To address this gap, we present the Vehicles in the Middle East (VME) dataset, designed explicitly for vehicle detection in high-resolution satellite images from Middle Eastern countries. Sourced from Maxar, the VME dataset spans 54 cities across 12 countries, comprising over 4,000 image tiles and more than 100,000 vehicles, annotated using both manual and semi-automated methods. Additionally, we introduce the largest benchmark dataset for Car Detection in Satellite Imagery (CDSI), combining images from multiple sources to enhance global car detection. Our experiments demonstrate that models trained on existing datasets perform poorly on Middle Eastern images, while the VME dataset significantly improves detection accuracy in this region. Moreover, state-of-the-art models trained on CDSI achieve substantial improvements in global car detection.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 8.1%">
                            Computer Vision
                        </span>
                <!-- Medicine: 4.5 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Datasets: 3.0 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.351
                </span>
                <a href="https://arxiv.org/abs/2505.22636" target="_blank" rel="noopener noreferrer">ObjectClear: Complete Object Removal via Object-Effect Attention</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jixin Zhao, Shangchen Zhou, Zhouxia Wang, Peiqing Yang, Chen Change Loy
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Object removal requires eliminating not only the target object but also its effects, such as shadows and reflections. However, diffusion-based inpainting methods often produce artifacts, hallucinate content, alter background, and struggle to remove object effects accurately. To address this challeng</span>
                
                <span class="abstract-full" style="display: none;">Object removal requires eliminating not only the target object but also its effects, such as shadows and reflections. However, diffusion-based inpainting methods often produce artifacts, hallucinate content, alter background, and struggle to remove object effects accurately. To address this challenge, we introduce a new dataset for OBject-Effect Removal, named OBER, which provides paired images with and without object effects, along with precise masks for both objects and their associated visual artifacts. The dataset comprises high-quality captured and simulated data, covering diverse object categories and complex multi-object scenes. Building on OBER, we propose a novel framework, ObjectClear, which incorporates an object-effect attention mechanism to guide the model toward the foreground removal regions by learning attention masks, effectively decoupling foreground removal from background reconstruction. Furthermore, the predicted attention map enables an attention-guided fusion strategy during inference, greatly preserving background details. Extensive experiments demonstrate that ObjectClear outperforms existing methods, achieving improved object-effect removal quality and background fidelity, especially in complex scenarios.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 7.5%">
                            Computer Vision
                        </span>
                <!-- Medicine: 4.5 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- HPO and AutoML: 2.0 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.3027
                </span>
                <a href="https://arxiv.org/abs/2505.21705" target="_blank" rel="noopener noreferrer">Preconditioning transformations of adjoint systems for evolution equations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Brian K. Tran, Ben S. Southworth, Hannah F. Blumhoefer, Samuel Olivier
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Achieving robust control and optimization in high-fidelity physics simulations is extremely challenging, especially for evolutionary systems whose solutions span vast scales across space, time, and physical variables. In conjunction with gradient-based methods, adjoint systems are widely used in the</span>
                
                <span class="abstract-full" style="display: none;">Achieving robust control and optimization in high-fidelity physics simulations is extremely challenging, especially for evolutionary systems whose solutions span vast scales across space, time, and physical variables. In conjunction with gradient-based methods, adjoint systems are widely used in the optimization of systems subject to differential equation constraints. In optimization, gradient-based methods are often transformed using suitable preconditioners to accelerate the convergence of the optimization algorithm. Inspired by preconditioned gradient descent methods, we introduce a framework for the preconditioning of adjoint systems associated to evolution equations, which allows one to reshape the dynamics of the adjoint system. We develop two classes of adjoint preconditioning transformations: those that transform both the state dynamics and the adjoint equation and those that transform only the adjoint equation while leaving the state dynamics invariant. Both classes of transformations have the flexibility to include generally nonlinear state-dependent transformations. Using techniques from symplectic geometry and Hamiltonian mechanics, we further show that these preconditioned adjoint systems preserve the property that the adjoint system backpropagates the derivative of an objective function. We then apply this framework to the setting of coupled evolution equations, where we develop a notion of scale preconditioning of the adjoint equations when the state dynamics exhibit large scale-separation. We demonstrate the proposed scale preconditioning on an inverse problem for the radiation diffusion equations. Naive gradient descent is unstable for any practical gradient descent step size, whereas our proposed scale-preconditioned adjoint descent converges in 10-15 gradient-based optimization iterations, with highly accurate reproduction of the wavefront at the final time.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #31bb31" title="Confidence: 6.7%">
                            Bayesian Optimization
                        </span>
                <!-- Federated Learning: 4.6 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Evolutionary Algorithms: 2.7 -->
                    
                <!-- Math: 2.5 -->
                    
                <!-- Medicine: 2.2 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- LLMs: 1.4 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2698
                </span>
                <a href="https://arxiv.org/abs/2505.22604" target="_blank" rel="noopener noreferrer">Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ruixuan Zhang, He Wang, Zhengyu Zhao, Zhiqing Guo, Xun Yang, Yunfeng Diao, Meng Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Rapid advances in Artificial Intelligence Generated Images (AIGI) have facilitated malicious use, such as forgery and misinformation. Therefore, numerous methods have been proposed to detect fake images. Although such detectors have been proven to be universally vulnerable to adversarial attacks, de</span>
                
                <span class="abstract-full" style="display: none;">Rapid advances in Artificial Intelligence Generated Images (AIGI) have facilitated malicious use, such as forgery and misinformation. Therefore, numerous methods have been proposed to detect fake images. Although such detectors have been proven to be universally vulnerable to adversarial attacks, defenses in this field are scarce. In this paper, we first identify that adversarial training (AT), widely regarded as the most effective defense, suffers from performance collapse in AIGI detection. Through an information-theoretic lens, we further attribute the cause of collapse to feature entanglement, which disrupts the preservation of feature-label mutual information. Instead, standard detectors show clear feature separation. Motivated by this difference, we propose Training-free Robust Detection via Information-theoretic Measures (TRIM), the first training-free adversarial defense for AIGI detection. TRIM builds on standard detectors and quantifies feature shifts using prediction entropy and KL divergence. Extensive experiments across multiple datasets and attacks validate the superiority of our TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%) on ProGAN (GenImage), while well maintaining original accuracy.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 5.0%">
                            Computer Vision
                        </span>
                <!-- LLMs: 4.8 -->
                    
                <!-- GNN: 2.9 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2384
                </span>
                <a href="https://arxiv.org/abs/2505.21649" target="_blank" rel="noopener noreferrer">Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Keanu Nichols, Nazia Tasnim, Yan Yuting, Nicholas Ikechukwu, Elva Zou, Deepti Ghadiyaram, Bryan Plummer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Object orientation understanding represents a fundamental challenge in visual perception critical for applications like robotic manipulation and augmented reality. Current vision-language benchmarks fail to isolate this capability, often conflating it with positional relationships and general scene </span>
                
                <span class="abstract-full" style="display: none;">Object orientation understanding represents a fundamental challenge in visual perception critical for applications like robotic manipulation and augmented reality. Current vision-language benchmarks fail to isolate this capability, often conflating it with positional relationships and general scene understanding. We introduce DORI (Discriminative Orientation Reasoning Intelligence), a comprehensive benchmark establishing object orientation perception as a primary evaluation target. DORI assesses four dimensions of orientation comprehension: frontal alignment, rotational transformations, relative directional relationships, and canonical orientation understanding. Through carefully curated tasks from 11 datasets spanning 67 object categories across synthetic and real-world scenarios, DORI provides insights on how multi-modal systems understand object orientations. Our evaluation of 15 state-of-the-art vision-language models reveals critical limitations: even the best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular orientation judgments, with performance deteriorating for tasks requiring reference frame shifts or compound rotations. These findings demonstrate the need for dedicated orientation representation mechanisms, as models show systematic inability to perform precise angular estimations, track orientation changes across viewpoints, and understand compound rotations - suggesting limitations in their internal 3D spatial representations. As the first diagnostic framework specifically designed for orientation awareness in multimodal systems, DORI offers implications for improving robotic control, 3D scene reconstruction, and human-AI interaction in physical environments. DORI data: https://huggingface.co/datasets/appledora/DORI-Benchmark</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 6.0%">
                            Computer Vision
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.9%">
                            LLMs
                        </span>
                <!-- Medicine: 4.1 -->
                    
                <!-- Hardware: 2.3 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2333
                </span>
                <a href="https://arxiv.org/abs/2505.22421" target="_blank" rel="noopener noreferrer">GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Anthony Chen, Wenzhao Zheng, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Kurt Keutzer, Shangbang Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advancements in world models have revolutionized dynamic environment simulation, allowing systems to foresee future states and assess potential actions. In autonomous driving, these capabilities help vehicles anticipate the behavior of other road users, perform risk-aware planning, accelerate</span>
                
                <span class="abstract-full" style="display: none;">Recent advancements in world models have revolutionized dynamic environment simulation, allowing systems to foresee future states and assess potential actions. In autonomous driving, these capabilities help vehicles anticipate the behavior of other road users, perform risk-aware planning, accelerate training in simulation, and adapt to novel scenarios, thereby enhancing safety and reliability. Current approaches exhibit deficiencies in maintaining robust 3D geometric consistency or accumulating artifacts during occlusion handling, both critical for reliable safety assessment in autonomous navigation tasks. To address this, we introduce GeoDrive, which explicitly integrates robust 3D geometry conditions into driving world models to enhance spatial understanding and action controllability. Specifically, we first extract a 3D representation from the input frame and then obtain its 2D rendering based on the user-specified ego-car trajectory. To enable dynamic modeling, we propose a dynamic editing module during training to enhance the renderings by editing the positions of the vehicles. Extensive experiments demonstrate that our method significantly outperforms existing models in both action accuracy and 3D spatial awareness, leading to more realistic, adaptable, and reliable scene modeling for safer autonomous driving. Additionally, our model can generalize to novel trajectories and offers interactive scene editing capabilities, such as object editing and object trajectory control.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 8.5%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 5.5%">
                            Computer Vision
                        </span>
                <!-- Medicine: 3.4 -->
                    
                <!-- 3D: 3.4 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.204
                </span>
                <a href="https://arxiv.org/abs/2502.02121" target="_blank" rel="noopener noreferrer">BILBO: BILevel Bayesian Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ruth Wan Theng Chew, Quoc Phong Nguyen, Bryan Kian Hsiang Low
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Bilevel optimization is characterized by a two-level optimization structure, where the upper-level problem is constrained by optimal lower-level solutions, and such structures are prevalent in real-world problems. The constraint by optimal lower-level solutions poses significant challenges, especial</span>
                
                <span class="abstract-full" style="display: none;">Bilevel optimization is characterized by a two-level optimization structure, where the upper-level problem is constrained by optimal lower-level solutions, and such structures are prevalent in real-world problems. The constraint by optimal lower-level solutions poses significant challenges, especially in noisy, constrained, and derivative-free settings, as repeating lower-level optimizations is sample inefficient and predicted lower-level solutions may be suboptimal. We present BILevel Bayesian Optimization (BILBO), a novel Bayesian optimization algorithm for general bilevel problems with blackbox functions, which optimizes both upper- and lower-level problems simultaneously, without the repeated lower-level optimization required by existing methods. BILBO samples from confidence-bounds based trusted sets, which bounds the suboptimality on the lower level. Moreover, BILBO selects only one function query per iteration, where the function query selection strategy incorporates the uncertainty of estimated lower-level solutions and includes a conditional reassignment of the query to encourage exploration of the lower-level objective. The performance of BILBO is theoretically guaranteed with a sublinear regret bound for commonly used kernels and is empirically evaluated on several synthetic and real-world problems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #31bb31" title="Confidence: 5.7%">
                            Bayesian Optimization
                        </span>
                <!-- Evolutionary Algorithms: 2.7 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Medicine: 2.2 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- LLMs: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.1958
                </span>
                <a href="https://arxiv.org/abs/2505.21974" target="_blank" rel="noopener noreferrer">BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yu-Heng Hung, Kai-Jie Lin, Yu-Heng Lin, Chien-YiWang, Cheng Sun, Ping-Chun Hsieh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Bayesian optimization (BO) offers an efficient pipeline for optimizing black-box functions with the help of a Gaussian process prior and an acquisition function (AF). Recently, in the context of single-objective BO, learning-based AFs witnessed promising empirical results given its favorable non-myo</span>
                
                <span class="abstract-full" style="display: none;">Bayesian optimization (BO) offers an efficient pipeline for optimizing black-box functions with the help of a Gaussian process prior and an acquisition function (AF). Recently, in the context of single-objective BO, learning-based AFs witnessed promising empirical results given its favorable non-myopic nature. Despite this, the direct extension of these approaches to multi-objective Bayesian optimization (MOBO) suffer from the \textit{hypervolume identifiability issue}, which results from the non-Markovian nature of MOBO problems. To tackle this, inspired by the non-Markovian RL literature and the success of Transformers in language modeling, we present a generalized deep Q-learning framework and propose \textit{BOFormer}, which substantiates this framework for MOBO via sequence modeling. Through extensive evaluation, we demonstrate that BOFormer constantly outperforms the benchmark rule-based and learning-based algorithms in various synthetic MOBO and real-world multi-objective hyperparameter optimization problems. We have made the source code publicly available to encourage further research in this direction.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #31bb31" title="Confidence: 5.1%">
                            Bayesian Optimization
                        </span>
                <!-- LLMs: 3.3 -->
                    
                <!-- Federated Learning: 2.8 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.1949
                </span>
                <a href="https://arxiv.org/abs/2505.21721" target="_blank" rel="noopener noreferrer">Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kyurae Kim, Yi-An Ma, Trevor Campbell, Jacob R. Gardner
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We prove that, given a mean-field location-scale variational family, black-box variational inference (BBVI) with the reparametrization gradient converges at an almost dimension-independent rate. Specifically, for strongly log-concave and log-smooth targets, the number of iterations for BBVI with a s</span>
                
                <span class="abstract-full" style="display: none;">We prove that, given a mean-field location-scale variational family, black-box variational inference (BBVI) with the reparametrization gradient converges at an almost dimension-independent rate. Specifically, for strongly log-concave and log-smooth targets, the number of iterations for BBVI with a sub-Gaussian family to achieve an objective $\epsilon$-close to the global optimum is $\mathrm{O}(\log d)$, which improves over the $\mathrm{O}(d)$ dependence of full-rank location-scale families. For heavy-tailed families, we provide a weaker $\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the number of finite moments. Additionally, if the Hessian of the target log-density is constant, the complexity is free of any explicit dimension dependence. We also prove that our bound on the gradient variance, which is key to our result, cannot be improved using only spectral bounds on the Hessian of the target log-density.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #31bb31" title="Confidence: 5.1%">
                            Bayesian Optimization
                        </span>
                <!-- Math: 3.6 -->
                    
                <!-- Federated Learning: 3.6 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Cryptography: 1.4 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.1448
                </span>
                <a href="https://arxiv.org/abs/2505.22499" target="_blank" rel="noopener noreferrer">The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aixuan Li, Mochu Xiang, Jing Zhang, Yuchao Dai
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">3D object detection is a critical component in autonomous driving systems. It allows real-time recognition and detection of vehicles, pedestrians and obstacles under varying environmental conditions. Among existing methods, 3D object detection in the Bird's Eye View (BEV) has emerged as the mainstre</span>
                
                <span class="abstract-full" style="display: none;">3D object detection is a critical component in autonomous driving systems. It allows real-time recognition and detection of vehicles, pedestrians and obstacles under varying environmental conditions. Among existing methods, 3D object detection in the Bird's Eye View (BEV) has emerged as the mainstream framework. To guarantee a safe, robust and trustworthy 3D object detection, 3D adversarial attacks are investigated, where attacks are placed in 3D environments to evaluate the model performance, e.g., putting a film on a car, clothing a pedestrian. The vulnerability of 3D object detection models to 3D adversarial attacks serves as an important indicator to evaluate the robustness of the model against perturbations. To investigate this vulnerability, we generate non-invasive 3D adversarial objects tailored for real-world attack scenarios. Our method verifies the existence of universal adversarial objects that are spatially consistent across time and camera views. Specifically, we employ differentiable rendering techniques to accurately model the spatial relationship between adversarial objects and the target vehicle. Furthermore, we introduce an occlusion-aware module to enhance visual consistency and realism under different viewpoints. To maintain attack effectiveness across multiple frames, we design a BEV spatial feature-guided optimization strategy. Experimental results demonstrate that our approach can reliably suppress vehicle predictions from state-of-the-art 3D object detectors, serving as an important tool to test robustness of 3D object detection models before deployment. Moreover, the generated adversarial objects exhibit strong generalization capabilities, retaining its effectiveness at various positions and distances in the scene.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 11.6%">
                            Computer Vision
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #76aa96" title="Confidence: 5.4%">
                            3D
                        </span>
                <!-- Medicine: 2.9 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- LLMs: 2.1 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0407
                </span>
                <a href="https://arxiv.org/abs/2505.21854" target="_blank" rel="noopener noreferrer">Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jun Chen, Xinke Li, Mingyue Xu, Tianrui Li, Chongshou Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Gradient-based adversarial attacks have become a dominant approach for evaluating the robustness of point cloud classification models. However, existing methods often rely on uniform update rules that fail to consider the heterogeneous nature of point clouds, resulting in excessive and perceptible p</span>
                
                <span class="abstract-full" style="display: none;">Gradient-based adversarial attacks have become a dominant approach for evaluating the robustness of point cloud classification models. However, existing methods often rely on uniform update rules that fail to consider the heterogeneous nature of point clouds, resulting in excessive and perceptible perturbations. In this paper, we rethink the design of gradient-based attacks by analyzing the limitations of conventional gradient update mechanisms and propose two new strategies to improve both attack effectiveness and imperceptibility. First, we introduce WAAttack, a novel framework that incorporates weighted gradients and an adaptive step-size strategy to account for the non-uniform contribution of points during optimization. This approach enables more targeted and subtle perturbations by dynamically adjusting updates according to the local structure and sensitivity of each point. Second, we propose SubAttack, a complementary strategy that decomposes the point cloud into subsets and focuses perturbation efforts on structurally critical regions. Together, these methods represent a principled rethinking of gradient-based adversarial attacks for 3D point cloud classification. Extensive experiments demonstrate that our approach outperforms state-of-the-art baselines in generating highly imperceptible adversarial examples. Code will be released upon paper acceptance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.3%">
                            LLMs
                        </span>
                <!-- Medicine: 3.8 -->
                    
                <!-- Federated Learning: 3.7 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.102
                </span>
                <a href="https://arxiv.org/abs/2505.21848" target="_blank" rel="noopener noreferrer">FPAN: Mitigating Replication in Diffusion Models through the Fine-Grained Probabilistic Addition of Noise to Token Embeddings</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jingqi Xu, Chenghao Li, Yuke Zhang, Peter A. Beerel
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Diffusion models have demonstrated remarkable potential in generating high-quality images. However, their tendency to replicate training data raises serious privacy concerns, particularly when the training datasets contain sensitive or private information. Existing mitigation strategies primarily fo</span>
                
                <span class="abstract-full" style="display: none;">Diffusion models have demonstrated remarkable potential in generating high-quality images. However, their tendency to replicate training data raises serious privacy concerns, particularly when the training datasets contain sensitive or private information. Existing mitigation strategies primarily focus on reducing image duplication, modifying the cross-attention mechanism, and altering the denoising backbone architecture of diffusion models. Moreover, recent work has shown that adding a consistent small amount of noise to text embeddings can reduce replication to some degree. In this work, we begin by analyzing the impact of adding varying amounts of noise. Based on our analysis, we propose a fine-grained noise injection technique that probabilistically adds a larger amount of noise to token embeddings. We refer to our method as Fine-grained Probabilistic Addition of Noise (FPAN). Through our extensive experiments, we show that our proposed FPAN can reduce replication by an average of 28.78% compared to the baseline diffusion model without significantly impacting image quality, and outperforms the prior consistent-magnitude-noise-addition approach by 26.51%. Moreover, when combined with other existing mitigation methods, our FPAN approach can further reduce replication by up to 16.82% with similar, if not improved, image quality.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 7.1%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #b3ebae" title="Confidence: 5.1%">
                            Federated Learning
                        </span>
                <!-- GNN: 2.6 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Medicine: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1381
                </span>
                <a href="https://arxiv.org/abs/2505.19634" target="_blank" rel="noopener noreferrer">Faster and Better LLMs via Latency-Aware Test-Time Scaling</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zili Wang, Tianyu Zhang, Lei Zhu, Haoli Bai, Lu Hou, Shiming Xiang, Xianzhi Yu, Wulong Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Test-Time Scaling (TTS) has proven effective in improving the performance of Large Language Models (LLMs) during inference. However, existing research has overlooked the efficiency of TTS from a latency-sensitive perspective. Through a latency-aware evaluation of representative TTS methods, we demon</span>
                
                <span class="abstract-full" style="display: none;">Test-Time Scaling (TTS) has proven effective in improving the performance of Large Language Models (LLMs) during inference. However, existing research has overlooked the efficiency of TTS from a latency-sensitive perspective. Through a latency-aware evaluation of representative TTS methods, we demonstrate that a compute-optimal TTS does not always result in the lowest latency in scenarios where latency is critical. To address this gap and achieve latency-optimal TTS, we propose two key approaches by optimizing the concurrency configurations: (1) branch-wise parallelism, which leverages multiple concurrent inference branches, and (2) sequence-wise parallelism, enabled by speculative decoding. By integrating these two approaches and allocating computational resources properly to each, our latency-optimal TTS enables a 32B model to reach 82.3% accuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4% within 10 seconds. Our work emphasizes the importance of latency-aware TTS and demonstrates its ability to deliver both speed and accuracy in latency-sensitive scenarios.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 16.6%">
                            LLMs
                        </span>
                <!-- Medicine: 2.7 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.2055
                </span>
                <a href="https://arxiv.org/abs/2505.21999" target="_blank" rel="noopener noreferrer">Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ashim Gupta, Maitrey Mehta, Zhichao Xu, Vivek Srikumar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language models (LLMs) provide detailed and impressive responses to queries in English. However, are they really consistent at responding to the same query in other languages? The popular way of evaluating for multilingual performance of LLMs requires expensive-to-collect annotated datasets. F</span>
                
                <span class="abstract-full" style="display: none;">Large language models (LLMs) provide detailed and impressive responses to queries in English. However, are they really consistent at responding to the same query in other languages? The popular way of evaluating for multilingual performance of LLMs requires expensive-to-collect annotated datasets. Further, evaluating for tasks like open-ended generation, where multiple correct answers may exist, is nontrivial. Instead, we propose to evaluate the predictability of model response across different languages. In this work, we propose a framework to evaluate LLM's cross-lingual consistency based on a simple Translate then Evaluate strategy. We instantiate this evaluation framework along two dimensions of consistency: information and empathy. Our results reveal pronounced inconsistencies in popular LLM responses across thirty languages, with severe performance deficits in certain language families and scripts, underscoring critical weaknesses in their multilingual capabilities. These findings necessitate cross-lingual evaluations that are consistent along multiple dimensions. We invite practitioners to use our framework for future multilingual LLM benchmarking.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 28.8%">
                            LLMs
                        </span>
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Medicine: 1.5 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.2746
                </span>
                <a href="https://arxiv.org/abs/2503.12051" target="_blank" rel="noopener noreferrer">TLUE: A Tibetan Language Understanding Evaluation Benchmark</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fan Gao, Cheng Huang, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Hao Wang Xiao Feng, Yongbin Yu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language models (LLMs) have made tremendous progress in recent years, but low-resource languages, such as Tibetan, remain significantly underrepresented in their evaluation. Despite Tibetan being spoken by over seven million people, it has largely been neglected in the development and assessme</span>
                
                <span class="abstract-full" style="display: none;">Large language models (LLMs) have made tremendous progress in recent years, but low-resource languages, such as Tibetan, remain significantly underrepresented in their evaluation. Despite Tibetan being spoken by over seven million people, it has largely been neglected in the development and assessment of LLMs. To address this gap, we present TLUE (A Tibetan Language Understanding Evaluation Benchmark), the first large-scale benchmark for assessing LLMs' capabilities in Tibetan. TLUE comprises two major components: (1) a comprehensive multi-task understanding benchmark spanning 5 domains and 67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a diverse set of state-of-the-art LLMs. Experimental results demonstrate that most LLMs perform below the random baseline, highlighting the considerable challenges LLMs face in processing Tibetan, a low-resource language. TLUE provides an essential foundation for driving future research and progress in Tibetan language understanding and underscores the need for greater inclusivity in LLM development.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 24.7%">
                            LLMs
                        </span>
                <!-- Medicine: 2.4 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Datasets: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.4087
                </span>
                <a href="https://arxiv.org/abs/2410.13080" target="_blank" rel="noopener noreferrer">Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Linhao Luo, Zicheng Zhao, Gholamreza Haffari, Yuan-Fang Li, Chen Gong, Shirui Pan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language models (LLMs) have demonstrated impressive reasoning abilities, but they still struggle with faithful reasoning due to knowledge gaps and hallucinations. To address these issues, knowledge graphs (KGs) have been utilized to enhance LLM reasoning through their structured knowledge. How</span>
                
                <span class="abstract-full" style="display: none;">Large language models (LLMs) have demonstrated impressive reasoning abilities, but they still struggle with faithful reasoning due to knowledge gaps and hallucinations. To address these issues, knowledge graphs (KGs) have been utilized to enhance LLM reasoning through their structured knowledge. However, existing KG-enhanced methods, either retrieval-based or agent-based, encounter difficulties in accurately retrieving knowledge and efficiently traversing KGs at scale. In this work, we introduce graph-constrained reasoning (GCR), a novel framework that bridges structured knowledge in KGs with unstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures faithful KG-grounded reasoning by integrating KG structure into the LLM decoding process through KG-Trie, a trie-based index that encodes KG reasoning paths. KG-Trie constrains the decoding process, allowing LLMs to directly reason on graphs and generate faithful reasoning paths grounded in KGs. Additionally, GCR leverages a lightweight KG-specialized LLM for graph-constrained reasoning alongside a powerful general LLM for inductive reasoning over multiple reasoning paths, resulting in accurate reasoning with zero reasoning hallucination. Extensive experiments on several KGQA benchmarks demonstrate that GCR achieves state-of-the-art performance and exhibits strong zero-shot generalizability to unseen KGs without additional training.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 57.2%">
                            LLMs
                        </span>
                <!-- Computer Vision: 2.6 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Medicine: 1.8 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.1085
                </span>
                <a href="https://arxiv.org/abs/2505.22608" target="_blank" rel="noopener noreferrer">Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Haoning Xu, Zhaoqing Li, Youjun Chen, Huimeng Wang, Guinan Li, Mengzhe Geng, Chengxi Deng, Xunying Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents a novel approach for speech foundation models compression that tightly integrates model pruning and parameter update into a single stage. Highly compact layer-level tied self-pinching gates each containing only a single learnable threshold are jointly trained with uncompressed mo</span>
                
                <span class="abstract-full" style="display: none;">This paper presents a novel approach for speech foundation models compression that tightly integrates model pruning and parameter update into a single stage. Highly compact layer-level tied self-pinching gates each containing only a single learnable threshold are jointly trained with uncompressed models and used in fine-grained neuron level pruning. Experiments conducted on the LibriSpeech-100hr corpus suggest that our approach reduces the number of parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60% respectively, while incurring no statistically significant word error rate (WER) increase on the test-clean dataset. Compared to previously published methods on the same task, our approach not only achieves the lowest WER of 7.05% on the test-clean dataset under a comparable model compression ratio of 4.26x, but also operates with at least 25% less model compression time.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.2%">
                            Medicine
                        </span>
                <!-- LLMs: 4.4 -->
                    
                <!-- Federated Learning: 3.5 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2124
                </span>
                <a href="https://arxiv.org/abs/2505.22141" target="_blank" rel="noopener noreferrer">FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Guanwen Feng, Zhiyuan Ma, Yunan Li, Junwei Jing, Jiahao Yang, Qiguang Miao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advances in audio-driven talking head generation have achieved impressive results in lip synchronization and emotional expression. However, they largely overlook the crucial task of facial attribute editing. This capability is crucial for achieving deep personalization and expanding the range</span>
                
                <span class="abstract-full" style="display: none;">Recent advances in audio-driven talking head generation have achieved impressive results in lip synchronization and emotional expression. However, they largely overlook the crucial task of facial attribute editing. This capability is crucial for achieving deep personalization and expanding the range of practical applications, including user-tailored digital avatars, engaging online education content, and brand-specific digital customer service. In these key domains, the flexible adjustment of visual attributes-such as hairstyle, accessories, and subtle facial features is essential for aligning with user preferences, reflecting diverse brand identities, and adapting to varying contextual demands. In this paper, we present FaceEditTalker, a unified framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos. Our method consists of two key components: an image feature space editing module, which extracts semantic and detail features and allows flexible control over attributes like expression, hairstyle, and accessories; and an audio-driven video generation module, which fuses these edited features with audio-guided facial landmarks to drive a diffusion-based generator. This design ensures temporal coherence, visual fidelity, and identity preservation across frames. Extensive experiments on public datasets demonstrate that our method outperforms state-of-the-art approaches in lip-sync accuracy, video quality, and attribute controllability. Project page: https://peterfanfan.github.io/FaceEditTalker/</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 8.6%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.4%">
                            Medicine
                        </span>
                <!-- Computer Vision: 2.3 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2148
                </span>
                <a href="https://arxiv.org/abs/2411.13924" target="_blank" rel="noopener noreferrer">Robust Data-Driven Predictive Control for Mixed Platoons under Noise and Attacks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shuai Li, Chaoyi Chen, Haotian Zheng, Jiawei Wang, Qing Xu, Jianqiang Wang, Keqiang Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Controlling mixed platoons, which consist of both connected and automated vehicles (CAVs) and human-driven vehicles (HDVs), poses significant challenges due to the uncertain and unknown human driving behaviors. Data-driven control methods offer promising solutions by leveraging available trajectory </span>
                
                <span class="abstract-full" style="display: none;">Controlling mixed platoons, which consist of both connected and automated vehicles (CAVs) and human-driven vehicles (HDVs), poses significant challenges due to the uncertain and unknown human driving behaviors. Data-driven control methods offer promising solutions by leveraging available trajectory data, but their performance can be compromised by noise and attacks. To address this issue, this paper proposes a Robust Data-EnablEd Predictive Leading Cruise Control (RDeeP-LCC) framework based on data-driven reachability analysis. The framework over-approximates system dynamics under noise and attack using a matrix zonotope set derived from data, and develops a stabilizing feedback control law. By decoupling the mixed platoon system into nominal and error components, we employ data-driven reachability sets to recursively compute error reachable sets that account for noise and attacks, and obtain tightened safety constraints of the nominal system. This leads to a robust data-driven predictive control framework, solved in a tube-based control manner. Numerical simulations and human-in-the-loop experiments demonstrate that the RDeeP-LCC method significantly improves robustness against noise and attacks, while enhancing tracking accuracy, control efficiency, energy economy, and driving comfort.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.7%">
                            Medicine
                        </span>
                <!-- LLMs: 4.7 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2426
                </span>
                <a href="https://arxiv.org/abs/2502.03424" target="_blank" rel="noopener noreferrer">Prediction of the Most Fire-Sensitive Point in Building Structures with Differentiable Agents for Thermal Simulators</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yuan Xinjie, Khalid M. Mosalam
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Fire safety is crucial for ensuring the stability of building structures, yet evaluating whether a structure meets fire safety requirement is challenging. Fires can originate at any point within a structure, and simulating every potential fire scenario is both expensive and time-consuming. To addres</span>
                
                <span class="abstract-full" style="display: none;">Fire safety is crucial for ensuring the stability of building structures, yet evaluating whether a structure meets fire safety requirement is challenging. Fires can originate at any point within a structure, and simulating every potential fire scenario is both expensive and time-consuming. To address this challenge, we propose the concept of the Most Fire-Sensitive Point (MFSP) and an efficient machine learning framework for its identification. The MFSP is defined as the location at which a fire, if initiated, would cause the most severe detrimental impact on the building's stability, effectively representing the worst-case fire scenario. In our framework, a Graph Neural Network (GNN) serves as an efficient and differentiable agent for conventional Finite Element Analysis (FEA) simulators by predicting the Maximum Interstory Drift Ratio (MIDR) under fire, which then guides the training and evaluation of the MFSP predictor. Additionally, we enhance our framework with a novel edge update mechanism and a transfer learning-based training scheme. Evaluations on a large-scale simulation dataset demonstrate the good performance of the proposed framework in identifying the MFSP, offering a transformative tool for optimizing fire safety assessments in structural design. All developed datasets and codes are open-sourced online.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.4%">
                            Medicine
                        </span>
                <!-- GNN: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Cryptography: 1.1 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2429
                </span>
                <a href="https://arxiv.org/abs/2505.21723" target="_blank" rel="noopener noreferrer">Are Statistical Methods Obsolete in the Era of Deep Learning?</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Skyler Wu, Shihao Yang, S. C. Kou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In the era of AI, neural networks have become increasingly popular for modeling, inference, and prediction, largely due to their potential for universal approximation. With the proliferation of such deep learning models, a question arises: are leaner statistical methods still relevant? To shed insig</span>
                
                <span class="abstract-full" style="display: none;">In the era of AI, neural networks have become increasingly popular for modeling, inference, and prediction, largely due to their potential for universal approximation. With the proliferation of such deep learning models, a question arises: are leaner statistical methods still relevant? To shed insight on this question, we employ the mechanistic nonlinear ordinary differential equation (ODE) inverse problem as a testbed, using physics-informed neural network (PINN) as a representative of the deep learning paradigm and manifold-constrained Gaussian process inference (MAGI) as a representative of statistically principled methods. Through case studies involving the SEIR model from epidemiology and the Lorenz model from chaotic dynamics, we demonstrate that statistical methods are far from obsolete, especially when working with sparse and noisy observations. On tasks such as parameter inference and trajectory reconstruction, statistically principled methods consistently achieve lower bias and variance, while using far fewer parameters and requiring less hyperparameter tuning. Statistical methods can also decisively outperform deep learning models on out-of-sample future prediction, where the absence of relevant data often leads overparameterized models astray. Additionally, we find that statistically principled approaches are more robust to accumulation of numerical imprecision and can represent the underlying system more faithful to the true governing ODEs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.4%">
                            Medicine
                        </span>
                <!-- LLMs: 2.6 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3373
                </span>
                <a href="https://arxiv.org/abs/2505.21549" target="_blank" rel="noopener noreferrer">Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Daniel Csizmadia, Andrei Codreanu, Victor Sim, Vighnesh Prabeau, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original model's strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hi</span>
                
                <span class="abstract-full" style="display: none;">We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original model's strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hinder their effectiveness in retrieval tasks that require fine-grained cross-modal understanding. DCLIP addresses these challenges through a meta teacher-student distillation framework, where a cross-modal transformer teacher is fine-tuned to produce enriched embeddings via bidirectional cross-attention between YOLO-extracted image regions and corresponding textual spans. These semantically and spatially aligned global representations guide the training of a lightweight student model using a hybrid loss that combines contrastive learning and cosine similarity objectives. Despite being trained on only ~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a fraction of CLIP's original dataset-DCLIP significantly improves image-text retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's zero-shot classification performance. These results demonstrate that DCLIP effectively mitigates the trade-off between task specialization and generalization, offering a resource-efficient, domain-adaptive, and detail-sensitive solution for advanced vision-language tasks. Code available at https://anonymous.4open.science/r/DCLIP-B772/README.md.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.4%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.6%">
                            Medicine
                        </span>
                <!-- Federated Learning: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3691
                </span>
                <a href="https://arxiv.org/abs/2502.09082" target="_blank" rel="noopener noreferrer">CoSER: Coordinating LLM-Based Persona Simulation of Established Roles</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xintao Wang, Heng Wang, Yifei Zhang, Xinfeng Yuan, Rui Xu, Jen-tse Huang, Siyu Yuan, Haoran Guo, Jiangjie Chen, Shuchang Zhou, Wei Wang, Yanghua Xiao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper</span>
                
                <span class="abstract-full" style="display: none;">Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 10.6%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.1%">
                            Medicine
                        </span>
                <!-- Hardware: 2.8 -->
                    
                <!-- Datasets: 2.5 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3718
                </span>
                <a href="https://arxiv.org/abs/2505.21527" target="_blank" rel="noopener noreferrer">VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jianheng Zhuo, Yifan Yang, Yiwen Shao, Yong Xu, Dong Yu, Kai Yu, Xie Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Automatic speech recognition (ASR) has made remarkable progress but heavily relies on large-scale labeled data, which is scarce for low-resource languages like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve promising performance, their efficacy remains inadequate in terms o</span>
                
                <span class="abstract-full" style="display: none;">Automatic speech recognition (ASR) has made remarkable progress but heavily relies on large-scale labeled data, which is scarce for low-resource languages like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve promising performance, their efficacy remains inadequate in terms of training costs, latency, and accessibility. To address these issues, we propose VietASR, a novel ASR training pipeline that leverages vast amounts of unlabeled data and a small set of labeled data. Through multi-iteration ASR-biased self-supervised learning on a large-scale unlabeled dataset, VietASR offers a cost-effective and practical solution for enhancing ASR performance. Experiments demonstrate that pre-training on 70,000-hour unlabeled data and fine-tuning on merely 50-hour labeled data yield a lightweight but powerful ASR model. It outperforms Whisper Large-v3 and commercial ASR systems on real-world data. Our code and models will be open-sourced to facilitate research in low-resource ASR.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.9%">
                            Medicine
                        </span>
                <!-- LLMs: 4.1 -->
                    
                <!-- Blockchain: 2.5 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Datasets: 2.0 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Decision Trees: 1.7 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4227
                </span>
                <a href="https://arxiv.org/abs/2505.17982" target="_blank" rel="noopener noreferrer">Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical t</span>
                
                <span class="abstract-full" style="display: none;">Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical tissue structures. However, existing methods often face two key limitations: (1) insufficient modeling of interactions within the same modalities across scales (e.g., 5x and 20x) and (2) inadequate alignment between visual and textual modalities on the same scale. To address these gaps, we propose HiVE-MIL, a hierarchical vision-language framework that constructs a unified graph consisting of (1) parent-child links between coarse (5x) and fine (20x) visual/textual nodes to capture hierarchical relationships, and (2) heterogeneous intra-scale edges linking visual and textual nodes on the same scale. To further enhance semantic consistency, HiVE-MIL incorporates a two-stage, text-guided dynamic filtering mechanism that removes weakly correlated patch-text pairs, and introduces a hierarchical contrastive loss to align textual semantics across scales. Extensive experiments on TCGA breast, lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently outperforms both traditional MIL and recent VLM-based MIL approaches, achieving gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate the value of jointly modeling hierarchical structure and multimodal alignment for efficient and scalable learning from limited pathology data. The code is available at https://github.com/bryanwong17/HiVE-MIL</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.2%">
                            Medicine
                        </span>
                <!-- LLMs: 5.0 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4293
                </span>
                <a href="https://arxiv.org/abs/2505.22532" target="_blank" rel="noopener noreferrer">Gautschi-type and implicit-explicit integrators for constrained wave equations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: R. Altmann, B. D\"orich, C. Zimmer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper deals with the construction and analysis of two integrators for (semi-linear) second-order partial differential-algebraic equations of semi-explicit type. More precisely, we consider an implicit-explicit Crank-Nicolson scheme as well as an exponential integrator of Gautschi type. For this</span>
                
                <span class="abstract-full" style="display: none;">This paper deals with the construction and analysis of two integrators for (semi-linear) second-order partial differential-algebraic equations of semi-explicit type. More precisely, we consider an implicit-explicit Crank-Nicolson scheme as well as an exponential integrator of Gautschi type. For this, well-known wave integrators for unconstrained systems are combined with techniques known from the field of differential-algebraic equations. The result are efficient time stepping schemes, which are provable of second order. Moreover, we discuss the practical implementation of the Gautschi-type method, which involves the solution of certain saddle point problems. The theoretical results are verified by numerical experiments for the the wave equation with kinetic boundary conditions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.2%">
                            Medicine
                        </span>
                <!-- Hardware: 3.1 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4891
                </span>
                <a href="https://arxiv.org/abs/2505.21597" target="_blank" rel="noopener noreferrer">Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Abdullah Al Mamun, Pollob Chandra Ray, Md Rahat Ul Nasib, Akash Das, Jia Uddin, Md Nurul Absur
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rapid advancement of deep learning in medical image analysis has greatly enhanced the accuracy of skin cancer classification. However, current state-of-the-art models, especially those based on transfer learning like ResNet50, come with significant computational overhead, rendering them impracti</span>
                
                <span class="abstract-full" style="display: none;">The rapid advancement of deep learning in medical image analysis has greatly enhanced the accuracy of skin cancer classification. However, current state-of-the-art models, especially those based on transfer learning like ResNet50, come with significant computational overhead, rendering them impractical for deployment in resource-constrained environments. This study proposes a custom CNN model that achieves a 96.7\% reduction in parameters (from 23.9 million in ResNet50 to 692,000) while maintaining a classification accuracy deviation of less than 0.022\%. Our empirical analysis of the HAM10000 dataset reveals that although transfer learning models provide a marginal accuracy improvement of approximately 0.022\%, they result in a staggering 13,216.76\% increase in FLOPs, considerably raising computational costs and inference latency. In contrast, our lightweight CNN architecture, which encompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion, significantly reduces energy consumption, memory footprint, and inference time. These findings underscore the trade-off between the complexity of deep models and their real-world feasibility, positioning our optimized CNN as a practical solution for mobile and edge-based skin cancer diagnostics.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.8%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.5%">
                            LLMs
                        </span>
                <!-- Datasets: 2.3 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5571
                </span>
                <a href="https://arxiv.org/abs/2505.22434" target="_blank" rel="noopener noreferrer">Distance Transform Guided Mixup for Alzheimer's Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zobia Batool, Huseyin Ozkan, Erchan Aptoula
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Alzheimer's detection efforts aim to develop accurate models for early disease diagnosis. Significant advances have been achieved with convolutional neural networks and vision transformer based approaches. However, medical datasets suffer heavily from class imbalance, variations in imaging protocols</span>
                
                <span class="abstract-full" style="display: none;">Alzheimer's detection efforts aim to develop accurate models for early disease diagnosis. Significant advances have been achieved with convolutional neural networks and vision transformer based approaches. However, medical datasets suffer heavily from class imbalance, variations in imaging protocols, and limited dataset diversity, which hinder model generalization. To overcome these challenges, this study focuses on single-domain generalization by extending the well-known mixup method. The key idea is to compute the distance transform of MRI scans, separate them spatially into multiple layers and then combine layers stemming from distinct samples to produce augmented images. The proposed approach generates diverse data while preserving the brain's structure. Experimental results show generalization performance improvement across both ADNI and AIBL datasets.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.7%">
                            Medicine
                        </span>
                <!-- Computer Vision: 4.1 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- HPO and AutoML: 2.4 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Decision Trees: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5795
                </span>
                <a href="https://arxiv.org/abs/2505.10764" target="_blank" rel="noopener noreferrer">Benchmarking performance, explainability, and evaluation strategies of vision-language models for surgery: Challenges and opportunities</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jiajun Cheng, Xianwu Zhao, Shan Lin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Minimally invasive surgery (MIS) presents significant visual challenges, including a limited field of view, specular reflections, and inconsistent lighting conditions due to the small incision and the use of endoscopes. Over the past decade, many machine learning and deep learning models have been d</span>
                
                <span class="abstract-full" style="display: none;">Minimally invasive surgery (MIS) presents significant visual challenges, including a limited field of view, specular reflections, and inconsistent lighting conditions due to the small incision and the use of endoscopes. Over the past decade, many machine learning and deep learning models have been developed to identify and detect instruments and anatomical structures in surgical videos. However, these models are typically trained on manually labeled, procedure- and task-specific datasets that are relatively small, resulting in limited generalization to unseen data.In practice, hospitals generate a massive amount of raw surgical data every day, including videos captured during various procedures. Labeling this data is almost impractical, as it requires highly specialized expertise. The recent success of vision-language models (VLMs), which can be trained on large volumes of raw image-text pairs and exhibit strong adaptability, offers a promising alternative for leveraging unlabeled surgical data. While some existing work has explored applying VLMs to surgical tasks, their performance remains limited. To support future research in developing more effective VLMs for surgical applications, this paper aims to answer a key question: How well do existing VLMs, both general-purpose and surgery-specific perform on surgical data, and what types of scenes do they struggle with? To address this, we conduct a benchmarking study of several popular VLMs across diverse laparoscopic datasets. Specifically, we visualize the model's attention to identify which regions of the image it focuses on when making predictions for surgical tasks. We also propose a metric to evaluate whether the model attends to task-relevant regions. Our findings reveal a mismatch between prediction accuracy and visual grounding, indicating that models may make correct predictions while focusing on irrelevant areas of the image.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.2%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.0%">
                            LLMs
                        </span>
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6082
                </span>
                <a href="https://arxiv.org/abs/2505.22352" target="_blank" rel="noopener noreferrer">State and Input Constrained Adaptive Tracking Control of Uncertain Euler-Lagrange Systems with Robustness and Feasibility Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Poulomee Ghosh, Shubhendu Bhasin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper proposes an adaptive tracking controller for uncertain Euler-Lagrange (E-L) systems with user-defined state and input constraints in presence of bounded external disturbances. A barrier Lyapunov function (BLF) is employed for state constraint satisfaction, integrated with a saturated cont</span>
                
                <span class="abstract-full" style="display: none;">This paper proposes an adaptive tracking controller for uncertain Euler-Lagrange (E-L) systems with user-defined state and input constraints in presence of bounded external disturbances. A barrier Lyapunov function (BLF) is employed for state constraint satisfaction, integrated with a saturated controller that ensures the control input remains within pre-specified bounds. To the best of the authors' knowledge, this is the first result on tracking control of state and input-constrained uncertain E-L systems that provides verifiable conditions for the existence of a feasible control policy. The efficacy of the proposed controller in terms of constraint satisfaction and tracking performance is demonstrated through simulation on a robotic manipulator system.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.4%">
                            Medicine
                        </span>
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- LLMs: 2.2 -->
                    
                <!-- Bayesian Optimization: 2.1 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6349
                </span>
                <a href="https://arxiv.org/abs/2505.22343" target="_blank" rel="noopener noreferrer">Empowering Intelligent Low-altitude Economy with Large AI Model Deployment</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhonghao Lyu, Yulan Gao, Junting Chen, Hongyang Du, Jie Xu, Kaibin Huang, Dong In Kim
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Low-altitude economy (LAE) represents an emerging economic paradigm that redefines commercial and social aerial activities. Large artificial intelligence models (LAIMs) offer transformative potential to further enhance the intelligence of LAE services. However, deploying LAIMs in LAE poses several c</span>
                
                <span class="abstract-full" style="display: none;">Low-altitude economy (LAE) represents an emerging economic paradigm that redefines commercial and social aerial activities. Large artificial intelligence models (LAIMs) offer transformative potential to further enhance the intelligence of LAE services. However, deploying LAIMs in LAE poses several challenges, including the significant gap between their computational/storage demands and the limited onboard resources of LAE entities, the mismatch between lab-trained LAIMs and dynamic physical environments, and the inefficiencies of traditional decoupled designs for sensing, communication, and computation. To address these issues, we first propose a hierarchical system architecture tailored for LAIM deployment and present representative LAE application scenarios. Next, we explore key enabling techniques that facilitate the mutual co-evolution of LAIMs and low-altitude systems, and introduce a task-oriented execution pipeline for scalable and adaptive service delivery. Then, the proposed framework is validated through real-world case studies. Finally, we outline open challenges to inspire future research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.5%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.3%">
                            Medicine
                        </span>
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6387
                </span>
                <a href="https://arxiv.org/abs/2505.22634" target="_blank" rel="noopener noreferrer">LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rui Li, Zixuan Hu, Wenxi Qu, Jinouwen Zhang, Zhenfei Yin, Sha Zhang, Xuantuo Huang, Hanqing Wang, Tai Wang, Jiangmiao Pang, Wanli Ouyang, Lei Bai, Wangmeng Zuo, Ling-Yu Duan, Dongzhan Zhou, Shixiang Tang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Scientific embodied agents play a crucial role in modern laboratories by automating complex experimental workflows. Compared to typical household environments, laboratory settings impose significantly higher demands on perception of physical-chemical transformations and long-horizon planning, making</span>
                
                <span class="abstract-full" style="display: none;">Scientific embodied agents play a crucial role in modern laboratories by automating complex experimental workflows. Compared to typical household environments, laboratory settings impose significantly higher demands on perception of physical-chemical transformations and long-horizon planning, making them an ideal testbed for advancing embodied intelligence. However, its development has been long hampered by the lack of suitable simulator and benchmarks. In this paper, we address this gap by introducing LabUtopia, a comprehensive simulation and benchmarking suite designed to facilitate the development of generalizable, reasoning-capable embodied agents in laboratory settings. Specifically, it integrates i) LabSim, a high-fidelity simulator supporting multi-physics and chemically meaningful interactions; ii) LabScene, a scalable procedural generator for diverse scientific scenes; and iii) LabBench, a hierarchical benchmark spanning five levels of complexity from atomic actions to long-horizon mobile manipulation. LabUtopia supports 30 distinct tasks and includes more than 200 scene and instrument assets, enabling large-scale training and principled evaluation in high-complexity environments. We demonstrate that LabUtopia offers a powerful platform for advancing the integration of perception, planning, and control in scientific-purpose agents and provides a rigorous testbed for exploring the practical capabilities and generalization limits of embodied intelligence in future research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 8.5%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.2%">
                            Medicine
                        </span>
                <!-- Hardware: 2.0 -->
                    
                <!-- Datasets: 2.0 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6724
                </span>
                <a href="https://arxiv.org/abs/2409.13477" target="_blank" rel="noopener noreferrer">A Plug-and-Play Method for Guided Multi-contrast MRI Reconstruction based on Content/Style Modeling</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chinmay Rao, Matthias van Osch, Nicola Pezzotti, Jeroen de Bresser, Laurens Beljaards, Jakob Meineke, Elwin de Weerdt, Huangling Lu, Mariya Doneva, Marius Staring
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Since multiple MRI contrasts of the same anatomy contain redundant information, one contrast can guide the reconstruction of an undersampled subsequent contrast. To this end, several end-to-end learning-based guided reconstruction methods have been proposed. However, a key challenge is the requireme</span>
                
                <span class="abstract-full" style="display: none;">Since multiple MRI contrasts of the same anatomy contain redundant information, one contrast can guide the reconstruction of an undersampled subsequent contrast. To this end, several end-to-end learning-based guided reconstruction methods have been proposed. However, a key challenge is the requirement of large paired training datasets comprising raw data and aligned reference images. We propose a modular two-stage approach addressing this issue, additionally providing an explanatory framework for the multi-contrast problem based on the shared and non-shared generative factors underlying two given contrasts. A content/style model of two-contrast image data is learned from a largely unpaired image-domain dataset and is subsequently applied as a plug-and-play operator in iterative reconstruction. The disentanglement of content and style allows explicit representation of contrast-independent and contrast-specific factors. Consequently, incorporating prior information into the reconstruction reduces to a simple replacement of the aliased content of the reconstruction iterate with high-quality content derived from the reference scan. Combining this component with a data consistency step and introducing a general corrective process for the content yields an iterative scheme. We name this novel approach PnP-CoSMo. Various aspects like interpretability and convergence are explored via simulations. Furthermore, its practicality is demonstrated on the NYU fastMRI DICOM dataset, showing improved generalizability compared to end-to-end methods, and on two in-house multi-coil raw datasets, offering up to 32.6% more acceleration over learning-based non-guided reconstruction for a given SSIM. In a small radiological task, PnP-CoSMo allowed 33.3% more acceleration over clinical reconstruction at diagnostic quality.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.7%">
                            Medicine
                        </span>
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7024
                </span>
                <a href="https://arxiv.org/abs/2503.20652" target="_blank" rel="noopener noreferrer">Imitating Radiological Scrolling: A Global-Local Attention Model for 3D Chest CT Volumes Multi-Label Anomaly Classification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rapid increase in the number of Computed Tomography (CT) scan examinations has created an urgent need for automated tools, such as organ segmentation, anomaly classification, and report generation, to assist radiologists with their growing workload. Multi-label classification of Three-Dimensiona</span>
                
                <span class="abstract-full" style="display: none;">The rapid increase in the number of Computed Tomography (CT) scan examinations has created an urgent need for automated tools, such as organ segmentation, anomaly classification, and report generation, to assist radiologists with their growing workload. Multi-label classification of Three-Dimensional (3D) CT scans is a challenging task due to the volumetric nature of the data and the variety of anomalies to be detected. Existing deep learning methods based on Convolutional Neural Networks (CNNs) struggle to capture long-range dependencies effectively, while Vision Transformers require extensive pre-training, posing challenges for practical use. Additionally, these existing methods do not explicitly model the radiologist's navigational behavior while scrolling through CT scan slices, which requires both global context understanding and local detail awareness. In this study, we present CT-Scroll, a novel global-local attention model specifically designed to emulate the scrolling behavior of radiologists during the analysis of 3D CT scans. Our approach is evaluated on two public datasets, demonstrating its efficacy through comprehensive experiments and an ablation study that highlights the contribution of each model component.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.8%">
                            Medicine
                        </span>
                <!-- Federated Learning: 4.4 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Bayesian Optimization: 2.1 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7129
                </span>
                <a href="https://arxiv.org/abs/2505.21996" target="_blank" rel="noopener noreferrer">Learning World Models for Interactive Video Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Taiye Chen, Xun Hu, Zihan Ding, Chi Jin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Foundational world models must be both interactive and preserve spatiotemporal coherence for effective future planning with action choices. However, present models for long video generation have limited inherent world modeling capabilities due to two main challenges: compounding errors and insuffici</span>
                
                <span class="abstract-full" style="display: none;">Foundational world models must be both interactive and preserve spatiotemporal coherence for effective future planning with action choices. However, present models for long video generation have limited inherent world modeling capabilities due to two main challenges: compounding errors and insufficient memory mechanisms. We enhance image-to-video models with interactive capabilities through additional action conditioning and autoregressive framework, and reveal that compounding error is inherently irreducible in autoregressive video generation, while insufficient memory mechanism leads to incoherence of world models. We propose video retrieval augmented generation (VRAG) with explicit global state conditioning, which significantly reduces long-term compounding errors and increases spatiotemporal consistency of world models. In contrast, naive autoregressive generation with extended context windows and retrieval-augmented generation prove less effective for video generation, primarily due to the limited in-context learning capabilities of current video models. Our work illuminates the fundamental challenges in video world models and establishes a comprehensive benchmark for improving video generation models with internal world modeling capabilities.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 12.3%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 7.6%">
                            Medicine
                        </span>
                <!-- Hardware: 2.6 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Datasets: 1.8 -->
                    
                <!-- Decision Trees: 1.7 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7193
                </span>
                <a href="https://arxiv.org/abs/2409.11064" target="_blank" rel="noopener noreferrer">A Hybrid Multi-Factor Network with Dynamic Sequence Modeling for Early Warning of Intraoperative Hypotension</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mingyue Cheng, Jintao Zhang, Zhiding Liu, Chunli Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Intraoperative hypotension (IOH) prediction using past physiological signals is crucial, as IOH may lead to inadequate organ perfusion and significantly elevate the risk of severe complications and mortality. However, current methods often rely on static modeling, overlooking the complex temporal de</span>
                
                <span class="abstract-full" style="display: none;">Intraoperative hypotension (IOH) prediction using past physiological signals is crucial, as IOH may lead to inadequate organ perfusion and significantly elevate the risk of severe complications and mortality. However, current methods often rely on static modeling, overlooking the complex temporal dependencies and the inherently non-stationary nature of physiological signals. We propose a Hybrid Multi-Factor (HMF) network that formulates IOH prediction as a dynamic sequence forecasting task, explicitly capturing both temporal dependencies and physiological non-stationarity. We represent signal dynamics as multivariate time series and decompose them into trend and seasonal components, enabling separate modeling of long-term and periodic variations. Each component is encoded with a patch-based Transformer to balance computational efficiency and feature representation. To address distributional drift from evolving signals, we introduce a symmetric normalization mechanism. Experiments on both public and real-world clinical datasets show that HMF significantly outperforms competitive baselines. We hope HMF offers new insights into IOH prediction and ultimately promotes safer surgical care. Our code is available at https://github.com/Mingyue-Cheng/HMF.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 7.8%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.3%">
                            LLMs
                        </span>
                <!-- Blockchain: 1.9 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7476
                </span>
                <a href="https://arxiv.org/abs/2505.22091" target="_blank" rel="noopener noreferrer">A simulation framework for autonomous lunar construction work</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mattias Linde, Daniel Lindmark, Sandra {\AA}lstig, Martin Servin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a simulation framework for lunar construction work involving multiple autonomous machines. The framework supports modelling of construction scenarios and autonomy solutions, execution of the scenarios in simulation, and analysis of work time and energy consumption throughout the construct</span>
                
                <span class="abstract-full" style="display: none;">We present a simulation framework for lunar construction work involving multiple autonomous machines. The framework supports modelling of construction scenarios and autonomy solutions, execution of the scenarios in simulation, and analysis of work time and energy consumption throughout the construction project. The simulations are based on physics-based models for contacting multibody dynamics and deformable terrain, including vehicle-soil interaction forces and soil flow in real time. A behaviour tree manages the operational logic and error handling, which enables the representation of complex behaviours through a discrete set of simpler tasks in a modular hierarchical structure. High-level decision-making is separated from lower-level control algorithms, with the two connected via ROS2. Excavation movements are controlled through inverse kinematics and tracking controllers. The framework is tested and demonstrated on two different lunar construction scenarios.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 7.0%">
                            Medicine
                        </span>
                <!-- LLMs: 3.8 -->
                    
                <!-- Datasets: 2.3 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7613
                </span>
                <a href="https://arxiv.org/abs/2505.21806" target="_blank" rel="noopener noreferrer">Towards Operational Automated Greenhouse Gas Plume Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Brian D. Bue, Jake H. Lee, Andrew K. Thorpe, Philip G. Brodrick, Daniel Cusworth, Alana Ayasse, Vassiliki Mancoridis, Anagha Satish, Shujun Xiong, Riley Duren
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Operational deployment of a fully automated greenhouse gas (GHG) plume detection system remains an elusive goal for imaging spectroscopy missions, despite recent advances in deep learning approaches. With the dramatic increase in data availability, however, automation continues to increase in import</span>
                
                <span class="abstract-full" style="display: none;">Operational deployment of a fully automated greenhouse gas (GHG) plume detection system remains an elusive goal for imaging spectroscopy missions, despite recent advances in deep learning approaches. With the dramatic increase in data availability, however, automation continues to increase in importance for natural and anthropogenic emissions monitoring. This work reviews and addresses several key obstacles in the field: data and label quality control, prevention of spatiotemporal biases, and correctly aligned modeling objectives. We demonstrate through rigorous experiments using multicampaign data from airborne and spaceborne instruments that convolutional neural networks (CNNs) are able to achieve operational detection performance when these obstacles are alleviated. We demonstrate that a multitask model that learns both instance detection and pixelwise segmentation simultaneously can successfully lead towards an operational pathway. We evaluate the model's plume detectability across emission source types and regions, identifying thresholds for operational deployment. Finally, we provide analysis-ready data, models, and source code for reproducibility, and work to define a set of best practices and validation standards to facilitate future contributions to the field.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.9%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.2%">
                            LLMs
                        </span>
                <!-- Computer Vision: 3.1 -->
                    
                <!-- Decision Trees: 1.9 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7837
                </span>
                <a href="https://arxiv.org/abs/2505.12363" target="_blank" rel="noopener noreferrer">Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Qi Feng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data </span>
                
                <span class="abstract-full" style="display: none;">While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 8.1%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 7.1%">
                            LLMs
                        </span>
                <!-- Hardware: 1.9 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8729
                </span>
                <a href="https://arxiv.org/abs/2505.22527" target="_blank" rel="noopener noreferrer">Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Agnideep Aich, Ashit Aich, Bruce Wade
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce the Symplectic Generative Network (SGN), a deep generative model that leverages Hamiltonian mechanics to construct an invertible, volume-preserving mapping between a latent space and the data space. By endowing the latent space with a symplectic structure and modeling data generation as</span>
                
                <span class="abstract-full" style="display: none;">We introduce the Symplectic Generative Network (SGN), a deep generative model that leverages Hamiltonian mechanics to construct an invertible, volume-preserving mapping between a latent space and the data space. By endowing the latent space with a symplectic structure and modeling data generation as the time evolution of a Hamiltonian system, SGN achieves exact likelihood evaluation without incurring the computational overhead of Jacobian determinant calculations. In this work, we provide a rigorous mathematical foundation for SGNs through a comprehensive theoretical framework that includes: (i) complete proofs of invertibility and volume preservation, (ii) a formal complexity analysis with theoretical comparisons to Variational Autoencoders and Normalizing Flows, (iii) strengthened universal approximation results with quantitative error bounds, (iv) an information-theoretic analysis based on the geometry of statistical manifolds, and (v) an extensive stability analysis with adaptive integration guarantees. These contributions highlight the fundamental advantages of SGNs and establish a solid foundation for future empirical investigations and applications to complex, high-dimensional data.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.9%">
                            Medicine
                        </span>
                <!-- LLMs: 4.2 -->
                    
                <!-- Federated Learning: 3.2 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8958
                </span>
                <a href="https://arxiv.org/abs/2505.21658" target="_blank" rel="noopener noreferrer">STACI: Spatio-Temporal Aleatoric Conformal Inference</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Brandon R. Feng, David Keetae Park, Xihaier Luo, Arantxa Urdangarin, Shinjae Yoo, Brian J. Reich
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty quantification for estimation of spatio-temporal fields. Spatio-temporal deep learning models, while scalable, typically assume a simplistic independent covariance matrix for the response, failing to capture the underlying</span>
                
                <span class="abstract-full" style="display: none;">Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty quantification for estimation of spatio-temporal fields. Spatio-temporal deep learning models, while scalable, typically assume a simplistic independent covariance matrix for the response, failing to capture the underlying correlation structure. However, spatio-temporal GPs suffer from issues of scalability and various forms of approximation bias resulting from restrictive assumptions of the covariance kernel function. We propose STACI, a novel framework consisting of a variational Bayesian neural network approximation of non-stationary spatio-temporal GP along with a novel spatio-temporal conformal inference algorithm. STACI is highly scalable, taking advantage of GPU training capabilities for neural network models, and provides statistically valid prediction intervals for uncertainty quantification. STACI outperforms competing GPs and deep methods in accurately approximating spatio-temporal processes and we show it easily scales to datasets with millions of observations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 8.2%">
                            Medicine
                        </span>
                <!-- LLMs: 3.5 -->
                    
                <!-- Hardware: 2.4 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Bayesian Optimization: 2.0 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9187
                </span>
                <a href="https://arxiv.org/abs/2404.06762" target="_blank" rel="noopener noreferrer">Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhengyuan Liu, Stella Xin Yin, Geyu Lin, Nancy F. Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language lea</span>
                
                <span class="abstract-full" style="display: none;">Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student's persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher's adaptive scaffolding strategies.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 30.1%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.4%">
                            Medicine
                        </span>
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.0923
                </span>
                <a href="https://arxiv.org/abs/2505.22022" target="_blank" rel="noopener noreferrer">A High Accuracy Symplectic Scheme for Advection Diffusion Reaction Models in Bioseparation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Farjana Siddiqua, Catalin Trenchea
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We analyze an advection-diffusion-reaction problem with non-homogeneous boundary conditions that models the chromatography process, a vital stage in bioseparation. We prove stability and error estimates for both constant and affine adsorption, using the symplectic one-step implicit midpoint method f</span>
                
                <span class="abstract-full" style="display: none;">We analyze an advection-diffusion-reaction problem with non-homogeneous boundary conditions that models the chromatography process, a vital stage in bioseparation. We prove stability and error estimates for both constant and affine adsorption, using the symplectic one-step implicit midpoint method for time discretization and finite elements for spatial discretization. In addition, we perform the stability analysis for the nonlinear, explicit adsorption in the continuous and semi-discrete cases. For the nonlinear, explicit adsorption, we also complete the error analysis for the semi-discrete case and prove the existence of a solution for the fully discrete case. The numerical tests validate our theoretical results.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 8.4%">
                            Medicine
                        </span>
                <!-- Computer Vision: 2.8 -->
                    
                <!-- Hardware: 2.4 -->
                    
                <!-- LLMs: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2973
                </span>
                <a href="https://arxiv.org/abs/2505.21790" target="_blank" rel="noopener noreferrer">Faster Rates for Private Adversarial Bandits</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hilal Asi, Vinod Raman, Kunal Talwar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We design new differentially private algorithms for the problems of adversarial bandits and bandits with expert advice. For adversarial bandits, we give a simple and efficient conversion of any non-private bandit algorithm to a private bandit algorithm. Instantiating our conversion with existing non</span>
                
                <span class="abstract-full" style="display: none;">We design new differentially private algorithms for the problems of adversarial bandits and bandits with expert advice. For adversarial bandits, we give a simple and efficient conversion of any non-private bandit algorithm to a private bandit algorithm. Instantiating our conversion with existing non-private bandit algorithms gives a regret upper bound of $O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all $\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first known separation between central and local differential privacy for this problem. For bandits with expert advice, we give the first differentially private algorithms, with expected regret $O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right), O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and $\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} + \frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of actions and experts respectively. These rates allow us to get sublinear regret for different combinations of small and large $K, N$ and $\epsilon.$</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 9.2%">
                            Medicine
                        </span>
                <!-- LLMs: 3.4 -->
                    
                <!-- Federated Learning: 3.1 -->
                    
                <!-- Hardware: 2.9 -->
                    
                <!-- Evolutionary Algorithms: 2.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3164
                </span>
                <a href="https://arxiv.org/abs/2505.21699" target="_blank" rel="noopener noreferrer">STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhengbo Zhou, Dooman Arefan, Margarita Zuley, Jules Sumkin, Shandong Wu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Predicting the risk of developing breast cancer is an important clinical tool to guide early intervention and tailoring personalized screening strategies. Early risk models have limited performance and recently machine learning-based analysis of mammogram images showed encouraging risk prediction ef</span>
                
                <span class="abstract-full" style="display: none;">Predicting the risk of developing breast cancer is an important clinical tool to guide early intervention and tailoring personalized screening strategies. Early risk models have limited performance and recently machine learning-based analysis of mammogram images showed encouraging risk prediction effects. These models however are limited to the use of a single exam or tend to overlook nuanced breast tissue evolvement in spatial and temporal details of longitudinal imaging exams that are indicative of breast cancer risk. In this paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk Prediction), a novel Transformer-based model that captures fine-grained mammographic imaging evolution simultaneously from bilateral and longitudinal asymmetries for breast cancer risk prediction. STA-Risk is innovative by the side encoding and temporal encoding to learn spatial-temporal asymmetries, regulated by a customized asymmetry loss. We performed extensive experiments with two independent mammogram datasets and achieved superior performance than four representative SOTA models for 1- to 5-year future risk prediction. Source codes will be released upon publishing of the paper.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 9.8%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.5%">
                            LLMs
                        </span>
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3564
                </span>
                <a href="https://arxiv.org/abs/2505.21356" target="_blank" rel="noopener noreferrer">Towards Robust Automated Perceptual Voice Quality Assessment with Speech Foundation Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, Yu Tsao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Perceptual voice quality assessment is essential for diagnosing and monitoring voice disorders. Traditionally, expert raters use scales such as the CAPE-V and GRBAS. However, these are subjective and prone to inter-rater variability, motivating the need for automated, objective assessment methods. T</span>
                
                <span class="abstract-full" style="display: none;">Perceptual voice quality assessment is essential for diagnosing and monitoring voice disorders. Traditionally, expert raters use scales such as the CAPE-V and GRBAS. However, these are subjective and prone to inter-rater variability, motivating the need for automated, objective assessment methods. This study proposes VOQANet, a deep learning framework with an attention mechanism that leverages a Speech Foundation Model (SFM) to extract high-level acoustic and prosodic information from raw speech. To improve robustness and interpretability, we introduce VOQANet+, which integrates handcrafted acoustic features such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM embeddings into a hybrid representation. Unlike prior work focusing only on vowel-based phonation (PVQD-A subset) from the Perceptual Voice Quality Dataset (PVQD), we evaluate our models on both vowel-based and sentence-level speech (PVQD-S subset) for better generalizability. Results show that sentence-based input outperforms vowel-based input, particularly at the patient level, highlighting the benefit of longer utterances for capturing voice attributes. VOQANet consistently surpasses baseline methods in root mean squared error and Pearson correlation across CAPE-V and GRBAS dimensions, with VOQANet+ achieving further improvements. Additional tests under noisy conditions show that VOQANet+ maintains high prediction accuracy, supporting its use in real-world and telehealth settings. These findings demonstrate the value of combining SFM embeddings with domain-informed acoustic features for interpretable and robust voice quality assessment.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 10.2%">
                            Medicine
                        </span>
                <!-- LLMs: 3.6 -->
                    
                <!-- Computer Vision: 2.7 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- HPO and AutoML: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4838
                </span>
                <a href="https://arxiv.org/abs/2505.21783" target="_blank" rel="noopener noreferrer">P-DROP: Poisson-Based Dropout for Graph Neural Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hyunsik Yun
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Over-smoothing remains a major challenge in Graph Neural Networks (GNNs), where repeated message passing causes node representations to converge and lose discriminative power. To address this, we propose a novel node selection strategy based on Poisson processes, introducing stochastic but structure</span>
                
                <span class="abstract-full" style="display: none;">Over-smoothing remains a major challenge in Graph Neural Networks (GNNs), where repeated message passing causes node representations to converge and lose discriminative power. To address this, we propose a novel node selection strategy based on Poisson processes, introducing stochastic but structure-aware updates. Specifically, we equip each node with an independent Poisson clock, enabling asynchronous and localized updates that preserve structural diversity. We explore two applications of this strategy: as a replacement for dropout-based regularization and as a dynamic subgraph training scheme. Experimental results on standard benchmarks (Cora, Citeseer, Pubmed) demonstrate that our Poisson-based method yields competitive or improved accuracy compared to traditional Dropout, DropEdge, and DropNode approaches, particularly in later training stages.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 7.5%">
                            GNN
                        </span>
                <!-- LLMs: 4.7 -->
                    
                <!-- Medicine: 3.3 -->
                    
                <!-- 3D: 2.6 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4859
                </span>
                <a href="https://arxiv.org/abs/2505.18927" target="_blank" rel="noopener noreferrer">Moderating Harm: Benchmarking Large Language Models for Cyberbullying Detection in YouTube Comments</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Amel Muminovic
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from hi</span>
                
                <span class="abstract-full" style="display: none;">As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse threads in gaming, lifestyle, food vlog, and music channels. The dataset comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and Indonesian, annotated independently by two reviewers with substantial agreement (Cohen's kappa = 0.83). Using a unified prompt and deterministic settings, GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful posts (recall = 0.875) but its precision fell to 0.767 due to frequent false positives. Claude delivered the highest precision at 0.920 and the lowest false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative analysis showed that all three models struggle with sarcasm, coded insults, and mixed-language slang. These results underscore the need for moderation pipelines that combine complementary models, incorporate conversational context, and fine-tune for under-represented languages and implicit abuse. A de-identified version of the dataset and full prompts is publicly released to promote reproducibility and further progress in automated content moderation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 13.4%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 7.8%">
                            Medicine
                        </span>
                <!-- Datasets: 2.5 -->
                    
                <!-- Hardware: 2.2 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.6724
                </span>
                <a href="https://arxiv.org/abs/2505.21919" target="_blank" rel="noopener noreferrer">Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yue Zhu, Hao Yu, Chen Wang, Zhuoran Liu, Eun Kyung Lee
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The increasing adoption of large language models (LLMs) with extended context windows necessitates efficient Key-Value Cache (KVC) management to optimize inference performance. Inference workloads like Retrieval-Augmented Generation (RAG) and agents exhibit high cache reusability, making efficient c</span>
                
                <span class="abstract-full" style="display: none;">The increasing adoption of large language models (LLMs) with extended context windows necessitates efficient Key-Value Cache (KVC) management to optimize inference performance. Inference workloads like Retrieval-Augmented Generation (RAG) and agents exhibit high cache reusability, making efficient caching critical to reducing redundancy and improving speed. We analyze real-world KVC access patterns using publicly available traces and evaluate commercial key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1] and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of tailored storage solution for KVC prefilling, underscores the need for an efficient distributed caching system with optimized metadata management for LLM workloads, and provides insights into designing improved KVC management systems for scalable, low-latency inference.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 13.1%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 9.4%">
                            Medicine
                        </span>
                <!-- Hardware: 3.8 -->
                    
                <!-- HPO and AutoML: 2.5 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.9122
                </span>
                <a href="https://arxiv.org/abs/2311.15876" target="_blank" rel="noopener noreferrer">End-to-End Breast Cancer Radiotherapy Planning via LMMs with Consistency Embedding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kwanyoung Kim, Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Joongyo Lee, Jin Sung Kim, Yong Bae Kim, Jong Chul Ye
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advances in AI foundation models have significant potential for lightening the clinical workload by mimicking the comprehensive and multi-faceted approaches used by medical professionals. In the field of radiation oncology, the integration of multiple modalities holds great importance, so the</span>
                
                <span class="abstract-full" style="display: none;">Recent advances in AI foundation models have significant potential for lightening the clinical workload by mimicking the comprehensive and multi-faceted approaches used by medical professionals. In the field of radiation oncology, the integration of multiple modalities holds great importance, so the opportunity of foundational model is abundant. Inspired by this, here we present RO-LMM, a multi-purpose, comprehensive large multimodal model (LMM) tailored for the field of radiation oncology. This model effectively manages a series of tasks within the clinical workflow, including clinical context summarization, radiation treatment plan suggestion, and plan-guided target volume segmentation by leveraging the capabilities of LMM. In particular, to perform consecutive clinical tasks without error accumulation, we present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which boosts LMM's robustness to noisy inputs while preserving the consistency of handling clean inputs. We further extend this concept to LMM-driven segmentation framework, leading to a novel Consistency Embedding Segmentation (CESEG) techniques. Experimental results including multi-centre validation confirm that our RO-LMM with CEFTune and CESEG results in promising performance for multiple clinical tasks with generalization capabilities.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 10.7%">
                            Medicine
                        </span>
                <!-- Federated Learning: 4.5 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.9642
                </span>
                <a href="https://arxiv.org/abs/2505.05151" target="_blank" rel="noopener noreferrer">Overcoming Dimensional Factorization Limits in Discrete Diffusion Models through Quantum Joint Distribution Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chuangtao Chen, Qinglin Zhao, MengChu Zhou, Zhimin He, Haozhen Situ
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study explores quantum-enhanced discrete diffusion models to overcome classical limitations in learning high-dimensional distributions. We rigorously prove that classical discrete diffusion models, which calculate per-dimension transition probabilities to avoid exponential computational cost, e</span>
                
                <span class="abstract-full" style="display: none;">This study explores quantum-enhanced discrete diffusion models to overcome classical limitations in learning high-dimensional distributions. We rigorously prove that classical discrete diffusion models, which calculate per-dimension transition probabilities to avoid exponential computational cost, exhibit worst-case linear scaling of Kullback-Leibler (KL) divergence with data dimension. To address this, we propose a Quantum Discrete Denoising Diffusion Probabilistic Model (QD3PM), which enables joint probability learning through diffusion and denoising in exponentially large Hilbert spaces. By deriving posterior states through quantum Bayes' theorem, similar to the crucial role of posterior probabilities in classical diffusion models, and by learning the joint probability, we establish a solid theoretical foundation for quantum-enhanced diffusion models. For denoising, we design a quantum circuit using temporal information for parameter sharing and learnable classical-data-controlled rotations for encoding. Exploiting joint distribution learning, our approach enables single-step sampling from pure noise, eliminating iterative requirements of existing models. Simulations demonstrate the proposed model's superior accuracy in modeling complex distributions compared to factorization methods. Hence, this paper establishes a new theoretical paradigm in generative models by leveraging the quantum advantage in joint distribution learning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.3%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 5.8%">
                            Quantum Computing
                        </span>
                <!-- Medicine: 2.4 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                <!-- Networks: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.295
                </span>
                <a href="https://arxiv.org/abs/2410.02622" target="_blank" rel="noopener noreferrer">Diss-l-ECT: Dissecting Graph Data with Local Euler Characteristic Transforms</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Julius von Rohrscheidt, Bastian Rieck
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The Euler Characteristic Transform (ECT) is an efficiently-computable geometrical-topological invariant that characterizes the global shape of data. In this paper, we introduce the Local Euler Characteristic Transform ($\ell$-ECT), a novel extension of the ECT particularly designed to enhance expres</span>
                
                <span class="abstract-full" style="display: none;">The Euler Characteristic Transform (ECT) is an efficiently-computable geometrical-topological invariant that characterizes the global shape of data. In this paper, we introduce the Local Euler Characteristic Transform ($\ell$-ECT), a novel extension of the ECT particularly designed to enhance expressivity and interpretability in graph representation learning. Unlike traditional Graph Neural Networks (GNNs), which may lose critical local details through aggregation, the $\ell$-ECT provides a lossless representation of local neighborhoods. This approach addresses key limitations in GNNs by preserving nuanced local structures while maintaining global interpretability. Moreover, we construct a rotation-invariant metric based on $\ell$-ECTs for spatial alignment of data spaces. Our method exhibits superior performance compared to standard GNNs on a variety of node-classification tasks, while also offering theoretical guarantees that demonstrate its effectiveness.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 8.6%">
                            GNN
                        </span>
                <!-- Federated Learning: 4.5 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3243
                </span>
                <a href="https://arxiv.org/abs/2502.01397" target="_blank" rel="noopener noreferrer">Message-Passing GNNs Fail to Approximate Sparse Triangular Factorizations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vladislav Trifonov, Ekaterina Muravleva, Ivan Oseledets
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph Neural Networks (GNNs) have been proposed as a tool for learning sparse matrix preconditioners, which are key components in accelerating linear solvers. This position paper argues that message-passing GNNs are fundamentally incapable of approximating sparse triangular factorizations. We demons</span>
                
                <span class="abstract-full" style="display: none;">Graph Neural Networks (GNNs) have been proposed as a tool for learning sparse matrix preconditioners, which are key components in accelerating linear solvers. This position paper argues that message-passing GNNs are fundamentally incapable of approximating sparse triangular factorizations. We demonstrate that message-passing GNNs fundamentally fail to approximate sparse triangular factorizations for classes of matrices for which high-quality preconditioners exist but require non-local dependencies. To illustrate this, we construct a set of baselines using both synthetic matrices and real-world examples from the SuiteSparse collection. Across a range of GNN architectures, including Graph Attention Networks and Graph Transformers, we observe severe performance degradation compared to exact or K-optimal factorizations, with cosine similarity dropping below $0.6$ in key cases. Our theoretical and empirical results suggest that architectural innovations beyond message-passing are necessary for applying GNNs to scientific computing tasks such as matrix factorization. Experiments demonstrate that overcoming non-locality alone is insufficient. Tailored architectures are necessary to capture the required dependencies since even a completely non-local Graph Transformer fails to match the proposed baselines.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 10.6%">
                            GNN
                        </span>
                <!-- LLMs: 3.1 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Medicine: 1.9 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3873
                </span>
                <a href="https://arxiv.org/abs/2411.11835" target="_blank" rel="noopener noreferrer">Describe Now: User-Driven Audio Description for Blind and Low Vision Individuals</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Maryam Cheema, Hasti Seifi, Pooyan Fazli
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Audio descriptions (AD) make videos accessible for blind and low vision (BLV) users by describing visual elements that cannot be understood from the main audio track. AD created by professionals or novice describers is time-consuming and offers little customization or control to BLV viewers on descr</span>
                
                <span class="abstract-full" style="display: none;">Audio descriptions (AD) make videos accessible for blind and low vision (BLV) users by describing visual elements that cannot be understood from the main audio track. AD created by professionals or novice describers is time-consuming and offers little customization or control to BLV viewers on description length and content and when they receive it. To address this gap, we explore user-driven AI-generated descriptions, enabling BLV viewers to control both the timing and level of detail of the descriptions they receive. In a study, 20 BLV participants activated audio descriptions for seven different video genres with two levels of detail: concise and detailed. Our findings reveal differences in the preferred frequency and level of detail of ADs for different videos, participants' sense of control with this style of AD delivery, and its limitations. We discuss the implications of these findings for the development of future AD tools for BLV users.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 10.2%">
                            Medicine
                        </span>
                <!-- LLMs: 4.3 -->
                    
                <!-- Hardware: 2.5 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Datasets: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8295
                </span>
                <a href="https://arxiv.org/abs/2505.22079" target="_blank" rel="noopener noreferrer">Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware Learning for Medical Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hanbin Ko, Chang-Min Park
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The development of large-scale image-text pair datasets has significantly advanced self-supervised learning in Vision-Language Processing (VLP). However, directly applying general-domain architectures such as CLIP to medical data presents challenges, particularly in handling negations and addressing</span>
                
                <span class="abstract-full" style="display: none;">The development of large-scale image-text pair datasets has significantly advanced self-supervised learning in Vision-Language Processing (VLP). However, directly applying general-domain architectures such as CLIP to medical data presents challenges, particularly in handling negations and addressing the inherent data imbalance of medical datasets. To address these issues, we propose a novel approach that integrates clinically-enhanced dynamic soft labels and medical graphical alignment, thereby improving clinical comprehension and the applicability of contrastive loss in medical contexts. Furthermore, we introduce negation-based hard negatives to deepen the model's understanding of the complexities of clinical language. Our approach is easily integrated into the medical CLIP training pipeline and achieves state-of-the-art performance across multiple tasks, including zero-shot, fine-tuned classification, and report retrieval. To comprehensively evaluate our model's capacity for understanding clinical language, we introduce CXR-Align, a benchmark uniquely designed to evaluate the understanding of negation and clinical information within chest X-ray (CXR) datasets. Experimental results demonstrate that our proposed methods are straightforward to implement and generalize effectively across contrastive learning frameworks, enhancing medical VLP capabilities and advancing clinical language understanding in medical imaging.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 15.1%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.7%">
                            LLMs
                        </span>
                <!-- Federated Learning: 2.2 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.0872
                </span>
                <a href="https://arxiv.org/abs/2405.19933" target="_blank" rel="noopener noreferrer">Learning Latent Graph Structures and their Uncertainty</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alessandro Manenti, Daniele Zambon, Cesare Alippi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph neural networks use relational information as an inductive bias to enhance prediction performance. Not rarely, task-relevant relations are unknown and graph structure learning approaches have been proposed to learn them from data. Given their latent nature, no graph observations are available </span>
                
                <span class="abstract-full" style="display: none;">Graph neural networks use relational information as an inductive bias to enhance prediction performance. Not rarely, task-relevant relations are unknown and graph structure learning approaches have been proposed to learn them from data. Given their latent nature, no graph observations are available to provide a direct training signal to the learnable relations. Therefore, graph topologies are typically learned on the prediction task alongside the other graph neural network parameters. In this paper, we demonstrate that minimizing point-prediction losses does not guarantee proper learning of the latent relational information and its associated uncertainty. Conversely, we prove that suitable loss functions on the stochastic model outputs simultaneously grant solving two tasks: (i) learning the unknown distribution of the latent graph and (ii) achieving optimal predictions of the target variable. Finally, we propose a sampling-based method that solves this joint learning task. Empirical results validate our theoretical claims and demonstrate the effectiveness of the proposed approach.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 10.6%">
                            GNN
                        </span>
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Federated Learning: 2.8 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- LLMs: 1.8 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.1084
                </span>
                <a href="https://arxiv.org/abs/2409.10803" target="_blank" rel="noopener noreferrer">Quantum Kernel Learning for Small Dataset Modeling in Semiconductor Fabrication: Application to Ohmic Contact</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zeheng Wang, Fangzhou Wang, Liang Li, Zirui Wang, Timothy van der Laan, Ross C. C. Leon, Jing-Kai Huang, Muhammad Usman
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modeling complex semiconductor fabrication processes such as Ohmic contact formation remains challenging due to high-dimensional parameter spaces and limited experimental data. While classical machine learning (CML) approaches have been successful in many domains, their performance degrades in small</span>
                
                <span class="abstract-full" style="display: none;">Modeling complex semiconductor fabrication processes such as Ohmic contact formation remains challenging due to high-dimensional parameter spaces and limited experimental data. While classical machine learning (CML) approaches have been successful in many domains, their performance degrades in small-sample, nonlinear scenarios. In this work, we investigate quantum machine learning (QML) as an alternative, exploiting quantum kernels to capture intricate correlations from compact datasets. Using only 159 experimental GaN HEMT samples, we develop a quantum kernel-aligned regressor (QKAR) combining a shallow Pauli-Z feature map with a trainable quantum kernel alignment (QKA) layer. All models, including seven baseline CML regressors, are evaluated under a unified PCA-based preprocessing pipeline to ensure a fair comparison. QKAR consistently outperforms classical baselines across multiple metrics (MAE, MSE, RMSE), achieving a mean absolute error of 0.338 Omega mm when validated on experimental data. We further assess noise robustness and generalization through cross-validation and new device fabrication. These findings suggest that carefully constructed QML models could provide predictive advantages in data-constrained semiconductor modeling, offering a foundation for practical deployment on near-term quantum hardware. While challenges remain for both QML and CML, this study demonstrates QML's potential as a complementary approach in complex process modeling tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 10.2%">
                            Quantum Computing
                        </span>
                <!-- LLMs: 4.5 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.8562
                </span>
                <a href="https://arxiv.org/abs/2305.19717" target="_blank" rel="noopener noreferrer">An Empirical Evaluation of Rewiring Approaches in Graph Neural Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alessio Micheli, Domenico Tortorella
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph neural networks compute node representations by performing multiple message-passing steps that consist in local aggregations of node features. Having deep models that can leverage longer-range interactions between nodes is hindered by the issues of over-smoothing and over-squashing. In particu</span>
                
                <span class="abstract-full" style="display: none;">Graph neural networks compute node representations by performing multiple message-passing steps that consist in local aggregations of node features. Having deep models that can leverage longer-range interactions between nodes is hindered by the issues of over-smoothing and over-squashing. In particular, the latter is attributed to the graph topology which guides the message-passing, causing a node representation to become insensitive to information contained at distant nodes. Many graph rewiring methods have been proposed to remedy or mitigate this problem. However, properly evaluating the benefits of these methods is made difficult by the coupling of over-squashing with other issues strictly related to model training, such as vanishing gradients. Therefore, we propose an evaluation setting based on message-passing models that do not require training to compute node and graph representations. We perform a systematic experimental comparison on real-world node and graph classification tasks, showing that rewiring the underlying graph rarely does confer a practical benefit for message-passing.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 14.1%">
                            GNN
                        </span>
                <!-- LLMs: 3.5 -->
                    
                <!-- Federated Learning: 3.1 -->
                    
                <!-- Bayesian Optimization: 1.9 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Medicine: 1.7 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.0621
                </span>
                <a href="https://arxiv.org/abs/2412.18519" target="_blank" rel="noopener noreferrer">Pilot-Quantum: A Quantum-HPC Middleware for Resource, Workload and Task Management</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pradeep Mantha, Florian J. Kiwit, Nishant Saurabh, Shantenu Jha, Andre Luckow
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As quantum hardware advances, integrating quantum processing units (QPUs) into HPC environments and managing diverse infrastructure and software stacks becomes increasingly essential. Pilot-Quantum addresses these challenges as a middleware designed to provide unified application-level management of</span>
                
                <span class="abstract-full" style="display: none;">As quantum hardware advances, integrating quantum processing units (QPUs) into HPC environments and managing diverse infrastructure and software stacks becomes increasingly essential. Pilot-Quantum addresses these challenges as a middleware designed to provide unified application-level management of resources and workloads across hybrid quantum-classical environments. It is built on a rigorous analysis of existing quantum middleware systems and application execution patterns. It implements the Pilot Abstraction conceptual model, originally developed for HPC, to manage resources, workloads, and tasks. It is designed for quantum applications that rely on task parallelism, including (i) hybrid algorithms, such as variational approaches, and (ii) circuit cutting systems, used to partition and execute large quantum circuits. Pilot-Quantum facilitates seamless integration of QPUs, classical CPUs, and GPUs, while supporting high-level programming frameworks like Qiskit and Pennylane. This enables users to efficiently design and execute hybrid workflows across diverse computing resources. The capabilities of Pilot-Quantum are demonstrated through mini-apps -- simplified yet representative kernels focusing on critical performance bottlenecks. We demonstrate the capabilities of Pilot-Quantum through multiple mini-apps, including different circuit executions (e.g., using IBM\'s Eagle QPU and simulators), circuit cutting, and quantum machine learning scenarios.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 8.4%">
                            Quantum Computing
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.7%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.4%">
                            LLMs
                        </span>
                <!-- Hardware: 2.8 -->
                    
                <!-- Blockchain: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.3528
                </span>
                <a href="https://arxiv.org/abs/2505.22612" target="_blank" rel="noopener noreferrer">BPMN to Smart Contract by Business Analyst</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: C. G. Liu, P. Bodorik, D. Jutla
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper addresses the challenge of creating smart contracts for applications represented using Business Process Management and Notation (BPMN) models. In our prior work we presented a methodology that automates the generation of smart contracts from BPMN models. This approach abstracts the BPMN f</span>
                
                <span class="abstract-full" style="display: none;">This paper addresses the challenge of creating smart contracts for applications represented using Business Process Management and Notation (BPMN) models. In our prior work we presented a methodology that automates the generation of smart contracts from BPMN models. This approach abstracts the BPMN flow control, making it independent of the underlying blockchain infrastructure, with only the BPMN task elements requiring coding. In subsequent research, we enhanced our approach by adding support for nested transactions and enabling a smart contract repair and/or upgrade. To empower Business Analysts (BAs) to generate smart contracts without relying on software developers, we tackled the challenge of generating smart contracts from BPMN models without assistance of a software developer. We exploit the Decision Model and Notation (DMN) standard to represent the decisions and the business logic of the BPMN task elements and amended our methodology for transformation of BPMN models into smart contracts to support also the generation script to represent the business logic represented by the DMN models. To support such transformation, we describe how the BA documents, using the BPMN elements, the flow of information along with the flow of execution. Thus, if the BA is successful in representing the blockchain application requirements using BPMN and DMN models, our methodology and the tool, called TABS, that we developed as a proof of concept, is used to generate the smart contracts directly from those models without developer assistance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #3cc377" title="Confidence: 5.4%">
                            Blockchain
                        </span>
                <!-- Federated Learning: 3.9 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- LLMs: 1.6 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.6711
                </span>
                <a href="https://arxiv.org/abs/2505.22362" target="_blank" rel="noopener noreferrer">Directed Homophily-Aware Graph Neural Network</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aihu Zhang, Jiaxing Xu, Mengcheng Lan, Shili Xiang, Yiping Ke
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph Neural Networks (GNNs) have achieved significant success in various learning tasks on graph-structured data. Nevertheless, most GNNs struggle to generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the directional nature of real-world graphs, resulting in suboptimal perform</span>
                
                <span class="abstract-full" style="display: none;">Graph Neural Networks (GNNs) have achieved significant success in various learning tasks on graph-structured data. Nevertheless, most GNNs struggle to generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the directional nature of real-world graphs, resulting in suboptimal performance on directed graphs with asymmetric structures. In this work, we propose Directed Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses these limitations by incorporating homophily-aware and direction-sensitive components. DHGNN employs a resettable gating mechanism to adaptively modulate message contributions based on homophily levels and informativeness, and a structure-aware noise-tolerant fusion module to effectively integrate node representations from the original and reverse directions. Extensive experiments on both homophilic and heterophilic directed graph datasets demonstrate that DHGNN outperforms state-of-the-art methods in node classification and link prediction. In particular, DHGNN improves over the best baseline by up to 15.07% in link prediction. Our analysis further shows that the gating mechanism captures directional homophily gaps and fluctuating homophily across layers, providing deeper insights into message-passing behavior on complex graph structures.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 18.3%">
                            GNN
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.4%">
                            LLMs
                        </span>
                <!-- Computer Vision: 3.1 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.8013
                </span>
                <a href="https://arxiv.org/abs/2505.22592" target="_blank" rel="noopener noreferrer">Comparative Analysis of Machine Learning Models for Lung Cancer Mutation Detection and Staging Using 3D CT Scans</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yiheng Li, Francisco Carrillo-Perez, Mohammed Alawad, Olivier Gevaert
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Lung cancer is the leading cause of cancer mortality worldwide, and non-invasive methods for detecting key mutations and staging are essential for improving patient outcomes. Here, we compare the performance of two machine learning models - FMCIB+XGBoost, a supervised model with domain-specific pret</span>
                
                <span class="abstract-full" style="display: none;">Lung cancer is the leading cause of cancer mortality worldwide, and non-invasive methods for detecting key mutations and staging are essential for improving patient outcomes. Here, we compare the performance of two machine learning models - FMCIB+XGBoost, a supervised model with domain-specific pretraining, and Dinov2+ABMIL, a self-supervised model with attention-based multiple-instance learning - on 3D lung nodule data from the Stanford Radiogenomics and Lung-CT-PT-Dx cohorts. In the task of KRAS and EGFR mutation detection, FMCIB+XGBoost consistently outperformed Dinov2+ABMIL, achieving accuracies of 0.846 and 0.883 for KRAS and EGFR mutations, respectively. In cancer staging, Dinov2+ABMIL demonstrated competitive generalization, achieving an accuracy of 0.797 for T-stage prediction in the Lung-CT-PT-Dx cohort, suggesting SSL's adaptability across diverse datasets. Our results emphasize the clinical utility of supervised models in mutation detection and highlight the potential of SSL to improve staging generalization, while identifying areas for enhancement in mutation sensitivity.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 23.4%">
                            Medicine
                        </span>
                <!-- LLMs: 3.9 -->
                    
                <!-- Hardware: 3.0 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -11.1816
                </span>
                <a href="https://arxiv.org/abs/2306.11677" target="_blank" rel="noopener noreferrer">Pseudorandom unitaries are neither real nor sparse nor noise-robust</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tobias Haug, Kishor Bharti, Dax Enshan Koh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Pseudorandom quantum states (PRSs) and pseudorandom unitaries (PRUs) possess the dual nature of being efficiently constructible while appearing completely random to any efficient quantum algorithm. In this study, we establish fundamental bounds on pseudorandomness. We show that PRSs and PRUs exist o</span>
                
                <span class="abstract-full" style="display: none;">Pseudorandom quantum states (PRSs) and pseudorandom unitaries (PRUs) possess the dual nature of being efficiently constructible while appearing completely random to any efficient quantum algorithm. In this study, we establish fundamental bounds on pseudorandomness. We show that PRSs and PRUs exist only when the probability that an error occurs is negligible, ruling out their generation on noisy intermediate-scale and early fault-tolerant quantum computers. Further, we show that PRUs need imaginarity while PRS do not have this restriction. This implies that quantum randomness requires in general a complex-valued formalism of quantum mechanics, while for random quantum states real numbers suffice. Additionally, we derive lower bounds on the coherence of PRSs and PRUs, ruling out the existence of sparse PRUs and PRSs. We also show that the notions of PRS, PRUs and pseudorandom scramblers (PRSSs) are distinct in terms of resource requirements. We introduce the concept of pseudoresources, where states which contain a low amount of a given resource masquerade as high-resource states. We define pseudocoherence, pseudopurity and pseudoimaginarity, and identify three distinct types of pseudoresources in terms of their masquerading capabilities. Our work also establishes rigorous bounds on the efficiency of property testing, demonstrating the exponential complexity in distinguishing real quantum states from imaginary ones, in contrast to the efficient measurability of unitary imaginarity. Further, we show an exponential advantage in imaginarity testing when having access to the complex conjugate of the state. Lastly, we show that the transformation from a complex to a real model of quantum computation is inefficient, in contrast to the reverse process, which is efficient. Our results establish fundamental limits on property testing and provide valuable insights into quantum pseudorandomness.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 16.8%">
                            Quantum Computing
                        </span>
                <!-- LLMs: 4.2 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Medicine: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -12.5591
                </span>
                <a href="https://arxiv.org/abs/2505.22070" target="_blank" rel="noopener noreferrer">Physical Reduced Stochastic Equations for Continuously Monitored Non-Markovian Quantum Systems with a Markovian Embedding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hendra I. Nurdin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">An effective approach to modeling non-Markovian quantum systems is to embed a principal (quantum) system of interest into a larger quantum system. A widely employed embedding is one that uses another quantum system, referred to as the auxiliary system, which is coupled to the principal system, and b</span>
                
                <span class="abstract-full" style="display: none;">An effective approach to modeling non-Markovian quantum systems is to embed a principal (quantum) system of interest into a larger quantum system. A widely employed embedding is one that uses another quantum system, referred to as the auxiliary system, which is coupled to the principal system, and both the principal and auxiliary can be coupled to quantum white noise processes. The principal and auxiliary together form a quantum Markov system and the quantum white noises act as a bath (environment) for this system.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 22.0%">
                            Quantum Computing
                        </span>
                <!-- Medicine: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- LLMs: 1.6 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -16.672
                </span>
                <a href="https://arxiv.org/abs/2505.18478" target="_blank" rel="noopener noreferrer">Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lucas Tecot, Di Luo, Cho-Jui Hsieh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Advancements in quantum computing have spurred significant interest in harnessing its potential for speedups over classical systems. However, noise remains a major obstacle to achieving reliable quantum algorithms. In this work, we present a provably noise-resilient training theory and algorithm to </span>
                
                <span class="abstract-full" style="display: none;">Advancements in quantum computing have spurred significant interest in harnessing its potential for speedups over classical systems. However, noise remains a major obstacle to achieving reliable quantum algorithms. In this work, we present a provably noise-resilient training theory and algorithm to enhance the robustness of parameterized quantum circuit classifiers. Our method, with a natural connection to Evolutionary Strategies, guarantees resilience to parameter noise with minimal adjustments to commonly used optimization algorithms. Our approach is function-agnostic and adaptable to various quantum circuits, successfully demonstrated in quantum phase classification tasks. By developing provably guaranteed optimization theory with quantum circuits, our work opens new avenues for practical, robust applications of near-term quantum computers.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 27.1%">
                            Quantum Computing
                        </span>
                <!-- LLMs: 4.6 -->
                    
                <!-- Evolutionary Algorithms: 3.7 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Medicine: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Hardware: 1.0 -->
                    
                <!-- 3D: 1.0 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -19.9226
                </span>
                <a href="https://arxiv.org/abs/2505.22193" target="_blank" rel="noopener noreferrer">Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Marco Parigi, Stefano Martina, Francesco Aldo Venturelli, Filippo Caruso
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI that aims to use quantum properties to improve the performances of their classical counterparts. However, existing algorithms are not easily scalable due to the limitations of near-term quantum devices. Following our previous </span>
                
                <span class="abstract-full" style="display: none;">Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI that aims to use quantum properties to improve the performances of their classical counterparts. However, existing algorithms are not easily scalable due to the limitations of near-term quantum devices. Following our previous work on QDMs, here we propose and implement two physics-inspired protocols. In the first, we use the formalism of quantum stochastic walks, showing that a specific interplay of quantum and classical dynamics in the forward process produces statistically more robust models generating sets of MNIST images with lower Fr\'echet Inception Distance (FID) than using totally classical dynamics. In the second approach, we realize an algorithm to generate images by exploiting the intrinsic noise of real IBM quantum hardware with only four qubits. Our work could be a starting point to pave the way for new scenarios for large-scale algorithms in quantum Generative AI, where quantum noise is neither mitigated nor corrected, but instead exploited as a useful resource.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 27.4%">
                            Quantum Computing
                        </span>
                <!-- LLMs: 3.1 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Evolutionary Algorithms: 2.5 -->
                    
                <!-- Medicine: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -19.9788
                </span>
                <a href="https://arxiv.org/abs/2505.22619" target="_blank" rel="noopener noreferrer">Smart Contracts for SMEs and Large Companies</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: C. G. Liu, P. Bodorik, D. Jutla
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Research on blockchains addresses multiple issues, with one being writing smart contracts. In our previous research we described methodology and a tool to generate, in automated fashion, smart contracts from BPMN models. The generated smart contracts provide support for multi-step transactions that </span>
                
                <span class="abstract-full" style="display: none;">Research on blockchains addresses multiple issues, with one being writing smart contracts. In our previous research we described methodology and a tool to generate, in automated fashion, smart contracts from BPMN models. The generated smart contracts provide support for multi-step transactions that facilitate repair/upgrade of smart contracts. In this paper we show how the approach is used to support collaborations via smart contracts for companies ranging from SMEs with little IT capabilities to companies with IT using blockchain smart contracts. Furthermore, we also show how the approach is used for certain applications to generate smart contracts by a BPMN modeler who does not need any knowledge of blockchain technology or smart contract development - thus we are hoping to facilitate democratization of smart contracts and blockchain technology.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #3cc377" title="Confidence: 17.3%">
                            Blockchain
                        </span>
                <!-- LLMs: 4.1 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Medicine: 2.6 -->
                    
                <!-- Evolutionary Algorithms: 2.5 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -20.645
                </span>
                <a href="https://arxiv.org/abs/2505.22142" target="_blank" rel="noopener noreferrer">Interpolation of Quantum Polar Codes and Quantum Reed-Muller Codes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Keita Hidaka, Dina Abdelhadi, Ruediger Urbanke
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Good quantum error-correcting codes that fulfill practical considerations, such as simple encoding circuits and efficient decoders, are essential for functional quantum information processing systems. Quantum polar codes satisfy some of these requirements but lack certain critical features, thereby </span>
                
                <span class="abstract-full" style="display: none;">Good quantum error-correcting codes that fulfill practical considerations, such as simple encoding circuits and efficient decoders, are essential for functional quantum information processing systems. Quantum polar codes satisfy some of these requirements but lack certain critical features, thereby hindering their widespread use. Existing constructions either require entanglement assistance to produce valid quantum codes, suffer from poor finite-size performance, or fail to tailor polar codes to the underlying channel properties. Meanwhile, quantum Reed-Muller (RM) codes demonstrate strong performance, though no known efficient decoding algorithm exists for them. In this work, we propose strategies to interpolate between quantum polar codes and quantum RM codes, thus addressing the challenges of designing valid quantum polar codes without entanglement assistance and improving finite-size code performance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 30.9%">
                            Quantum Computing
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.1%">
                            LLMs
                        </span>
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Medicine: 1.8 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -67.2338
                </span>
                <a href="https://arxiv.org/abs/2412.18208" target="_blank" rel="noopener noreferrer">Quantum framework for Reinforcement Learning: Integrating Markov decision process, quantum arithmetic, and trajectory search</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Thet Htar Su, Shaswot Shresthamali, Masaaki Kondo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces a quantum framework for addressing reinforcement learning (RL) tasks, grounded in the quantum principles and leveraging a fully quantum model of the classical Markov decision process (MDP). By employing quantum concepts and a quantum search algorithm, this work presents the imp</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces a quantum framework for addressing reinforcement learning (RL) tasks, grounded in the quantum principles and leveraging a fully quantum model of the classical Markov decision process (MDP). By employing quantum concepts and a quantum search algorithm, this work presents the implementation and optimization of the agent-environment interactions entirely within the quantum domain, eliminating reliance on classical computations. Key contributions include the quantum-based state transitions, return calculation, and trajectory search mechanism that utilize quantum principles to demonstrate the realization of RL processes through quantum phenomena. The implementation emphasizes the fundamental role of quantum superposition in enhancing computational efficiency for RL tasks. Results demonstrate the capacity of a quantum model to achieve quantum enhancement in RL, highlighting the potential of fully quantum implementations in decision-making tasks. This work not only underscores the applicability of quantum computing in machine learning but also contributes to the field of quantum reinforcement learning (QRL) by offering a robust framework for understanding and exploiting quantum computing in RL systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #d37d97" title="Confidence: 75.6%">
                            Quantum Computing
                        </span>
                <!-- Medicine: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- LLMs: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
    </div>
    
    <div class="date-section">
        <h2 class="date-header">2025-05-28</h2>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.4602
                </span>
                <a href="https://arxiv.org/abs/2505.18133" target="_blank" rel="noopener noreferrer">Joint Encryption and Error Correction for Secure Quantum Communication</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nitin Jha, Abhishek Parakh, Mahadevan Subramaniam
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Secure quantum networks are a bedrock requirement for developing a future quantum internet. However, quantum channels are susceptible to channel noise that introduce errors in the transmitted data. The traditional approach to providing error correction typically encapsulates the message in an error </span>
                
                <span class="abstract-full" style="display: none;">Secure quantum networks are a bedrock requirement for developing a future quantum internet. However, quantum channels are susceptible to channel noise that introduce errors in the transmitted data. The traditional approach to providing error correction typically encapsulates the message in an error correction code after encryption. Such separate processes incur overhead that must be avoided when possible. We, consequently, provide a single integrated process that allows for encryption as well as error correction. This is a first attempt to do so for secure quantum communication and combines the Calderbank-Shor-Steane (CSS) code with the three-stage secure quantum communication protocol. Lastly, it allows for arbitrary qubits to be transmitted from sender to receiver making the proposed protocol general purpose.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 14.9%">
                            Quantum Computing
                        </span>
                <!-- Networks: 2.2 -->
                    
                <!-- Cryptography: 1.8 -->
                    
                <!-- Medicine: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- LLMs: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
    </div>
    
    <div class="date-section">
        <h2 class="date-header">2025-05-27</h2>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.8123
                </span>
                <a href="https://arxiv.org/abs/2405.10148" target="_blank" rel="noopener noreferrer">SpecDETR: A Transformer-based Hyperspectral Point Object Detection Network</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhaoxu Li, Wei An, Gaowei Guo, Longguang Wang, Yingqian Wang, Zaiping Lin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Hyperspectral target detection (HTD) aims to identify specific materials based on spectral information in hyperspectral imagery and can detect extremely small-sized objects, some of which occupy a smaller than one-pixel area. However, existing HTD methods are developed based on per-pixel binary clas</span>
                
                <span class="abstract-full" style="display: none;">Hyperspectral target detection (HTD) aims to identify specific materials based on spectral information in hyperspectral imagery and can detect extremely small-sized objects, some of which occupy a smaller than one-pixel area. However, existing HTD methods are developed based on per-pixel binary classification, neglecting the three-dimensional cube structure of hyperspectral images (HSIs) that integrates both spatial and spectral dimensions. The synergistic existence of spatial and spectral features in HSIs enable objects to simultaneously exhibit both, yet the per-pixel HTD framework limits the joint expression of these features. In this paper, we rethink HTD from the perspective of spatial-spectral synergistic representation and propose hyperspectral point object detection as an innovative task framework. We introduce SpecDETR, the first specialized network for hyperspectral multi-class point object detection, which eliminates dependence on pre-trained backbone networks commonly required by vision-based object detectors. SpecDETR uses a multi-layer Transformer encoder with self-excited subpixel-scale attention modules to directly extract deep spatial-spectral joint features from hyperspectral cubes. We develop a simulated hyperspectral point object detection benchmark termed SPOD, and for the first time, evaluate and compare the performance of visual object detection networks and HTD methods on hyperspectral point object detection. Extensive experiments demonstrate that our proposed SpecDETR outperforms SOTA visual object detection networks and HTD methods. Our code and dataset are available at https://github.com/ZhaoxuLi123/SpecDETR.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 13.9%">
                            Computer Vision
                        </span>
                <!-- Medicine: 3.8 -->
                    
                <!-- LLMs: 2.8 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.6452
                </span>
                <a href="https://arxiv.org/abs/2505.19731" target="_blank" rel="noopener noreferrer">Accelerating Nash Learning from Human Feedback via Mirror Prox</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Daniil Tiapkin, Daniele Calandriello, Denis Belomestny, Eric Moulines, Alexey Naumov, Kashif Rasul, Michal Valko, Pierre Menard
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on reward models, frequently assuming preference structures like the Bradley-Terry model, which may not accurately capture the complexities of real human preferences (e.g., intransitivity). Nash Learning from Human Feedback (</span>
                
                <span class="abstract-full" style="display: none;">Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on reward models, frequently assuming preference structures like the Bradley-Terry model, which may not accurately capture the complexities of real human preferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF) offers a more direct alternative by framing the problem as finding a Nash equilibrium of a game defined by these preferences. In this work, we introduce Nash Mirror Prox ($\mathtt{Nash-MP}$), an online NLHF algorithm that leverages the Mirror Prox optimization scheme to achieve fast and stable convergence to the Nash equilibrium. Our theoretical analysis establishes that Nash-MP exhibits last-iterate linear convergence towards the $\beta$-regularized Nash equilibrium. Specifically, we prove that the KL-divergence to the optimal policy decreases at a rate of order $(1+2\beta)^{-N/2}$, where $N$ is a number of preference queries. We further demonstrate last-iterate linear convergence for the exploitability gap and uniformly for the span semi-norm of log-probabilities, with all these rates being independent of the size of the action space. Furthermore, we propose and analyze an approximate version of Nash-MP where proximal steps are estimated using stochastic policy gradients, making the algorithm closer to applications. Finally, we detail a practical implementation strategy for fine-tuning large language models and present experiments that demonstrate its competitive performance and compatibility with existing methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #44f899" title="Confidence: 5.2%">
                            Reinforcement Learning
                        </span>
                <!-- Federated Learning: 4.0 -->
                    
                <!-- LLMs: 2.7 -->
                    
                <!-- Bayesian Optimization: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Medicine: 1.8 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Game Theory: 1.2 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.3527
                </span>
                <a href="https://arxiv.org/abs/2505.20001" target="_blank" rel="noopener noreferrer">NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-ID</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shihao Li, Chenglong Li, Aihua Zheng, Andong Lu, Jin Tang, Jixin Ma
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multi-modal object re-identification (ReID) aims to extract identity features across heterogeneous spectral modalities to enable accurate recognition and retrieval in complex real-world scenarios. However, most existing methods rely on implicit feature fusion structures, making it difficult to model</span>
                
                <span class="abstract-full" style="display: none;">Multi-modal object re-identification (ReID) aims to extract identity features across heterogeneous spectral modalities to enable accurate recognition and retrieval in complex real-world scenarios. However, most existing methods rely on implicit feature fusion structures, making it difficult to model fine-grained recognition strategies under varying challenging conditions. Benefiting from the powerful semantic understanding capabilities of Multi-modal Large Language Models (MLLMs), the visual appearance of an object can be effectively translated into descriptive text. In this paper, we propose a reliable multi-modal caption generation method based on attribute confidence, which significantly reduces the unknown recognition rate of MLLMs in multi-modal semantic generation and improves the quality of generated text. Additionally, we propose a novel ReID framework NEXT, the Multi-grained Mixture of Experts via Text-Modulation for Multi-modal Object Re-Identification. Specifically, we decouple the recognition problem into semantic and structural expert branches to separately capture modality-specific appearance and intrinsic structure. For semantic recognition, we propose the Text-Modulated Semantic-sampling Experts (TMSE), which leverages randomly sampled high-quality semantic texts to modulate expert-specific sampling of multi-modal features and mining intra-modality fine-grained semantic cues. Then, to recognize coarse-grained structure features, we propose the Context-Shared Structure-aware Experts (CSSE) that focuses on capturing the holistic object structure across modalities and maintains inter-modality structural consistency through a soft routing mechanism. Finally, we propose the Multi-Modal Feature Aggregation (MMFA), which adopts a unified feature fusion strategy to simply and effectively integrate semantic and structural expert outputs into the final identity representations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 7.0%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 5.5%">
                            Computer Vision
                        </span>
                <!-- Federated Learning: 3.9 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- GNN: 2.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.313
                </span>
                <a href="https://arxiv.org/abs/2505.18986" target="_blank" rel="noopener noreferrer">VL-SAM-V2: Open-World Object Detection with General and Specific Query Fusion</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhiwei Lin, Yongtao Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Current perception models have achieved remarkable success by leveraging large-scale labeled datasets, but still face challenges in open-world environments with novel objects. To address this limitation, researchers introduce open-set perception models to detect or segment arbitrary test-time user-i</span>
                
                <span class="abstract-full" style="display: none;">Current perception models have achieved remarkable success by leveraging large-scale labeled datasets, but still face challenges in open-world environments with novel objects. To address this limitation, researchers introduce open-set perception models to detect or segment arbitrary test-time user-input categories. However, open-set models rely on human involvement to provide predefined object categories as input during inference. More recently, researchers have framed a more realistic and challenging task known as open-ended perception that aims to discover unseen objects without requiring any category-level input from humans at inference time. Nevertheless, open-ended models suffer from low performance compared to open-set models. In this paper, we present VL-SAM-V2, an open-world object detection framework that is capable of discovering unseen objects while achieving favorable performance. To achieve this, we combine queries from open-set and open-ended models and propose a general and specific query fusion module to allow different queries to interact. By adjusting queries from open-set models, we enable VL-SAM-V2 to be evaluated in the open-set or open-ended mode. In addition, to learn more diverse queries, we introduce ranked learnable queries to match queries with proposals from open-ended models by sorting. Moreover, we design a denoising point training strategy to facilitate the training process. Experimental results on LVIS show that our method surpasses the previous open-set and open-ended methods, especially on rare objects.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 8.4%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 6.6%">
                            Computer Vision
                        </span>
                <!-- Federated Learning: 3.1 -->
                    
                <!-- GNN: 3.1 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- HPO and AutoML: 2.0 -->
                    
                <!-- Decision Trees: 1.8 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.3056
                </span>
                <a href="https://arxiv.org/abs/2412.00592" target="_blank" rel="noopener noreferrer">LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in Real-World Scenes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shing-Hei Ho, Bao Thach, Minghan Zhu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data for autonomous driving. Our framework edits real-world LiDAR scans by introducing new object layouts while preserving the realism of the background environment. Compared to end-to-end frameworks that generate LiDAR point clo</span>
                
                <span class="abstract-full" style="display: none;">We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data for autonomous driving. Our framework edits real-world LiDAR scans by introducing new object layouts while preserving the realism of the background environment. Compared to end-to-end frameworks that generate LiDAR point clouds from scratch, LiDAR-EDIT offers users full control over the object layout, including the number, type, and pose of objects, while keeping most of the original real-world background. Our method also provides object labels for the generated data. Compared to novel view synthesis techniques, our framework allows for the creation of counterfactual scenarios with object layouts significantly different from the original real-world scene. LiDAR-EDIT uses spherical voxelization to enforce correct LiDAR projective geometry in the generated point clouds by construction. During object removal and insertion, generative models are employed to fill the unseen background and object parts that were occluded in the original real LiDAR scans. Experimental results demonstrate that our framework produces realistic LiDAR scans with practical value for downstream tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 5.7%">
                            Computer Vision
                        </span>
                <!-- Medicine: 3.0 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2972
                </span>
                <a href="https://arxiv.org/abs/2405.14273" target="_blank" rel="noopener noreferrer">A fast algorithm to minimize prediction loss of the optimal solution in inverse optimization problem of MILP</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Akira Kitaoka
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We consider the inverse optimization problem of estimating the weights of the objective function such that the given solution is an optimal solution for a mixed integer linear program (MILP). In this inverse optimization problem, the known methods exhibit inefficient convergence. Specifically, if $d</span>
                
                <span class="abstract-full" style="display: none;">We consider the inverse optimization problem of estimating the weights of the objective function such that the given solution is an optimal solution for a mixed integer linear program (MILP). In this inverse optimization problem, the known methods exhibit inefficient convergence. Specifically, if $d$ denotes the dimension of the weights and $k$ the number of iterations, then the error of the weights is bounded by $O(k^{-1/(d-1)})$, leading to slow convergence as $d$ increases. We propose a projected subgradient method with a step size of $k^{-1/2}$ based on suboptimality loss. We theoretically show and demonstrate that the proposed method efficiently learns the weights. In particular, we show that there exists a constant $\gamma > 0$ such that the distance between the learned and true weights is bounded by $ O\left(k^{-1/(1+\gamma)} \exp\left(-\frac{\gamma k^{1/2}}{2+\gamma}\right)\right), $ or the optimal solution is exactly recovered. Furthermore, experiments demonstrate that the proposed method solves the inverse optimization problems of MILP using fewer than $1/7$ the number of MILP calls required by known methods, and converges within a finite number of iterations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #31bb31" title="Confidence: 13.0%">
                            Bayesian Optimization
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #b3ebae" title="Confidence: 5.9%">
                            Federated Learning
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #a49950" title="Confidence: 5.3%">
                            Math
                        </span>
                <!-- Reinforcement Learning: 4.2 -->
                    
                <!-- Evolutionary Algorithms: 3.2 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Cryptography: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Medicine: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Finance: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2944
                </span>
                <a href="https://arxiv.org/abs/2409.19833" target="_blank" rel="noopener noreferrer">HazyDet: Open-Source Benchmark for Drone-View Object Detection with Depth-Cues in Hazy Scenes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Changfeng Feng, Zhenyuan Chen, Xiang Li, Chunping Wang, Jian Yang, Ming-Ming Cheng, Yimian Dai, Qiang Fu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Object detection from aerial platforms under adverse atmospheric conditions, particularly haze, is paramount for robust drone autonomy. Yet, this domain remains largely underexplored, primarily hindered by the absence of specialized benchmarks. To bridge this gap, we present \textit{HazyDet}, the fi</span>
                
                <span class="abstract-full" style="display: none;">Object detection from aerial platforms under adverse atmospheric conditions, particularly haze, is paramount for robust drone autonomy. Yet, this domain remains largely underexplored, primarily hindered by the absence of specialized benchmarks. To bridge this gap, we present \textit{HazyDet}, the first, large-scale benchmark specifically designed for drone-view object detection in hazy conditions. Comprising 383,000 real-world instances derived from both naturally hazy captures and synthetically hazed scenes augmented from clear images, HazyDet provides a challenging and realistic testbed for advancing detection algorithms. To address the severe visual degradation induced by haze, we propose the Depth-Conditioned Detector (DeCoDet), a novel architecture that integrates a Depth-Conditioned Kernel to dynamically modulate feature representations based on depth cues. The practical efficacy and robustness of DeCoDet are further enhanced by its training with a Progressive Domain Fine-Tuning (PDFT) strategy to navigate synthetic-to-real domain shifts, and a Scale-Invariant Refurbishment Loss (SIRLoss) to ensure resilient learning from potentially noisy depth annotations. Comprehensive empirical validation on HazyDet substantiates the superiority of our unified DeCoDet framework, which achieves state-of-the-art performance, surpassing the closest competitor by a notable +1.5\% mAP on challenging real-world hazy test scenarios. Our dataset and toolkit are available at https://github.com/GrokCV/HazyDet.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 5.5%">
                            Computer Vision
                        </span>
                <!-- Medicine: 4.5 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- LLMs: 1.8 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Hardware: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2775
                </span>
                <a href="https://arxiv.org/abs/2505.04119" target="_blank" rel="noopener noreferrer">GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zixiang Ai, Zichen Liu, Yuanhang Lei, Zhenyu Cui, Xu Zou, Jiahuan Zhou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data. However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive. Existing parameter-efficient fine-tuning (PEFT) approaches, which focus</span>
                
                <span class="abstract-full" style="display: none;">Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data. However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive. Existing parameter-efficient fine-tuning (PEFT) approaches, which focus primarily on input token prompting, struggle to achieve competitive performance due to their limited ability to capture the geometric information inherent in point clouds. To address this challenge, we propose a novel Geometry-Aware Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the adaptability of 3D vision models. First, we introduce a Point Prompt that serves as an auxiliary input alongside the original point cloud, explicitly guiding the model to capture fine-grained geometric details. Additionally, we present a Point Shift Prompter designed to extract global shape information from the point cloud, enabling instance-specific geometric adjustments at the input level. Moreover, our proposed Prompt Propagation mechanism incorporates the shape information into the model's feature extraction process, further strengthening its ability to capture essential geometric characteristics. Extensive experiments demonstrate that GAPrompt significantly outperforms state-of-the-art PEFT methods and achieves competitive results compared to full fine-tuning on various benchmarks, while utilizing only 2.19% of trainable parameters. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 5.5%">
                            Computer Vision
                        </span>
                <!-- GNN: 4.6 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- 3D: 3.7 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2646
                </span>
                <a href="https://arxiv.org/abs/2505.19420" target="_blank" rel="noopener noreferrer">ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wenhua Wu, Chenpeng Su, Siting Zhu, Tianchen Deng, Zhe Liu, Hesheng Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advancements in Neural Radiance Fields (NeRF) and 3D Gaussian-based Simultaneous Localization and Mapping (SLAM) methods have demonstrated exceptional localization precision and remarkable dense mapping performance. However, dynamic objects introduce critical challenges by disrupting scene co</span>
                
                <span class="abstract-full" style="display: none;">Recent advancements in Neural Radiance Fields (NeRF) and 3D Gaussian-based Simultaneous Localization and Mapping (SLAM) methods have demonstrated exceptional localization precision and remarkable dense mapping performance. However, dynamic objects introduce critical challenges by disrupting scene consistency, leading to tracking drift and mapping artifacts. Existing methods that employ semantic segmentation or object detection for dynamic identification and filtering typically rely on predefined categorical priors, while discarding dynamic scene information crucial for robotic applications such as dynamic obstacle avoidance and environmental interaction. To overcome these challenges, we propose ADD-SLAM: an Adaptive Dynamic Dense SLAM framework based on Gaussian splitting. We design an adaptive dynamic identification mechanism grounded in scene consistency analysis, comparing geometric and textural discrepancies between real-time observations and historical maps. Ours requires no predefined semantic category priors and adaptively discovers scene dynamics. Precise dynamic object recognition effectively mitigates interference from moving targets during localization. Furthermore, we propose a dynamic-static separation mapping strategy that constructs a temporal Gaussian model to achieve online incremental dynamic modeling. Experiments conducted on multiple dynamic datasets demonstrate our method's flexible and accurate dynamic segmentation capabilities, along with state-of-the-art performance in both localization and mapping.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 6.1%">
                            Computer Vision
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.9%">
                            LLMs
                        </span>
                <!-- Medicine: 4.0 -->
                    
                <!-- 3D: 3.7 -->
                    
                <!-- HPO and AutoML: 1.9 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2631
                </span>
                <a href="https://arxiv.org/abs/2405.05648" target="_blank" rel="noopener noreferrer">ASGrasp: Generalizable Transparent Object Reconstruction and 6-DoF Grasp Detection from RGB-D Active Stereo Camera</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jun Shi, Yong A, Yixiang Jin, Dingzhe Li, Haoyu Niu, Zhezhu Jin, He Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we tackle the problem of grasping transparent and specular objects. This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras. For the first time, we propose ASGrasp, a 6-DoF grasp detection net</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we tackle the problem of grasping transparent and specular objects. This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras. For the first time, we propose ASGrasp, a 6-DoF grasp detection network that uses an RGB-D active stereo camera. ASGrasp utilizes a two-layer learning-based stereo network for the purpose of transparent object reconstruction, enabling material-agnostic object grasping in cluttered environments. In contrast to existing RGB-D based grasp detection methods, which heavily depend on depth restoration networks and the quality of depth maps generated by depth cameras, our system distinguishes itself by its ability to directly utilize raw IR and RGB images for transparent object geometry reconstruction. We create an extensive synthetic dataset through domain randomization, which is based on GraspNet-1Billion. Our experiments demonstrate that ASGrasp can achieve over 90% success rate for generalizable transparent object grasping in both simulation and the real via seamless sim-to-real transfer. Our method significantly outperforms SOTA networks and even surpasses the performance upper bound set by perfect visible point cloud inputs.Project page: https://pku-epic.github.io/ASGrasp</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 5.3%">
                            Computer Vision
                        </span>
                <!-- LLMs: 4.1 -->
                    
                <!-- Medicine: 3.7 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2617
                </span>
                <a href="https://arxiv.org/abs/2505.19227" target="_blank" rel="noopener noreferrer">Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Frederik Kunstner, Francis Bach
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent works have highlighted optimization difficulties faced by gradient descent in training the first and last layers of transformer-based language models, which are overcome by optimizers such as Adam. These works suggest that the difficulty is linked to the heavy-tailed distribution of words in </span>
                
                <span class="abstract-full" style="display: none;">Recent works have highlighted optimization difficulties faced by gradient descent in training the first and last layers of transformer-based language models, which are overcome by optimizers such as Adam. These works suggest that the difficulty is linked to the heavy-tailed distribution of words in text data, where the frequency of the $k$th most frequent word $\pi_k$ is proportional to $1/k$, following Zipf's law. To better understand the impact of the data distribution on training performance, we study a linear bigram model for next-token prediction when the tokens follow a power law $\pi_k \propto 1/k^\alpha$ parameterized by the exponent $\alpha > 0$. We derive optimization scaling laws for deterministic gradient descent and sign descent as a proxy for Adam as a function of the exponent $\alpha$. Existing theoretical investigations in scaling laws assume that the eigenvalues of the data decay as a power law with exponent $\alpha > 1$. This assumption effectively makes the problem ``finite dimensional'' as most of the loss comes from a few of the largest eigencomponents. In comparison, we show that the problem is more difficult when the data have heavier tails. The case $\alpha = 1$ as found in text data is ``worst-case'' for gradient descent, in that the number of iterations required to reach a small relative error scales almost linearly with dimension. While the performance of sign descent also depends on the dimension, for Zipf-distributed data the number of iterations scales only with the square-root of the dimension, leading to a large improvement for large vocabularies.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #31bb31" title="Confidence: 6.3%">
                            Bayesian Optimization
                        </span>
                <!-- Math: 2.8 -->
                    
                <!-- Federated Learning: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Medicine: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Cryptography: 1.6 -->
                    
                <!-- Finance: 1.4 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2421
                </span>
                <a href="https://arxiv.org/abs/2405.07760" target="_blank" rel="noopener noreferrer">CAGES: Cost-Aware Gradient Entropy Search for Efficient Local Multi-Fidelity Bayesian Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wei-Ting Tang, Joel A. Paulson
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Bayesian optimization (BO) is a popular approach for optimizing expensive-to-evaluate black-box objective functions. An important challenge in BO is its application to high-dimensional search spaces due in large part to the curse of dimensionality. One way to overcome this challenge is to focus on l</span>
                
                <span class="abstract-full" style="display: none;">Bayesian optimization (BO) is a popular approach for optimizing expensive-to-evaluate black-box objective functions. An important challenge in BO is its application to high-dimensional search spaces due in large part to the curse of dimensionality. One way to overcome this challenge is to focus on local BO methods that aim to efficiently learn gradients, which have shown strong empirical performance on high-dimensional problems including policy search in reinforcement learning (RL). Current local BO methods assume access to only a single high-fidelity information source whereas, in many problems, one has access to multiple cheaper approximations of the objective. We propose a novel algorithm, Cost-Aware Gradient Entropy Search (CAGES), for local BO of multi-fidelity black-box functions. CAGES makes no assumption about the relationship between different information sources, making it more flexible than other multi-fidelity methods. It also employs a new information-theoretic acquisition function, which enables systematic identification of samples that maximize the information gain about the unknown gradient per evaluation cost. We demonstrate CAGES can achieve significant performance improvements compared to other state-of-the-art methods on synthetic and benchmark RL problems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #31bb31" title="Confidence: 7.3%">
                            Bayesian Optimization
                        </span>
                <!-- Federated Learning: 3.3 -->
                    
                <!-- GNN: 2.9 -->
                    
                <!-- Evolutionary Algorithms: 2.6 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Medicine: 1.0 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2157
                </span>
                <a href="https://arxiv.org/abs/2505.20021" target="_blank" rel="noopener noreferrer">Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hyunsik Chae, Seungwoo Yoon, Jaden Park, Chloe Yewon Chun, Yongin Cho, Mu Cai, Yong Jae Lee, Ernest K. Ryu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal comprehension and reasoning capabilities, yet they often struggle with trivially simple visual tasks. In this work, we focus on the domain of basic 2D Euclidean geometry and systematically categorize the fundamental, indivi</span>
                
                <span class="abstract-full" style="display: none;">Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal comprehension and reasoning capabilities, yet they often struggle with trivially simple visual tasks. In this work, we focus on the domain of basic 2D Euclidean geometry and systematically categorize the fundamental, indivisible visual perception skills, which we refer to as atomic visual skills. We then introduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the atomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find that they struggle with these tasks, despite being trivial for adult humans. Our findings highlight the need for purpose-built datasets to train and evaluate VLMs on atomic, rather than composite, visual perception tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.6%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #753a22" title="Confidence: 5.3%">
                            Computer Vision
                        </span>
                <!-- Medicine: 4.3 -->
                    
                <!-- Hardware: 2.2 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2128
                </span>
                <a href="https://arxiv.org/abs/2505.19850" target="_blank" rel="noopener noreferrer">DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Leander Diaz-Bone, Marco Bagatella, Jonas H\"ubotter, Andreas Krause
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Sparse-reward reinforcement learning (RL) can model a wide range of highly complex tasks. Solving sparse-reward tasks is RL's core premise - requiring efficient exploration coupled with long-horizon credit assignment - and overcoming these challenges is key for building self-improving agents with su</span>
                
                <span class="abstract-full" style="display: none;">Sparse-reward reinforcement learning (RL) can model a wide range of highly complex tasks. Solving sparse-reward tasks is RL's core premise - requiring efficient exploration coupled with long-horizon credit assignment - and overcoming these challenges is key for building self-improving agents with superhuman ability. We argue that solving complex and high-dimensional tasks requires solving simpler tasks that are relevant to the target task. In contrast, most prior work designs strategies for selecting exploratory tasks with the objective of solving any task, making exploration of challenging high-dimensional, long-horizon tasks intractable. We find that the sense of direction, necessary for effective exploration, can be extracted from existing RL algorithms, without needing any prior information. Based on this finding, we propose a method for directed sparse-reward goal-conditioned very long-horizon RL (DISCOVER), which selects exploratory goals in the direction of the target task. We connect DISCOVER to principled exploration in bandits, formally bounding the time until the target task becomes achievable in terms of the agent's initial distance to the target, but independent of the volume of the space of all tasks. Empirically, we perform a thorough evaluation in high-dimensional environments. We find that the directed goal selection of DISCOVER solves exploration problems that are beyond the reach of prior state-of-the-art exploration methods in RL.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #31bb31" title="Confidence: 5.1%">
                            Bayesian Optimization
                        </span>
                <!-- LLMs: 3.4 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Game Theory: 1.2 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2008
                </span>
                <a href="https://arxiv.org/abs/2505.19841" target="_blank" rel="noopener noreferrer">Efficient Deconvolution in Populational Inverse Problems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Arnaud Vadeboncoeur, Mark Girolami, Andrew M. Stuart
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This work is focussed on the inversion task of inferring the distribution over parameters of interest leading to multiple sets of observations. The potential to solve such distributional inversion problems is driven by increasing availability of data, but a major roadblock is blind deconvolution, ar</span>
                
                <span class="abstract-full" style="display: none;">This work is focussed on the inversion task of inferring the distribution over parameters of interest leading to multiple sets of observations. The potential to solve such distributional inversion problems is driven by increasing availability of data, but a major roadblock is blind deconvolution, arising when the observational noise distribution is unknown. However, when data originates from collections of physical systems, a population, it is possible to leverage this information to perform deconvolution. To this end, we propose a methodology leveraging large data sets of observations, collected from different instantiations of the same physical processes, to simultaneously deconvolve the data corrupting noise distribution, and to identify the distribution over model parameters defining the physical processes. A parameter-dependent mathematical model of the physical process is employed. A loss function characterizing the match between the observed data and the output of the mathematical model is defined; it is minimized as a function of the both the parameter inputs to the model of the physics and the parameterized observational noise. This coupled problem is addressed with a modified gradient descent algorithm that leverages specific structure in the noise model. Furthermore, a new active learning scheme is proposed, based on adaptive empirical measures, to train a surrogate model to be accurate in parameter regions of interest; this approach accelerates computation and enables automatic differentiation of black-box, potentially nondifferentiable, code computing parameter-to-solution maps. The proposed methodology is demonstrated on porous medium flow, damped elastodynamics, and simplified models of atmospheric dynamics.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b3ebae" title="Confidence: 6.7%">
                            Federated Learning
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #31bb31" title="Confidence: 5.7%">
                            Bayesian Optimization
                        </span>
                <!-- Evolutionary Algorithms: 2.5 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.1966
                </span>
                <a href="https://arxiv.org/abs/2307.12179" target="_blank" rel="noopener noreferrer">Leveraging Knowledge Graphs for Zero-Shot Object-agnostic State Classification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Filipos Gouidis, Theodore Patkos, Antonis Argyros, Dimitris Plexousakis
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We investigate the problem of Object State Classification (OSC) as a zero-shot learning problem. Specifically, we propose the first Object-agnostic State Classification (OaSC) method that infers the state of a certain object without relying on the knowledge or the estimation of the object class. In </span>
                
                <span class="abstract-full" style="display: none;">We investigate the problem of Object State Classification (OSC) as a zero-shot learning problem. Specifically, we propose the first Object-agnostic State Classification (OaSC) method that infers the state of a certain object without relying on the knowledge or the estimation of the object class. In that direction, we capitalize on Knowledge Graphs (KGs) for structuring and organizing knowledge, which, in combination with visual information, enable the inference of the states of objects in object/state pairs that have not been encountered in the method's training set. A series of experiments investigate the performance of the proposed method in various settings, against several hypotheses and in comparison with state of the art approaches for object attribute classification. The experimental results demonstrate that the knowledge of an object class is not decisive for the prediction of its state. Moreover, the proposed OaSC method outperforms existing methods in all datasets and benchmarks by a great margin.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #31bb31" title="Confidence: 5.1%">
                            Bayesian Optimization
                        </span>
                <!-- Federated Learning: 3.8 -->
                    
                <!-- Medicine: 2.6 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.0157
                </span>
                <a href="https://arxiv.org/abs/2505.19737" target="_blank" rel="noopener noreferrer">Weighted Leave-One-Out Cross Validation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Luc Pronzato (RT-UQ), Maria-Jo\~ao Rendas
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a weighted version of Leave-One-Out (LOO) cross-validation for estimating the Integrated Squared Error (ISE) when approximating an unknown function by a predictor that depends linearly on evaluations of the function over a finite collection of sites. The method relies on the construction </span>
                
                <span class="abstract-full" style="display: none;">We present a weighted version of Leave-One-Out (LOO) cross-validation for estimating the Integrated Squared Error (ISE) when approximating an unknown function by a predictor that depends linearly on evaluations of the function over a finite collection of sites. The method relies on the construction of the best linear estimator of the squared prediction error at an arbitrary unsampled site based on squared LOO residuals, assuming that the function is a realization of a Gaussian Process (GP). A theoretical analysis of performance of the ISE estimator is presented, and robustness with respect to the choice of the GP kernel is investigated first analytically, then through numerical examples. Overall, the estimation of ISE is significantly more precise than with classical, unweighted, LOO cross validation. Application to model selection is briefly considered through examples.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #31bb31" title="Confidence: 6.3%">
                            Bayesian Optimization
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #a49950" title="Confidence: 5.9%">
                            Math
                        </span>
                <!-- Federated Learning: 3.4 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Cryptography: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Finance: 1.5 -->
                    
                <!-- Game Theory: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- LLMs: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18793" target="_blank" rel="noopener noreferrer">Genie Centurion: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wenhao Wang, Jianheng Song, Chiming Liu, Jiayao Ma, Siyuan Feng, Jingyuan Wang, Yuxin Jiang, Kylin Chen, Sikang Zhan, Yi Wang, Tong Meng, Modi Shi, Xindong He, Guanghui Ren, Yang Yang, Maoqing Yao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While Vision-Language-Action (VLA) models show strong generalizability in various tasks, real-world deployment of robotic policy still requires large-scale, high-quality human expert demonstrations. However, passive data collection via human teleoperation is costly, hard to scale, and often biased t</span>
                
                <span class="abstract-full" style="display: none;">While Vision-Language-Action (VLA) models show strong generalizability in various tasks, real-world deployment of robotic policy still requires large-scale, high-quality human expert demonstrations. However, passive data collection via human teleoperation is costly, hard to scale, and often biased toward passive demonstrations with limited diversity. To address this, we propose Genie Centurion (GCENT), a scalable and general data collection paradigm based on human rewind-and-refine guidance. When the robot execution failures occur, GCENT enables the system revert to a previous state with a rewind mechanism, after which a teleoperator provides corrective demonstrations to refine the policy. This framework supports a one-human-to-many-robots supervision scheme with a Task Sentinel module, which autonomously predicts task success and solicits human intervention when necessary, enabling scalable supervision. Empirical results show that GCENT achieves up to 40% higher task success rates than state-of-the-art data collection methods, and reaches comparable performance using less than half the data. We also quantify the data yield-to-effort ratio under multi-robot scenarios, demonstrating GCENT's potential for scalable and cost-efficient robot policy training in real-world environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.1 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18798" target="_blank" rel="noopener noreferrer">Governing Equation Discovery from Data Based on Differential Invariants</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lexiang Hu, Yikang Li, Zhouchen Lin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The explicit governing equation is one of the simplest and most intuitive forms for characterizing physical laws. However, directly discovering partial differential equations (PDEs) from data poses significant challenges, primarily in determining relevant terms from a vast search space. Symmetry, as</span>
                
                <span class="abstract-full" style="display: none;">The explicit governing equation is one of the simplest and most intuitive forms for characterizing physical laws. However, directly discovering partial differential equations (PDEs) from data poses significant challenges, primarily in determining relevant terms from a vast search space. Symmetry, as a crucial prior knowledge in scientific fields, has been widely applied in tasks such as designing equivariant networks and guiding neural PDE solvers. In this paper, we propose a pipeline for governing equation discovery based on differential invariants, which can losslessly reduce the search space of existing equation discovery methods while strictly adhering to symmetry. Specifically, we compute the set of differential invariants corresponding to the infinitesimal generators of the symmetry group and select them as the relevant terms for equation discovery. Taking DI-SINDy (SINDy based on Differential Invariants) as an example, we demonstrate that its success rate and accuracy in PDE discovery surpass those of other symmetry-informed governing equation discovery methods across a series of PDEs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.6 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Medicine: 2.6 -->
                    
                <!-- Hardware: 2.4 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18806" target="_blank" rel="noopener noreferrer">Mal-D2GAN: Double-Detector based GAN for Malware Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nam Hoang Thanh, Trung Pham Duy, Lam Bui Thu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Machine learning (ML) has been developed to detect malware in recent years. Most researchers focused their efforts on improving the detection performance but ignored the robustness of the ML models. In addition, many machine learning algorithms are very vulnerable to intentional attacks. To solve th</span>
                
                <span class="abstract-full" style="display: none;">Machine learning (ML) has been developed to detect malware in recent years. Most researchers focused their efforts on improving the detection performance but ignored the robustness of the ML models. In addition, many machine learning algorithms are very vulnerable to intentional attacks. To solve these problems, adversarial malware examples are generated by GANs to enhance the robustness of the malware detector. However, since current GAN models suffer from limitations such as unstable training and weak adversarial examples, we propose the Mal-D2GAN model to address these problems. Specifically, the Mal-D2GAN architecture was designed with double-detector and a least square loss function and tested on a dataset of 20,000 samples. The results show that the Mal-D2GAN model reduced the detection accuracy (true positive rate) in 8 malware detectors. The performance was then compared with that of the existing MalGAN and Mal- LSGAN models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 3.6 -->
                    
                <!-- Computer Vision: 3.1 -->
                    
                <!-- Medicine: 3.1 -->
                    
                <!-- LLMs: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Bayesian Optimization: 1.7 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18810" target="_blank" rel="noopener noreferrer">Discrete gradient methods for port-Hamiltonian differential-algebraic equations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Philipp L. Kinon, Riccardo Morandin, Philipp Schulze
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Discrete gradient methods are a powerful tool for the time discretization of dynamical systems, since they are structure-preserving regardless of the form of the total energy. In this work, we discuss the application of discrete gradient methods to the system class of nonlinear port-Hamiltonian diff</span>
                
                <span class="abstract-full" style="display: none;">Discrete gradient methods are a powerful tool for the time discretization of dynamical systems, since they are structure-preserving regardless of the form of the total energy. In this work, we discuss the application of discrete gradient methods to the system class of nonlinear port-Hamiltonian differential-algebraic equations - as they emerge from the port- and energy-based modeling of physical systems in various domains. We introduce a novel numerical scheme tailored for semi-explicit differential-algebraic equations and further address more general settings using the concepts of discrete gradient pairs and Dirac-dissipative structures. Additionally, the behavior under system transformations is investigated and we demonstrate that under suitable assumptions port-Hamiltonian differential-algebraic equations admit a representation which consists of a parametrized port-Hamiltonian semi-explicit system and an unstructured equation. Finally, we present the application to multibody system dynamics and discuss numerical results to demonstrate the capabilities of our approach.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.0 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- Federated Learning: 3.0 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18818" target="_blank" rel="noopener noreferrer">Automated Verification of Monotonic Data Structure Traversals in C</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Matthew Sotoudeh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Bespoke data structure operations are common in real-world C code. We identify one common subclass, monotonic data structure traversals (MDSTs), that iterate monotonically through the structure. For example, strlen iterates from start to end of a character array until a null byte is found, and a bin</span>
                
                <span class="abstract-full" style="display: none;">Bespoke data structure operations are common in real-world C code. We identify one common subclass, monotonic data structure traversals (MDSTs), that iterate monotonically through the structure. For example, strlen iterates from start to end of a character array until a null byte is found, and a binary search tree insert iterates from the tree root towards a leaf. We describe a new automated verification tool, Shrinker, to verify MDSTs written in C. Shrinker uses a new program analysis strategy called scapegoating size descent, which is designed to take advantage of the fact that many MDSTs produce very similar traces when executed on an input (e.g., some large list) as when executed on a 'shrunk' version of the input (e.g., the same list but with its first element deleted). We introduce a new benchmark set containing over one hundred instances proving correctness, equivalence, and memory safety properties of dozens of MDSTs found in major C codebases including Linux, NetBSD, OpenBSD, QEMU, Git, and Musl. Shrinker significantly increases the number of monotonic string and list traversals that can be verified vs. a portfolio of state-of-the-art tools.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.9 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Medicine: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Cryptography: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Game Theory: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18824" target="_blank" rel="noopener noreferrer">FlatAttention: Dataflow and Fabric Collectives Co-Optimization for Efficient Multi-Head Attention on Tile-Based Many-PE Accelerators</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chi Zhang, Luca Colagrande, Renzo Andri, Thomas Benz, Gamze Islamoglu, Alessandro Nadalini, Francesco Conti, Yawei Li, Luca Benini
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multi-Head Attention (MHA) is a critical computational kernel in transformer-based AI models. Emerging scalable tile-based accelerator architectures integrate increasing numbers of tightly-packed processing elements (PEs) with tensor units. MHA dataflow mapping is crucial for achieving high utilizat</span>
                
                <span class="abstract-full" style="display: none;">Multi-Head Attention (MHA) is a critical computational kernel in transformer-based AI models. Emerging scalable tile-based accelerator architectures integrate increasing numbers of tightly-packed processing elements (PEs) with tensor units. MHA dataflow mapping is crucial for achieving high utilization of the available units. We propose FlatAttention, a new dataflow for MHA on tile-based many-PE accelerators, minimizing costly main memory (HBM) accesses by leveraging collective primitives integrated into the on-chip network fabric. FlatAttention achieves up to 89.3% utilization, and 4.1x performance speedup over FlashAttention-3 dataflow on tile-based accelerators whilst reducing HBM traffic by 16x. Through algorithm-architecture co-exploration, we identify an optimal configuration for a large scaled-out tile-based accelerator featuring a 32x32 tile mesh with 1024 TFLOPS @ FP16 peak performance, comparable to the state-of-the-art Nvidia H100 GPU. FlatAttention in this configuration achieves up to 1.3x higher utilization over FlashAttention-3 on the H100 GPU. Meanwhile, this tile-based accelerator configuration requires 40% less HBM bandwidth compared to the H100, enabling a 1.8x reduction in die size, estimated on the same technology node.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Hardware: 2.6 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- LLMs: 2.0 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- Cryptography: 1.2 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18825" target="_blank" rel="noopener noreferrer">How to build a consistency model: Learning flow maps via self-distillation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nicholas M. Boffi, Michael S. Albergo, Eric Vanden-Eijnden
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Building on the framework proposed in Boffi et al. (2024), we present a systematic approach for learning flow maps associated with flow and diffusion models. Flow map-based models, commonly known as consistency models, encompass recent efforts to improve the efficiency of generative models based on </span>
                
                <span class="abstract-full" style="display: none;">Building on the framework proposed in Boffi et al. (2024), we present a systematic approach for learning flow maps associated with flow and diffusion models. Flow map-based models, commonly known as consistency models, encompass recent efforts to improve the efficiency of generative models based on solutions to differential equations. By exploiting a relationship between the velocity field underlying a continuous-time flow and the instantaneous rate of change of the flow map, we show how to convert existing distillation schemes into direct training algorithms via self-distillation, eliminating the need for pre-trained models. We empirically evaluate several instantiations of our framework, finding that high-dimensional tasks like image synthesis benefit from objective functions that avoid temporal and spatial derivatives of the flow map, while lower-dimensional tasks can benefit from objectives incorporating higher-order derivatives to capture sharp features.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 3.5 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.7 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18828" target="_blank" rel="noopener noreferrer">Improved Regret and Contextual Linear Extension for Pandora's Box and Prophet Inequality</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Junyan Liu, Ziyun Chen, Kun Wang, Haipeng Luo, Lillian J. Ratliff
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the Pandora's Box problem in an online learning setting with semi-bandit feedback. In each round, the learner sequentially pays to open up to $n$ boxes with unknown reward distributions, observes rewards upon opening, and decides when to stop. The utility of the learner is the maximum obser</span>
                
                <span class="abstract-full" style="display: none;">We study the Pandora's Box problem in an online learning setting with semi-bandit feedback. In each round, the learner sequentially pays to open up to $n$ boxes with unknown reward distributions, observes rewards upon opening, and decides when to stop. The utility of the learner is the maximum observed reward minus the cumulative cost of opened boxes, and the goal is to minimize regret defined as the gap between the cumulative expected utility and that of the optimal policy. We propose a new algorithm that achieves $\widetilde{O}(\sqrt{nT})$ regret after $T$ rounds, which improves the $\widetilde{O}(n\sqrt{T})$ bound of Agarwal et al. [2024] and matches the known lower bound up to logarithmic factors. To better capture real-life applications, we then extend our results to a natural but challenging contextual linear setting, where each box's expected reward is linear in some known but time-varying $d$-dimensional context and the noise distribution is fixed over time. We design an algorithm that learns both the linear function and the noise distributions, achieving $\widetilde{O}(nd\sqrt{T})$ regret. Finally, we show that our techniques also apply to the online Prophet Inequality problem, where the learner must decide immediately whether or not to accept a revealed reward. In both non-contextual and contextual settings, our approach achieves similar improvements and regret bounds.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 3.9 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- LLMs: 2.2 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Medicine: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18836" target="_blank" rel="noopener noreferrer">Distributed Incremental SAT Solving with Mallob: Report and Case Study with Hierarchical Planning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dominik Schreiber
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This report describes an extension of the distributed job scheduling and SAT solving platform Mallob by incremental SAT solving, embedded in a case study on SAT-based hierarchical planning. We introduce a low-latency interface for incremental jobs and specifically for IPASIR-style incremental SAT so</span>
                
                <span class="abstract-full" style="display: none;">This report describes an extension of the distributed job scheduling and SAT solving platform Mallob by incremental SAT solving, embedded in a case study on SAT-based hierarchical planning. We introduce a low-latency interface for incremental jobs and specifically for IPASIR-style incremental SAT solving to Mallob. This also allows to process many independent planning instances in parallel via Mallob's scheduling capabilities. In an experiment where 587 planning inputs are resolved in parallel on 2348 cores, we observe significant speedups for several planning domains where SAT solving constitutes a major part of the planner's running time. These findings indicate that our approach to distributed incremental SAT solving may be useful for a wide range of SAT applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.7 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Game Theory: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18838" target="_blank" rel="noopener noreferrer">Unconditionally Stable Mixed Finite Element Methods for Darcy Flow</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Maicon R. Correa, Abimael F. D. Loula
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Unconditionally stable finite element methods for Darcy flow are derived by adding least-squares residual forms of the governing equations to the classical mixed formulations. The proposed methods are free of mesh dependent stabilization parameters and allow the use of the classical continuous Lagra</span>
                
                <span class="abstract-full" style="display: none;">Unconditionally stable finite element methods for Darcy flow are derived by adding least-squares residual forms of the governing equations to the classical mixed formulations. The proposed methods are free of mesh dependent stabilization parameters and allow the use of the classical continuous Lagrangian finite element spaces of any order for the velocity and the potential. Stability, convergence and error estimates are derived and numerical experiments are presented to demonstrate the flexibility of the proposed finite element formulations and to confirm the predicted rates of convergence.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.3 -->
                    
                <!-- Hardware: 3.0 -->
                    
                <!-- Bayesian Optimization: 2.7 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- LLMs: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18839" target="_blank" rel="noopener noreferrer">DNF Learning via Locally Mixing Random Walks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Josh Alman, Shivam Nadimpalli, Shyamal Patel, Rocco A. Servedio
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We give two results on PAC learning DNF formulas using membership queries in the challenging "distribution-free" learning framework, where learning algorithms must succeed for an arbitrary and unknown distribution over $\{0,1\}^n$.</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.1 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- HPO and AutoML: 2.3 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- Decision Trees: 2.0 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                <!-- Cryptography: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Datasets: 1.0 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18850" target="_blank" rel="noopener noreferrer">The Theory of the Unique Latent Pattern: A Formal Epistemic Framework for Structural Singularity in Complex Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mohamed Aly Bouke
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces the Theory of the Unique Latent Pattern (ULP), a formal epistemic framework that redefines the origin of apparent complexity in dynamic systems. Rather than attributing unpredictability to intrinsic randomness or emergent nonlinearity, ULP asserts that every analyzable system i</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces the Theory of the Unique Latent Pattern (ULP), a formal epistemic framework that redefines the origin of apparent complexity in dynamic systems. Rather than attributing unpredictability to intrinsic randomness or emergent nonlinearity, ULP asserts that every analyzable system is governed by a structurally unique, deterministic generative mechanism, one that remains hidden not due to ontological indeterminacy, but due to epistemic constraints. The theory is formalized using a non-universal generative mapping \( \mathcal{F}_S(P_S, t) \), where each system \( S \) possesses its own latent structure \( P_S \), irreducible and non-replicable across systems. Observed irregularities are modeled as projections of this generative map through observer-limited interfaces, introducing epistemic noise \( \varepsilon_S(t) \) as a measure of incomplete access. By shifting the locus of uncertainty from the system to the observer, ULP reframes chaos as a context-relative failure of representation. We contrast this position with foundational paradigms in chaos theory, complexity science, and statistical learning. While they assume or model shared randomness or collective emergence, ULP maintains that every instance harbors a singular structural identity. Although conceptual, the theory satisfies the criterion of falsifiability in the Popperian sense, it invites empirical challenge by asserting that no two systems governed by distinct latent mechanisms will remain indistinguishable under sufficient resolution. This opens avenues for structurally individuated models in AI, behavioral inference, and epistemic diagnostics.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.2 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Game Theory: 1.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Cryptography: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Finance: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Datasets: 1.0 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18854" target="_blank" rel="noopener noreferrer">Three-Dimensional Nonlinear Guidance with Impact Time and Field-of-view Constraints</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ashok R Samrat, Swati Singh, Shashi Ranjan Kumar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper addresses the time-constrained interception of targets at a predetermined time with bounded field-of-view capability of the seeker-equipped interceptors. We propose guidance laws using the effective lead angle and velocity lead angles of the interceptor to achieve a successful interceptio</span>
                
                <span class="abstract-full" style="display: none;">This paper addresses the time-constrained interception of targets at a predetermined time with bounded field-of-view capability of the seeker-equipped interceptors. We propose guidance laws using the effective lead angle and velocity lead angles of the interceptor to achieve a successful interception of the target. The former scheme extends the existing two-dimensional guidance strategy to a three-dimensional setting. We have shown that such an extension may result in high-frequency switching in the input demand, which may degrade the interceptor's performance. To overcome the potential limitations of such a guidance strategy, we propose an elegant solution using the velocity lead angles and the range error with a backstepping technique. Using the velocity lead angles as virtual inputs, the effective lead angle profile is subsequently regulated to satisfy the seeker's field-of-view bound. Unlike the existing strategies, the proposed guidance strategy does not rely on the time-to-go estimate, which is an appealing feature of the design, as the time-to-go estimate may not always be available with high precision. We provide a theoretical analysis of the error variable and subsequently analytically derive the bounds on achievable impact times. Numerical simulations are performed to support the theoretical findings. The performance of the proposed guidance strategy is compared with that of an existing one, and it has been shown to yield better performance. Finally, a study on different choices of virtual inputs is also provided.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Math: 3.8 -->
                    
                <!-- Networks: 3.7 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- Cryptography: 2.8 -->
                    
                <!-- Finance: 2.0 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Game Theory: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Medicine: 1.1 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18857" target="_blank" rel="noopener noreferrer">Hierarchical-embedding autoencoder with a predictor (HEAP) as efficient architecture for learning long-term evolution of complex multi-scale physical systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alexander Khrabry, Edward Startsev, Andrew Powis, Igor Kaganovich
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a novel efficient architecture for learning long-term evolution in complex multi-scale physical systems which is based on the idea of separation of scales. Structures of various scales that dynamically emerge in the system interact with each other only locally. Structures of similar scale</span>
                
                <span class="abstract-full" style="display: none;">We propose a novel efficient architecture for learning long-term evolution in complex multi-scale physical systems which is based on the idea of separation of scales. Structures of various scales that dynamically emerge in the system interact with each other only locally. Structures of similar scale can interact directly when they are in contact and indirectly when they are parts of larger structures that interact directly. This enables modeling a multi-scale system in an efficient way, where interactions between small-scale features that are apart from each other do not need to be modeled. The hierarchical fully-convolutional autoencoder transforms the state of a physical system not just into a single embedding layer, as it is done conventionally, but into a series of embedding layers which encode structures of various scales preserving spatial information at a corresponding resolution level. Shallower layers embed smaller structures on a finer grid, while deeper layers embed larger structures on a coarser grid. The predictor advances all embedding layers in sync. Interactions between features of various scales are modeled using a combination of convolutional operators. We compare the performance of our model to variations of a conventional ResNet architecture in application to the Hasegawa-Wakatani turbulence. A multifold improvement in long-term prediction accuracy was observed for crucial statistical characteristics of this system.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.3 -->
                    
                <!-- Federated Learning: 3.2 -->
                    
                <!-- Evolutionary Algorithms: 2.9 -->
                    
                <!-- LLMs: 2.3 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Game Theory: 1.7 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18861" target="_blank" rel="noopener noreferrer">Securing Credit Inquiries: The Role of Real-Time User Approval in Preventing SSN Identity Theft</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gogulakrishnan Thiyagarajan, Vinay Bist, Prabhudarshi Nayak
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Unauthorized credit inquiries are also a central entry point for identity theft, with Social Security Numbers (SSNs) being widely utilized in fraudulent cases. Traditional credit inquiry systems do not usually possess strict user authentication, making them vulnerable to unauthorized access. This pa</span>
                
                <span class="abstract-full" style="display: none;">Unauthorized credit inquiries are also a central entry point for identity theft, with Social Security Numbers (SSNs) being widely utilized in fraudulent cases. Traditional credit inquiry systems do not usually possess strict user authentication, making them vulnerable to unauthorized access. This paper proposes a real-time user authorization system to enhance security by enforcing explicit user approval before processing any credit inquiry. The system employs real-time verification and approval techniques. This ensures that the authorized user only approves or rejects a credit check request. It minimizes the risks of interference by third parties. Apart from enhancing security, this system complies with regulations like the General Data Protection Regulation (GDPR) and the Fair Credit Reporting Act (FCRA) while maintaining a seamless user experience. This article discusses the technical issues, scaling-up issues, and ways of implementing real-time user authorization in financial systems. Through this framework, financial institutions can drastically minimize the risk of identity theft, avert unauthorized credit checks, and increase customer trust in the credit verification system.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.5 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Cryptography: 1.9 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Game Theory: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Finance: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18872" target="_blank" rel="noopener noreferrer">Zero Trust Cybersecurity: Procedures and Considerations in Context</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Brady D. Lund, Tae Hee Lee, Ziang Wang, Ting Wang, Nishith Reddy Mannuru
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In response to the increasing complexity and sophistication of cyber threats, particularly those enhanced by advancements in artificial intelligence, traditional security methods are proving insufficient. This paper explores the Zero Trust cybersecurity framework, which operates on the principle of </span>
                
                <span class="abstract-full" style="display: none;">In response to the increasing complexity and sophistication of cyber threats, particularly those enhanced by advancements in artificial intelligence, traditional security methods are proving insufficient. This paper explores the Zero Trust cybersecurity framework, which operates on the principle of never trust, always verify to mitigate vulnerabilities within organizations. Specifically, it examines the applicability of Zero Trust principles in environments where large volumes of information are exchanged, such as schools and libraries. The discussion highlights the importance of continuous authentication, least privilege access, and breach assumption. The findings underscore avenues for future research that may help preserve the security of these vulnerable organizations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.8 -->
                    
                <!-- Blockchain: 4.0 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18876" target="_blank" rel="noopener noreferrer">DiffusionRL: Efficient Training of Diffusion Policies for Robotic Grasping Using RL-Adapted Large-Scale Datasets</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Maria Makarova, Qian Liu, Dzmitry Tsetserukou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Diffusion models have been successfully applied in areas such as image, video, and audio generation. Recent works show their promise for sequential decision-making and dexterous manipulation, leveraging their ability to model complex action distributions. However, challenges persist due to the data </span>
                
                <span class="abstract-full" style="display: none;">Diffusion models have been successfully applied in areas such as image, video, and audio generation. Recent works show their promise for sequential decision-making and dexterous manipulation, leveraging their ability to model complex action distributions. However, challenges persist due to the data limitations and scenario-specific adaptation needs. In this paper, we address these challenges by proposing an optimized approach to training diffusion policies using large, pre-built datasets that are enhanced using Reinforcement Learning (RL). Our end-to-end pipeline leverages RL-based enhancement of the DexGraspNet dataset, lightweight diffusion policy training on a dexterous manipulation task for a five-fingered robotic hand, and a pose sampling algorithm for validation. The pipeline achieved a high success rate of 80% for three DexGraspNet objects. By eliminating manual data collection, our approach lowers barriers to adopting diffusion models in robotics, enhancing generalization and robustness for real-world applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.7 -->
                    
                <!-- Medicine: 4.5 -->
                    
                <!-- Hardware: 2.4 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18879" target="_blank" rel="noopener noreferrer">Efficient Online Random Sampling via Randomness Recycling</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Thomas L. Draper, Feras A. Saad
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">``Randomness recycling'' is a powerful algorithmic technique for reusing a fraction of the random information consumed by a randomized algorithm to reduce its entropy requirements. This article presents a family of efficient randomness recycling algorithms for sampling a sequence $X_1, X_2, X_3, \do</span>
                
                <span class="abstract-full" style="display: none;">``Randomness recycling'' is a powerful algorithmic technique for reusing a fraction of the random information consumed by a randomized algorithm to reduce its entropy requirements. This article presents a family of efficient randomness recycling algorithms for sampling a sequence $X_1, X_2, X_3, \dots$ of discrete random variables whose joint distribution follows an arbitrary stochastic process. We develop randomness recycling strategies to reduce the entropy cost of a variety of prominent sampling algorithms, which include uniform sampling, inverse transform sampling, lookup table sampling, alias sampling, and discrete distribution generating (DDG) tree sampling. Our method achieves an expected amortized entropy cost of $H(X_1,\dots,X_k)/k + \varepsilon$ input random bits per output sample using $O(\log(1/\varepsilon))$ space, which is arbitrarily close to the optimal Shannon entropy rate. The combination of space, time, and entropy properties of our method improve upon the Han and Hoshi interval algorithm and Knuth and Yao entropy-optimal algorithm for sampling a discrete random sequence. An empirical evaluation of the algorithm shows that it achieves state-of-the-art runtime performance on the Fisher-Yates shuffle when using a cryptographically secure pseudorandom number generator. Accompanying the manuscript is a performant random sampling library in the C programming language.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Evolutionary Algorithms: 3.2 -->
                    
                <!-- Federated Learning: 3.2 -->
                    
                <!-- Medicine: 2.6 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Finance: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Cryptography: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18884" target="_blank" rel="noopener noreferrer">LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Borna Khodabandeh, Amirabbas Afzali, Amirhossein Afsharrad, Seyed Shahabeddin Mousavi, Sanjay Lall, Sajjad Amini, Seyed-Mohsen Moosavi-Dezfooli
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Visual encoders have become fundamental components in modern computer vision pipelines. However, ensuring robustness against adversarial perturbations remains a critical challenge. Recent efforts have explored both supervised and unsupervised adversarial fine-tuning strategies. We identify two key l</span>
                
                <span class="abstract-full" style="display: none;">Visual encoders have become fundamental components in modern computer vision pipelines. However, ensuring robustness against adversarial perturbations remains a critical challenge. Recent efforts have explored both supervised and unsupervised adversarial fine-tuning strategies. We identify two key limitations in these approaches: (i) they often suffer from instability, especially during the early stages of fine-tuning, resulting in suboptimal convergence and degraded performance on clean data, and (ii) they exhibit a suboptimal trade-off between robustness and clean data accuracy, hindering the simultaneous optimization of both objectives. To overcome these challenges, we propose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised adversarial fine-tuning framework. LORE utilizes constrained optimization, which offers a principled approach to balancing competing goals, such as improving robustness while preserving nominal performance. By enforcing embedding-space proximity constraints, LORE effectively maintains clean data performance throughout adversarial fine-tuning. Extensive experiments show that LORE significantly improves zero-shot adversarial robustness with minimal degradation in clean data accuracy. Furthermore, we demonstrate the effectiveness of the adversarially fine-tuned CLIP image encoder in out-of-distribution generalization and enhancing the interpretability of image embeddings.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.8 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- Blockchain: 2.5 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18885" target="_blank" rel="noopener noreferrer">The Parameterized Complexity of Computing the Linear Vertex Arboricity</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alexander Erhardt, Alexander Wolff
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The \emph{linear vertex arboricity} of a graph is the smallest number of sets into which the vertices of a graph can be partitioned so that each of these sets induces a linear forest. Chaplick et al. [JoCG 2020] showed that, somewhat surprisingly, the linear vertex arboricity of a graph is the same </span>
                
                <span class="abstract-full" style="display: none;">The \emph{linear vertex arboricity} of a graph is the smallest number of sets into which the vertices of a graph can be partitioned so that each of these sets induces a linear forest. Chaplick et al. [JoCG 2020] showed that, somewhat surprisingly, the linear vertex arboricity of a graph is the same as the \emph{3D weak line cover number} of the graph, that is, the minimum number of straight lines necessary to cover the vertices of a crossing-free straight-line drawing of the graph in $\mathbb{R}^3$. Chaplick et al. [JGAA 2023] showed that deciding whether a given graph has linear vertex arboricity 2 is NP-hard.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Math: 4.3 -->
                    
                <!-- Networks: 3.9 -->
                    
                <!-- Cryptography: 3.0 -->
                    
                <!-- Bayesian Optimization: 2.8 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Game Theory: 2.0 -->
                    
                <!-- Pathfinding: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Finance: 1.5 -->
                    
                <!-- Medicine: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- LLMs: 1.3 -->
                    
                <!-- Hardware: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18892" target="_blank" rel="noopener noreferrer">Climate Implications of Diffusion-based Generative Visual AI Systems and their Mass Adoption</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vanessa Utz, Steve DiPaola
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Climate implications of rapidly developing digital technologies, such as blockchains and the associated crypto mining and NFT minting, have been well documented and their massive GPU energy use has been identified as a cause for concern. However, we postulate that due to their more mainstream consum</span>
                
                <span class="abstract-full" style="display: none;">Climate implications of rapidly developing digital technologies, such as blockchains and the associated crypto mining and NFT minting, have been well documented and their massive GPU energy use has been identified as a cause for concern. However, we postulate that due to their more mainstream consumer appeal, the GPU use of text-prompt based diffusion AI art systems also requires thoughtful considerations. Given the recent explosion in the number of highly sophisticated generative art systems and their rapid adoption by consumers and creative professionals, the impact of these systems on the climate needs to be carefully considered. In this work, we report on the growth of diffusion-based visual AI systems, their patterns of use, growth and the implications on the climate. Our estimates show that the mass adoption of these tools potentially contributes considerably to global energy consumption. We end this paper with our thoughts on solutions and future areas of inquiry as well as associated difficulties, including the lack of publicly available data.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 4.5 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Blockchain: 3.3 -->
                    
                <!-- Hardware: 2.8 -->
                    
                <!-- Evolutionary Algorithms: 2.7 -->
                    
                <!-- Medicine: 2.3 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18894" target="_blank" rel="noopener noreferrer">Digital Overconsumption and Waste: A Closer Look at the Impacts of Generative AI</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vanessa Utz, Steve DiPaola
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generative Artificial Intelligence (AI) systems currently contribute negatively to the production of digital waste, via the associated energy consumption and the related CO2 emissions. At this moment, a discussion is urgently needed on the replication of harmful consumer behavior, such as overconsum</span>
                
                <span class="abstract-full" style="display: none;">Generative Artificial Intelligence (AI) systems currently contribute negatively to the production of digital waste, via the associated energy consumption and the related CO2 emissions. At this moment, a discussion is urgently needed on the replication of harmful consumer behavior, such as overconsumption, in the digital space. We outline our previous work on the climate implications of commercially available generative AI systems and the sentiment of generative AI users when confronted with AI-related climate research. We expand on this work via a discussion of digital overconsumption and waste, other related societal impacts, and a possible solution pathway</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.9 -->
                    
                <!-- Blockchain: 3.3 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- Federated Learning: 3.1 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Hardware: 2.3 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18903" target="_blank" rel="noopener noreferrer">StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Valentin Barriere, Nahuel Gomez, Leo Hemamou, Sofia Callejas, Brian Ravenet
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Aiming towards improving current computational models of humor detection, we propose a new multimodal dataset of stand-up comedies, in seven languages: English, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset of more than 330 hours, is at the time of writing the biggest availa</span>
                
                <span class="abstract-full" style="display: none;">Aiming towards improving current computational models of humor detection, we propose a new multimodal dataset of stand-up comedies, in seven languages: English, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset of more than 330 hours, is at the time of writing the biggest available for this type of task, and the most diverse. The whole dataset is automatically annotated in laughter (from the audience), and the subpart left for model validation is manually annotated. Contrary to contemporary approaches, we do not frame the task of humor detection as a binary sequence classification, but as word-level sequence labeling, in order to take into account all the context of the sequence and to capture the continuous joke tagging mechanism typically occurring in natural conversations. As par with unimodal baselines results, we propose a method for e propose a method to enhance the automatic laughter detection based on Audio Speech Recognition errors. Our code and data are available online: https://tinyurl.com/EMNLPHumourStandUpPublic</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.0 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Datasets: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18912" target="_blank" rel="noopener noreferrer">Robust Stability Analysis of Positive Lure System with Neural Network Feedback</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hamidreza Montazeri Hedesh, Moh. Kamalul Wafi, Bahram Shafai, Milad Siami
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper investigates the robustness of the Lur'e problem under positivity constraints, drawing on results from the positive Aizerman conjecture and the robustness properties of Metzler matrices. Specifically, we consider a control system of Lur'e type in which not only the linear part includes pa</span>
                
                <span class="abstract-full" style="display: none;">This paper investigates the robustness of the Lur'e problem under positivity constraints, drawing on results from the positive Aizerman conjecture and the robustness properties of Metzler matrices. Specifically, we consider a control system of Lur'e type in which not only the linear part includes parametric uncertainty but also the nonlinear sector bound is unknown. We investigate tools from positive linear systems to effectively solve the problems in complicated and uncertain nonlinear systems. By leveraging the positivity characteristic of the system, we derive an explicit formula for the stability radius of Lur'e systems. Furthermore, we extend our analysis to systems with neural network (NN) feedback loops. Building on this approach, we also propose a refinement method for sector bounds of feedforward neural networks (FFNNs). This study introduces a scalable and efficient approach for robustness analysis of both Lur'e and NN-controlled systems. Finally, the proposed results are supported by illustrative examples.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.9 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- LLMs: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Math: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18925" target="_blank" rel="noopener noreferrer">Words as Geometric Features: Estimating Homography using Optical Character Recognition as Compressed Image Representation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ross Greer, Alisha Ukani, Katherine Izhikevich, Earlence Fernandes, Stefan Savage, Alex C. Snoeren
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Document alignment and registration play a crucial role in numerous real-world applications, such as automated form processing, anomaly detection, and workflow automation. Traditional methods for document alignment rely on image-based features like keypoints, edges, and textures to estimate geometri</span>
                
                <span class="abstract-full" style="display: none;">Document alignment and registration play a crucial role in numerous real-world applications, such as automated form processing, anomaly detection, and workflow automation. Traditional methods for document alignment rely on image-based features like keypoints, edges, and textures to estimate geometric transformations, such as homographies. However, these approaches often require access to the original document images, which may not always be available due to privacy, storage, or transmission constraints. This paper introduces a novel approach that leverages Optical Character Recognition (OCR) outputs as features for homography estimation. By utilizing the spatial positions and textual content of OCR-detected words, our method enables document alignment without relying on pixel-level image data. This technique is particularly valuable in scenarios where only OCR outputs are accessible. Furthermore, the method is robust to OCR noise, incorporating RANSAC to handle outliers and inaccuracies in the OCR data. On a set of test documents, we demonstrate that our OCR-based approach even performs more accurately than traditional image-based methods, offering a more efficient and scalable solution for document registration tasks. The proposed method facilitates applications in document processing, all while reducing reliance on high-dimensional image data.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.3 -->
                    
                <!-- Computer Vision: 3.2 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18945" target="_blank" rel="noopener noreferrer">Echo Planning for Autonomous Driving: From Current Observations to Future Trajectories and Back</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jintao Sun, Hu Zhang, Gangyi Ding, Zhedong Zheng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modern end-to-end autonomous driving systems suffer from a critical limitation: their planners lack mechanisms to enforce temporal consistency between predicted trajectories and evolving scene dynamics. This absence of self-supervision allows early prediction errors to compound catastrophically over</span>
                
                <span class="abstract-full" style="display: none;">Modern end-to-end autonomous driving systems suffer from a critical limitation: their planners lack mechanisms to enforce temporal consistency between predicted trajectories and evolving scene dynamics. This absence of self-supervision allows early prediction errors to compound catastrophically over time. We introduce Echo Planning, a novel self-correcting framework that establishes a closed-loop Current - Future - Current (CFC) cycle to harmonize trajectory prediction with scene coherence. Our key insight is that plausible future trajectories must be bi-directionally consistent, ie, not only generated from current observations but also capable of reconstructing them. The CFC mechanism first predicts future trajectories from the Bird's-Eye-View (BEV) scene representation, then inversely maps these trajectories back to estimate the current BEV state. By enforcing consistency between the original and reconstructed BEV representations through a cycle loss, the framework intrinsically penalizes physically implausible or misaligned trajectories. Experiments on nuScenes demonstrate state-of-the-art performance, reducing L2 error by 0.04 m and collision rate by 0.12% compared to one-shot planners. Crucially, our method requires no additional supervision, leveraging the CFC cycle as an inductive bias for robust planning. This work offers a deployable solution for safety-critical autonomous systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.9 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18946" target="_blank" rel="noopener noreferrer">SANNet: A Semantic-Aware Agentic AI Networking Framework for Multi-Agent Cross-Layer Coordination</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yong Xiao, Haoran Zhou, Xubo Li, Yayu Gao, Guangming Shi, Ping Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Agentic AI networking (AgentNet) is a novel AI-native networking paradigm that relies on a large number of specialized AI agents to collaborate and coordinate for autonomous decision-making, dynamic environmental adaptation, and complex goal achievement. It has the potential to facilitate real-time </span>
                
                <span class="abstract-full" style="display: none;">Agentic AI networking (AgentNet) is a novel AI-native networking paradigm that relies on a large number of specialized AI agents to collaborate and coordinate for autonomous decision-making, dynamic environmental adaptation, and complex goal achievement. It has the potential to facilitate real-time network management alongside capabilities for self-configuration, self-optimization, and self-adaptation across diverse and complex networking environments, laying the foundation for fully autonomous networking systems in the future. Despite its promise, AgentNet is still in the early stage of development, and there still lacks an effective networking framework to support automatic goal discovery and multi-agent self-orchestration and task assignment. This paper proposes SANNet, a novel semantic-aware agentic AI networking architecture that can infer the semantic goal of the user and automatically assign agents associated with different layers of a mobile system to fulfill the inferred goal. Motivated by the fact that one of the major challenges in AgentNet is that different agents may have different and even conflicting objectives when collaborating for certain goals, we introduce a dynamic weighting-based conflict-resolving mechanism to address this issue. We prove that SANNet can provide theoretical guarantee in both conflict-resolving and model generalization performance for multi-agent collaboration in dynamic environment. We develop a hardware prototype of SANNet based on the open RAN and 5GS core platform. Our experimental results show that SANNet can significantly improve the performance of multi-agent networking systems, even when agents with conflicting objectives are selected to collaborate for the same goal.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.9 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18952" target="_blank" rel="noopener noreferrer">Online Knowledge Distillation with Reward Guidance</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chen Jia
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This work studies knowledge distillation (KD) for large language models (LLMs) through preference optimization. We propose a reward-guided imitation learning framework for sequential KD, formulating a min-max optimization problem between the policy and reward model (RM) to minimize the performance g</span>
                
                <span class="abstract-full" style="display: none;">This work studies knowledge distillation (KD) for large language models (LLMs) through preference optimization. We propose a reward-guided imitation learning framework for sequential KD, formulating a min-max optimization problem between the policy and reward model (RM) to minimize the performance gap between the student and teacher policies. Specifically, the reward optimization is constrained to achieve near-optimality within a confidence set for preference alignment. For preference data construction, we explore both offline and online preference-based KD. Additionally, we reformulate the RM using the $Q$-value function and extend the framework to white-box KD, where the teacher policy's predicted probabilities are accessible. Theoretical analysis and empirical results demonstrate the effectiveness of the proposed framework.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.2 -->
                    
                <!-- Computer Vision: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- Bayesian Optimization: 2.5 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18975" target="_blank" rel="noopener noreferrer">FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aotao Wang, Haikuo Shao, Shaobo Ma, Zhongfeng Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">State Space Models (SSMs), like recent Mamba2, have achieved remarkable performance and received extensive attention. However, deploying Mamba2 on resource-constrained edge devices encounters many problems: severe outliers within the linear layer challenging the quantization, diverse and irregular e</span>
                
                <span class="abstract-full" style="display: none;">State Space Models (SSMs), like recent Mamba2, have achieved remarkable performance and received extensive attention. However, deploying Mamba2 on resource-constrained edge devices encounters many problems: severe outliers within the linear layer challenging the quantization, diverse and irregular element-wise tensor operations, and hardware-unfriendly nonlinear functions in the SSM block. To address these issues, this paper presents FastMamba, a dedicated accelerator on FPGA with hardware-algorithm co-design to promote the deployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit quantization for linear layers through Hadamard transformation to eliminate outliers. Moreover, a hardware-friendly and fine-grained power-of-two quantization framework is presented for the SSM block and convolution layer, and a first-order linear approximation is developed to optimize the nonlinear functions. Based on the accurate algorithm quantization, we propose an accelerator that integrates parallel vector processing units, pipelined execution dataflow, and an efficient SSM Nonlinear Approximation Unit, which enhances computational efficiency and reduces hardware complexity. Finally, we evaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on Mamba2-130M, FastMamba achieves 68.80\times and 8.90\times speedup over Intel Xeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode experiment with Mamba2-2.7B, FastMamba attains 6\times higher energy efficiency than RTX 3090 GPU.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- Computer Vision: 3.5 -->
                    
                <!-- Hardware: 3.3 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18980" target="_blank" rel="noopener noreferrer">Improving Anomalous Sound Detection through Pseudo-anomalous Set Selection and Pseudo-label Utilization under Unlabeled Conditions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ibuki Kuroyanagi, Takuya Fujimura, Kazuya Takeda, Tomoki Toda
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper addresses performance degradation in anomalous sound detection (ASD) when neither sufficiently similar machine data nor operational state labels are available. We present an integrated pipeline that combines three complementary components derived from prior work and extends them to the un</span>
                
                <span class="abstract-full" style="display: none;">This paper addresses performance degradation in anomalous sound detection (ASD) when neither sufficiently similar machine data nor operational state labels are available. We present an integrated pipeline that combines three complementary components derived from prior work and extends them to the unlabeled ASD setting. First, we adapt an anomaly score based selector to curate external audio data resembling the normal sounds of the target machine. Second, we utilize triplet learning to assign pseudo-labels to unlabeled data, enabling finer classification of operational sounds and detection of subtle anomalies. Third, we employ iterative training to refine both the pseudo-anomalous set selection and pseudo-label assignment, progressively improving detection accuracy. Experiments on the DCASE2022-2024 Task 2 datasets demonstrate that, in unlabeled settings, our approach achieves an average AUC increase of over 6.6 points compared to conventional methods. In labeled settings, incorporating external data from the pseudo-anomalous set further boosts performance. These results highlight the practicality and robustness of our methods in scenarios with scarce machine data and labels, facilitating ASD deployment across diverse industrial settings with minimal annotation effort.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.0 -->
                    
                <!-- Computer Vision: 3.7 -->
                    
                <!-- LLMs: 3.6 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Decision Trees: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18984" target="_blank" rel="noopener noreferrer">Self-supervised learning method using multiple sampling strategies for general-purpose audio representation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ibuki Kuroyanagi, Tatsuya Komatsu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a self-supervised learning method using multiple sampling strategies to obtain general-purpose audio representation. Multiple sampling strategies are used in the proposed method to construct contrastive losses from different perspectives and learn representations based on them. In this st</span>
                
                <span class="abstract-full" style="display: none;">We propose a self-supervised learning method using multiple sampling strategies to obtain general-purpose audio representation. Multiple sampling strategies are used in the proposed method to construct contrastive losses from different perspectives and learn representations based on them. In this study, in addition to the widely used clip-level sampling strategy, we introduce two new strategies, a frame-level strategy and a task-specific strategy. The proposed multiple strategies improve the performance of frame-level classification and other tasks like pitch detection, which are not the focus of the conventional single clip-level sampling strategy. We pre-trained the method on a subset of Audioset and applied it to a downstream task with frozen weights. The proposed method improved clip classification, sound event detection, and pitch detection performance by 25%, 20%, and 3.6%.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 3.5 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Evolutionary Algorithms: 3.0 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18987" target="_blank" rel="noopener noreferrer">Error estimates for the interpolation and approximation of gradients and vector fields on protected Delaunay meshes in $\mathbb{R}^d$</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: David M. Williams, Mathijs Wintraecken
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">One frequently needs to interpolate or approximate gradients on simplicial meshes. Unfortunately, there are very few explicit mathematical results governing the interpolation or approximation of vector-valued functions on Delaunay meshes in more than two dimensions. Most of the existing results are </span>
                
                <span class="abstract-full" style="display: none;">One frequently needs to interpolate or approximate gradients on simplicial meshes. Unfortunately, there are very few explicit mathematical results governing the interpolation or approximation of vector-valued functions on Delaunay meshes in more than two dimensions. Most of the existing results are tailored towards interpolation with piecewise linear polynomials. In contrast, interpolation with piecewise high-order polynomials is not well understood. In particular, the results in this area are sometimes difficult to immediately interpret, or to specialize to the Delaunay setting. In order to address this issue, we derive explicit error estimates for high-order, piecewise polynomial gradient interpolation and approximation on protected Delaunay meshes. In addition, we generalize our analysis beyond gradients, and obtain error estimates for sufficiently-smooth vector fields. Throughout the paper, we show that the quality of interpolation and approximation often depends (in part) on the minimum thickness of simplices in the mesh. Fortunately, the minimum thickness can be precisely controlled on protected Delaunay meshes in $\mathbb{R}^d$.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 3.0 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- LLMs: 2.2 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Game Theory: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18988" target="_blank" rel="noopener noreferrer">NTIRE 2025 Challenge on Video Quality Enhancement for Video Conferencing: Datasets, Methods and Results</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Varun Jain, Zongwei Wu, Quan Zou, Louis Florentin, Henrik Turbell, Sandeep Siddhartha, Radu Timofte, others
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents a comprehensive review of the 1st Challenge on Video Quality Enhancement for Video Conferencing held at the NTIRE workshop at CVPR 2025, and highlights the problem statement, datasets, proposed solutions, and results. The aim of this challenge was to design a Video Quality Enhanc</span>
                
                <span class="abstract-full" style="display: none;">This paper presents a comprehensive review of the 1st Challenge on Video Quality Enhancement for Video Conferencing held at the NTIRE workshop at CVPR 2025, and highlights the problem statement, datasets, proposed solutions, and results. The aim of this challenge was to design a Video Quality Enhancement (VQE) model to enhance video quality in video conferencing scenarios by (a) improving lighting, (b) enhancing colors, (c) reducing noise, and (d) enhancing sharpness - giving a professional studio-like effect. Participants were given a differentiable Video Quality Assessment (VQA) model, training, and test videos. A total of 91 participants registered for the challenge. We received 10 valid submissions that were evaluated in a crowdsourced framework.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.5 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Hardware: 2.3 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18994" target="_blank" rel="noopener noreferrer">Designing Pin-pression Gripper and Learning its Dexterous Grasping with Online In-hand Adjustment</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hewen Xiao, Xiuping Liu, Hang Zhao, Jian Liu, Kai Xu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce a novel design of parallel-jaw grippers drawing inspiration from pin-pression toys. The proposed pin-pression gripper features a distinctive mechanism in which each finger integrates a 2D array of pins capable of independent extension and retraction. This unique design allows the grippe</span>
                
                <span class="abstract-full" style="display: none;">We introduce a novel design of parallel-jaw grippers drawing inspiration from pin-pression toys. The proposed pin-pression gripper features a distinctive mechanism in which each finger integrates a 2D array of pins capable of independent extension and retraction. This unique design allows the gripper to instantaneously customize its finger's shape to conform to the object being grasped by dynamically adjusting the extension/retraction of the pins. In addition, the gripper excels in in-hand re-orientation of objects for enhanced grasping stability again via dynamically adjusting the pins. To learn the dynamic grasping skills of pin-pression grippers, we devise a dedicated reinforcement learning algorithm with careful designs of state representation and reward shaping. To achieve a more efficient grasp-while-lift grasping mode, we propose a curriculum learning scheme. Extensive evaluations demonstrate that our design, together with the learned skills, leads to highly flexible and robust grasping with much stronger generality to unseen objects than alternatives. We also highlight encouraging physical results of sim-to-real transfer on a physically manufactured pin-pression gripper, demonstrating the practical significance of our novel gripper design and grasping skill. Demonstration videos for this paper are available at https://github.com/siggraph-pin-pression-gripper/pin-pression-gripper-video.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 3.8 -->
                    
                <!-- Medicine: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- LLMs: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18998" target="_blank" rel="noopener noreferrer">Property Directed Reachability with Extended Resolution</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andrew Luka, Yakir Vizel
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Property Directed Reachability (\textsc{Pdr}), also known as IC3, is a state-of-the-art model checking algorithm widely used for verifying safety properties. While \textsc{Pdr} is effective in finding inductive invariants, its underlying proof system, Resolution, limits its ability to construct shor</span>
                
                <span class="abstract-full" style="display: none;">Property Directed Reachability (\textsc{Pdr}), also known as IC3, is a state-of-the-art model checking algorithm widely used for verifying safety properties. While \textsc{Pdr} is effective in finding inductive invariants, its underlying proof system, Resolution, limits its ability to construct short proofs for certain verification problems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.8 -->
                    
                <!-- HPO and AutoML: 3.4 -->
                    
                <!-- Medicine: 2.9 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.18999" target="_blank" rel="noopener noreferrer">Lightweight Embeddings with Graph Rewiring for Collaborative Filtering</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xurong Liang, Tong Chen, Wei Yuan, Hongzhi Yin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As recommendation services scale rapidly and their deployment now commonly involves resource-constrained edge devices, GNN-based recommender systems face significant challenges, including high embedding storage costs and runtime latency from graph propagations. Our previous work, LEGCF, effectively </span>
                
                <span class="abstract-full" style="display: none;">As recommendation services scale rapidly and their deployment now commonly involves resource-constrained edge devices, GNN-based recommender systems face significant challenges, including high embedding storage costs and runtime latency from graph propagations. Our previous work, LEGCF, effectively reduced embedding storage costs but struggled to maintain recommendation performance under stricter storage limits. Additionally, LEGCF did not address the extensive runtime computation costs associated with graph propagation, which involves heavy multiplication and accumulation operations (MACs). These challenges consequently hinder effective training and inference on resource-constrained edge devices. To address these limitations, we propose Lightweight Embeddings with Rewired Graph for Graph Collaborative Filtering (LERG), an improved extension of LEGCF. LERG retains LEGCFs compositional codebook structure but introduces quantization techniques to reduce the storage cost, enabling the inclusion of more meta-embeddings within the same storage. To optimize graph propagation, we pretrain the quantized compositional embedding table using the full interaction graph on resource-rich servers, after which a fine-tuning stage is engaged to identify and prune low-contribution entities via a gradient-free binary integer programming approach, constructing a rewired graph that excludes these entities (i.e., user/item nodes) from propagating signals. The quantized compositional embedding table with selective embedding participation and sparse rewired graph are transferred to edge devices which significantly reduce computation memory and inference time. Experiments on three public benchmark datasets, including an industry-scale dataset, demonstrate that LERG achieves superior recommendation performance while dramatically reducing storage and computation costs for graph-based recommendation services.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.0 -->
                    
                <!-- GNN: 4.9 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Computer Vision: 2.6 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- HPO and AutoML: 2.0 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.19001" target="_blank" rel="noopener noreferrer">DARTH: Declarative Recall Through Early Termination for Approximate Nearest Neighbor Search</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Manos Chatzakis, Yannis Papakonstantinou, Themis Palpanas
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Approximate Nearest Neighbor Search (ANNS) presents an inherent tradeoff between performance and recall (i.e., result quality). Each ANNS algorithm provides its own algorithm-dependent parameters to allow applications to influence the recall/performance tradeoff of their searches. This situation is </span>
                
                <span class="abstract-full" style="display: none;">Approximate Nearest Neighbor Search (ANNS) presents an inherent tradeoff between performance and recall (i.e., result quality). Each ANNS algorithm provides its own algorithm-dependent parameters to allow applications to influence the recall/performance tradeoff of their searches. This situation is doubly problematic. First, the application developers have to experiment with these algorithm-dependent parameters to fine-tune the parameters that produce the desired recall for each use case. This process usually takes a lot of effort. Even worse, the chosen parameters may produce good recall for some queries, but bad recall for hard queries. To solve these problems, we present DARTH, a method that uses target declarative recall. DARTH uses a novel method for providing target declarative recall on top of an ANNS index by employing an adaptive early termination strategy integrated into the search algorithm. Through a wide range of experiments, we demonstrate that DARTH effectively meets user-defined recall targets while achieving significant speedups, up to 14.6x (average: 6.8x; median: 5.7x) faster than the search without early termination for HNSW and up to 41.8x (average: 13.6x; median: 8.1x) for IVF. This paper appeared in ACM SIGMOD 2026.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 3.2 -->
                    
                <!-- Medicine: 3.1 -->
                    
                <!-- Evolutionary Algorithms: 2.8 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.19002" target="_blank" rel="noopener noreferrer">Semi-pessimistic Reinforcement Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jin Zhu, Xin Zhou, Jiaang Yao, Gholamali Aminian, Omar Rivasplata, Simon Little, Lexin Li, Chengchun Shi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected data. However, it faces challenges of distributional shift, where the learned policy may encounter unseen scenarios not covered in the offline data. Additionally, numerous applications suffer from a scarcity of la</span>
                
                <span class="abstract-full" style="display: none;">Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected data. However, it faces challenges of distributional shift, where the learned policy may encounter unseen scenarios not covered in the offline data. Additionally, numerous applications suffer from a scarcity of labeled reward data. Relying on labeled data alone often leads to a narrow state-action distribution, further amplifying the distributional shift, and resulting in suboptimal policy learning. To address these issues, we first recognize that the volume of unlabeled data is typically substantially larger than that of labeled data. We then propose a semi-pessimistic RL method to effectively leverage abundant unlabeled data. Our approach offers several advantages. It considerably simplifies the learning process, as it seeks a lower bound of the reward function, rather than that of the Q-function or state transition function. It is highly flexible, and can be integrated with a range of model-free and model-based RL algorithms. It enjoys the guaranteed improvement when utilizing vast unlabeled data, but requires much less restrictive conditions. We compare our method with a number of alternative solutions, both analytically and numerically, and demonstrate its clear competitiveness. We further illustrate with an application to adaptive deep brain stimulation for Parkinson's disease.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.7 -->
                    
                <!-- Federated Learning: 3.2 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.19010" target="_blank" rel="noopener noreferrer">Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Md. Mithun Hossain, Md. Shakil Hossain, Sudipto Chaki, M. F. Mridha
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multi-modal learning has become a critical research area because integrating text and image data can significantly improve performance in tasks such as classification, retrieval, and scene understanding. However, despite progress with pre-trained models, current approaches are limited by inadequate </span>
                
                <span class="abstract-full" style="display: none;">Multi-modal learning has become a critical research area because integrating text and image data can significantly improve performance in tasks such as classification, retrieval, and scene understanding. However, despite progress with pre-trained models, current approaches are limited by inadequate cross-modal interactions and static fusion strategies that do not fully exploit the complementary nature of different modalities. To address these shortcomings, we introduce a novel multi-modal Co-AttenDWG architecture that leverages dual-path encoding, co-attention with dimension-wise gating, and advanced expert fusion. Our approach begins by projecting text and image features into a common embedding space, where a dedicated co-attention mechanism enables simultaneous, fine-grained interactions between modalities. This mechanism is further enhanced by a dimension-wise gating network that adaptively regulates the feature contributions at the channel level, ensuring that only the most relevant information is emphasized. In parallel, dual-path encoders refine the representations by processing cross-modal information separately before an additional cross-attention layer further aligns modalities. The refined features are then aggregated via an expert fusion module that combines learned gating and self-attention to produce a robust, unified representation. We validate our approach on the MIMIC and SemEval Memotion 1.0, where experimental results demonstrate significant improvements in cross-modal alignment and state-of-the-art performance, underscoring the potential of our model for a wide range of multi-modal applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.5 -->
                    
                <!-- LLMs: 4.2 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.19013" target="_blank" rel="noopener noreferrer">Faithful Group Shapley Value</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kiljae Lee, Ziqi Liu, Weijing Tang, Yuan Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Data Shapley is an important tool for data valuation, which quantifies the contribution of individual data points to machine learning models. In practice, group-level data valuation is desirable when data providers contribute data in batch. However, we identify that existing group-level extensions o</span>
                
                <span class="abstract-full" style="display: none;">Data Shapley is an important tool for data valuation, which quantifies the contribution of individual data points to machine learning models. In practice, group-level data valuation is desirable when data providers contribute data in batch. However, we identify that existing group-level extensions of Data Shapley are vulnerable to shell company attacks, where strategic group splitting can unfairly inflate valuations. We propose Faithful Group Shapley Value (FGSV) that uniquely defends against such attacks. Building on original mathematical insights, we develop a provably fast and accurate approximation algorithm for computing FGSV. Empirical experiments demonstrate that our algorithm significantly outperforms state-of-the-art methods in computational efficiency and approximation accuracy, while ensuring faithful group-level valuation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.9 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Decision Trees: 2.7 -->
                    
                <!-- GNN: 2.6 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Bayesian Optimization: 2.1 -->
                    
                <!-- HPO and AutoML: 2.0 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.19026" target="_blank" rel="noopener noreferrer">Staircase Recognition and Location Based on Polarization Vision</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Weifeng Kong, Zhiying Tan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Staircase is one of the most common structures in artificial scenes. However, it is difficult for humanoid robots and people with lower limb disabilities or visual impairment to cross the scene without the help of sensors and intelligent algorithms. Staircase scene perception technology is a prerequ</span>
                
                <span class="abstract-full" style="display: none;">Staircase is one of the most common structures in artificial scenes. However, it is difficult for humanoid robots and people with lower limb disabilities or visual impairment to cross the scene without the help of sensors and intelligent algorithms. Staircase scene perception technology is a prerequisite for recognition and localization. This technology is of great significance for the mode switching of the robot and the calculation of the footprint position to adapt to the discontinuous terrain. However, there are still many problems that constrain the application of this technology, such as low recognition accuracy, high initial noise from sensors, unstable output signals and high computational requirements. In terms of scene reconstruction, the binocular and time of flight (TOF) reconstruction of the scene can be easily affected by environmental light and the surface material of the target object. In contrast, due to the special structure of the polarizer, the polarization can selectively transmit polarized light in a specific direction and this reconstruction method relies on the polarization information of the object surface. So the advantages of polarization reconstruction are reflected, which are less affected by environmental light and not dependent on the texture information of the object surface. In this paper, in order to achieve the detection of staircase, this paper proposes a contrast enhancement algorithm that integrates polarization and light intensity information, and integrates point cloud segmentation based on YOLOv11. To realize the high-quality reconstruction, we proposed a method of fusing polarized binocular and TOF depth information to realize the three-dimensional (3D) reconstruction of the staircase. Besides, it also proposes a joint calibration algorithm of monocular camera and TOF camera based on ICP registration and improved gray wolf optimization algorithm.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 4.6 -->
                    
                <!-- Medicine: 3.1 -->
                    
                <!-- Evolutionary Algorithms: 2.6 -->
                    
                <!-- Hardware: 2.5 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.19027" target="_blank" rel="noopener noreferrer">High Throughput QC-LDPC Decoder With Optimized Schedule Policy in Layered Decoding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dongxu Chang, Qingqing Peng, Guanghui Wang, Guiying Yan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this study, a scheduling policy of layered decoding for quasi-cycle (QC) low-density parity-check (LDPC) codes with high throughput and good performance is designed. The influence of scheduling on the delay of the decoder's hardware implementation and on the decoding performance are considered si</span>
                
                <span class="abstract-full" style="display: none;">In this study, a scheduling policy of layered decoding for quasi-cycle (QC) low-density parity-check (LDPC) codes with high throughput and good performance is designed. The influence of scheduling on the delay of the decoder's hardware implementation and on the decoding performance are considered simultaneously. Specifically, we analyze the idle time required under various scheduling sequences within a pipelined decoding architecture and formulate the problem as a traveling salesman problem (TSP) aiming at minimizing idle time. Furthermore, considering that different scheduling sequences can affect decoding performance, we refine the graph used to solve the TSP based on scheduling characteristics that promote improved decoding outcomes. Simulation results demonstrate that the identified scheduling sequence achieves a low number of hardware delays while maintaining excellent decoding performance for 5G New Radio (NR) LDPC codes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 3.4 -->
                    
                <!-- Hardware: 2.4 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- LLMs: 2.1 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Networks: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0542
                </span>
                <a href="https://arxiv.org/abs/2505.19820" target="_blank" rel="noopener noreferrer">InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Feifei Li, Mi Zhang, Zhaoxiang Wang, Min Yang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Interpretability of point cloud (PC) models becomes imperative given their deployment in safety-critical scenarios such as autonomous vehicles. We focus on attributing PC model outputs to interpretable critical concepts, defined as meaningful subsets of the input point cloud. To enable human-underst</span>
                
                <span class="abstract-full" style="display: none;">Interpretability of point cloud (PC) models becomes imperative given their deployment in safety-critical scenarios such as autonomous vehicles. We focus on attributing PC model outputs to interpretable critical concepts, defined as meaningful subsets of the input point cloud. To enable human-understandable diagnostics of model failures, an ideal critical subset should be *faithful* (preserving points that causally influence predictions) and *conceptually coherent* (forming semantically meaningful structures that align with human perception). We propose InfoCons, an explanation framework that applies information-theoretic principles to decompose the point cloud into 3D concepts, enabling the examination of their causal effect on model predictions with learnable priors. We evaluate InfoCons on synthetic datasets for classification, comparing it qualitatively and quantitatively with four baselines. We further demonstrate its scalability and flexibility on two real-world datasets and in two applications that utilize critical scores of PC.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 7.6%">
                            LLMs
                        </span>
                <!-- Medicine: 3.9 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0593
                </span>
                <a href="https://arxiv.org/abs/2505.19896" target="_blank" rel="noopener noreferrer">Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alejandro Carrasco, Victor Rodriguez-Fernandez, Richard Linares
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Control in space, enabling LLMs to play a significant role in the decision-making process for </span>
                
                <span class="abstract-full" style="display: none;">Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Control in space, enabling LLMs to play a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. The project comprises several open repositories to facilitate replication and further research. The codebase is accessible on \href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models and datasets are available on \href{https://huggingface.co/OhhTuRnz}{Hugging Face}. Additionally, experiment tracking and detailed results can be reviewed on \href{https://wandb.ai/carrusk/huggingface}{Weights \& Biases</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 8.3%">
                            LLMs
                        </span>
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Medicine: 1.5 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Game Theory: 1.2 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0608
                </span>
                <a href="https://arxiv.org/abs/2505.19462" target="_blank" rel="noopener noreferrer">VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Puyuan Peng, Shang-Wen Li, Abdelrahman Mohamed, David Harwath
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuatio</span>
                
                <span class="abstract-full" style="display: none;">We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training/inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form/extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and model weights: https://github.com/jasonppy/VoiceStar</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.1%">
                            LLMs
                        </span>
                <!-- Medicine: 5.0 -->
                    
                <!-- Computer Vision: 3.6 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0692
                </span>
                <a href="https://arxiv.org/abs/2505.19696" target="_blank" rel="noopener noreferrer">Modeling Beyond MOS: Quality Assessment Models Must Integrate Context, Reasoning, and Multimodality</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mohamed Amine Kerkouri, Marouane Tliba, Aladine Chetouani, Nour Aburaed, Alessandro Bruno
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This position paper argues that Mean Opinion Score (MOS), while historically foundational, is no longer sufficient as the sole supervisory signal for multimedia quality assessment models. MOS reduces rich, context-sensitive human judgments to a single scalar, obscuring semantic failures, user intent</span>
                
                <span class="abstract-full" style="display: none;">This position paper argues that Mean Opinion Score (MOS), while historically foundational, is no longer sufficient as the sole supervisory signal for multimedia quality assessment models. MOS reduces rich, context-sensitive human judgments to a single scalar, obscuring semantic failures, user intent, and the rationale behind quality decisions. We contend that modern quality assessment models must integrate three interdependent capabilities: (1) context-awareness, to adapt evaluations to task-specific goals and viewing conditions; (2) reasoning, to produce interpretable, evidence-grounded justifications for quality judgments; and (3) multimodality, to align perceptual and semantic cues using vision-language models. We critique the limitations of current MOS-centric benchmarks and propose a roadmap for reform: richer datasets with contextual metadata and expert rationales, and new evaluation metrics that assess semantic alignment, reasoning fidelity, and contextual sensitivity. By reframing quality assessment as a contextual, explainable, and multimodal modeling task, we aim to catalyze a shift toward more robust, human-aligned, and trustworthy evaluation systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 9.7%">
                            LLMs
                        </span>
                <!-- Medicine: 4.1 -->
                    
                <!-- Decision Trees: 2.1 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.0 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0832
                </span>
                <a href="https://arxiv.org/abs/2503.02398" target="_blank" rel="noopener noreferrer">PersonaX: A Recommendation Agent Oriented User Modeling Framework for Long Behavior Sequence</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yunxiao Shi, Wujiang Xu, Zeqi Zhang, Xing Zi, Qiang Wu, Min Xu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">User profile embedded in the prompt template of personalized recommendation agents play a crucial role in shaping their decision-making process. High-quality user profiles are essential for aligning agent behavior with real user interests. Typically, these profiles are constructed by leveraging LLMs</span>
                
                <span class="abstract-full" style="display: none;">User profile embedded in the prompt template of personalized recommendation agents play a crucial role in shaping their decision-making process. High-quality user profiles are essential for aligning agent behavior with real user interests. Typically, these profiles are constructed by leveraging LLMs for user profile modeling (LLM-UM). However, this process faces several challenges: (1) LLMs struggle with long user behaviors due to context length limitations and performance degradation. (2) Existing methods often extract only partial segments from full historical behavior sequence, inevitably discarding diverse user interests embedded in the omitted content, leading to incomplete modeling and suboptimal profiling. (3) User profiling is often tightly coupled with the inference context, requiring online processing, which introduces significant latency overhead. In this paper, we propose PersonaX, an agent-agnostic LLM-UM framework to address these challenges. It augments downstream recommendation agents to achieve better recommendation performance and inference efficiency. PersonaX (a) segments complete historical behaviors into clustered groups, (b) selects multiple sub behavior sequences (SBS) with a balance of prototypicality and diversity to form a high quality core set, (c) performs offline multi-persona profiling to capture diverse user interests and generate fine grained, cached textual personas, and (d) decouples user profiling from online inference, enabling profile retrieval instead of real time generation. Extensive experiments demonstrate its effectiveness: using only 30 to 50% of behavioral data (sequence length 480), PersonaX enhances AgentCF by 3 to 11% and Agent4Rec by 10 to 50%. As a scalable and model-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user modeling.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 11.6%">
                            LLMs
                        </span>
                <!-- Medicine: 2.9 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.085
                </span>
                <a href="https://arxiv.org/abs/2505.12728" target="_blank" rel="noopener noreferrer">FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zihua Wang, Ruibo Li, Haozhe Du, Joey Tianyi Zhou, Yu Zhang, Xu Yang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language and multimodal models (LLMs and LMMs) exhibit strong inference capabilities but are often limited by slow decoding speeds. This challenge is especially acute in LMMs, where visual inputs typically comprise more tokens with lower information density than text -- an issue exacerbated by</span>
                
                <span class="abstract-full" style="display: none;">Large language and multimodal models (LLMs and LMMs) exhibit strong inference capabilities but are often limited by slow decoding speeds. This challenge is especially acute in LMMs, where visual inputs typically comprise more tokens with lower information density than text -- an issue exacerbated by recent trends toward finer-grained visual tokenizations to boost performance. Speculative decoding has been effective in accelerating LLM inference by using a smaller draft model to generate candidate tokens, which are then selectively verified by the target model, improving speed without sacrificing output quality. While this strategy has been extended to LMMs, existing methods largely overlook the unique properties of visual inputs and depend solely on text-based draft models. In this work, we propose \textbf{FLASH} (Fast Latent-Aware Semi-Autoregressive Heuristics), a speculative decoding framework designed specifically for LMMs, which leverages two key properties of multimodal data to design the draft model. First, to address redundancy in visual tokens, we propose a lightweight latent-aware token compression mechanism. Second, recognizing that visual objects often co-occur within a scene, we employ a semi-autoregressive decoding strategy to generate multiple tokens per forward pass. These innovations accelerate draft decoding while maintaining high acceptance rates, resulting in faster overall inference. Experiments show that FLASH significantly outperforms prior speculative decoding approaches in both unimodal and multimodal settings, achieving up to \textbf{2.68$\times$} speed-up on video captioning and \textbf{2.55$\times$} on visual instruction tuning tasks compared to the original LMM. Our code is available \href{https://github.com/ZihuaEvan/FlashSD/}{[here]}.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 12.7%">
                            LLMs
                        </span>
                <!-- Federated Learning: 3.0 -->
                    
                <!-- Computer Vision: 2.8 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Medicine: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1129
                </span>
                <a href="https://arxiv.org/abs/2505.20047" target="_blank" rel="noopener noreferrer">Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Debargha Ganguly, Vikash Singh, Sreehari Sankar, Biyao Zhang, Xuecen Zhang, Srinivasan Iyengar, Xiaotian Han, Amit Sharma, Shivkumar Kalyanaraman, Vipin Chaudhary
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by </span>
                
                <span class="abstract-full" style="display: none;">Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 16.9%">
                            LLMs
                        </span>
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.116
                </span>
                <a href="https://arxiv.org/abs/2504.13211" target="_blank" rel="noopener noreferrer">Mirror: Multimodal Cognitive Reframing Therapy for Rolling with Resistance</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Subin Kim, Hoonrae Kim, Jihyun Lee, Yejin Jeon, Gary Geunbae Lee
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent studies have explored the use of large language models (LLMs) in psychotherapy; however, text-based cognitive behavioral therapy (CBT) models often struggle with client resistance, which can weaken therapeutic alliance. To address this, we propose a multimodal approach that incorporates nonve</span>
                
                <span class="abstract-full" style="display: none;">Recent studies have explored the use of large language models (LLMs) in psychotherapy; however, text-based cognitive behavioral therapy (CBT) models often struggle with client resistance, which can weaken therapeutic alliance. To address this, we propose a multimodal approach that incorporates nonverbal cues, which allows the AI therapist to better align its responses with the client's negative emotional state. Specifically, we introduce a new synthetic dataset, Mirror (Multimodal Interactive Rolling with Resistance), which is a novel synthetic dataset that pairs each client's statements with corresponding facial images. Using this dataset, we train baseline vision language models (VLMs) so that they can analyze facial cues, infer emotions, and generate empathetic responses to effectively manage client resistance. These models are then evaluated in terms of both their counseling skills as a therapist, and the strength of therapeutic alliance in the presence of client resistance. Our results demonstrate that Mirror significantly enhances the AI therapist's ability to handle resistance, which outperforms existing text-based CBT approaches. Human expert evaluations further confirm the effectiveness of our approach in managing client resistance and fostering therapeutic alliance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 15.1%">
                            LLMs
                        </span>
                <!-- Federated Learning: 3.9 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1365
                </span>
                <a href="https://arxiv.org/abs/2505.19897" target="_blank" rel="noopener noreferrer">ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Qiushi Sun, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu, Kanzhi Cheng, Zhaoyang Liu, Jianing Wang, Qintong Li, Xiangru Tang, Tianbao Xie, Xiachong Feng, Xiang Li, Ben Kao, Wenhai Wang, Biqing Qi, Lingpeng Kong, Zhiyong Wu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among</span>
                
                <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers' workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at https://qiushisun.github.io/ScienceBoard-Home/.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 16.4%">
                            LLMs
                        </span>
                <!-- Medicine: 2.8 -->
                    
                <!-- Blockchain: 2.4 -->
                    
                <!-- Hardware: 2.2 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1751
                </span>
                <a href="https://arxiv.org/abs/2402.13606" target="_blank" rel="noopener noreferrer">MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Boyang Xue, Hongru Wang, Rui Wang, Sheng Wang, Zezhong Wang, Yiming Du, Bin Liang, Wenxuan Zhang, Kam-Fai Wong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The tendency of Large Language Models (LLMs) to generate hallucinations raises concerns regarding their reliability. Therefore, confidence estimations indicating the extent of trustworthiness of the generations become essential. However, current LLM confidence estimations in languages other than Eng</span>
                
                <span class="abstract-full" style="display: none;">The tendency of Large Language Models (LLMs) to generate hallucinations raises concerns regarding their reliability. Therefore, confidence estimations indicating the extent of trustworthiness of the generations become essential. However, current LLM confidence estimations in languages other than English remain underexplored. This paper addresses this gap by introducing a comprehensive investigation of Multilingual Confidence estimation (MlingConf) on LLMs, focusing on both language-agnostic (LA) and language-specific (LS) tasks to explore the performance and language dominance effects of multilingual confidence estimations on different tasks. The benchmark comprises four meticulously checked and human-evaluated high-quality multilingual datasets for LA tasks and one for the LS task tailored to specific social, cultural, and geographical contexts of a language. Our experiments reveal that on LA tasks English exhibits notable linguistic dominance in confidence estimations than other languages, while on LS tasks, using question-related language to prompt LLMs demonstrates better linguistic dominance in multilingual confidence estimations. The phenomena inspire a simple yet effective native-tone prompting strategy by employing language-specific prompts for LS tasks, effectively improving LLMs' reliability and accuracy in LS scenarios.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 19.3%">
                            LLMs
                        </span>
                <!-- Medicine: 2.5 -->
                    
                <!-- Blockchain: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.5092
                </span>
                <a href="https://arxiv.org/abs/2505.18992" target="_blank" rel="noopener noreferrer">VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tianchen Deng, Wenhua Wu, Junjie He, Yue Pan, Xirui Jiang, Shenghai Yuan, Danwei Wang, Hesheng Wang, Weidong Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">3D Gaussian Splatting has recently shown promising results in dense visual SLAM. However, existing 3DGS-based SLAM methods are all constrained to small-room scenarios and struggle with memory explosion in large-scale scenes and long sequences. To this end, we propose VPGS-SLAM, the first 3DGS-based </span>
                
                <span class="abstract-full" style="display: none;">3D Gaussian Splatting has recently shown promising results in dense visual SLAM. However, existing 3DGS-based SLAM methods are all constrained to small-room scenarios and struggle with memory explosion in large-scale scenes and long sequences. To this end, we propose VPGS-SLAM, the first 3DGS-based large-scale RGBD SLAM framework for both indoor and outdoor scenarios. We design a novel voxel-based progressive 3D Gaussian mapping method with multiple submaps for compact and accurate scene representation in large-scale and long-sequence scenes. This allows us to scale up to arbitrary scenes and improves robustness (even under pose drifts). In addition, we propose a 2D-3D fusion camera tracking method to achieve robust and accurate camera tracking in both indoor and outdoor large-scale scenes. Furthermore, we design a 2D-3D Gaussian loop closure method to eliminate pose drift. We further propose a submap fusion method with online distillation to achieve global consistency in large-scale scenes when detecting a loop. Experiments on various indoor and outdoor datasets demonstrate the superiority and generalizability of the proposed framework. The code will be open source on https://github.com/dtc111111/vpgs-slam.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.0%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #76aa96" title="Confidence: 5.0%">
                            3D
                        </span>
                <!-- Computer Vision: 3.1 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.617
                </span>
                <a href="https://arxiv.org/abs/2505.01726" target="_blank" rel="noopener noreferrer">Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jie Liu, Pan Zhou, Zehao Xiao, Jiayi Shen, Wenzhe Yin, Jan-Jakob Sonke, Efstratios Gavves
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentatio</span>
                
                <span class="abstract-full" style="display: none;">Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model's ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.7%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #76aa96" title="Confidence: 5.6%">
                            3D
                        </span>
                <!-- Medicine: 3.9 -->
                    
                <!-- GNN: 3.8 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Computer Vision: 2.7 -->
                    
                <!-- HPO and AutoML: 2.0 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Decision Trees: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.114
                </span>
                <a href="https://arxiv.org/abs/2505.19926" target="_blank" rel="noopener noreferrer">Bounding Width on Graph Classes of Constant Diameter</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Konrad K. Dabrowski, Tala Eagling-Vose, Noleen K\"ohler, Sebastian Ordyniak, Dani\"el Paulusma
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We determine if the width of a graph class ${\cal G}$ changes from unbounded to bounded if we consider only those graphs from ${\cal G}$ whose diameter is bounded. As parameters we consider treedepth, pathwidth, treewidth and clique-width, and as graph classes we consider classes defined by forbiddi</span>
                
                <span class="abstract-full" style="display: none;">We determine if the width of a graph class ${\cal G}$ changes from unbounded to bounded if we consider only those graphs from ${\cal G}$ whose diameter is bounded. As parameters we consider treedepth, pathwidth, treewidth and clique-width, and as graph classes we consider classes defined by forbidding some specific graph $F$ as a minor, induced subgraph or subgraph, respectively. Our main focus is on treedepth for $F$-subgraph-free graphs of diameter at most~$d$ for some fixed integer $d$. We give classifications of boundedness of treedepth for $d\in \{4,5,\ldots\}$ and partial classifications for $d=2$ and $d=3$.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.2%">
                            Medicine
                        </span>
                <!-- LLMs: 3.2 -->
                    
                <!-- GNN: 3.1 -->
                    
                <!-- Hardware: 2.5 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.1504
                </span>
                <a href="https://arxiv.org/abs/2501.01587" target="_blank" rel="noopener noreferrer">EGPT-PINN: Entropy-enhanced Generative Pre-Trained Physics Informed Neural Networks for parameterized nonlinear conservation laws</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yajie Ji, Yanlai Chen, Zhenli Xu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose an entropy-enhanced Generative Pre-Trained Physics-Informed Neural Network with a transform layer (EGPT-PINN) for solving parameterized nonlinear conservation laws. The EGPT-PINN extends the traditional physics-informed neural networks and its recently proposed generative pre-trained stra</span>
                
                <span class="abstract-full" style="display: none;">We propose an entropy-enhanced Generative Pre-Trained Physics-Informed Neural Network with a transform layer (EGPT-PINN) for solving parameterized nonlinear conservation laws. The EGPT-PINN extends the traditional physics-informed neural networks and its recently proposed generative pre-trained strategy for linear model reduction to nonlinear model reduction and shock-capturing domains. By utilizing an adaptive meta-network, a simultaneously trained transform layer, entropy enhancement strategies, implementable shock interaction analysis, and a separable training process, the EGPT-PINN efficiently captures complex parameter-dependent shock formations and interactions. Numerical results of EGPT-PINN applied to the families of inviscid Burgers' equation and the Euler equations, parameterized by their initial conditions, demonstrate the robustness and accuracy of the proposed technique. It accurately solves the viscosity solution via very few neurons without leveraging any {\it a priori} knowledge of the equations or its initial condition. Moreover, via a simple augmentation of the loss function by model-data mismatch, we demonstrate the robustness of EGPT-PINN in solving inverse problems more accurately than the vanilla and entropy-enhanced versions of PINN.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.4%">
                            Medicine
                        </span>
                <!-- Federated Learning: 2.0 -->
                    
                <!-- LLMs: 2.0 -->
                    
                <!-- Bayesian Optimization: 1.9 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.1756
                </span>
                <a href="https://arxiv.org/abs/2505.20136" target="_blank" rel="noopener noreferrer">Engineering Trustworthy Machine-Learning Operations with Zero-Knowledge Proofs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Filippo Scaramuzza, Giovanni Quattrocchi, Damian A. Tamburri
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As Artificial Intelligence (AI) systems, particularly those based on machine learning (ML), become integral to high-stakes applications, their probabilistic and opaque nature poses significant challenges to traditional verification and validation methods. These challenges are exacerbated in regulate</span>
                
                <span class="abstract-full" style="display: none;">As Artificial Intelligence (AI) systems, particularly those based on machine learning (ML), become integral to high-stakes applications, their probabilistic and opaque nature poses significant challenges to traditional verification and validation methods. These challenges are exacerbated in regulated sectors requiring tamper-proof, auditable evidence, as highlighted by apposite legal frameworks, e.g., the EU AI Act. Conversely, Zero-Knowledge Proofs (ZKPs) offer a cryptographic solution that enables provers to demonstrate, through verified computations, adherence to set requirements without revealing sensitive model details or data. Through a systematic survey of ZKP protocols, we identify five key properties (non-interactivity, transparent setup, standard representations, succinctness, and post-quantum security) critical for their application in AI validation and verification pipelines. Subsequently, we perform a follow-up systematic survey analyzing ZKP-enhanced ML applications across an adaptation of the Team Data Science Process (TDSP) model (Data & Preprocessing, Training & Offline Metrics, Inference, and Online Metrics), detailing verification objectives, ML models, and adopted protocols. Our findings indicate that current research on ZKP-Enhanced ML primarily focuses on inference verification, while the data preprocessing and training stages remain underexplored. Most notably, our analysis identifies a significant convergence within the research domain toward the development of a unified Zero-Knowledge Machine Learning Operations (ZKMLOps) framework. This emerging framework leverages ZKPs to provide robust cryptographic guarantees of correctness, integrity, and privacy, thereby promoting enhanced accountability, transparency, and compliance with Trustworthy AI principles.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.5%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.3%">
                            Medicine
                        </span>
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2107
                </span>
                <a href="https://arxiv.org/abs/2505.19018" target="_blank" rel="noopener noreferrer">CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Md. Mithun Hossain, Md. Shakil Hossain, Sudipto Chaki, Md. Rajib Hossain, Md. Saifur Rahman, A. B. M. Shawkat Ali
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Aspect-Based Sentiment Analysis (ABSA) is a fundamental task in natural language processing, offering fine-grained insights into opinions expressed in text. While existing research has largely focused on resource-rich languages like English which leveraging large annotated datasets, pre-trained mode</span>
                
                <span class="abstract-full" style="display: none;">Aspect-Based Sentiment Analysis (ABSA) is a fundamental task in natural language processing, offering fine-grained insights into opinions expressed in text. While existing research has largely focused on resource-rich languages like English which leveraging large annotated datasets, pre-trained models, and language-specific tools. These resources are often unavailable for low-resource languages such as Bengali. The ABSA task in Bengali remains poorly explored and is further complicated by its unique linguistic characteristics and a lack of annotated data, pre-trained models, and optimized hyperparameters. To address these challenges, this research propose CrosGrpsABS, a novel hybrid framework that leverages bidirectional cross-attention between syntactic and semantic graphs to enhance aspect-level sentiment classification. The CrosGrpsABS combines transformerbased contextual embeddings with graph convolutional networks, built upon rule-based syntactic dependency parsing and semantic similarity computations. By employing bidirectional crossattention, the model effectively fuses local syntactic structure with global semantic context, resulting in improved sentiment classification performance across both low- and high-resource settings. We evaluate CrosGrpsABS on four low-resource Bengali ABSA datasets and the high-resource English SemEval 2014 Task 4 dataset. The CrosGrpsABS consistently outperforms existing approaches, achieving notable improvements, including a 0.93% F1-score increase for the Restaurant domain and a 1.06% gain for the Laptop domain in the SemEval 2014 Task 4 benchmark.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.7%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.5%">
                            Medicine
                        </span>
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2424
                </span>
                <a href="https://arxiv.org/abs/2411.06608" target="_blank" rel="noopener noreferrer">MolMiner: Towards Controllable, 3D-Aware, Fragment-Based Molecular Design</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Raul Ortega-Ochoa, Tejs Vegge, Jes Frellsen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce MolMiner, a fragment-based, geometry-aware, and order-agnostic autoregressive model for molecular design. MolMiner supports conditional generation of molecules over twelve properties, enabling flexible control across physicochemical and structural targets. Molecules are built via symmet</span>
                
                <span class="abstract-full" style="display: none;">We introduce MolMiner, a fragment-based, geometry-aware, and order-agnostic autoregressive model for molecular design. MolMiner supports conditional generation of molecules over twelve properties, enabling flexible control across physicochemical and structural targets. Molecules are built via symmetry-aware fragment attachments, with 3D geometry dynamically updated during generation using forcefields. A probabilistic conditioning mechanism allows users to specify any subset of target properties while sampling the rest. MolMiner achieves calibrated conditional generation across most properties and offers competitive unconditional performance. We also propose improved benchmarking methods for both unconditional and conditional generation, including distributional comparisons via Wasserstein distance and calibration plots for property control. To our knowledge, this is the first model to unify dynamic geometry, symmetry handling, order-agnostic fragment-based generation, and high-dimensional multi-property conditioning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 7.0%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.0%">
                            Medicine
                        </span>
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2544
                </span>
                <a href="https://arxiv.org/abs/2505.19028" target="_blank" rel="noopener noreferrer">InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Minzhi Lin, Tianchi Xie, Mengchen Liu, Yilin Ye, Changjian Chen, Shixia Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Understanding infographic charts with design-driven visual elements (e.g., pictograms, icons) requires both visual recognition and reasoning, posing challenges for multimodal large language models (MLLMs). However, existing visual-question answering benchmarks fall short in evaluating these capabili</span>
                
                <span class="abstract-full" style="display: none;">Understanding infographic charts with design-driven visual elements (e.g., pictograms, icons) requires both visual recognition and reasoning, posing challenges for multimodal large language models (MLLMs). However, existing visual-question answering benchmarks fall short in evaluating these capabilities of MLLMs due to the lack of paired plain charts and visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a benchmark for evaluating MLLMs on infographic chart understanding. It includes 5,642 pairs of infographic and plain charts, each sharing the same underlying data but differing in visual presentations. We further design visual-element-based questions to capture their unique visual designs and communicative intent. Evaluation of 20 MLLMs reveals a substantial performance decline on infographic charts, particularly for visual-element-based questions related to metaphors. The paired infographic and plain charts enable fine-grained error analysis and ablation studies, which highlight new opportunities for advancing MLLMs in infographic chart understanding. We release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 8.2%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.0%">
                            Medicine
                        </span>
                <!-- Hardware: 3.0 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Datasets: 2.1 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.275
                </span>
                <a href="https://arxiv.org/abs/2504.09797" target="_blank" rel="noopener noreferrer">IGL-DT: Iterative Global-Local Feature Learning with Dual-Teacher Semantic Segmentation Framework under Limited Annotation Scheme</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dinh Dai Quan Tran, Hoang-Thien Nguyen, Thanh-Huy Nguyen, Gia-Van To, Tien-Huy Nguyen, Quan Nguyen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Semi-Supervised Semantic Segmentation (SSSS) aims to improve segmentation accuracy by leveraging a small set of labeled images alongside a larger pool of unlabeled data. Recent advances primarily focus on pseudo-labeling, consistency regularization, and co-training strategies. However, existing meth</span>
                
                <span class="abstract-full" style="display: none;">Semi-Supervised Semantic Segmentation (SSSS) aims to improve segmentation accuracy by leveraging a small set of labeled images alongside a larger pool of unlabeled data. Recent advances primarily focus on pseudo-labeling, consistency regularization, and co-training strategies. However, existing methods struggle to balance global semantic representation with fine-grained local feature extraction. To address this challenge, we propose a novel tri-branch semi-supervised segmentation framework incorporating a dual-teacher strategy, named IGL-DT. Our approach employs SwinUnet for high-level semantic guidance through Global Context Learning and ResUnet for detailed feature refinement via Local Regional Learning. Additionally, a Discrepancy Learning mechanism mitigates over-reliance on a single teacher, promoting adaptive feature learning. Extensive experiments on benchmark datasets demonstrate that our method outperforms state-of-the-art approaches, achieving superior segmentation performance across various data regimes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.4%">
                            Medicine
                        </span>
                <!-- Federated Learning: 4.0 -->
                    
                <!-- GNN: 4.0 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- 3D: 3.2 -->
                    
                <!-- Computer Vision: 2.6 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2935
                </span>
                <a href="https://arxiv.org/abs/2412.03318" target="_blank" rel="noopener noreferrer">Domain-Agnostic Stroke Lesion Segmentation Using Physics-Constrained Synthetic Data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Liam Chalcroft, Jenny Crinion, Cathy J. Price, John Ashburner
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Segmenting stroke lesions in MRI is challenging due to diverse acquisition protocols that limit model generalisability. In this work, we introduce two physics-constrained approaches to generate synthetic quantitative MRI (qMRI) images that improve segmentation robustness across heterogeneous domains</span>
                
                <span class="abstract-full" style="display: none;">Segmenting stroke lesions in MRI is challenging due to diverse acquisition protocols that limit model generalisability. In this work, we introduce two physics-constrained approaches to generate synthetic quantitative MRI (qMRI) images that improve segmentation robustness across heterogeneous domains. Our first method, $\texttt{qATLAS}$, trains a neural network to estimate qMRI maps from standard MPRAGE images, enabling the simulation of varied MRI sequences with realistic tissue contrasts. The second method, $\texttt{qSynth}$, synthesises qMRI maps directly from tissue labels using label-conditioned Gaussian mixture models, ensuring physical plausibility. Extensive experiments on multiple out-of-domain datasets show that both methods outperform a baseline UNet, with $\texttt{qSynth}$ notably surpassing previous synthetic data approaches. These results highlight the promise of integrating MRI physics into synthetic data generation for robust, generalisable stroke lesion segmentation. Code is available at https://github.com/liamchalcroft/qsynth</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.9%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.6%">
                            LLMs
                        </span>
                <!-- GNN: 3.6 -->
                    
                <!-- Federated Learning: 2.8 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Decision Trees: 2.2 -->
                    
                <!-- Bayesian Optimization: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3497
                </span>
                <a href="https://arxiv.org/abs/2505.19582" target="_blank" rel="noopener noreferrer">Guard Me If You Know Me: Protecting Specific Face-Identity from Deepfakes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kaiqing Lin, Zhiyuan Yan, Ke-Yue Zhang, Li Hao, Yue Zhou, Yuzhen Lin, Weixiang Li, Taiping Yao, Shouhong Ding, Bin Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Securing personal identity against deepfake attacks is increasingly critical in the digital age, especially for celebrities and political figures whose faces are easily accessible and frequently targeted. Most existing deepfake detection methods focus on general-purpose scenarios and often ignore th</span>
                
                <span class="abstract-full" style="display: none;">Securing personal identity against deepfake attacks is increasingly critical in the digital age, especially for celebrities and political figures whose faces are easily accessible and frequently targeted. Most existing deepfake detection methods focus on general-purpose scenarios and often ignore the valuable prior knowledge of known facial identities, e.g., "VIP individuals" whose authentic facial data are already available. In this paper, we propose \textbf{VIPGuard}, a unified multimodal framework designed to capture fine-grained and comprehensive facial representations of a given identity, compare them against potentially fake or similar-looking faces, and reason over these comparisons to make accurate and explainable predictions. Specifically, our framework consists of three main stages. First, fine-tune a multimodal large language model (MLLM) to learn detailed and structural facial attributes. Second, we perform identity-level discriminative learning to enable the model to distinguish subtle differences between highly similar faces, including real and fake variations. Finally, we introduce user-specific customization, where we model the unique characteristics of the target face identity and perform semantic reasoning via MLLM to enable personalized and explainable deepfake detection. Our framework shows clear advantages over previous detection works, where traditional detectors mainly rely on low-level visual cues and provide no human-understandable explanations, while other MLLM-based models often lack a detailed understanding of specific face identities. To facilitate the evaluation of our method, we built a comprehensive identity-aware benchmark called \textbf{VIPBench} for personalized deepfake detection, involving the latest 7 face-swapping and 7 entire face synthesis techniques for generation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.2%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.6%">
                            Medicine
                        </span>
                <!-- Computer Vision: 3.4 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3642
                </span>
                <a href="https://arxiv.org/abs/2505.20290" target="_blank" rel="noopener noreferrer">EgoZero: Robot Learning from Smart Glasses</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vincent Liu, Ademi Adeniji, Haotian Zhan, Raunaq Bhirangi, Pieter Abbeel, Lerrel Pinto
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Despite recent progress in general purpose robotics, robot policies still lag far behind basic human capabilities in the real world. Humans interact constantly with the physical world, yet this rich data resource remains largely untapped in robot learning. We propose EgoZero, a minimal system that l</span>
                
                <span class="abstract-full" style="display: none;">Despite recent progress in general purpose robotics, robot policies still lag far behind basic human capabilities in the real world. Humans interact constantly with the physical world, yet this rich data resource remains largely untapped in robot learning. We propose EgoZero, a minimal system that learns robust manipulation policies from human demonstrations captured with Project Aria smart glasses, $\textbf{and zero robot data}$. EgoZero enables: (1) extraction of complete, robot-executable actions from in-the-wild, egocentric, human demonstrations, (2) compression of human visual observations into morphology-agnostic state representations, and (3) closed-loop policy learning that generalizes morphologically, spatially, and semantically. We deploy EgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot transfer with 70% success rate over 7 manipulation tasks and only 20 minutes of data collection per task. Our results suggest that in-the-wild human data can serve as a scalable foundation for real-world robot learning - paving the way toward a future of abundant, diverse, and naturalistic training data for robots. Code and videos are available at https://egozero-robot.github.io.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.6%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.2%">
                            LLMs
                        </span>
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Datasets: 2.2 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3959
                </span>
                <a href="https://arxiv.org/abs/2505.18947" target="_blank" rel="noopener noreferrer">OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhenhao Zhang, Ye Shi, Lingxiao Yang, Suting Ni, Qi Ye, Jingya Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Understanding and synthesizing realistic 3D hand-object interactions (HOI) is critical for applications ranging from immersive AR/VR to dexterous robotics. Existing methods struggle with generalization, performing well on closed-set objects and predefined tasks but failing to handle unseen objects o</span>
                
                <span class="abstract-full" style="display: none;">Understanding and synthesizing realistic 3D hand-object interactions (HOI) is critical for applications ranging from immersive AR/VR to dexterous robotics. Existing methods struggle with generalization, performing well on closed-set objects and predefined tasks but failing to handle unseen objects or open-vocabulary instructions. We introduce OpenHOI, the first framework for open-world HOI synthesis, capable of generating long-horizon manipulation sequences for novel objects guided by free-form language commands. Our approach integrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint affordance grounding and semantic task decomposition, enabling precise localization of interaction regions (e.g., handles, buttons) and breakdown of complex instructions (e.g., "Find a water bottle and take a sip") into executable sub-tasks. To synthesize physically plausible interactions, we propose an affordance-driven diffusion model paired with a training-free physics refinement stage that minimizes penetration and optimizes affordance alignment. Evaluations across diverse scenarios demonstrate OpenHOI's superiority over state-of-the-art methods in generalizing to novel object categories, multi-stage tasks, and complex language instructions. Our project page at \href{https://openhoi.github.io}</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 12.3%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.1%">
                            Medicine
                        </span>
                <!-- Computer Vision: 3.2 -->
                    
                <!-- 3D: 2.9 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4031
                </span>
                <a href="https://arxiv.org/abs/2411.13981" target="_blank" rel="noopener noreferrer">On the Fairness, Diversity and Reliability of Text-to-Image Generative Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jordan Vice, Naveed Akhtar, Leonid Sigal, Richard Hartley, Ajmal Mian
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rapid proliferation of multimodal generative models has sparked critical discussions on their reliability, fairness and potential for misuse. While text-to-image models excel at producing high-fidelity, user-guided content, they often exhibit unpredictable behaviors and vulnerabilities that can </span>
                
                <span class="abstract-full" style="display: none;">The rapid proliferation of multimodal generative models has sparked critical discussions on their reliability, fairness and potential for misuse. While text-to-image models excel at producing high-fidelity, user-guided content, they often exhibit unpredictable behaviors and vulnerabilities that can be exploited to manipulate class or concept representations. To address this, we propose an evaluation framework to assess model reliability by analyzing responses to global and local perturbations in the embedding space, enabling the identification of inputs that trigger unreliable or biased behavior. Beyond social implications, fairness and diversity are fundamental to defining robust and trustworthy model behavior. Our approach offers deeper insights into these essential aspects by evaluating: (i) generative diversity, measuring the breadth of visual representations for learned concepts, and (ii) generative fairness, which examines the impact that removing concepts from input prompts has on control, under a low guidance setup. Beyond these evaluations, our method lays the groundwork for detecting unreliable, bias-injected models and tracing the provenance of embedded biases. Our code is publicly available at https://github.com/JJ-Vice/T2I_Fairness_Diversity_Reliability.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.4%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.4%">
                            Medicine
                        </span>
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Decision Trees: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4061
                </span>
                <a href="https://arxiv.org/abs/2503.04342" target="_blank" rel="noopener noreferrer">TRANSIT your events into a new mass: Fast background interpolation for weakly-supervised anomaly searches</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ivan Oleksiyuk, Svyatoslav Voloshynovskiy, Tobias Golling
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce a new model for conditional and continuous data morphing called TRansport Adversarial Network for Smooth InTerpolation (TRANSIT). We apply it to create a background data template for weakly-supervised searches at the LHC. The method smoothly transforms sideband events to match signal re</span>
                
                <span class="abstract-full" style="display: none;">We introduce a new model for conditional and continuous data morphing called TRansport Adversarial Network for Smooth InTerpolation (TRANSIT). We apply it to create a background data template for weakly-supervised searches at the LHC. The method smoothly transforms sideband events to match signal region mass distributions. We demonstrate the performance of TRANSIT using the LHC Olympics R\&amp;D dataset. The model captures non-linear mass correlations of features and produces a template that offers a competitive anomaly sensitivity compared to state-of-the-art transport-based template generators. Moreover, the computational training time required for TRANSIT is an order of magnitude lower than that of competing deep learning methods. This makes it ideal for analyses that iterate over many signal regions and signal models. Unlike generative models, which must learn a full probability density distribution, i.e., the correlations between all the variables, the proposed transport model only has to learn a smooth conditional shift of the distribution. This allows for a simpler, more efficient residual architecture, enabling mass uncorrelated features to pass the network unchanged while the mass correlated features are adjusted accordingly. Furthermore, we show that the latent space of the model provides a set of mass decorrelated features useful for anomaly detection without background sculpting.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.1%">
                            Medicine
                        </span>
                <!-- Networks: 2.6 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Bayesian Optimization: 2.3 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Finance: 1.5 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- LLMs: 1.1 -->
                    
                <!-- Cryptography: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4681
                </span>
                <a href="https://arxiv.org/abs/2505.11232" target="_blank" rel="noopener noreferrer">AW-GATCN: Adaptive Weighted Graph Attention Convolutional Network for Event Camera Data Joint Denoising and Object Recognition</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Haiyu Li, Charith Abhayaratne
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Event cameras, which capture brightness changes with high temporal resolution, inherently generate a significant amount of redundant and noisy data beyond essential object structures. The primary challenge in event-based object recognition lies in effectively removing this noise without losing criti</span>
                
                <span class="abstract-full" style="display: none;">Event cameras, which capture brightness changes with high temporal resolution, inherently generate a significant amount of redundant and noisy data beyond essential object structures. The primary challenge in event-based object recognition lies in effectively removing this noise without losing critical spatial-temporal information. To address this, we propose an Adaptive Graph-based Noisy Data Removal framework for Event-based Object Recognition. Specifically, our approach integrates adaptive event segmentation based on normalized density analysis, a multifactorial edge-weighting mechanism, and adaptive graph-based denoising strategies. These innovations significantly enhance the integration of spatiotemporal information, effectively filtering noise while preserving critical structural features for robust recognition. Experimental evaluations on four challenging datasets demonstrate that our method achieves superior recognition accuracies of 83.77%, 76.79%, 99.30%, and 96.89%, surpassing existing graph-based methods by up to 8.79%, and improving noise reduction performance by up to 19.57%, with an additional accuracy gain of 6.26% compared to traditional Euclidean-based techniques.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.7%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 5.1%">
                            LLMs
                        </span>
                <!-- Computer Vision: 3.6 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4696
                </span>
                <a href="https://arxiv.org/abs/2505.20036" target="_blank" rel="noopener noreferrer">Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hazem Alsamkary, Mohamed Elshaffei, Mohamed Soudy, Sara Ossman, Abdallah Amr, Nehal Adel Abdelsalam, Mohamed Elkerdawy, Ahmed Elnaggar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Protein-protein interactions (PPIs) are fundamental to numerous cellular processes, and their characterization is vital for understanding disease mechanisms and guiding drug discovery. While protein language models (PLMs) have demonstrated remarkable success in predicting protein structure and funct</span>
                
                <span class="abstract-full" style="display: none;">Protein-protein interactions (PPIs) are fundamental to numerous cellular processes, and their characterization is vital for understanding disease mechanisms and guiding drug discovery. While protein language models (PLMs) have demonstrated remarkable success in predicting protein structure and function, their application to sequence-based PPI binding affinity prediction remains relatively underexplored. This gap is often attributed to the scarcity of high-quality, rigorously refined datasets and the reliance on simple strategies for concatenating protein representations. In this work, we address these limitations. First, we introduce a meticulously curated version of the PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction entries, by resolving annotation inconsistencies and duplicate entries for multi-chain protein interactions. This dataset incorporates a stringent, less than or equal to 30%, sequence identity threshold to ensure robust splitting into training, validation, and test sets, minimizing data leakage. Second, we propose and systematically evaluate four architectures for adapting PLMs to PPI binding affinity prediction: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). These architectures were assessed using two training methods: full fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM features. Our comprehensive experiments across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures consistently outperform conventional concatenation methods, achieving up to 12% increase in terms of Spearman correlation. These results highlight the necessity of sophisticated architectural designs to fully exploit the capabilities of PLMs for nuanced PPI binding affinity prediction.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 7.1%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.2%">
                            Medicine
                        </span>
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4746
                </span>
                <a href="https://arxiv.org/abs/2505.19031" target="_blank" rel="noopener noreferrer">Medical Large Vision Language Models with Multi-Image Visual Ability</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xikai Yang, Juzheng Miao, Yuchen Yuan, Jiaze Wang, Qi Dou, Jinpeng Li, Pheng-Ann Heng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Medical large vision-language models (LVLMs) have demonstrated promising performance across various single-image question answering (QA) benchmarks, yet their capability in processing multi-image clinical scenarios remains underexplored. Unlike single image based tasks, medical tasks involving multi</span>
                
                <span class="abstract-full" style="display: none;">Medical large vision-language models (LVLMs) have demonstrated promising performance across various single-image question answering (QA) benchmarks, yet their capability in processing multi-image clinical scenarios remains underexplored. Unlike single image based tasks, medical tasks involving multiple images often demand sophisticated visual understanding capabilities, such as temporal reasoning and cross-modal analysis, which are poorly supported by current medical LVLMs. To bridge this critical gap, we present the Med-MIM instruction dataset, comprising 83.2K medical multi-image QA pairs that span four types of multi-image visual abilities (temporal understanding, reasoning, comparison, co-reference). Using this dataset, we fine-tune Mantis and LLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and Med-Mantis, both optimized for multi-image analysis. Additionally, we develop the Med-MIM benchmark to comprehensively evaluate the medical multi-image understanding capabilities of LVLMs. We assess eight popular LVLMs, including our two models, on the Med-MIM benchmark. Experimental results show that both Med-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and held-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM instruction dataset effectively enhances LVLMs' multi-image understanding capabilities in the medical domain.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 8.9%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.1%">
                            Medicine
                        </span>
                <!-- Computer Vision: 3.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4816
                </span>
                <a href="https://arxiv.org/abs/2505.19614" target="_blank" rel="noopener noreferrer">Multiplicity is an Inevitable and Inherent Challenge in Multimodal Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sanghyuk Chun
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multimodal learning has seen remarkable progress, particularly with the emergence of large-scale pre-training across various modalities. However, most current approaches are built on the assumption of a deterministic, one-to-one alignment between modalities. This oversimplifies real-world multimodal</span>
                
                <span class="abstract-full" style="display: none;">Multimodal learning has seen remarkable progress, particularly with the emergence of large-scale pre-training across various modalities. However, most current approaches are built on the assumption of a deterministic, one-to-one alignment between modalities. This oversimplifies real-world multimodal relationships, where their nature is inherently many-to-many. This phenomenon, named multiplicity, is not a side-effect of noise or annotation error, but an inevitable outcome of semantic abstraction, representational asymmetry, and task-dependent ambiguity in multimodal tasks. This position paper argues that multiplicity is a fundamental bottleneck that manifests across all stages of the multimodal learning pipeline: from data construction to training and evaluation. This paper examines the causes and consequences of multiplicity, and highlights how multiplicity introduces training uncertainty, unreliable evaluation, and low dataset quality. This position calls for new research directions on multimodal learning: novel multiplicity-aware learning frameworks and dataset construction protocols considering multiplicity.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.9%">
                            Medicine
                        </span>
                <!-- LLMs: 3.4 -->
                    
                <!-- Blockchain: 2.7 -->
                    
                <!-- Datasets: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5107
                </span>
                <a href="https://arxiv.org/abs/2505.18995" target="_blank" rel="noopener noreferrer">FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Carlos Jude G. Maminta (Institution College of Computer, Information Sciences, Polytechnic University of the Philippines, Sta. Mesa, Manila), Isaiah Job Enriquez (Institution College of Computer, Information Sciences, Polytechnic University of the Philippines, Sta. Mesa, Manila), Deandre Nigel Nunez (Institution College of Computer, Information Sciences, Polytechnic University of the Philippines, Sta. Mesa, Manila), Michael B. Dela Fuente (Institution College of Computer, Information Sciences, Polytechnic University of the Philippines, Sta. Mesa, Manila)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study presents FiLLM, a Filipino-optimized large language model, designed to enhance natural language processing (NLP) capabilities in the Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintainin</span>
                
                <span class="abstract-full" style="display: none;">This study presents FiLLM, a Filipino-optimized large language model, designed to enhance natural language processing (NLP) capabilities in the Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining task-specific performance. The model was trained and evaluated on diverse Filipino datasets to address key NLP tasks, including Named Entity Recognition (NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text Summarization. Performance comparisons with the CalamanCy model were conducted using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap metrics. Results indicate that Calamancy outperforms FILLM in several aspects, demonstrating its effectiveness in processing Filipino text with improved linguistic comprehension and adaptability. This research contributes to the advancement of Filipino NLP applications by providing an optimized, efficient, and scalable language model tailored for local linguistic needs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 12.1%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.1%">
                            Medicine
                        </span>
                <!-- Federated Learning: 3.3 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5715
                </span>
                <a href="https://arxiv.org/abs/2505.19641" target="_blank" rel="noopener noreferrer">SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for </span>
                
                <span class="abstract-full" style="display: none;">Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 9.1%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.0%">
                            Medicine
                        </span>
                <!-- Hardware: 2.3 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- Datasets: 2.1 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6674
                </span>
                <a href="https://arxiv.org/abs/2505.19157" target="_blank" rel="noopener noreferrer">Efficient and robust solvers for a cell-by-cell dual-poroelasticity problem</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Marius Causemann, Miroslav Kuchta
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents a scalable and robust solver for a cell-by-cell dual-poroelasticity model, describing the mechanical interactions between brains cells embedded in extracellular space. Explicitly representing the complex cellular shapes, the proposed approach models both intracellular and extrace</span>
                
                <span class="abstract-full" style="display: none;">This paper presents a scalable and robust solver for a cell-by-cell dual-poroelasticity model, describing the mechanical interactions between brains cells embedded in extracellular space. Explicitly representing the complex cellular shapes, the proposed approach models both intracellular and extracellular spaces as distinct poroelastic media, separated by a permeable cell membrane which allows hydrostatic and osmotic pressure-driven fluid exchange. The solver, which employs a three-field (displacement, total pressure, and fluid pressure) formulation, leverages the framework of norm-equivalent preconditioning and appropriately fitted norms to ensure robustness across all material parameters of the model. Scalability for large and complex geometries is achieved through efficient Algebraic Multigrid (AMG) approximations of the preconditioners' individual blocks. Furthermore, we accommodate diverse boundary conditions, including full Dirichlet boundary conditions for displacement, which we handle efficiently using the Sherman-Morrison-Woodbury formula. Numerical experiments demonstrate the preconditioners' robustness and performance across various parameters relevant to realistic scenarios, and a large scale example of cellular swelling on a dense reconstruction of the mouse visual cortex highlights the method's potential for investigating complex physiological processes like cellular volume regulation in detailed biological structures.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.6%">
                            Medicine
                        </span>
                <!-- Hardware: 2.5 -->
                    
                <!-- LLMs: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7129
                </span>
                <a href="https://arxiv.org/abs/2504.05634" target="_blank" rel="noopener noreferrer">Simplifying Data Integration: SLM-Driven Systems for Unified Semantic Queries Across Heterogeneous Databases</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Teng Lin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The integration of heterogeneous databases into a unified querying framework remains a critical challenge, particularly in resource-constrained environments. This paper presents a novel Small Language Model(SLM)-driven system that synergizes advancements in lightweight Retrieval-Augmented Generation</span>
                
                <span class="abstract-full" style="display: none;">The integration of heterogeneous databases into a unified querying framework remains a critical challenge, particularly in resource-constrained environments. This paper presents a novel Small Language Model(SLM)-driven system that synergizes advancements in lightweight Retrieval-Augmented Generation (RAG) and semantic-aware data structuring to enable efficient, accurate, and scalable query resolution across diverse data formats. By integrating MiniRAG's semantic-aware heterogeneous graph indexing and topology-enhanced retrieval with SLM-powered structured data extraction, our system addresses the limitations of traditional methods in handling Multi-Entity Question Answering (Multi-Entity QA) and complex semantic queries. Experimental results demonstrate superior performance in accuracy and efficiency, while the introduction of semantic entropy as an unsupervised evaluation metric provides robust insights into model uncertainty. This work pioneers a cost-effective, domain-agnostic solution for next-generation database systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 7.9%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 7.2%">
                            Medicine
                        </span>
                <!-- Hardware: 2.1 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Datasets: 1.8 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8952
                </span>
                <a href="https://arxiv.org/abs/2505.19213" target="_blank" rel="noopener noreferrer">Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shaohao Rui, Kaitao Chen, Weijie Ma, Xiaosong Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advances in reinforcement learning with verifiable, rule-based rewards have greatly enhanced the reasoning capabilities and out-of-distribution generalization of VLMs/LLMs, obviating the need for manually crafted reasoning chains. Despite these promising developments in the general domain, th</span>
                
                <span class="abstract-full" style="display: none;">Recent advances in reinforcement learning with verifiable, rule-based rewards have greatly enhanced the reasoning capabilities and out-of-distribution generalization of VLMs/LLMs, obviating the need for manually crafted reasoning chains. Despite these promising developments in the general domain, their translation to medical imaging remains limited. Current medical reinforcement fine-tuning (RFT) methods predominantly focus on close-ended VQA, thereby restricting the model's ability to engage in world knowledge retrieval and flexible task adaptation. More critically, these methods fall short of addressing the critical clinical demand for open-ended, reasoning-intensive decision-making. To bridge this gap, we introduce \textbf{MedCCO}, the first multimodal reinforcement learning framework tailored for medical VQA that unifies close-ended and open-ended data within a curriculum-driven RFT paradigm. Specifically, MedCCO is initially fine-tuned on a diverse set of close-ended medical VQA tasks to establish domain-grounded reasoning capabilities, and is then progressively adapted to open-ended tasks to foster deeper knowledge enhancement and clinical interpretability. We validate MedCCO across eight challenging medical VQA benchmarks, spanning both close-ended and open-ended settings. Experimental results show that MedCCO consistently enhances performance and generalization, achieving a 11.4\% accuracy gain across three in-domain tasks, and a 5.7\% improvement on five out-of-domain benchmarks. These findings highlight the promise of curriculum-guided RL in advancing robust, clinically-relevant reasoning in medical multimodal language models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 8.6%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 6.8%">
                            LLMs
                        </span>
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8959
                </span>
                <a href="https://arxiv.org/abs/2411.12919" target="_blank" rel="noopener noreferrer">Robust multi-coil MRI reconstruction via self-supervised denoising</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Asad Aali, Marius Arvinte, Sidharth Kumar, Yamin I. Arefeen, Jonathan I. Tamir
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the effect of incorporating self-supervised denoising as a pre-processing step for training deep learning (DL) based reconstruction methods on data corrupted by Gaussian noise. K-space data employed for training are typically multi-coil and inherently noisy. Although DL-based reconstruction</span>
                
                <span class="abstract-full" style="display: none;">We study the effect of incorporating self-supervised denoising as a pre-processing step for training deep learning (DL) based reconstruction methods on data corrupted by Gaussian noise. K-space data employed for training are typically multi-coil and inherently noisy. Although DL-based reconstruction methods trained on fully sampled data can enable high reconstruction quality, obtaining large, noise-free datasets is impractical. We leverage Generalized Stein's Unbiased Risk Estimate (GSURE) for denoising. We evaluate two DL-based reconstruction methods: Diffusion Probabilistic Models (DPMs) and Model-Based Deep Learning (MoDL). We evaluate the impact of denoising on the performance of these DL-based methods in solving accelerated multi-coil magnetic resonance imaging (MRI) reconstruction. The experiments were carried out on T2-weighted brain and fat-suppressed proton-density knee scans. We observed that self-supervised denoising enhances the quality and efficiency of MRI reconstructions across various scenarios. Specifically, employing denoised images rather than noisy counterparts when training DL networks results in lower normalized root mean squared error (NRMSE), higher structural similarity index measure (SSIM) and peak signal-to-noise ratio (PSNR) across different SNR levels, including 32dB, 22dB, and 12dB for T2-weighted brain data, and 24dB, 14dB, and 4dB for fat-suppressed knee data. Overall, we showed that denoising is an essential pre-processing technique capable of improving the efficacy of DL-based MRI reconstruction methods under diverse conditions. By refining the quality of input data, denoising enables training more effective DL networks, potentially bypassing the need for noise-free reference MRI scans.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 8.8%">
                            Medicine
                        </span>
                <!-- Hardware: 2.5 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.7 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9096
                </span>
                <a href="https://arxiv.org/abs/2505.19445" target="_blank" rel="noopener noreferrer">MetaGMT: Improving Actionable Interpretability of Graph Multilinear Networks via Meta-Learning Filtration</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rishabh Bhattacharya, Hari Shankar, Vaishnavi Shivkumar, Ponnurangam Kumaraguru
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The growing adoption of Graph Neural Networks (GNNs) in high-stakes domains like healthcare and finance demands reliable explanations of their decision-making processes. While inherently interpretable GNN architectures like Graph Multi-linear Networks (GMT) have emerged, they remain vulnerable to ge</span>
                
                <span class="abstract-full" style="display: none;">The growing adoption of Graph Neural Networks (GNNs) in high-stakes domains like healthcare and finance demands reliable explanations of their decision-making processes. While inherently interpretable GNN architectures like Graph Multi-linear Networks (GMT) have emerged, they remain vulnerable to generating explanations based on spurious correlations, potentially undermining trust in critical applications. We present MetaGMT, a meta-learning framework that enhances explanation fidelity through a novel bi-level optimization approach. We demonstrate that MetaGMT significantly improves both explanation quality (AUC-ROC, Precision@K) and robustness to spurious patterns, across BA-2Motifs, MUTAG, and SP-Motif benchmarks. Our approach maintains competitive classification accuracy while producing more faithful explanations (with an increase up to 8% of Explanation ROC on SP-Motif 0.5) compared to baseline methods. These advancements in interpretability could enable safer deployment of GNNs in sensitive domains by (1) facilitating model debugging through more reliable explanations, (2) supporting targeted retraining when biases are identified, and (3) enabling meaningful human oversight. By addressing the critical challenge of explanation reliability, our work contributes to building more trustworthy and actionable GNN systems for real-world applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 10.0%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 5.5%">
                            GNN
                        </span>
                <!-- Medicine: 2.2 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9423
                </span>
                <a href="https://arxiv.org/abs/2505.20033" target="_blank" rel="noopener noreferrer">EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Maurice Kraus, Felix Friedrich, Huu Nguyen, Krishna Kalyan, Kourosh Nadi, Kristian Kersting, S\"oren Auer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Effective human-AI interaction relies on AI's ability to accurately perceive and interpret human emotions. Current benchmarks for vision and vision-language models are severely limited, offering a narrow emotional spectrum that overlooks nuanced states (e.g., bitterness, intoxication) and fails to d</span>
                
                <span class="abstract-full" style="display: none;">Effective human-AI interaction relies on AI's ability to accurately perceive and interpret human emotions. Current benchmarks for vision and vision-language models are severely limited, offering a narrow emotional spectrum that overlooks nuanced states (e.g., bitterness, intoxication) and fails to distinguish subtle differences between related feelings (e.g., shame vs. embarrassment). Existing datasets also often use uncontrolled imagery with occluded faces and lack demographic diversity, risking significant bias. To address these critical gaps, we introduce EmoNet Face, a comprehensive benchmark suite. EmoNet Face features: (1) A novel 40-category emotion taxonomy, meticulously derived from foundational research to capture finer details of human emotional experiences. (2) Three large-scale, AI-generated datasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and controlled demographic balance across ethnicity, age, and gender. (3) Rigorous, multi-expert annotations for training and high-fidelity evaluation. (4) We build Empathic Insight Face, a model achieving human-expert-level performance on our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets, and model - provides a robust foundation for developing and evaluating AI systems with a deeper understanding of human emotions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 8.8%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 7.1%">
                            LLMs
                        </span>
                <!-- Computer Vision: 2.5 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.0091
                </span>
                <a href="https://arxiv.org/abs/2505.19024" target="_blank" rel="noopener noreferrer">Learn Beneficial Noise as Graph Augmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Siqi Huang, Yanchen Xu, Hongyuan Zhang, Xuelong Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Although graph contrastive learning (GCL) has been widely investigated, it is still a challenge to generate effective and stable graph augmentations. Existing methods often apply heuristic augmentation like random edge dropping, which may disrupt important graph structures and result in unstable GCL</span>
                
                <span class="abstract-full" style="display: none;">Although graph contrastive learning (GCL) has been widely investigated, it is still a challenge to generate effective and stable graph augmentations. Existing methods often apply heuristic augmentation like random edge dropping, which may disrupt important graph structures and result in unstable GCL performance. In this paper, we propose Positive-incentive Noise driven Graph Data Augmentation (PiNGDA), where positive-incentive noise (pi-noise) scientifically analyzes the beneficial effect of noise under the information theory. To bridge the standard GCL and pi-noise framework, we design a Gaussian auxiliary variable to convert the loss function to information entropy. We prove that the standard GCL with pre-defined augmentations is equivalent to estimate the beneficial noise via the point estimation. Following our analysis, PiNGDA is derived from learning the beneficial noise on both topology and attributes through a trainable noise generator for graph augmentations, instead of the simple estimation. Since the generator learns how to produce beneficial perturbations on graph topology and node attributes, PiNGDA is more reliable compared with the existing methods. Extensive experimental results validate the effectiveness and stability of PiNGDA.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 5.2%">
                            GNN
                        </span>
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Medicine: 2.2 -->
                    
                <!-- LLMs: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1427
                </span>
                <a href="https://arxiv.org/abs/2505.18906" target="_blank" rel="noopener noreferrer">Federated Retrieval-Augmented Generation: A Systematic Mapping Study</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Abhijit Chakraborty, Chahana Dahal, Vivek Gupta
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Federated Retrieval-Augmented Generation (Federated RAG) combines Federated Learning (FL), which enables distributed model training without exposing raw data, with Retrieval-Augmented Generation (RAG), which improves the factual accuracy of language models by grounding outputs in external knowledge.</span>
                
                <span class="abstract-full" style="display: none;">Federated Retrieval-Augmented Generation (Federated RAG) combines Federated Learning (FL), which enables distributed model training without exposing raw data, with Retrieval-Augmented Generation (RAG), which improves the factual accuracy of language models by grounding outputs in external knowledge. As large language models are increasingly deployed in privacy-sensitive domains such as healthcare, finance, and personalized assistance, Federated RAG offers a promising framework for secure, knowledge-intensive natural language processing (NLP). To the best of our knowledge, this paper presents the first systematic mapping study of Federated RAG, covering literature published between 2020 and 2025. Following Kitchenham's guidelines for evidence-based software engineering, we develop a structured classification of research focuses, contribution types, and application domains. We analyze architectural patterns, temporal trends, and key challenges, including privacy-preserving retrieval, cross-client heterogeneity, and evaluation limitations. Our findings synthesize a rapidly evolving body of research, identify recurring design patterns, and surface open questions, providing a foundation for future work at the intersection of RAG and federated systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 12.8%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.0%">
                            Medicine
                        </span>
                <!-- Federated Learning: 3.0 -->
                    
                <!-- Hardware: 2.9 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Datasets: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2355
                </span>
                <a href="https://arxiv.org/abs/2505.19077" target="_blank" rel="noopener noreferrer">An Autocovariance Least-Squares-Based Data-Driven Kalman Filter for Unknown Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Suyang Hu, Xiaoxu Lyu, Peihu Duan, Dawei Shi, Ling Shi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This article investigates the problem of data-driven state estimation for linear systems with both unknown system dynamics and noise covariances. We propose an Autocovariance Least-squares-based Data-driven Kalman Filter (ADKF), which provides a unified framework for simultaneous system identificati</span>
                
                <span class="abstract-full" style="display: none;">This article investigates the problem of data-driven state estimation for linear systems with both unknown system dynamics and noise covariances. We propose an Autocovariance Least-squares-based Data-driven Kalman Filter (ADKF), which provides a unified framework for simultaneous system identification and state estimation by utilizing pre-collected input-output trajectories and estimated initial states. Specifically, we design a SDP-based algorithm for estimating the noise covariances. We quantify the impact of model inaccuracy on noise covariances estimation using this identification algorithm, and introduce a feedback control mechanism for data collection to enhance the accuracy and stability of noise covariance estimation. The estimated noise covariances account for model inaccuracy, which are shown to be more suitable for state estimation. We also quantify the performance gap between the ADKF and the traditional Kalman filter with known system dynamics and noise covariances, showing that this gap decreases as the number and length of pre-collected trajectories increase. Finally, numerical simulations validate the robustness and effectiveness of the proposed ADKF.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 9.7%">
                            Medicine
                        </span>
                <!-- Computer Vision: 2.9 -->
                    
                <!-- Hardware: 2.6 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- LLMs: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.254
                </span>
                <a href="https://arxiv.org/abs/2505.19562" target="_blank" rel="noopener noreferrer">AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ying Xiao, Jie Huang, Ruijuan He, Jing Xiao, Mohammad Reza Mousavi, Yepang Liu, Kezhi Li, Zhenpeng Chen, Jie M. Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language models (LLMs) are reaching expert-level accuracy on medical diagnosis questions, yet their mistakes and the biases behind them pose life-critical risks. Bias linked to race, sex, and socioeconomic status is already well known, but a consistent and automatic testbed for measuring it is</span>
                
                <span class="abstract-full" style="display: none;">Large language models (LLMs) are reaching expert-level accuracy on medical diagnosis questions, yet their mistakes and the biases behind them pose life-critical risks. Bias linked to race, sex, and socioeconomic status is already well known, but a consistent and automatic testbed for measuring it is missing. To fill this gap, this paper presents AMQA -- an Adversarial Medical Question-Answering dataset -- built for automated, large-scale bias evaluation of LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the United States Medical Licensing Examination (USMLE) dataset, generated using a multi-agent framework to create diverse adversarial descriptions and question pairs. Using AMQA, we benchmark five representative LLMs and find surprisingly substantial disparities: even GPT-4.1, the least biased model tested, answers privileged-group questions over 10 percentage points more accurately than unprivileged ones. Compared with the existing benchmark CPV, AMQA reveals 15% larger accuracy gaps on average between privileged and unprivileged groups. Our dataset and code are publicly available at https://github.com/XY-Showing/AMQA to support reproducible research and advance trustworthy, bias-aware medical AI.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 13.9%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 8.6%">
                            Medicine
                        </span>
                <!-- Computer Vision: 2.7 -->
                    
                <!-- Datasets: 2.3 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4673
                </span>
                <a href="https://arxiv.org/abs/2501.17758" target="_blank" rel="noopener noreferrer">Glioma Multimodal MRI Analysis System for Tumor Layered Diagnosis via Multi-task Semi-supervised Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yihao Liu, Zhihao Cui, Liming Li, Junjie You, Xinle Feng, Jianxin Wang, Xiangyu Wang, Qing Liu, Minghua Wu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Gliomas are the most common primary tumors of the central nervous system. Multimodal MRI is widely used for the preliminary screening of gliomas and plays a crucial role in auxiliary diagnosis, therapeutic efficacy, and prognostic evaluation. Currently, the computer-aided diagnostic studies of gliom</span>
                
                <span class="abstract-full" style="display: none;">Gliomas are the most common primary tumors of the central nervous system. Multimodal MRI is widely used for the preliminary screening of gliomas and plays a crucial role in auxiliary diagnosis, therapeutic efficacy, and prognostic evaluation. Currently, the computer-aided diagnostic studies of gliomas using MRI have focused on independent analysis events such as tumor segmentation, grading, and radiogenomic classification, without studying inter-dependencies among these events. In this study, we propose a Glioma Multimodal MRI Analysis System (GMMAS) that utilizes a deep learning network for processing multiple events simultaneously, leveraging their inter-dependencies through an uncertainty-based multi-task learning architecture and synchronously outputting tumor region segmentation, glioma histological subtype, IDH mutation genotype, and 1p/19q chromosome disorder status. Compared with the reported single-task analysis models, GMMAS improves the precision across tumor layered diagnostic tasks. Additionally, we have employed a two-stage semi-supervised learning method, enhancing model performance by fully exploiting both labeled and unlabeled MRI samples. Further, by utilizing an adaptation module based on knowledge self-distillation and contrastive learning for cross-modal feature extraction, GMMAS exhibited robustness in situations of modality absence and revealed the differing significance of each MRI modal. Finally, based on the analysis outputs of the GMMAS, we created a visual and user-friendly platform for doctors and patients, introducing GMMAS-GPT to generate personalized prognosis evaluations and suggestions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 9.0%">
                            Medicine
                        </span>
                <!-- LLMs: 3.1 -->
                    
                <!-- Computer Vision: 2.7 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Hardware: 2.4 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4783
                </span>
                <a href="https://arxiv.org/abs/2505.18927" target="_blank" rel="noopener noreferrer">Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Amel Muminovic (International Balkan University)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from hi</span>
                
                <span class="abstract-full" style="display: none;">As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse threads in gaming, lifestyle, food vlog, and music channels. The dataset comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and Indonesian, annotated independently by two reviewers with substantial agreement (Cohen's kappa = 0.83). Using a unified prompt and deterministic settings, GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful posts (recall = 0.875) but its precision fell to 0.767 due to frequent false positives. Claude delivered the highest precision at 0.920 and the lowest false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative analysis showed that all three models struggle with sarcasm, coded insults, and mixed-language slang. These results underscore the need for moderation pipelines that combine complementary models, incorporate conversational context, and fine-tune for under-represented languages and implicit abuse. A de-identified version of the dataset and full prompts is publicly released to promote reproducibility and further progress in automated content moderation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 13.7%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 7.8%">
                            Medicine
                        </span>
                <!-- Datasets: 2.5 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.5688
                </span>
                <a href="https://arxiv.org/abs/2303.13111" target="_blank" rel="noopener noreferrer">Boosting Convolution with Efficient MLP-Permutation for Volumetric Medical Image Segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yi Lin, Xiao Fang, Dong Zhang, Kwang-Ting Cheng, Hao Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recently, the advent of vision Transformer (ViT) has brought substantial advancements in 3D dataset benchmarks, particularly in 3D volumetric medical image segmentation (Vol-MedSeg). Concurrently, multi-layer perceptron (MLP) network has regained popularity among researchers due to their comparable </span>
                
                <span class="abstract-full" style="display: none;">Recently, the advent of vision Transformer (ViT) has brought substantial advancements in 3D dataset benchmarks, particularly in 3D volumetric medical image segmentation (Vol-MedSeg). Concurrently, multi-layer perceptron (MLP) network has regained popularity among researchers due to their comparable results to ViT, albeit with the exclusion of the resource-intensive self-attention module. In this work, we propose a novel permutable hybrid network for Vol-MedSeg, named PHNet, which capitalizes on the strengths of both convolution neural networks (CNNs) and MLP. PHNet addresses the intrinsic isotropy problem of 3D volumetric data by employing a combination of 2D and 3D CNNs to extract local features. Besides, we propose an efficient multi-layer permute perceptron (MLPP) module that captures long-range dependence while preserving positional information. This is achieved through an axis decomposition operation that permutes the input tensor along different axes, thereby enabling the separate encoding of the positional information. Furthermore, MLPP tackles the resolution sensitivity issue of MLP in Vol-MedSeg with a token segmentation operation, which divides the feature into smaller tokens and processes them individually. Extensive experimental results validate that PHNet outperforms the state-of-the-art methods with lower computational costs on the widely-used yet challenging COVID-19-20 and Synapse benchmarks. The ablation study also demonstrates the effectiveness of PHNet in harnessing the strengths of both CNNs and MLP. The code is available on Github: \href{https://github.com/xiaofang007/PHNet}{https://github.com/xiaofang007/PHNet}.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 9.4%">
                            Medicine
                        </span>
                <!-- Federated Learning: 4.1 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7015
                </span>
                <a href="https://arxiv.org/abs/2505.19389" target="_blank" rel="noopener noreferrer">Curation and Analysis of MIMICEL -- An Event Log for MIMIC-IV Emergency Department</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jia Wei, Chun Ouyang, Bemali Wickramanayake, Zhipeng He, Keshara Perera, Catarina Moreira
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The global issue of overcrowding in emergency departments (ED) necessitates the analysis of patient flow through ED to enhance efficiency and alleviate overcrowding. However, traditional analytical methods are time-consuming and costly. The healthcare industry is embracing process mining tools to an</span>
                
                <span class="abstract-full" style="display: none;">The global issue of overcrowding in emergency departments (ED) necessitates the analysis of patient flow through ED to enhance efficiency and alleviate overcrowding. However, traditional analytical methods are time-consuming and costly. The healthcare industry is embracing process mining tools to analyse healthcare processes and patient flows. Process mining aims to discover, monitor, and enhance processes by obtaining knowledge from event log data. However, the availability of event logs is a prerequisite for applying process mining techniques. Hence, this paper aims to generate an event log for analysing processes in ED. In this study, we extract an event log from the MIMIC-IV-ED dataset and name it MIMICEL. MIMICEL captures the process of patient journey in ED, allowing for analysis of patient flows and improving ED efficiency. We present analyses conducted using MIMICEL to demonstrate the utility of the dataset. The curation of MIMICEL facilitates extensive use of MIMIC-IV-ED data for ED analysis using process mining techniques, while also providing the process mining research communities with a valuable dataset for study.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 9.9%">
                            Medicine
                        </span>
                <!-- LLMs: 4.1 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Datasets: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- GNN: 1.0 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.74
                </span>
                <a href="https://arxiv.org/abs/2505.20099" target="_blank" rel="noopener noreferrer">Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chuangtao Ma, Yongrui Chen, Tianxing Wu, Arijit Khan, Haofen Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language models (LLMs) have demonstrated remarkable performance on question-answering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, an</span>
                
                <span class="abstract-full" style="display: none;">Large language models (LLMs) have demonstrated remarkable performance on question-answering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, and hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs) for QA to address the above challenges. In this survey, we propose a new structured taxonomy that categorizes the methodology of synthesizing LLMs and KGs for QA according to the categories of QA and the KG's role when integrating with LLMs. We systematically survey state-of-the-art advances in synthesizing LLMs and KGs for QA and compare and analyze these approaches in terms of strength, limitations, and KG requirements. We then align the approaches with QA and discuss how these approaches address the main challenges of different complex QA. Finally, we summarize the advancements, evaluation metrics, and benchmark datasets and highlight open challenges and opportunities.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 37.6%">
                            LLMs
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 6.1%">
                            Medicine
                        </span>
                <!-- Hardware: 2.0 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3742
                </span>
                <a href="https://arxiv.org/abs/2505.19195" target="_blank" rel="noopener noreferrer">CardioCoT: Hierarchical Reasoning for Multimodal Survival Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shaohao Rui, Haoyang Su, Jinyi Xiang, Lian-Ming Wu, Xiaosong Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate prediction of major adverse cardiovascular events recurrence risk in acute myocardial infarction patients based on postoperative cardiac MRI and associated clinical notes is crucial for precision treatment and personalized intervention. Existing methods primarily focus on risk stratificatio</span>
                
                <span class="abstract-full" style="display: none;">Accurate prediction of major adverse cardiovascular events recurrence risk in acute myocardial infarction patients based on postoperative cardiac MRI and associated clinical notes is crucial for precision treatment and personalized intervention. Existing methods primarily focus on risk stratification capability while overlooking the need for intermediate robust reasoning and model interpretability in clinical practice. Moreover, end-to-end risk prediction using LLM/VLM faces significant challenges due to data limitations and modeling complexity. To bridge this gap, we propose CardioCoT, a novel two-stage hierarchical reasoning-enhanced survival analysis framework designed to enhance both model interpretability and predictive performance. In the first stage, we employ an evidence-augmented self-refinement mechanism to guide LLM/VLMs in generating robust hierarchical reasoning trajectories based on associated radiological findings. In the second stage, we integrate the reasoning trajectories with imaging data for risk model training and prediction. CardioCoT demonstrates superior performance in MACE recurrence risk prediction while providing interpretable reasoning processes, offering valuable insights for clinical decision-making.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 13.5%">
                            Medicine
                        </span>
                <!-- LLMs: 4.7 -->
                    
                <!-- Computer Vision: 4.3 -->
                    
                <!-- Hardware: 2.4 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.6672
                </span>
                <a href="https://arxiv.org/abs/2505.19762" target="_blank" rel="noopener noreferrer">Language Model-Enhanced Message Passing for Heterophilic Graph Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wenjun Wang, Dawei Cheng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Traditional graph neural networks (GNNs), which rely on homophily-driven message passing, struggle with heterophilic graphs where connected nodes exhibit dissimilar features and different labels. While existing methods address heterophily through graph structure refinement or adaptation of neighbor </span>
                
                <span class="abstract-full" style="display: none;">Traditional graph neural networks (GNNs), which rely on homophily-driven message passing, struggle with heterophilic graphs where connected nodes exhibit dissimilar features and different labels. While existing methods address heterophily through graph structure refinement or adaptation of neighbor aggregation functions, they often overlook the semantic potential of node text, rely on suboptimal message representation for propagation and compromise performance on homophilic graphs. To address these limitations, we propose a novel language model (LM)-enhanced message passing approach for heterophilic graph leaning (LEMP4HG). Specifically, in the context of text-attributed graph, we provide paired node texts for LM to generate their connection analysis, which are encoded and then fused with paired node textual embeddings through a gating mechanism. The synthesized messages are semantically enriched and adaptively balanced with both nodes' information, which mitigates contradictory signals when neighbor aggregation in heterophilic regions. Furthermore, we introduce an active learning strategy guided by our heuristic MVRD (Modulated Variation of Reliable Distance), selectively enhancing node pairs suffer most from message passing, reducing the cost of analysis generation and side effects on homophilic regions. Extensive experiments validate that our approach excels on heterophilic graphs and performs robustly on homophilic ones, with a graph convolutional network (GCN) backbone and a practical budget.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 5.5%">
                            GNN
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 5.5%">
                            Medicine
                        </span>
                <!-- LLMs: 3.7 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.6183
                </span>
                <a href="https://arxiv.org/abs/2505.19846" target="_blank" rel="noopener noreferrer">Zero-Shot Pseudo Labels Generation Using SAM and CLIP for Semi-Supervised Semantic Segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nagito Saito, Shintaro Ito, Koichi Ito, Takafumi Aoki
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Semantic segmentation is a fundamental task in medical image analysis and autonomous driving and has a problem with the high cost of annotating the labels required in training. To address this problem, semantic segmentation methods based on semi-supervised learning with a small number of labeled dat</span>
                
                <span class="abstract-full" style="display: none;">Semantic segmentation is a fundamental task in medical image analysis and autonomous driving and has a problem with the high cost of annotating the labels required in training. To address this problem, semantic segmentation methods based on semi-supervised learning with a small number of labeled data have been proposed. For example, one approach is to train a semantic segmentation model using images with annotated labels and pseudo labels. In this approach, the accuracy of the semantic segmentation model depends on the quality of the pseudo labels, and the quality of the pseudo labels depends on the performance of the model to be trained and the amount of data with annotated labels. In this paper, we generate pseudo labels using zero-shot annotation with the Segment Anything Model (SAM) and Contrastive Language-Image Pretraining (CLIP), improve the accuracy of the pseudo labels using the Unified Dual-Stream Perturbations Approach (UniMatch), and use them as enhanced labels to train a semantic segmentation model. The effectiveness of the proposed method is demonstrated through the experiments using the public datasets: PASCAL and MS COCO.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 15.2%">
                            Medicine
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #b3ebae" title="Confidence: 5.3%">
                            Federated Learning
                        </span>
                <!-- Bayesian Optimization: 3.2 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- LLMs: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.8727
                </span>
                <a href="https://arxiv.org/abs/2310.10187" target="_blank" rel="noopener noreferrer">An Interpretable Deep-Learning Framework for Predicting Hospital Readmissions From Electronic Health Records</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fabio Azzalini, Tommaso Dolci, Marco Vagaggini
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">With the increasing availability of patient data, modern medicine is shifting towards prospective healthcare. Electronic health records offer a variety of information useful for clinical patient characterization and the development of predictive models, given that similar medical histories often lea</span>
                
                <span class="abstract-full" style="display: none;">With the increasing availability of patient data, modern medicine is shifting towards prospective healthcare. Electronic health records offer a variety of information useful for clinical patient characterization and the development of predictive models, given that similar medical histories often lead to analogous health progressions. One application is the prediction of unplanned hospital readmissions, an essential task for reducing healthcare costs and improving patient outcomes. While predictive models demonstrate strong performances especially with deep learning approaches, they are often criticized for their lack of interpretability, a critical requirement in the medical domain where incorrect predictions may have severe consequences for patient safety. In this paper, we propose a novel and interpretable deep learning framework for predicting unplanned hospital readmissions, supported by NLP findings on word embeddings and by ConvLSTM neural networks for better handling temporal data. We validate the framework on two predictive tasks for hospital readmission within 30 and 180 days, using real-world data. Additionally, we introduce and evaluate a model-dependent technique designed to enhance result interpretability for medical professionals. Our solution outperforms traditional machine learning models in prediction accuracy while simultaneously providing more interpretable results.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 19.5%">
                            Medicine
                        </span>
                <!-- LLMs: 3.0 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Hardware: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.9373
                </span>
                <a href="https://arxiv.org/abs/2505.19188" target="_blank" rel="noopener noreferrer">Chordless Structure: A Pathway to Simple and Expressive GNNs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hongxu Pan, Shuxian Hu, Mo Zhou, Zhibin Wang, Rong Gu, Chen Tian, Kun Yang, Sheng Zhong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Researchers have proposed various methods of incorporating more structured information into the design of Graph Neural Networks (GNNs) to enhance their expressiveness. However, these methods are either computationally expensive or lacking in provable expressiveness. In this paper, we observe that th</span>
                
                <span class="abstract-full" style="display: none;">Researchers have proposed various methods of incorporating more structured information into the design of Graph Neural Networks (GNNs) to enhance their expressiveness. However, these methods are either computationally expensive or lacking in provable expressiveness. In this paper, we observe that the chords increase the complexity of the graph structure while contributing little useful information in many cases. In contrast, chordless structures are more efficient and effective for representing the graph. Therefore, when leveraging the information of cycles, we choose to omit the chords. Accordingly, we propose a Chordless Structure-based Graph Neural Network (CSGNN) and prove that its expressiveness is strictly more powerful than the k-hop GNN (KPGNN) with polynomial complexity. Experimental results on real-world datasets demonstrate that CSGNN outperforms existing GNNs across various graph tasks while incurring lower computational costs and achieving better performance than the GNNs of 3-WL expressiveness.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 11.8%">
                            GNN
                        </span>
                <!-- LLMs: 4.2 -->
                    
                <!-- Federated Learning: 3.1 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.1092
                </span>
                <a href="https://arxiv.org/abs/2505.19270" target="_blank" rel="noopener noreferrer">Effect of noise and topologies on multi-photon quantum protocols</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nitin Jha, Abhishek Parakh, Mahadevan Subramaniam
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Quantum-augmented networks aim to use quantum phenomena to improve detection and protection against malicious actors in a classical communication network. This may include multiplexing quantum signals into classical fiber optical channels and incorporating purely quantum links alongside classical li</span>
                
                <span class="abstract-full" style="display: none;">Quantum-augmented networks aim to use quantum phenomena to improve detection and protection against malicious actors in a classical communication network. This may include multiplexing quantum signals into classical fiber optical channels and incorporating purely quantum links alongside classical links in the network. In such hybrid networks, quantum protocols based on single photons become a bottleneck for transmission distances and data speeds, thereby reducing entire network performance. Furthermore, many of the security assumptions of the single-photon protocols do not hold up in practice because of the impossibility of manufacturing single-photon emitters. Multi-photon quantum protocols, on the other hand, are designed to operate under practical assumptions and do not require single photon emitters. As a result, they provide higher levels of security guarantees and longer transmission distances. However, the effect of channel and device noise on multiphoton protocols in terms of security, transmission distances, and bit rates has not been investigated. In this paper, we focus on channel noise and present our observations on the effect of various types of noise on multi-photon protocols. We also investigate the effect of topologies such as ring, star, and torus on the noise characteristics of the multi-photon protocols. Our results show the possible advantages of switching to multi-photon protocols and give insights into the repeater placement and topology choice for quantum-augmented networks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 6.4%">
                            Quantum Computing
                        </span>
                <!-- Medicine: 3.3 -->
                    
                <!-- Hardware: 3.3 -->
                    
                <!-- Blockchain: 3.2 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- LLMs: 1.9 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.4669
                </span>
                <a href="https://arxiv.org/abs/2406.17441" target="_blank" rel="noopener noreferrer">A Matrix Product State Model for Simultaneous Classification and Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alex Mossi, Bojan \v{Z}unkovic, Kyriakos Flouris
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Quantum machine learning (QML) is a rapidly expanding field that merges the principles of quantum computing with the techniques of machine learning. One of the powerful mathematical frameworks in this domain is tensor networks. These networks are used to approximate high-order tensors by contracting</span>
                
                <span class="abstract-full" style="display: none;">Quantum machine learning (QML) is a rapidly expanding field that merges the principles of quantum computing with the techniques of machine learning. One of the powerful mathematical frameworks in this domain is tensor networks. These networks are used to approximate high-order tensors by contracting tensors with lower ranks. Initially developed for simulating quantum systems, tensor networks have become integral to quantum computing and, by extension, to QML. Drawing inspiration from these quantum methods, specifically the Matrix Product States (MPS), we apply them in a classical machine learning setting. Their ability to efficiently represent and manipulate complex, high-dimensional data makes them effective in a supervised learning framework. Here, we present an MPS model, in which the MPS functions as both a classifier and a generator. The dual functionality of this novel MPS model permits a strategy that enhances the traditional training of supervised MPS models. This framework is inspired by generative adversarial networks and is geared towards generating more realistic samples by reducing outliers. In addition, our contributions offer insights into the mechanics of tensor network methods for generation tasks. Specifically, we discuss alternative embedding functions and a new sampling method from non-normalized MPSs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 8.9%">
                            Quantum Computing
                        </span>
                <!-- Medicine: 4.2 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.0212
                </span>
                <a href="https://arxiv.org/abs/2503.08962" target="_blank" rel="noopener noreferrer">On the status of current quantum machine learning software</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Manish K. Gupta, Tomasz Rybotycki, Piotr Gawron
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The recent advancements in noisy intermediate-scale quantum (NISQ) devices implementation allow us to study their application to real-life computational problems. However, hardware challenges are not the only ones that hinder our quantum computation capabilities. Software limitations are the other, </span>
                
                <span class="abstract-full" style="display: none;">The recent advancements in noisy intermediate-scale quantum (NISQ) devices implementation allow us to study their application to real-life computational problems. However, hardware challenges are not the only ones that hinder our quantum computation capabilities. Software limitations are the other, less explored side of this medal. Using satellite image segmentation as a task example, we investigated how difficult it is to run a hybrid quantum-classical model on a real, publicly available quantum device. We also analyzed the costs of such endeavor and the change in quality of model.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 11.3%">
                            Quantum Computing
                        </span>
                <!-- LLMs: 3.0 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Medicine: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Cryptography: 1.1 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.1522
                </span>
                <a href="https://arxiv.org/abs/2501.18739" target="_blank" rel="noopener noreferrer">Beyond Message Passing: Neural Graph Pattern Machine</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zehong Wang, Zheyuan Zhang, Tianyi Ma, Nitesh V Chawla, Chuxu Zhang, Yanfang Ye
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph learning tasks often hinge on identifying key substructure patterns -- such as triadic closures in social networks or benzene rings in molecular graphs -- that underpin downstream performance. However, most existing graph neural networks (GNNs) rely on message passing, which aggregates local n</span>
                
                <span class="abstract-full" style="display: none;">Graph learning tasks often hinge on identifying key substructure patterns -- such as triadic closures in social networks or benzene rings in molecular graphs -- that underpin downstream performance. However, most existing graph neural networks (GNNs) rely on message passing, which aggregates local neighborhood information iteratively and struggles to explicitly capture such fundamental motifs, like triangles, k-cliques, and rings. This limitation hinders both expressiveness and long-range dependency modeling. In this paper, we introduce the Neural Graph Pattern Machine (GPM), a novel framework that bypasses message passing by learning directly from graph substructures. GPM efficiently extracts, encodes, and prioritizes task-relevant graph patterns, offering greater expressivity and improved ability to capture long-range dependencies. Empirical evaluations across four standard tasks -- node classification, link prediction, graph classification, and graph regression -- demonstrate that GPM outperforms state-of-the-art baselines. Further analysis reveals that GPM exhibits strong out-of-distribution generalization, desirable scalability, and enhanced interpretability. Code and datasets are available at: https://github.com/Zehong-Wang/GPM.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 14.6%">
                            GNN
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 7.6%">
                            LLMs
                        </span>
                <!-- Computer Vision: 2.6 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- Blockchain: 2.4 -->
                    
                <!-- HPO and AutoML: 2.2 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Decision Trees: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.2404
                </span>
                <a href="https://arxiv.org/abs/2411.11197" target="_blank" rel="noopener noreferrer">Stealing Training Graphs from Graph Neural Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Minhua Lin, Enyan Dai, Junjie Xu, Jinyuan Jia, Xiang Zhang, Suhang Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph Neural Networks (GNNs) have shown promising results in modeling graphs in various tasks. The training of GNNs, especially on specialized tasks such as bioinformatics, demands extensive expert annotations, which are expensive and usually contain sensitive information of data providers. The trai</span>
                
                <span class="abstract-full" style="display: none;">Graph Neural Networks (GNNs) have shown promising results in modeling graphs in various tasks. The training of GNNs, especially on specialized tasks such as bioinformatics, demands extensive expert annotations, which are expensive and usually contain sensitive information of data providers. The trained GNN models are often shared for deployment in the real world. As neural networks can memorize the training samples, the model parameters of GNNs have a high risk of leaking private training data. Our theoretical analysis shows the strong connections between trained GNN parameters and the training graphs used, confirming the training graph leakage issue. However, explorations into training data leakage from trained GNNs are rather limited. Therefore, we investigate a novel problem of stealing graphs from trained GNNs. To obtain high-quality graphs that resemble the target training set, a graph diffusion model with diffusion noise optimization is deployed as a graph generator. Furthermore, we propose a selection method that effectively leverages GNN model parameters to identify training graphs from samples generated by the graph diffusion model. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed framework in stealing training graphs from the trained GNN.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 16.2%">
                            GNN
                        </span>
                <!-- Medicine: 2.8 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- LLMs: 1.6 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.0683
                </span>
                <a href="https://arxiv.org/abs/2309.14326" target="_blank" rel="noopener noreferrer">Efficient Pauli channel estimation with logarithmic quantum memory</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sitan Chen, Weiyuan Gong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Here we revisit one of the prototypical tasks for characterizing the structure of noise in quantum devices: estimating every eigenvalue of an $n$-qubit Pauli noise channel to error $\epsilon$. Prior work [14] proved no-go theorems for this task in the practical regime where one has a limited amount </span>
                
                <span class="abstract-full" style="display: none;">Here we revisit one of the prototypical tasks for characterizing the structure of noise in quantum devices: estimating every eigenvalue of an $n$-qubit Pauli noise channel to error $\epsilon$. Prior work [14] proved no-go theorems for this task in the practical regime where one has a limited amount of quantum memory, e.g. any protocol with $\le 0.99n$ ancilla qubits of quantum memory must make exponentially many measurements, provided it is non-concatenating. Such protocols can only interact with the channel by repeatedly preparing a state, passing it through the channel, and measuring immediately afterward.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 13.3%">
                            Quantum Computing
                        </span>
                <!-- LLMs: 3.1 -->
                    
                <!-- Medicine: 3.1 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Cryptography: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.6767
                </span>
                <a href="https://arxiv.org/abs/2505.19006" target="_blank" rel="noopener noreferrer">A quantitative notion of economic security for smart contract compositions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Emily Priyadarshini, Massimo Bartoletti
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Decentralized applications are often composed of multiple interconnected smart contracts. This is especially evident in DeFi, where protocols are heavily intertwined and rely on a variety of basic building blocks such as tokens, decentralized exchanges and lending protocols. A crucial security chall</span>
                
                <span class="abstract-full" style="display: none;">Decentralized applications are often composed of multiple interconnected smart contracts. This is especially evident in DeFi, where protocols are heavily intertwined and rely on a variety of basic building blocks such as tokens, decentralized exchanges and lending protocols. A crucial security challenge in this setting arises when adversaries target individual components to cause systemic economic losses. Existing security notions focus on determining the existence of these attacks, but fail to quantify the effect of manipulating individual components on the overall economic security of the system. In this paper, we introduce a quantitative security notion that measures how an attack on a single component can amplify economic losses of the overall system. We study the fundamental properties of this notion and apply it to assess the security of key compositions. In particular, we analyse under-collateralized loan attacks in systems made of lending protocols and decentralized exchanges.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #3cc377" title="Confidence: 6.1%">
                            Blockchain
                        </span>
                <!-- LLMs: 3.9 -->
                    
                <!-- Federated Learning: 3.8 -->
                    
                <!-- Evolutionary Algorithms: 2.7 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Bayesian Optimization: 1.9 -->
                    
                <!-- Medicine: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -8.6533
                </span>
                <a href="https://arxiv.org/abs/2504.04041" target="_blank" rel="noopener noreferrer">Authenticated Sublinear Quantum Private Information Retrieval</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fengxia Liu, Zhiyong Zheng, Kun Tian, Yi Zhang, Heng Guo, Zhe Hu, Oleksiy Zhedanov, Zixian Gong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces a novel lower bound on communication complexity using quantum relative entropy and mutual information, refining previous classical entropy-based results. By leveraging Uhlmann's lemma and quantum Pinsker inequalities, the authors establish tighter bounds for information-theoret</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces a novel lower bound on communication complexity using quantum relative entropy and mutual information, refining previous classical entropy-based results. By leveraging Uhlmann's lemma and quantum Pinsker inequalities, the authors establish tighter bounds for information-theoretic security, demonstrating that quantum protocols inherently outperform classical counterparts in balancing privacy and efficiency. Also explores symmetric Quantum Private Information Retrieval (QPIR) protocols that achieve sub-linear communication complexity while ensuring robustness against specious adversaries: A post-quantum cryptography based protocol that can be authenticated for the specious server; A ring-LWE-based protocol for post-quantum security in a single-server setting, ensuring robustness against quantum attacks; A multi-server protocol optimized for hardware practicality, reducing implementation overhead while maintaining sub-linear efficiency. These protocols address critical gaps in secure database queries, offering exponential communication improvements over classical linear-complexity methods. The work also analyzes security trade-offs under quantum specious adversaries, providing theoretical guarantees for privacy and correctness.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 14.1%">
                            Quantum Computing
                        </span>
                <!-- LLMs: 4.1 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Decision Trees: 1.7 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Cryptography: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -8.9584
                </span>
                <a href="https://arxiv.org/abs/2505.16332" target="_blank" rel="noopener noreferrer">Is Quantum Optimization Ready? An Effort Towards Neural Network Compression using Adiabatic Quantum Computing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhehui Wanga, Benjamin Chen Ming Choonga, Tian Huang, Daniel Gerlinghoffa, Rick Siow Mong Goh, Cheng Liu, Tao Luo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Quantum optimization is the most mature quantum computing technology to date, providing a promising approach towards efficiently solving complex combinatorial problems. Methods such as adiabatic quantum computing (AQC) have been employed in recent years on important optimization problems across vari</span>
                
                <span class="abstract-full" style="display: none;">Quantum optimization is the most mature quantum computing technology to date, providing a promising approach towards efficiently solving complex combinatorial problems. Methods such as adiabatic quantum computing (AQC) have been employed in recent years on important optimization problems across various domains. In deep learning, deep neural networks (DNN) have reached immense sizes to support new predictive capabilities. Optimization of large-scale models is critical for sustainable deployment, but becomes increasingly challenging with ever-growing model sizes and complexity. While quantum optimization is suitable for solving complex problems, its application to DNN optimization is not straightforward, requiring thorough reformulation for compatibility with commercially available quantum devices. In this work, we explore the potential of adopting AQC for fine-grained pruning-quantization of convolutional neural networks. We rework established heuristics to formulate model compression as a quadratic unconstrained binary optimization (QUBO) problem, and assess the solution space offered by commercial quantum annealing devices. Through our exploratory efforts of reformulation, we demonstrate that AQC can achieve effective compression of practical DNN models. Experiments demonstrate that adiabatic quantum computing (AQC) not only outperforms classical algorithms like genetic algorithms and reinforcement learning in terms of time efficiency but also excels at identifying global optima.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 14.6%">
                            Quantum Computing
                        </span>
                <!-- Bayesian Optimization: 3.5 -->
                    
                <!-- Evolutionary Algorithms: 3.4 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- Federated Learning: 3.1 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- HPO and AutoML: 1.9 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.1385
                </span>
                <a href="https://arxiv.org/abs/2505.13525" target="_blank" rel="noopener noreferrer">Learning to Program Quantum Measurements for Machine Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Samuel Yen-Chi Chen, Huan-Hsin Tseng, Hsin-Yi Lin, Shinjae Yoo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rapid advancements in quantum computing (QC) and machine learning (ML) have sparked significant interest, driving extensive exploration of quantum machine learning (QML) algorithms to address a wide range of complex challenges. The development of high-performance QML models requires expert-level</span>
                
                <span class="abstract-full" style="display: none;">The rapid advancements in quantum computing (QC) and machine learning (ML) have sparked significant interest, driving extensive exploration of quantum machine learning (QML) algorithms to address a wide range of complex challenges. The development of high-performance QML models requires expert-level expertise, presenting a key challenge to the widespread adoption of QML. Critical obstacles include the design of effective data encoding strategies and parameterized quantum circuits, both of which are vital for the performance of QML models. Furthermore, the measurement process is often neglected-most existing QML models employ predefined measurement schemes that may not align with the specific requirements of the targeted problem. We propose an innovative framework that renders the observable of a quantum system-specifically, the Hermitian matrix-trainable. This approach employs an end-to-end differentiable learning framework, enabling simultaneous optimization of the neural network used to program the parameterized observables and the standard quantum circuit parameters. Notably, the quantum observable parameters are dynamically programmed by the neural network, allowing the observables to adapt in real time based on the input data stream. Through numerical simulations, we demonstrate that the proposed method effectively programs observables dynamically within variational quantum circuits, achieving superior results compared to existing approaches. Notably, it delivers enhanced performance metrics, such as higher classification accuracy, thereby significantly improving the overall effectiveness of QML models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 12.6%">
                            Quantum Computing
                        </span>
                <!-- Federated Learning: 4.7 -->
                    
                <!-- Reinforcement Learning: 3.3 -->
                    
                <!-- Bayesian Optimization: 2.9 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- LLMs: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.4089
                </span>
                <a href="https://arxiv.org/abs/2408.07086" target="_blank" rel="noopener noreferrer">Quantum algorithms for optimizers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Giacomo Nannicini
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This is a set of lecture notes for a Ph.D.-level course on quantum algorithms, with an emphasis on quantum optimization algorithms. It is developed for applied mathematicians and engineers, and requires no previous background in quantum mechanics. The main topics of this course, in addition to a rig</span>
                
                <span class="abstract-full" style="display: none;">This is a set of lecture notes for a Ph.D.-level course on quantum algorithms, with an emphasis on quantum optimization algorithms. It is developed for applied mathematicians and engineers, and requires no previous background in quantum mechanics. The main topics of this course, in addition to a rigorous introduction to the computational model, are: input/output models, quantum search, the quantum gradient algorithm, matrix manipulation algorithms, the mirror descent framework for semidefinite optimization (including the matrix multiplicative weights update algorithm), adiabatic optimization.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 14.1%">
                            Quantum Computing
                        </span>
                <!-- Medicine: 3.3 -->
                    
                <!-- Evolutionary Algorithms: 2.9 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- LLMs: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Cryptography: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.6918
                </span>
                <a href="https://arxiv.org/abs/2409.12104" target="_blank" rel="noopener noreferrer">Performance of Quantum Approximate Optimization with Quantum Error Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zichang He, David Amaro, Ruslan Shaydulin, Marco Pistoia
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Quantum algorithms must be scaled up to tackle real-world applications. Doing so requires overcoming the noise present on today's hardware. The quantum approximate optimization algorithm (QAOA) is a promising candidate for scaling up, due to its modest resource requirements and documented asymptotic</span>
                
                <span class="abstract-full" style="display: none;">Quantum algorithms must be scaled up to tackle real-world applications. Doing so requires overcoming the noise present on today's hardware. The quantum approximate optimization algorithm (QAOA) is a promising candidate for scaling up, due to its modest resource requirements and documented asymptotic speedup over state-of-the-art classical algorithms for some problems. However, achieving better-than-classical performance with QAOA is believed to require fault tolerance. In this paper, we demonstrate a partially fault-tolerant implementation of QAOA using the $[[k+2,k,2]]$ ``Iceberg'' error detection code. We observe that encoding the circuit with the Iceberg code improves the algorithmic performance as compared to the unencoded circuit for problems with up to $20$ logical qubits on a trapped-ion quantum computer. Additionally, we propose and calibrate a model for predicting the code performance. We use this model to characterize the limits of the Iceberg code and extrapolate its performance to future hardware with improved error rates. In particular, we show how our model can be used to determine the necessary conditions for QAOA to outperform the Goemans-Williamson algorithm on future hardware. To the best of our knowledge, our results demonstrate the largest universal quantum computing algorithm protected by partially fault-tolerant quantum error detection on practical applications to date, paving the way towards solving real-world applications with quantum computers.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 10.8%">
                            Quantum Computing
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #b3ebae" title="Confidence: 5.3%">
                            Federated Learning
                        </span>
                <!-- Evolutionary Algorithms: 4.0 -->
                    
                <!-- Computer Vision: 3.8 -->
                    
                <!-- Bayesian Optimization: 2.5 -->
                    
                <!-- Medicine: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Hardware: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -12.2946
                </span>
                <a href="https://arxiv.org/abs/2404.16366" target="_blank" rel="noopener noreferrer">Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yuanchen Bei, Sheng Zhou, Jinke Shi, Yao Ma, Haishuai Wang, Jiajun Bu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Unsupervised graph anomaly detection aims at identifying rare patterns that deviate from the majority in a graph without the aid of labels, which is important for a variety of real-world applications. Recent advances have utilized Graph Neural Networks (GNNs) to learn effective node representations </span>
                
                <span class="abstract-full" style="display: none;">Unsupervised graph anomaly detection aims at identifying rare patterns that deviate from the majority in a graph without the aid of labels, which is important for a variety of real-world applications. Recent advances have utilized Graph Neural Networks (GNNs) to learn effective node representations by aggregating information from neighborhoods. This is motivated by the hypothesis that nodes in the graph tend to exhibit consistent behaviors with their neighborhoods. However, such consistency can be disrupted by graph anomalies in multiple ways. Most existing methods directly employ GNNs to learn representations, disregarding the negative impact of graph anomalies on GNNs, resulting in sub-optimal node representations and anomaly detection performance. While a few recent approaches have redesigned GNNs for graph anomaly detection under semi-supervised label guidance, how to address the adverse effects of graph anomalies on GNNs in unsupervised scenarios and learn effective representations for anomaly detection are still under-explored. To bridge this gap, in this paper, we propose a simple yet effective framework for Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD). Specifically, G3AD first introduces two auxiliary networks along with correlation constraints to guard the GNNs against inconsistent information encoding. Furthermore, G3AD introduces an adaptive caching module to guard the GNNs from directly reconstructing the observed graph data that contains anomalies. Extensive experiments demonstrate that our G3AD can outperform twenty state-of-the-art methods on both synthetic and real-world graph anomaly datasets, with flexible generalization ability in different GNN backbones.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 29.5%">
                            GNN
                        </span>
                <!-- Computer Vision: 3.5 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- LLMs: 1.9 -->
                    
                <!-- Medicine: 1.9 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.0 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -35.2371
                </span>
                <a href="https://arxiv.org/abs/2505.18282" target="_blank" rel="noopener noreferrer">Towards a Quantum-classical Augmented Network</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nitin Jha, Abhishek Parakh, Mahadevan Subramaniam
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In the past decade, several small-scale quantum key distribution networks have been established. However, the deployment of large-scale quantum networks depends on the development of quantum repeaters, quantum channels, quantum memories, and quantum network protocols. To improve the security of exis</span>
                
                <span class="abstract-full" style="display: none;">In the past decade, several small-scale quantum key distribution networks have been established. However, the deployment of large-scale quantum networks depends on the development of quantum repeaters, quantum channels, quantum memories, and quantum network protocols. To improve the security of existing networks and adopt currently feasible quantum technologies, the next step is to augment classical networks with quantum devices, properties, and phenomena. To achieve this, we propose a change in the structure of the HTTP protocol such that it can carry both quantum and classical payload. This work lays the foundation for dividing one single network packet into classical and quantum payloads depending on the privacy needs. We implement logistic regression, CNN, LSTM, and BiLSTM models to classify the privacy label for outgoing communications. This enables reduced utilization of quantum resources allowing for a more efficient secure quantum network design. Experimental results using the proposed methods are presented.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 48.5%">
                            Quantum Computing
                        </span>
                <!-- Medicine: 3.2 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- LLMs: 1.2 -->
                    
                
            </div>
        </div>
        
    </div>
    
    
    <div id="jsonPopup" class="json-popup">
        <pre id="jsonContent"></pre>
        <button onclick="copyJson()">Copy to Clipboard</button>
        <button onclick="closePopup()">Close</button>
    </div>

    <script>
        function extractPaperData(paperElement) {
            const titleElement = paperElement.querySelector('.paper-title a');
            const metaElement = paperElement.querySelector('.paper-meta');
            const abstractElement = paperElement.querySelector('.paper-abstract');
            const tagsElement = paperElement.querySelector('.paper-tags');
            
            // Get the date from the parent date-section header
            const dateSection = paperElement.closest('.date-section');
            const dateText = dateSection.querySelector('.date-header').textContent.trim();
            
            const authorsText = metaElement.textContent.replace('Authors:', '').trim();
            
            const paperData = {
                title: titleElement.textContent,
                url: titleElement.href,
                authors: authorsText.split(',').map(author => author.trim()),
                created: dateText,
                abstract: abstractElement.querySelector('.abstract-full').textContent
            };
            
            return paperData;
        }

        function showJson(paperElement) {
            const popup = document.getElementById('jsonPopup');
            const content = document.getElementById('jsonContent');
            const paperData = extractPaperData(paperElement);
            content.textContent = JSON.stringify(paperData, null, null);
            popup.style.display = 'block';
            document.addEventListener('click', function closePopupOnClick(event) {
                if (!popup.contains(event.target)) {
                    popup.style.display = 'none';
                    document.removeEventListener('click', closePopupOnClick);
                }
            });
        }
        function toggleAbstract(element) {
            const abstract = element.parentElement;
            const short = abstract.querySelector('.abstract-short');
            const full = abstract.querySelector('.abstract-full');
            const lowConfidenceTags = abstract.parentElement.querySelectorAll('.tag-badge.low-confidence');
            
            if (element.textContent === '... more') {
                short.style.display = 'none';
                full.style.display = 'inline';
                element.textContent = ' less';
                lowConfidenceTags.forEach(tag => tag.style.display = 'inline-block');
            } else {
                short.style.display = 'inline';
                full.style.display = 'none';
                element.textContent = '... more';
                lowConfidenceTags.forEach(tag => tag.style.display = 'none');
            }
        }

        function closePopup() {
            document.getElementById('jsonPopup').style.display = 'none';
        }

        function copyJson() {
            const content = document.getElementById('jsonContent').textContent;
            navigator.clipboard.writeText(content).catch(() => {
                // If clipboard API is not available, just show the popup
                alert('Could not copy to clipboard. JSON is displayed in the popup.');
            });
        }

        // Close popup when clicking outside
        window.onclick = function(event) {
            const popup = document.getElementById('jsonPopup');
            if (event.target === popup) {
                popup.style.display = 'none';
            }
        }
    </script>
</body>
</html> 