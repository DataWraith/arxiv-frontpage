<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Frontpage</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .paper {
            margin-bottom: 30px;
            margin-top: 30px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .paper-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .paper-meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        .paper-abstract {
            margin-bottom: 10px;
        }
        .abstract-short {
            display: inline;
        }
        .abstract-full {
            display: none;
        }
        .more-link {
            color: blue;
            cursor: pointer;
            text-decoration: underline;
        }
        .tag-badge {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 5px;
            margin-bottom: 5px;
            border-radius: 3px;
            font-size: 0.8em;
            color: white;
        }
        .tag-badge.high-confidence {
            opacity: 1;
        }
        .tag-badge.low-confidence {
            opacity: 0.6;
            display: none;
        }
        .interestingness-score {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 10px;
            color: white;
            border-radius: 3px;
            font-weight: bold;
        }
        .interestingness-positive {
            background-color: #4CAF50;
        }
        .interestingness-negative {
            background-color: #f44336;
        }
        .interestingness-neutral {
            background-color: #9e9e9e;
        }
        .last-updated {
            text-align: right;
            color: #666;
            font-size: 0.9em;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .intro {
            text-align: center;
            max-width: 60em;
            margin: 0 auto;
            color: #888;
        }
        .copy-icon {
            display: inline-block;
            width: 16px;
            height: 16px;
            cursor: pointer;
            margin-left: 5px;
            opacity: 0.5;
        }
        .copy-icon:hover {
            opacity: 1;
        }
        .json-popup {
            display: none;
            position: fixed;
            top: 20px;
            right: 20px;
            background: white;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            max-width: 500px;
            max-height: 300px;
            overflow: auto;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        a {
            color: inherit;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h1 {
            text-align: center;
        }
        h1 a {
            text-decoration: underline;
        }
        .date-section {
            margin-bottom: 40px;
        }
        .date-header {
            color: #666;
            font-size: 1.5em;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #eee;
        }
    </style>
</head>
<body>
    <h1>
        <a href="https://github.com/DataWraith/arxiv-frontpage">DataWraith's</a> ArXiv Frontpage
    </h1>

    <div class="last-updated">
        Last updated: 2025-04-17
    </div>

    <p class="intro">
        This frontpage is made by scraping ArXiv's computer science RSS feed and tagging papers with a classifier.
    </p>

    <p class="intro">
        Each tag is weighted according to my preferences to compute a paper's <i>interestingness</i> score.
    </p>
    
    
    <div class="date-section">
        <h2 class="date-header">2025-04-17</h2>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.2236
                </span>
                <a href="https://arxiv.org/abs/2504.10001" target="_blank" rel="noopener noreferrer">GaussVideoDreamer: 3D Scene Generation with Video Diffusion and Inconsistency-Aware Gaussian Splatting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Junlin Hao, Peiheng Wang, Haoyang Wang, Xinggong Zhang, Zongming Guo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Single-image 3D scene reconstruction presents significant challenges due to its inherently ill-posed nature and limited input constraints. Recent advances have explored two promising directions: multiview generative models that train on 3D consistent datasets but struggle with out-of-distribution ge</span>
                
                <span class="abstract-full" style="display: none;">Single-image 3D scene reconstruction presents significant challenges due to its inherently ill-posed nature and limited input constraints. Recent advances have explored two promising directions: multiview generative models that train on 3D consistent datasets but struggle with out-of-distribution generalization, and 3D scene inpainting and completion frameworks that suffer from cross-view inconsistency and suboptimal error handling, as they depend exclusively on depth data or 3D smoothness, which ultimately degrades output quality and computational performance. Building upon these approaches, we present GaussVideoDreamer, which advances generative multimedia approaches by bridging the gap between image, video, and 3D generation, integrating their strengths through two key innovations: (1) A progressive video inpainting strategy that harnesses temporal coherence for improved multiview consistency and faster convergence. (2) A 3D Gaussian Splatting consistency mask to guide the video diffusion with 3D consistent multiview evidence. Our pipeline combines three core components: a geometry-aware initialization protocol, Inconsistency-Aware Gaussian Splatting, and a progressive video inpainting strategy. Experimental results demonstrate that our approach achieves 32% higher LLaVA-IQA scores and at least 2x speedup compared to existing methods while maintaining robust performance across diverse scenes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 13.2 -->
                    
                <!-- Medicine: 10.3 -->
                    
                <!-- 3D: 8.2 -->
                    
                <!-- RAG: 3.7 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- T2I: 2.1 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                
            </div>
        </div>
        
    </div>
    
    <div class="date-section">
        <h2 class="date-header">2025-04-16</h2>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    9.3078
                </span>
                <a href="https://arxiv.org/abs/2504.03784" target="_blank" rel="noopener noreferrer">Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, Chengchung Shi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preference</span>
                
                <span class="abstract-full" style="display: none;">Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #44f899" title="Confidence: 69.0%">
                            Reinforcement Learning
                        </span>
                <!-- LLMs: 11.5 -->
                    
                <!-- Medicine: 5.7 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10502" target="_blank" rel="noopener noreferrer">Human-Oriented Image Retrieval System (HORSE): A Neuro-Symbolic Approach to Optimizing Retrieval of Previewed Images</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Abraham Itzhak Weinberg
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Image retrieval remains a challenging task due to the complex interaction between human visual perception, memory, and computational processes. Current image search engines often struggle to efficiently retrieve images based on natural language descriptions, as they rely on time-consuming preprocess</span>
                
                <span class="abstract-full" style="display: none;">Image retrieval remains a challenging task due to the complex interaction between human visual perception, memory, and computational processes. Current image search engines often struggle to efficiently retrieve images based on natural language descriptions, as they rely on time-consuming preprocessing, tagging, and machine learning pipelines. This paper introduces the Human-Oriented Retrieval Search Engine for Images (HORSE), a novel approach that leverages neuro-symbolic indexing to improve image retrieval by focusing on human-oriented indexing. By integrating cognitive science insights with advanced computational techniques, HORSE enhances the retrieval process, making it more aligned with how humans perceive, store, and recall visual information. The neuro-symbolic framework combines the strengths of neural networks and symbolic reasoning, mitigating their individual limitations. The proposed system optimizes image retrieval, offering a more intuitive and efficient solution for users. We discuss the design and implementation of HORSE, highlight its potential applications in fields such as design error detection and knowledge management, and suggest future directions for research to further refine the system's metrics and capabilities.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.8 -->
                    
                <!-- LLMs: 9.0 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10521" target="_blank" rel="noopener noreferrer">Integrating Emotion Distribution Networks and Textual Message Analysis for X User Emotional State Classification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pardis Moradbeiki, Mohammad Ali Zare Chahooki
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As the popularity and reach of social networks continue to surge, a vast reservoir of opinions and sentiments across various subjects inundates these platforms. Among these, X social network (formerly Twitter) stands as a juggernaut, boasting approximately 420 million active users. Extracting users'</span>
                
                <span class="abstract-full" style="display: none;">As the popularity and reach of social networks continue to surge, a vast reservoir of opinions and sentiments across various subjects inundates these platforms. Among these, X social network (formerly Twitter) stands as a juggernaut, boasting approximately 420 million active users. Extracting users' emotional and mental states from their expressed opinions on social media has become a common pursuit. While past methodologies predominantly focused on the textual content of messages to analyze user sentiment, the interactive nature of these platforms suggests a deeper complexity. This study employs hybrid methodologies, integrating textual analysis, profile examination, follower analysis, and emotion dissemination patterns. Initially, user interactions are leveraged to refine emotion classification within messages, encompassing exchanges where users respond to each other. Introducing the concept of a communication tree, a model is extracted to map these interactions. Subsequently, users' bios and interests from this tree are juxtaposed with message text to enrich analysis. Finally, influential figures are identified among users' followers in the communication tree, categorized into different topics to gauge interests. The study highlights that traditional sentiment analysis methodologies, focusing solely on textual content, are inadequate in discerning sentiment towards significant events, notably the presidential election. Comparative analysis with conventional methods reveals a substantial improvement in accuracy with the incorporation of emotion distribution patterns and user profiles. The proposed approach yields a 12% increase in accuracy with emotion distribution patterns and a 15% increase when considering user profiles, underscoring its efficacy in capturing nuanced sentiment dynamics.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.7 -->
                    
                <!-- Medicine: 6.6 -->
                    
                <!-- Networks: 3.8 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Robotics: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10551" target="_blank" rel="noopener noreferrer">MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lili Zhao, Qi Liu, Wei Chen, Liyi Chen, Ruijun Sun, Min Hou, Yang Wang, Shijin Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Empirical Risk Minimization (ERM) models often rely on spurious correlations between features and labels during the learning process, leading to shortcut learning behavior that undermines robustness generalization performance. Current research mainly targets identifying or mitigating a single shortc</span>
                
                <span class="abstract-full" style="display: none;">Empirical Risk Minimization (ERM) models often rely on spurious correlations between features and labels during the learning process, leading to shortcut learning behavior that undermines robustness generalization performance. Current research mainly targets identifying or mitigating a single shortcut; however, in real-world scenarios, cues within the data are diverse and unknown. In empirical studies, we reveal that the models rely to varying extents on different shortcuts. Compared to weak shortcuts, models depend more heavily on strong shortcuts, resulting in their poor generalization ability. To address these challenges, we propose MiMu, a novel method integrated with Transformer-based ERMs designed to Mitigate Multiple shortcut learning behavior, which incorporates self-calibration strategy and self-improvement strategy. In the source model, we preliminarily propose the self-calibration strategy to prevent the model from relying on shortcuts and make overconfident predictions. Then, we further design self-improvement strategy in target model to reduce the reliance on multiple shortcuts. The random mask strategy involves randomly masking partial attention positions to diversify the focus of target model other than concentrating on a fixed region. Meanwhile, the adaptive attention alignment module facilitates the alignment of attention weights to the calibrated source model, without the need for post-hoc attention maps or supervision. Finally, extensive experiments conducted on Natural Language Processing (NLP) and Computer Vision (CV) demonstrate the effectiveness of MiMu in improving robustness generalization abilities.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.5 -->
                    
                <!-- Medicine: 5.6 -->
                    
                <!-- Reinforcement Learning: 4.6 -->
                    
                <!-- Federated Learning: 3.0 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10559" target="_blank" rel="noopener noreferrer">Efficient Process Reward Model Training via Active Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Keyu Duan, Zichen Liu, Xin Mao, Tianyu Pang, Changyu Chen, Qiguang Chen, Michael Qizhe Shieh, Longxu Dou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain </span>
                
                <span class="abstract-full" style="display: none;">Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs. During training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss with respect to the labels and update the PRM's weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduces 50% annotation, but achieving the comparable or even better performance. Beyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60% of the data. A subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same sized models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.9 -->
                    
                <!-- LLMs: 8.4 -->
                    
                <!-- Reinforcement Learning: 4.4 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10561" target="_blank" rel="noopener noreferrer">Self-Controlled Dynamic Expansion Model for Continual Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Runqing Wu, Fei Ye, Rongyao Hu, Guoxi Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Continual Learning (CL) epitomizes an advanced training paradigm wherein prior data samples remain inaccessible during the acquisition of new tasks. Numerous investigations have delved into leveraging a pre-trained Vision Transformer (ViT) to enhance model efficacy in continual learning. Nonetheless</span>
                
                <span class="abstract-full" style="display: none;">Continual Learning (CL) epitomizes an advanced training paradigm wherein prior data samples remain inaccessible during the acquisition of new tasks. Numerous investigations have delved into leveraging a pre-trained Vision Transformer (ViT) to enhance model efficacy in continual learning. Nonetheless, these approaches typically utilize a singular, static backbone, which inadequately adapts to novel tasks, particularly when engaging with diverse data domains, due to a substantial number of inactive parameters. This paper addresses this limitation by introducing an innovative Self-Controlled Dynamic Expansion Model (SCDEM), which orchestrates multiple distinct trainable pre-trained ViT backbones to furnish diverse and semantically enriched representations. Specifically, by employing the multi-backbone architecture as a shared module, the proposed SCDEM dynamically generates a new expert with minimal parameters to accommodate a new task. A novel Collaborative Optimization Mechanism (COM) is introduced to synergistically optimize multiple backbones by harnessing prediction signals from historical experts, thereby facilitating new task learning without erasing previously acquired knowledge. Additionally, a novel Feature Distribution Consistency (FDC) approach is proposed to align semantic similarity between previously and currently learned representations through an optimal transport distance-based mechanism, effectively mitigating negative knowledge transfer effects. Furthermore, to alleviate over-regularization challenges, this paper presents a novel Dynamic Layer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the penalization intensity on each trainable representation layer. An extensive series of experiments have been conducted to evaluate the proposed methodology's efficacy, with empirical results corroborating that the approach attains state-of-the-art performance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.7 -->
                    
                <!-- Networks: 5.1 -->
                    
                <!-- Reinforcement Learning: 4.3 -->
                    
                <!-- Medicine: 4.0 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- GNN: 3.0 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Attention: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- SpikingNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10609" target="_blank" rel="noopener noreferrer">Finding Pathways in Reaction Networks guided by Energy Barriers using Integer Linear Programming</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Adittya Pal, Rolf Fagerberg, Jakob Lykke Andersen, Peter Dittrich, Daniel Merkle
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Analyzing synthesis pathways for target molecules in a chemical reaction network annotated with information on the kinetics of individual reactions is an area of active study. This work presents a computational methodology for searching for pathways in reaction networks which is based on integer lin</span>
                
                <span class="abstract-full" style="display: none;">Analyzing synthesis pathways for target molecules in a chemical reaction network annotated with information on the kinetics of individual reactions is an area of active study. This work presents a computational methodology for searching for pathways in reaction networks which is based on integer linear programming and the modeling of reaction networks by directed hypergraphs. Often multiple pathways fit the given search criteria. To rank them, we develop an objective function based on physical arguments maximizing the probability of the pathway. We furthermore develop an automated pipeline to estimate the energy barriers of individual reactions in reaction networks. Combined, the methodology facilitates flexible and kinetically informed pathway investigations on large reaction networks by computational means, even for networks coming without kinetic annotation, such as those created via generative approaches for expanding molecular spaces.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.0 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- Math: 2.8 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- SpikingNN: 1.3 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10612" target="_blank" rel="noopener noreferrer">Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Michal Balcerak, Tamaz Amiranashvili, Suprosanna Shit, Antonio Terpin, Sebastian Kaltenbach, Petros Koumoutsakos, Bjoern Menze
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generative models often map noise to data by matching flows or scores, but these approaches become cumbersome for incorporating partial observations or additional priors. Inspired by recent advances in Wasserstein gradient flows, we propose Energy Matching, a framework that unifies flow-based approa</span>
                
                <span class="abstract-full" style="display: none;">Generative models often map noise to data by matching flows or scores, but these approaches become cumbersome for incorporating partial observations or additional priors. Inspired by recent advances in Wasserstein gradient flows, we propose Energy Matching, a framework that unifies flow-based approaches with the flexibility of energy-based models (EBMs). Far from the data manifold, samples move along curl-free, optimal transport paths from noise to data. As they approach the data manifold, an entropic energy term guides the system into a Boltzmann equilibrium distribution, explicitly capturing the underlying likelihood structure of the data. We parameterize this dynamic with a single time-independent scalar field, which serves as both a powerful generator and a flexible prior for effective regularization of inverse problems. Our method substantially outperforms existing EBMs on CIFAR-10 generation (FID 3.97 compared to 8.61), while retaining the simulation-free training of transport-based approaches away from the data manifold. Additionally, we exploit the flexibility of our method and introduce an interaction energy for diverse mode exploration. Our approach focuses on learning a static scalar potential energy -- without time conditioning, auxiliary generators, or additional networks -- marking a significant departure from recent EBM methods. We believe this simplified framework significantly advances EBM capabilities and paves the way for their broader adoption in generative modeling across diverse domains.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.2 -->
                    
                <!-- LLMs: 6.1 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- SpikingNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10622" target="_blank" rel="noopener noreferrer">Improving Upon the generalized c-mu rule: a Whittle approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhouzi Li, Keerthana Gurushankar, Mor Harchol-Balter, Alan Scheller-Wolf
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Scheduling a stream of jobs whose holding cost changes over time is a classic and practical problem. Specifically, each job is associated with a holding cost (penalty), where a job's instantaneous holding cost is some increasing function of its class and current age (the time it has spent in the sys</span>
                
                <span class="abstract-full" style="display: none;">Scheduling a stream of jobs whose holding cost changes over time is a classic and practical problem. Specifically, each job is associated with a holding cost (penalty), where a job's instantaneous holding cost is some increasing function of its class and current age (the time it has spent in the system since its arrival). The goal is to schedule the jobs to minimize the time-average total holding cost across all jobs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.0 -->
                    
                <!-- Medicine: 5.9 -->
                    
                <!-- Networks: 4.9 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10639" target="_blank" rel="noopener noreferrer">Secure Estimation of Battery Voltage Under Sensor Attacks: A Self-Learning Koopman Approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sanchita Ghosh, Tanushree Roy
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Cloud-based battery management system (BMS) requires accurate terminal voltage measurement data to ensure optimal and safe charging of Lithium-ion batteries. Unfortunately, an adversary can corrupt the battery terminal voltage data as it passes from the local-BMS to the cloud-BMS through the communi</span>
                
                <span class="abstract-full" style="display: none;">Cloud-based battery management system (BMS) requires accurate terminal voltage measurement data to ensure optimal and safe charging of Lithium-ion batteries. Unfortunately, an adversary can corrupt the battery terminal voltage data as it passes from the local-BMS to the cloud-BMS through the communication network, with the objective of under- or over-charging the battery. To ensure accurate terminal voltage data under such malicious sensor attacks, this paper investigates a Koopman-based secure terminal voltage estimation scheme using a two-stage error-compensated self-learning feedback. During the first stage of error correction, the potential Koopman prediction error is estimated to compensate for the error accumulation due to the linear approximation of Koopman operator. The second stage of error compensation aims to recover the error amassing from the higher-order dynamics of the Lithium-ion batteries missed by the self-learning strategy. Specifically, we have proposed two different methods for this second stage error compensation. First, an interpretable empirical correction strategy has been obtained using the open circuit voltage to state-of-charge mapping for the battery. Second, a Gaussian process regression-based data-driven method has been explored. Finally, we demonstrate the efficacy of the proposed secure estimator using both empirical and data-driven corrections.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.2 -->
                    
                <!-- Reinforcement Learning: 4.6 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Math: 3.3 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- LLMs: 2.2 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10649" target="_blank" rel="noopener noreferrer">Ride-pool Assignment Algorithms: Modern Implementation and Swapping Heuristics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Matthew Zalesak, Hins Hu, Samitha Samaranayake
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">On-demand ride-pooling has emerged as a popular urban transportation solution, addressing the efficiency limitations of traditional ride-hailing services by grouping multiple riding requests with spatiotemporal proximity into a single vehicle. Although numerous algorithms have been developed for the</span>
                
                <span class="abstract-full" style="display: none;">On-demand ride-pooling has emerged as a popular urban transportation solution, addressing the efficiency limitations of traditional ride-hailing services by grouping multiple riding requests with spatiotemporal proximity into a single vehicle. Although numerous algorithms have been developed for the Ride-pool Assignment Problem (RAP) -- a core component of ride-pooling systems, there is a lack of open-source implementations, making it difficult to benchmark these algorithms on a common dataset and objective. In this paper, we present the implementation details of a ride-pool simulator that encompasses several key ride-pool assignment algorithms, along with associated components such as vehicle routing and rebalancing. We also open-source a highly optimized and modular C++ codebase, designed to facilitate the extension of new algorithms and features. Additionally, we introduce a family of swapping-based local-search heuristics to enhance existing ride-pool assignment algorithms, achieving a better balance between performance and computational efficiency. Extensive experiments on a large-scale, real-world dataset from Manhattan, NYC reveal that while all selected algorithms perform comparably, the newly proposed Multi-Round Linear Assignment with Cyclic Exchange (LA-MR-CE) algorithm achieves a state-of-the-art service rate with significantly reduced computational time. Furthermore, an in-depth analysis suggests that a performance barrier exists for all myopic ride-pool assignment algorithms due to the system's capacity bottleneck, and incorporating future information could be key to overcoming this limitation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.3 -->
                    
                <!-- LLMs: 5.0 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10657" target="_blank" rel="noopener noreferrer">Balanced TSP partitioning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Benjamin Aram Berendsohn, Hwi Kim, L\'aszl\'o Kozma
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The traveling salesman problem (TSP) famously asks for a shortest tour that a salesperson can take to visit a given set of cities in any order. In this paper, we ask how much faster $k \ge 2$ salespeople can visit the cities if they divide the task among themselves. We show that, in the two-dimensio</span>
                
                <span class="abstract-full" style="display: none;">The traveling salesman problem (TSP) famously asks for a shortest tour that a salesperson can take to visit a given set of cities in any order. In this paper, we ask how much faster $k \ge 2$ salespeople can visit the cities if they divide the task among themselves. We show that, in the two-dimensional Euclidean setting, two salespeople can always achieve a speedup of at least $\frac12 + \frac1\pi \approx 0.818$, for any given input, and there are inputs where they cannot do better. We also give (non-matching) upper and lower bounds for $k \geq 3$.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.3 -->
                    
                <!-- Quantum Computing: 6.1 -->
                    
                <!-- Medicine: 5.7 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10663" target="_blank" rel="noopener noreferrer">Characterizing Knowledge Manipulation in a Russian Wikipedia Fork</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mykola Trokhymovych, Oleksandr Kosovan, Nathan Forrester, Pablo Arag\'on, Diego Saez-Trumper, Ricardo Baeza-Yates
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Wikipedia is powered by MediaWiki, a free and open-source software that is also the infrastructure for many other wiki-based online encyclopedias. These include the recently launched website Ruwiki, which has copied and modified the original Russian Wikipedia content to conform to Russian law. To id</span>
                
                <span class="abstract-full" style="display: none;">Wikipedia is powered by MediaWiki, a free and open-source software that is also the infrastructure for many other wiki-based online encyclopedias. These include the recently launched website Ruwiki, which has copied and modified the original Russian Wikipedia content to conform to Russian law. To identify practices and narratives that could be associated with different forms of knowledge manipulation, this article presents an in-depth analysis of this Russian Wikipedia fork. We propose a methodology to characterize the main changes with respect to the original version. The foundation of this study is a comprehensive comparative analysis of more than 1.9M articles from Russian Wikipedia and its fork. Using meta-information and geographical, temporal, categorical, and textual features, we explore the changes made by Ruwiki editors. Furthermore, we present a classification of the main topics of knowledge manipulation in this fork, including a numerical estimation of their scope. This research not only sheds light on significant changes within Ruwiki, but also provides a methodology that could be applied to analyze other Wikipedia forks and similar collaborative projects.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.5 -->
                    
                <!-- Medicine: 6.1 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Math: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10671" target="_blank" rel="noopener noreferrer">Token Sliding Reconfiguration on DAGs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jona Dirks, Alexandre Vigny
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Given a graph $G$ and two independent sets of same size, the Independent Set Reconfiguration Problem under token sliding ask whether one can, in a step by step manner, transform the first independent set into the second one. In each step we must preserve the condition of independence. Further, refer</span>
                
                <span class="abstract-full" style="display: none;">Given a graph $G$ and two independent sets of same size, the Independent Set Reconfiguration Problem under token sliding ask whether one can, in a step by step manner, transform the first independent set into the second one. In each step we must preserve the condition of independence. Further, referring to solution vertices as tokens, we are only permitted to slide a token along an edge. Until the recent work of Ito et al. [Ito et al. MFCS 2022] this problem was only considered on undirected graphs. In this work, we study reconfiguration under token sliding focusing on DAGs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.0 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Hardware: 1.0 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10699" target="_blank" rel="noopener noreferrer">HyRRT-Connect: Bidirectional Motion Planning for Hybrid Dynamical Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nan Wang, Ricardo G. Sanfelice
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper proposes a bidirectional rapidly-exploring random trees (RRT) algorithm to solve the motion planning problem for hybrid systems. The proposed algorithm, called HyRRT-Connect, propagates in both forward and backward directions in hybrid time until an overlap between the forward and backwar</span>
                
                <span class="abstract-full" style="display: none;">This paper proposes a bidirectional rapidly-exploring random trees (RRT) algorithm to solve the motion planning problem for hybrid systems. The proposed algorithm, called HyRRT-Connect, propagates in both forward and backward directions in hybrid time until an overlap between the forward and backward propagation results is detected. Then, HyRRT-Connect constructs a motion plan through the reversal and concatenation of functions defined on hybrid time domains, ensuring that the motion plan satisfies the given hybrid dynamics. To address the potential discontinuity along the flow caused by tolerating some distance between the forward and backward partial motion plans, we reconstruct the backward partial motion plan by a forward-in-hybrid-time simulation from the final state of the forward partial motion plan. effectively eliminating the discontinuity. The proposed algorithm is applied to an actuated bouncing ball system and a walking robot example to highlight its computational improvement.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.9 -->
                    
                <!-- Reinforcement Learning: 4.1 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10702" target="_blank" rel="noopener noreferrer">Container-level Energy Observability in Kubernetes Clusters</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bjorn Pijnacker, Brian Setz, Vasilios Andrikopoulos
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Kubernetes has been for a number of years the default cloud orchestrator solution across multiple application and research domains. As such, optimizing the energy efficiency of Kubernetes-deployed workloads is of primary interest towards controlling operational expenses by reducing energy consumptio</span>
                
                <span class="abstract-full" style="display: none;">Kubernetes has been for a number of years the default cloud orchestrator solution across multiple application and research domains. As such, optimizing the energy efficiency of Kubernetes-deployed workloads is of primary interest towards controlling operational expenses by reducing energy consumption at data center level and allocated resources at application level. A lot of research in this direction aims on reducing the total energy usage of Kubernetes clusters without establishing an understanding of their workloads, i.e. the applications deployed on the cluster. This means that there are untapped potential improvements in energy efficiency that can be achieved through, for example, application refactoring or deployment optimization. For all these cases a prerequisite is establishing fine-grained observability down to the level of individual containers and their power draw over time. A state-of-the-art tool approved by the Cloud-Native Computing Foundation, Kepler, aims to provide this functionality, but has not been assessed for its accuracy and therefore fitness for purpose. In this work we start by developing an experimental procedure to this goal, and we conclude that the reported energy usage metrics provided by Kepler are not at a satisfactory level. As a reaction to this, we develop KubeWatt as an alternative to Kepler for specific use case scenarios, and demonstrate its higher accuracy through the same experimental procedure as we used for Kepler.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.9 -->
                    
                <!-- Medicine: 6.2 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10703" target="_blank" rel="noopener noreferrer">The Trie Measure, Revisited</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jarno N. Alanko, Ruben Becker, Davide Cenzato, Travis Gagie, Sung-Hwan Kim, Bojana Kodric, Nicola Prezza
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we study the following problem: given $n$ subsets $S_1, \dots, S_n$ of an integer universe $U = \{0,\dots, u-1\}$, having total cardinality $N = \sum_{i=1}^n |S_i|$, find a prefix-free encoding $enc : U \rightarrow \{0,1\}^+$ minimizing the so-called trie measure, i.e., the total numb</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we study the following problem: given $n$ subsets $S_1, \dots, S_n$ of an integer universe $U = \{0,\dots, u-1\}$, having total cardinality $N = \sum_{i=1}^n |S_i|$, find a prefix-free encoding $enc : U \rightarrow \{0,1\}^+$ minimizing the so-called trie measure, i.e., the total number of edges in the $n$ binary tries $\mathcal T_1, \dots, \mathcal T_n$, where $\mathcal T_i$ is the trie packing the encoded integers $\{enc(x):x\in S_i\}$. We first observe that this problem is equivalent to that of merging $u$ sets with the cheapest sequence of binary unions, a problem which in [Ghosh et al., ICDCS 2015] is shown to be NP-hard. Motivated by the hardness of the general problem, we focus on particular families of prefix-free encodings. We start by studying the fixed-length shifted encoding of [Gupta et al., Theoretical Computer Science 2007]. Given a parameter $0\le a < u$, this encoding sends each $x \in U$ to $(x + a) \mod u$, interpreted as a bit-string of $\log u$ bits. We develop the first efficient algorithms that find the value of $a$ minimizing the trie measure when this encoding is used. Our two algorithms run in $O(u + N\log u)$ and $O(N\log^2 u)$ time, respectively. We proceed by studying ordered encodings (a.k.a. monotone or alphabetic), and describe an algorithm finding the optimal such encoding in $O(N+u^3)$ time. Within the same running time, we show how to compute the best shifted ordered encoding, provably no worse than both the optimal shifted and optimal ordered encodings. We provide implementations of our algorithms and discuss how these encodings perform in practice.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.5 -->
                    
                <!-- Math: 4.0 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Medicine: 2.3 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Pathfinding: 1.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10710" target="_blank" rel="noopener noreferrer">On an efficient line smoother for the p-multigrid {\gamma}-cycle</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jos\'e Pablo Lucero Lorca, Duane Rosenberg, Isidora Jankov, Conor McCoid, Martin Jakob Gander
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As part of the development of a Poisson solver for the spectral element discretization used in the GeoFluid Object Workbench (GeoFLOW) code, we propose a solver for the linear system arising from a Gauss-Legendre-Lobatto global spectral method. We precondition using a p-multigrid {\gamma}-cycle with</span>
                
                <span class="abstract-full" style="display: none;">As part of the development of a Poisson solver for the spectral element discretization used in the GeoFluid Object Workbench (GeoFLOW) code, we propose a solver for the linear system arising from a Gauss-Legendre-Lobatto global spectral method. We precondition using a p-multigrid {\gamma}-cycle with highly-vectorizable smoothers, that we refer to as line smoothers. Our smoothers are restrictions of spectral and finite element discretizations to low-order one-dimensional problems along lines, that are solved by a reformulation of cyclic reduction as a direct multigrid method. We illustrate our method with numerical experiments showing the apparent boundedness of the iteration count for a fixed residual reduction over a range of moderately deformed domains, right hand sides and Dirichlet boundary conditions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.8 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Reinforcement Learning: 3.2 -->
                    
                <!-- Networks: 3.2 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- SpikingNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10717" target="_blank" rel="noopener noreferrer">FuzzSense: Towards A Modular Fuzzing Framework for Autonomous Driving Software</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andrew Roberts, Lorenz Teply, Mert D. Pese, Olaf Maennel, Mohammad Hamad, Sebastian Steinhorst
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Fuzz testing to find semantic control vulnerabilities is an essential activity to evaluate the robustness of autonomous driving (AD) software. Whilst there is a preponderance of disparate fuzzing tools that target different parts of the test environment, such as the scenario, sensors, and vehicle dy</span>
                
                <span class="abstract-full" style="display: none;">Fuzz testing to find semantic control vulnerabilities is an essential activity to evaluate the robustness of autonomous driving (AD) software. Whilst there is a preponderance of disparate fuzzing tools that target different parts of the test environment, such as the scenario, sensors, and vehicle dynamics, there is a lack of fuzzing strategies that ensemble these fuzzers to enable concurrent fuzzing, utilizing diverse techniques and targets. This research proposes FuzzSense, a modular, black-box, mutation-based fuzzing framework that is architected to ensemble diverse AD fuzzing tools. To validate the utility of FuzzSense, a LiDAR sensor fuzzer was developed as a plug-in, and the fuzzer was implemented in the new AD simulation platform AWSIM and Autoware.Universe AD software platform. The results demonstrated that FuzzSense was able to find vulnerabilities in the new Autoware.Universe software. We contribute to FuzzSense open-source with the aim of initiating a conversation in the community on the design of AD-specific fuzzers and the establishment of a community fuzzing framework to better target the diverse technology base of autonomous vehicles.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.6 -->
                    
                <!-- Medicine: 4.0 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Robotics: 2.3 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10720" target="_blank" rel="noopener noreferrer">Leveraging Deep Operator Networks (DeepONet) for Acoustic Full Waveform Inversion (FWI)</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kamaljyoti Nath, Khemraj Shukla, Victor C. Tsai, Umair bin Waheed, Christian Huber, Omer Alpak, Chuen-Song Chen, Ligang Lu, Amik St-Cyr
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Full Waveform Inversion (FWI) is an important geophysical technique considered in subsurface property prediction. It solves the inverse problem of predicting high-resolution Earth interior models from seismic data. Traditional FWI methods are computationally demanding. Inverse problems in geophysics</span>
                
                <span class="abstract-full" style="display: none;">Full Waveform Inversion (FWI) is an important geophysical technique considered in subsurface property prediction. It solves the inverse problem of predicting high-resolution Earth interior models from seismic data. Traditional FWI methods are computationally demanding. Inverse problems in geophysics often face challenges of non-uniqueness due to limited data, as data are often collected only on the surface. In this study, we introduce a novel methodology that leverages Deep Operator Networks (DeepONet) to attempt to improve both the efficiency and accuracy of FWI. The proposed DeepONet methodology inverts seismic waveforms for the subsurface velocity field. This approach is able to capture some key features of the subsurface velocity field. We have shown that the architecture can be applied to noisy seismic data with an accuracy that is better than some other machine learning methods. We also test our proposed method with out-of-distribution prediction for different velocity models. The proposed DeepONet shows comparable and better accuracy in some velocity models than some other machine learning methods. To improve the FWI workflow, we propose using the DeepONet output as a starting model for conventional FWI and that it may improve FWI performance. While we have only shown that DeepONet facilitates faster convergence than starting with a homogeneous velocity field, it may have some benefits compared to other approaches to constructing starting models. This integration of DeepONet into FWI may accelerate the inversion process and may also enhance its robustness and reliability.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.8 -->
                    
                <!-- LLMs: 6.6 -->
                    
                <!-- Quantum Computing: 4.4 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10726" target="_blank" rel="noopener noreferrer">Beyond the Classroom: Bridging the Gap Between Academia and Industry with a Hands-on Learning Approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mingyang Xu, Ryan Zheng He Liu, Mark Stoodley, Ladan Tahvildari
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modern software systems require various capabilities to meet architectural and operational demands, such as the ability to scale automatically and recover from sudden failures. Self-adaptive software systems have emerged as a critical focus in software design and operation due to their capacity to a</span>
                
                <span class="abstract-full" style="display: none;">Modern software systems require various capabilities to meet architectural and operational demands, such as the ability to scale automatically and recover from sudden failures. Self-adaptive software systems have emerged as a critical focus in software design and operation due to their capacity to autonomously adapt to changing environments. However, educating students on this topic is scarce in academia, and a survey among practitioners identified that the lack of knowledgeable individuals has hindered its adoption in the industry. In this paper, we present our experience teaching a course on self-adaptive software systems that integrates theoretical knowledge and hands-on learning with industry-relevant technologies. To close the gap between academic education and industry practices, we incorporated guest lectures from experts and showcases featuring industry professionals as judges, improving technical and communication skills for our students. Feedback based on surveys from 21 students indicates significant improvements in their understanding of self-adaptive systems. The empirical analysis of the developed course demonstrates the effectiveness of the proposed course syllabus and teaching methodology. In addition, we provide a summary of the educational challenges of running this unique course, including balancing theory and practice, addressing the diverse backgrounds and motivations of students, and integrating the industry-relevant technologies. We believe these insights can provide valuable guidance for educating students in other emerging topics within software engineering.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.3 -->
                    
                <!-- Medicine: 7.8 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Robotics: 2.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10728" target="_blank" rel="noopener noreferrer">Iterative Recommendations based on Monte Carlo Sampling and Trust Estimation in Multi-Stage Vehicular Traffic Routing Games</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Doris E. M. Brown, Venkata Sriram Siddhardh Nadendla, Sajal K. Das
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The shortest-time route recommendations offered by modern navigation systems fuel selfish routing in urban vehicular traffic networks and are therefore one of the main reasons for the growth of congestion. In contrast, intelligent transportation systems (ITS) prefer to steer driver-vehicle systems (</span>
                
                <span class="abstract-full" style="display: none;">The shortest-time route recommendations offered by modern navigation systems fuel selfish routing in urban vehicular traffic networks and are therefore one of the main reasons for the growth of congestion. In contrast, intelligent transportation systems (ITS) prefer to steer driver-vehicle systems (DVS) toward system-optimal route recommendations, which are primarily designed to mitigate network congestion. However, due to the misalignment in motives, drivers exhibit a lack of trust in the ITS. This paper models the interaction between a DVS and an ITS as a novel, multi-stage routing game where the DVS exhibits dynamics in its trust towards the recommendations of ITS based on counterfactual and observed game outcomes. Specifically, DVS and ITS are respectively modeled as a travel-time minimizer and network congestion minimizer, each having nonidentical prior beliefs about the network state. A novel approximate algorithm to compute the Bayesian Nash equilibrium, called ROSTER(Recommendation Outcome Sampling with Trust Estimation and Re-evaluation), is proposed based on Monte Carlo sampling with trust belief updating to determine the best response route recommendations of the ITS at each stage of the game. Simulation results demonstrate that the trust prediction error in the proposed algorithm converges to zero with a growing number of multi-stage DVS-ITS interactions and is effectively able to both mitigate congestion and reduce driver travel times when compared to alternative route recommendation strategies.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.5 -->
                    
                <!-- Medicine: 4.5 -->
                    
                <!-- Reinforcement Learning: 3.7 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Robotics: 2.4 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10730" target="_blank" rel="noopener noreferrer">PQ-CAN: A Framework for Simulating Post-Quantum Cryptography in Embedded Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mauro Conti, Francesco Marchiori, Sebastiano Matarazzo, Marco Rubin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rapid development of quantum computers threatens traditional cryptographic schemes, prompting the need for Post-Quantum Cryptography (PQC). Although the NIST standardization process has accelerated the development of such algorithms, their application in resource-constrained environments such as</span>
                
                <span class="abstract-full" style="display: none;">The rapid development of quantum computers threatens traditional cryptographic schemes, prompting the need for Post-Quantum Cryptography (PQC). Although the NIST standardization process has accelerated the development of such algorithms, their application in resource-constrained environments such as embedded systems remains a challenge. Automotive systems relying on the Controller Area Network (CAN) bus for communication are particularly vulnerable due to their limited computational capabilities, high traffic, and need for real-time response. These constraints raise concerns about the feasibility of implementing PQC in automotive environments, where legacy hardware and bit rate limitations must also be considered.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.9 -->
                    
                <!-- LLMs: 7.9 -->
                    
                <!-- Quantum Computing: 6.3 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10743" target="_blank" rel="noopener noreferrer">Robust Gittins for Stochastic Scheduling</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Benjamin Moseley, Heather Newman, Kirk Pruhs, Rudy Zhou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A common theme in stochastic optimization problems is that, theoretically, stochastic algorithms need to "know" relatively rich information about the underlying distributions. This is at odds with most applications, where distributions are rough predictions based on historical data. Thus, commonly, </span>
                
                <span class="abstract-full" style="display: none;">A common theme in stochastic optimization problems is that, theoretically, stochastic algorithms need to "know" relatively rich information about the underlying distributions. This is at odds with most applications, where distributions are rough predictions based on historical data. Thus, commonly, stochastic algorithms are making decisions using imperfect predicted distributions, while trying to optimize over some unknown true distributions. We consider the fundamental problem of scheduling stochastic jobs preemptively on a single machine to minimize expected mean completion time in the setting where the scheduler is only given imperfect predicted job size distributions. If the predicted distributions are perfect, then it is known that this problem can be solved optimally by the Gittins index policy. The goal of our work is to design a scheduling policy that is robust in the sense that it produces nearly optimal schedules even if there are modest discrepancies between the predicted distributions and the underlying real distributions. Our main contributions are:</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.6 -->
                    
                <!-- Quantum Computing: 4.6 -->
                    
                <!-- Math: 3.6 -->
                    
                <!-- GNN: 3.5 -->
                    
                <!-- Reinforcement Learning: 3.2 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- Medicine: 2.9 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                <!-- Multi-armed Bandit: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10746" target="_blank" rel="noopener noreferrer">Hearing Anywhere in Any Environment</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiulong Liu, Anurag Kumar, Paul Calamia, Sebastia V. Amengual, Calvin Murdock, Ishwarya Ananthabhotla, Philip Robinson, Eli Shlizerman, Vamsi Krishna Ithapu, Ruohan Gao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In mixed reality applications, a realistic acoustic experience in spatial environments is as crucial as the visual experience for achieving true immersion. Despite recent advances in neural approaches for Room Impulse Response (RIR) estimation, most existing methods are limited to the single environ</span>
                
                <span class="abstract-full" style="display: none;">In mixed reality applications, a realistic acoustic experience in spatial environments is as crucial as the visual experience for achieving true immersion. Despite recent advances in neural approaches for Room Impulse Response (RIR) estimation, most existing methods are limited to the single environment on which they are trained, lacking the ability to generalize to new rooms with different geometries and surface materials. We aim to develop a unified model capable of reconstructing the spatial acoustic experience of any environment with minimum additional measurements. To this end, we present xRIR, a framework for cross-room RIR prediction. The core of our generalizable approach lies in combining a geometric feature extractor, which captures spatial context from panorama depth images, with a RIR encoder that extracts detailed acoustic features from only a few reference RIR samples. To evaluate our method, we introduce ACOUSTICROOMS, a new dataset featuring high-fidelity simulation of over 300,000 RIRs from 260 rooms. Experiments show that our method strongly outperforms a series of baselines. Furthermore, we successfully perform sim-to-real transfer by evaluating our model on four real-world environments, demonstrating the generalizability of our approach and the realism of our dataset.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.9 -->
                    
                <!-- Medicine: 6.7 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10747" target="_blank" rel="noopener noreferrer">Encryption scheme based on Automorphism Group of Hermitian Function Field with Homomorphic Encryption</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gennady Khalimov, Yevgen Kotukh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This article proposes a comprehensive approach to implementing encryption schemes based on the automorphism group of the Hermitian function field. We utilize a three-parameter group with logarithmic representations outside the group's center. In this work, we enhance the Hermitian function field-bas</span>
                
                <span class="abstract-full" style="display: none;">This article proposes a comprehensive approach to implementing encryption schemes based on the automorphism group of the Hermitian function field. We utilize a three-parameter group with logarithmic representations outside the group's center. In this work, we enhance the Hermitian function field-based encryption scheme with homomorphic encryption capabilities, which constitutes a significant advantage of our implementation. Both the attack complexity and the encrypted message size are directly correlated with the order of the group.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.0 -->
                    
                <!-- LLMs: 4.6 -->
                    
                <!-- Reinforcement Learning: 4.6 -->
                    
                <!-- Math: 3.6 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- SpikingNN: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10751" target="_blank" rel="noopener noreferrer">Communication-aware Hierarchical Map Compression of Time-Varying Environments for Mobile Robots</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Daniel T. Larsson, Dipankar Maity
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we develop a systematic framework for the time-sequential compression of dynamic probabilistic occupancy grids. Our approach leverages ideas from signal compression theory to formulate an optimization problem that searches for a multi-resolution hierarchical encoder that balances the </span>
                
                <span class="abstract-full" style="display: none;">In this paper, we develop a systematic framework for the time-sequential compression of dynamic probabilistic occupancy grids. Our approach leverages ideas from signal compression theory to formulate an optimization problem that searches for a multi-resolution hierarchical encoder that balances the quality of the compressed map (distortion) with its description size, the latter of which relates to the bandwidth required to reliably transmit the map to other agents or to store map estimates in on-board memory. The resulting optimization problem allows for multi-resolution map compressions to be obtained that satisfy available communication or memory resources, and does not require knowledge of the occupancy map dynamics. We develop an algorithm to solve our problem, and demonstrate the utility of the proposed framework in simulation on both static (i.e., non-time varying) and dynamic (time-varying) occupancy maps.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.7 -->
                    
                <!-- Reinforcement Learning: 4.2 -->
                    
                <!-- Medicine: 4.0 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10754" target="_blank" rel="noopener noreferrer">auto-fpt: Automating Free Probability Theory Calculations for Machine Learning Theory</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Arjun Subramonian, Elvis Dohmatob
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A large part of modern machine learning theory often involves computing the high-dimensional expected trace of a rational expression of large rectangular random matrices. To symbolically compute such quantities using free probability theory, we introduce auto-fpt, a lightweight Python and SymPy-base</span>
                
                <span class="abstract-full" style="display: none;">A large part of modern machine learning theory often involves computing the high-dimensional expected trace of a rational expression of large rectangular random matrices. To symbolically compute such quantities using free probability theory, we introduce auto-fpt, a lightweight Python and SymPy-based tool that can automatically produce a reduced system of fixed-point equations which can be solved for the quantities of interest, and effectively constitutes a theory. We overview the algorithmic ideas underlying auto-fpt and its applications to various interesting problems, such as the high-dimensional error of linearized feed-forward neural networks, recovering well-known results. We hope that auto-fpt streamlines the majority of calculations involved in high-dimensional analysis, while helping the machine learning community reproduce known and uncover new phenomena.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.0 -->
                    
                <!-- LLMs: 6.2 -->
                    
                <!-- Quantum Computing: 5.9 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Math: 2.6 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10765" target="_blank" rel="noopener noreferrer">Minimal Sensing for Orienting a Solar Panel</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jeremy Klotz, Shree K. Nayar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A solar panel harvests the most energy when pointing in the direction that maximizes the total illumination (irradiance) falling on it. Given an arbitrary orientation of a panel and an arbitrary environmental illumination, we address the problem of finding the direction of maximum total irradiance. </span>
                
                <span class="abstract-full" style="display: none;">A solar panel harvests the most energy when pointing in the direction that maximizes the total illumination (irradiance) falling on it. Given an arbitrary orientation of a panel and an arbitrary environmental illumination, we address the problem of finding the direction of maximum total irradiance. We develop a minimal sensing approach where measurements from just four photodetectors are used to iteratively vary the tilt of the panel to maximize the irradiance. Many environments produce irradiance functions with multiple local maxima. As a result, simply measuring the gradient of the irradiance function and applying gradient ascent will not work. We show that a larger, optimized tilt between the detectors and the panel is equivalent to blurring the irradiance function. This has the effect of eliminating local maxima and turning the irradiance function into a unimodal one, whose maximum can be found using gradient ascent. We show that there is a close relationship between our approach and scale space theory. We have collected a large dataset of high-dynamic range lighting environments in New York City, called \textit{UrbanSky}. We used this dataset to conduct simulations to verify the robustness of our approach. Finally, we have built a portable solar panel with four compact detectors and an actuator to conduct experiments in various real-world settings: direct sunlight, cloudy sky, urban settings with occlusions and shadows, and complex indoor lighting. In all cases, we show significant improvements in harvested energy compared to standard approaches for controlling the orientation of a solar panel.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.4 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- Reinforcement Learning: 3.5 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10777" target="_blank" rel="noopener noreferrer">AtlasD: Automatic Local Symmetry Discovery</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Manu Bhat, Jonghyun Park, Jianke Yang, Nima Dehmamy, Robin Walters, Rose Yu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Existing symmetry discovery methods predominantly focus on global transformations across the entire system or space, but they fail to consider the symmetries in local neighborhoods. This may result in the reported symmetry group being a misrepresentation of the true symmetry. In this paper, we forma</span>
                
                <span class="abstract-full" style="display: none;">Existing symmetry discovery methods predominantly focus on global transformations across the entire system or space, but they fail to consider the symmetries in local neighborhoods. This may result in the reported symmetry group being a misrepresentation of the true symmetry. In this paper, we formalize the notion of local symmetry as atlas equivariance. Our proposed pipeline, automatic local symmetry discovery (AtlasD), recovers the local symmetries of a function by training local predictor networks and then learning a Lie group basis to which the predictors are equivariant. We demonstrate AtlasD is capable of discovering local symmetry groups with multiple connected components in top-quark tagging and partial differential equation experiments. The discovered local symmetry is shown to be a useful inductive bias that improves the performance of downstream tasks in climate segmentation and vision tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.2 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10783" target="_blank" rel="noopener noreferrer">Superfast Configuration-Space Convex Set Computation on GPUs for Online Motion Planning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Peter Werner, Richard Cheng, Tom Stewart, Russ Tedrake, Daniela Rus
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this work, we leverage GPUs to construct probabilistically collision-free convex sets in robot configuration space on the fly. This extends the use of modern motion planning algorithms that leverage such representations to changing environments. These planners rapidly and reliably optimize high-q</span>
                
                <span class="abstract-full" style="display: none;">In this work, we leverage GPUs to construct probabilistically collision-free convex sets in robot configuration space on the fly. This extends the use of modern motion planning algorithms that leverage such representations to changing environments. These planners rapidly and reliably optimize high-quality trajectories, without the burden of challenging nonconvex collision-avoidance constraints. We present an algorithm that inflates collision-free piecewise linear paths into sequences of convex sets (SCS) that are probabilistically collision-free using massive parallelism. We then integrate this algorithm into a motion planning pipeline, which leverages dynamic roadmaps to rapidly find one or multiple collision-free paths, and inflates them. We then optimize the trajectory through the probabilistically collision-free sets, simultaneously using the candidate trajectory to detect and remove collisions from the sets. We demonstrate the efficacy of our approach on a simulation benchmark and a KUKA iiwa 7 robot manipulator with perception in the loop. On our benchmark, our approach runs 17.1 times faster and yields a 27.9% increase in reliability over the nonlinear trajectory optimization baseline, while still producing high-quality motion plans.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.0 -->
                    
                <!-- Medicine: 5.4 -->
                    
                <!-- Quantum Computing: 4.2 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10793" target="_blank" rel="noopener noreferrer">SonicSieve: Bringing Directional Speech Extraction to Smartphones Using Acoustic Microstructures</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kuang Yuan, Yifeng Wang, Xiyuxing Zhang, Chengyi Shen, Swarun Kumar, Justin Chan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Imagine placing your smartphone on a table in a noisy restaurant and clearly capturing the voices of friends seated around you, or recording a lecturer's voice with clarity in a reverberant auditorium. We introduce SonicSieve, the first intelligent directional speech extraction system for smartphone</span>
                
                <span class="abstract-full" style="display: none;">Imagine placing your smartphone on a table in a noisy restaurant and clearly capturing the voices of friends seated around you, or recording a lecturer's voice with clarity in a reverberant auditorium. We introduce SonicSieve, the first intelligent directional speech extraction system for smartphones using a bio-inspired acoustic microstructure. Our passive design embeds directional cues onto incoming speech without any additional electronics. It attaches to the in-line mic of low-cost wired earphones which can be attached to smartphones. We present an end-to-end neural network that processes the raw audio mixtures in real-time on mobile devices. Our results show that SonicSieve achieves a signal quality improvement of 5.0 dB when focusing on a 30{\deg} angular region. Additionally, the performance of our system based on only two microphones exceeds that of conventional 5-microphone arrays.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.8 -->
                    
                <!-- Quantum Computing: 4.9 -->
                    
                <!-- Medicine: 4.5 -->
                    
                <!-- Reinforcement Learning: 3.2 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- GNN: 2.6 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- SpikingNN: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10800" target="_blank" rel="noopener noreferrer">Products of Recursive Programs for Hypersafety Verification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ruotong Cheng, Azadeh Farzan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the problem of automated hypersafety verification of infinite-state recursive programs. We propose an infinite class of product programs, specifically designed with recursion in mind, that reduce the hypersafety verification of a recursive program to standard safety verification. For this, </span>
                
                <span class="abstract-full" style="display: none;">We study the problem of automated hypersafety verification of infinite-state recursive programs. We propose an infinite class of product programs, specifically designed with recursion in mind, that reduce the hypersafety verification of a recursive program to standard safety verification. For this, we combine insights from language theory and concurrency theory to propose an algorithmic solution for constructing an infinite class of recursive product programs. One key insight is that, using the simple theory of visibly pushdown languages, one can maintain the recursive structure of syntactic program alignments which is vital to constructing a new product program that can be viewed as a classic recursive program -- that is, one that can be executed on a single stack. Another key insight is that techniques from concurrency theory can be generalized to help define product programs based on the view that the parallel composition of individual recursive programs includes all possible alignments from which a sound set of alignments that faithfully preserve the satisfaction of the hypersafety property can be selected. On the practical side, we formulate a family of parametric canonical product constructions that are intuitive to programmers and can be used as building blocks to specify recursive product programs for the purpose of relational and hypersafety verification, with the idea that the right product program can be verified automatically using existing techniques. We demonstrate the effectiveness of these techniques through an implementation and highly promising experimental results.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Math: 6.4 -->
                    
                <!-- Quantum Computing: 5.7 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- SpikingNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10807" target="_blank" rel="noopener noreferrer">Power-scaled Bayesian Inference with Score-based Generative mModels</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Huseyin Tuna Erdinc, Yunlin Zeng, Abhinav Prakash Gahlot, Felix J. Herrmann
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a score-based generative algorithm for sampling from power-scaled priors and likelihoods within the Bayesian inference framework. Our algorithm enables flexible control over prior-likelihood influence without requiring retraining for different power-scaling configurations. Specifically, w</span>
                
                <span class="abstract-full" style="display: none;">We propose a score-based generative algorithm for sampling from power-scaled priors and likelihoods within the Bayesian inference framework. Our algorithm enables flexible control over prior-likelihood influence without requiring retraining for different power-scaling configurations. Specifically, we focus on synthesizing seismic velocity models conditioned on imaged seismic. Our method enables sensitivity analysis by sampling from intermediate power posteriors, allowing us to assess the relative influence of the prior and likelihood on samples of the posterior distribution. Through a comprehensive set of experiments, we evaluate the effects of varying the power parameter in different settings: applying it solely to the prior, to the likelihood of a Bayesian formulation, and to both simultaneously. The results show that increasing the power of the likelihood up to a certain threshold improves the fidelity of posterior samples to the conditioning data (e.g., seismic images), while decreasing the prior power promotes greater structural diversity among samples. Moreover, we find that moderate scaling of the likelihood leads to a reduced shot data residual, confirming its utility in posterior refinement.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 5.2 -->
                    
                <!-- LLMs: 5.1 -->
                    
                <!-- Medicine: 3.7 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10810" target="_blank" rel="noopener noreferrer">PatrolVision: Automated License Plate Recognition in the wild</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Anmol Singhal Navya Singhal
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Adoption of AI driven techniques in public services remains low due to challenges related to accuracy and speed of information at population scale. Computer vision techniques for traffic monitoring have not gained much popularity despite their relative strength in areas such as autonomous driving. D</span>
                
                <span class="abstract-full" style="display: none;">Adoption of AI driven techniques in public services remains low due to challenges related to accuracy and speed of information at population scale. Computer vision techniques for traffic monitoring have not gained much popularity despite their relative strength in areas such as autonomous driving. Despite large number of academic methods for Automatic License Plate Recognition (ALPR) systems, very few provide an end to end solution for patrolling in the city. This paper presents a novel prototype for a low power GPU based patrolling system to be deployed in an urban environment on surveillance vehicles for automated vehicle detection, recognition and tracking. In this work, we propose a complete ALPR system for Singapore license plates having both single and double line creating our own YOLO based network. We focus on unconstrained capture scenarios as would be the case in real world application, where the license plate (LP) might be considerably distorted due to oblique views. In this work, we first detect the license plate from the full image using RFB-Net and rectify multiple distorted license plates in a single image. After that, the detected license plate image is fed to our network for character recognition. We evaluate the performance of our proposed system on a newly built dataset covering more than 16,000 images. The system was able to correctly detect license plates with 86\% precision and recognize characters of a license plate in 67\% of the test set, and 89\% accuracy with one incorrect character (partial match). We also test latency of our system and achieve 64FPS on Tesla P4 GPU</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.5 -->
                    
                <!-- LLMs: 6.2 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10811" target="_blank" rel="noopener noreferrer">FlexiContracts: A Novel and Efficient Scheme for Upgrading Smart Contracts in Ethereum Blockchain</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tahrim Hossain, Sakib Hassan, Faisal Haque Bappy, Muhammad Nur Yanhaona, Sarker Ahmed Rumee, Moinul Zaber, Tariqul Islam
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Blockchain technology has revolutionized contractual processes, enhancing efficiency and trust through smart contracts. Ethereum, as a pioneer in this domain, offers a platform for decentralized applications but is challenged by the immutability of smart contracts, which makes upgrades cumbersome. E</span>
                
                <span class="abstract-full" style="display: none;">Blockchain technology has revolutionized contractual processes, enhancing efficiency and trust through smart contracts. Ethereum, as a pioneer in this domain, offers a platform for decentralized applications but is challenged by the immutability of smart contracts, which makes upgrades cumbersome. Existing design patterns, while addressing upgradability, introduce complexity, increased development effort, and higher gas costs, thus limiting their effectiveness. In response, we introduce FlexiContracts, an innovative scheme that reimagines the evolution of smart contracts on Ethereum. By enabling secure, in-place upgrades without losing historical data, FlexiContracts surpasses existing approaches, introducing a previously unexplored path in smart contract evolution. Its streamlined design transcends the limitations of current design patterns by simplifying smart contract development, eliminating the need for extensive upfront planning, and significantly reducing the complexity of the design process. This advancement fosters an environment for continuous improvement and adaptation to new requirements, redefining the possibilities for dynamic, upgradable smart contracts.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.0 -->
                    
                <!-- LLMs: 8.9 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10819" target="_blank" rel="noopener noreferrer">Generalized Audio Deepfake Detection Using Frame-level Latent Information Entropy</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Botao Zhao, Zuheng Kang, Yayun He, Xiaoyang Qu, Junqing Peng, Jing Xiao, Jianzong Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generalizability, the capacity of a robust model to perform effectively on unseen data, is crucial for audio deepfake detection due to the rapid evolution of text-to-speech (TTS) and voice conversion (VC) technologies. A promising approach to differentiate between bonafide and spoof samples lies in </span>
                
                <span class="abstract-full" style="display: none;">Generalizability, the capacity of a robust model to perform effectively on unseen data, is crucial for audio deepfake detection due to the rapid evolution of text-to-speech (TTS) and voice conversion (VC) technologies. A promising approach to differentiate between bonafide and spoof samples lies in identifying intrinsic disparities to enhance model generalizability. From an information-theoretic perspective, we hypothesize the information content is one of the intrinsic differences: bonafide sample represents a dense, information-rich sampling of the real world, whereas spoof sample is typically derived from lower-dimensional, less informative representations. To implement this, we introduce frame-level latent information entropy detector(f-InfoED), a framework that extracts distinctive information entropy from latent representations at the frame level to identify audio deepfakes. Furthermore, we present AdaLAM, which extends large pre-trained audio models with trainable adapters for enhanced feature extraction. To facilitate comprehensive evaluation, the audio deepfake forensics 2024 (ADFF 2024) dataset was built by the latest TTS and VC methods. Extensive experiments demonstrate that our proposed approach achieves state-of-the-art performance and exhibits remarkable generalization capabilities. Further analytical studies confirms the efficacy of AdaLAM in extracting discriminative audio features and f-InfoED in leveraging latent entropy information for more generalized deepfake detection.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.8 -->
                    
                <!-- LLMs: 7.3 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10822" target="_blank" rel="noopener noreferrer">IlluSign: Illustrating Sign Language Videos by Leveraging the Attention Mechanism</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Janna Bruner, Amit Moryossef, Lior Wolf
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Sign languages are dynamic visual languages that involve hand gestures, in combination with non manual elements such as facial expressions. While video recordings of sign language are commonly used for education and documentation, the dynamic nature of signs can make it challenging to study them in </span>
                
                <span class="abstract-full" style="display: none;">Sign languages are dynamic visual languages that involve hand gestures, in combination with non manual elements such as facial expressions. While video recordings of sign language are commonly used for education and documentation, the dynamic nature of signs can make it challenging to study them in detail, especially for new learners and educators. This work aims to convert sign language video footage into static illustrations, which serve as an additional educational resource to complement video content. This process is usually done by an artist, and is therefore quite costly. We propose a method that illustrates sign language videos by leveraging generative models' ability to understand both the semantic and geometric aspects of images. Our approach focuses on transferring a sketch like illustration style to video footage of sign language, combining the start and end frames of a sign into a single illustration, and using arrows to highlight the hand's direction and motion. While many style transfer methods address domain adaptation at varying levels of abstraction, applying a sketch like style to sign languages, especially for hand gestures and facial expressions, poses a significant challenge. To tackle this, we intervene in the denoising process of a diffusion model, injecting style as keys and values into high resolution attention layers, and fusing geometric information from the image and edges as queries. For the final illustration, we use the attention mechanism to combine the attention weights from both the start and end illustrations, resulting in a soft combination. Our method offers a cost effective solution for generating sign language illustrations at inference time, addressing the lack of such resources in educational materials.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.7 -->
                    
                <!-- Medicine: 5.4 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10825" target="_blank" rel="noopener noreferrer">OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dianbing Xi, Jiepeng Wang, Yuanzhi Liang, Xi Qiu, Yuchi Huo, Rui Wang, Chi Zhang, Xuelong Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff, aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, whil</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff, aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality. This allows flexible manipulation of each modality's role, enabling support for a wide range of tasks. Consequently, our model supports three key functionalities: (1) Text-conditioned video generation: multi-modal visual video sequences (i.e., rgb, depth, canny, segmentaion) are generated based on the text conditions in one diffusion process; (2) Video understanding: OmniVDiff can estimate the depth, canny map, and semantic segmentation across the input rgb frames while ensuring coherence with the rgb input; and (3) X-conditioned video generation: OmniVDiff generates videos conditioned on fine-grained attributes (e.g., depth maps or segmentation maps). By integrating these diverse tasks into a unified video diffusion framework, OmniVDiff enhances the flexibility and scalability for controllable video diffusion, making it an effective tool for a variety of downstream applications, such as video-to-video translation. Extensive experiments demonstrate the effectiveness of our approach, highlighting its potential for various video-related applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.4 -->
                    
                <!-- LLMs: 6.8 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10831" target="_blank" rel="noopener noreferrer">Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hyojun Ahn, Seungcheol Oh, Gyu Seon Kim, Soyi Jung, Soohyun Park, Joongheon Kim
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper proposes SafeGPT, a two-tiered framework that integrates generative pretrained transformers (GPTs) with reinforcement learning (RL) for efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In the proposed design, a Global GPT module assigns high-level tasks such as s</span>
                
                <span class="abstract-full" style="display: none;">This paper proposes SafeGPT, a two-tiered framework that integrates generative pretrained transformers (GPTs) with reinforcement learning (RL) for efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In the proposed design, a Global GPT module assigns high-level tasks such as sector allocation, while an On-Device GPT manages real-time local route planning. An RL-based safety filter monitors each GPT decision and overrides unsafe actions that could lead to battery depletion or duplicate visits, effectively mitigating hallucinations. Furthermore, a dual replay buffer mechanism helps both the GPT modules and the RL agent refine their strategies over time. Simulation results demonstrate that SafeGPT achieves higher delivery success rates compared to a GPT-only baseline, while substantially reducing battery consumption and travel distance. These findings validate the efficacy of combining GPT-based semantic reasoning with formal safety guarantees, contributing a viable solution for robust and energy-efficient UAV logistics.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.3 -->
                    
                <!-- LLMs: 8.4 -->
                    
                <!-- Quantum Computing: 4.4 -->
                    
                <!-- Networks: 3.3 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10848" target="_blank" rel="noopener noreferrer">Ichiyo: Fragile and Transient Interaction in Neighborhood</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hirofumi Shibata, Ayako Yogo, Naoto Nishida, Yu Shimada, Toma Ishii
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As the Internet develops, social networking and other communication tools have transformed people's relationships into something fast, visible, and geographically huge. However, these communication tools have not expanded opportunities for acquainting oneself with neighbors outside one's social netw</span>
                
                <span class="abstract-full" style="display: none;">As the Internet develops, social networking and other communication tools have transformed people's relationships into something fast, visible, and geographically huge. However, these communication tools have not expanded opportunities for acquainting oneself with neighbors outside one's social network; rather, they have comparatively diminished occasions for interacting with unfamiliar neighbors by prioritizing communication with existing friends. Therefore, we invented the medium Ichiyo to increase the opportunities to think of neighbors walking along the same street or in the same neighborhood and to expand the imagination of those who pass by and those who used to be there. Thus, users can engage in indirect interaction. We used commercially available laser cutters to engrave QR codes on leaves that are naturally found in our living space to prevent environmental invasion. The QR codes lead to a communal space on the web where users can freely leave messages. By engraving QR codes, information can be virtually expanded to be presented. To get the feedback of Ichiyo, we let a total of several thousand people experience a new way of communication as a part of the exhibition ''iii Exhibition 2022'', an art exhibition at the University of Tokyo. A total of more than 1,000 leaves engraved with QR codes were prepared and scattered at the exhibition site and along the road from the nearest station to the venue.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.4 -->
                    
                <!-- Medicine: 4.9 -->
                    
                <!-- Reinforcement Learning: 3.5 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10849" target="_blank" rel="noopener noreferrer">Real-Time Word-Level Temporal Segmentation in Streaming Speech Recognition</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Naoto Nishida, Hirotaka Hiraki, Jun Rekimoto, Yoshio Ishiguro
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Rich-text captions are essential to help communication for Deaf and hard-of-hearing (DHH) people, second-language learners, and those with autism spectrum disorder (ASD). They also preserve nuances when converting speech to text, enhancing the realism of presentation scripts and conversation or spee</span>
                
                <span class="abstract-full" style="display: none;">Rich-text captions are essential to help communication for Deaf and hard-of-hearing (DHH) people, second-language learners, and those with autism spectrum disorder (ASD). They also preserve nuances when converting speech to text, enhancing the realism of presentation scripts and conversation or speech logs. However, current real-time captioning systems lack the capability to alter text attributes (ex. capitalization, sizes, and fonts) at the word level, hindering the accurate conveyance of speaker intent that is expressed in the tones or intonations of the speech. For example, ''YOU should do this'' tends to be considered as indicating ''You'' as the focus of the sentence, whereas ''You should do THIS'' tends to be ''This'' as the focus. This paper proposes a solution that changes the text decorations at the word level in real time. As a prototype, we developed an application that adjusts word size based on the loudness of each spoken word. Feedback from users implies that this system helped to convey the speaker's intent, offering a more engaging and accessible captioning experience.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.4 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Reinforcement Learning: 3.3 -->
                    
                <!-- Math: 2.7 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10850" target="_blank" rel="noopener noreferrer">How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Meiqi Liu, Zhuoqun Huang, Yue Xing
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">With the rise of powerful foundation models, a pre-training-fine-tuning paradigm becomes increasingly popular these days: A foundation model is pre-trained using a huge amount of data from various sources, and then the downstream users only need to fine-tune and adapt it to specific downstream tasks</span>
                
                <span class="abstract-full" style="display: none;">With the rise of powerful foundation models, a pre-training-fine-tuning paradigm becomes increasingly popular these days: A foundation model is pre-trained using a huge amount of data from various sources, and then the downstream users only need to fine-tune and adapt it to specific downstream tasks. However, due to the high computation complexity of adversarial training, it is not feasible to fine-tune the foundation model to improve its robustness on the downstream task. Observing the above challenge, we want to improve the downstream robustness without updating/accessing the weights in the foundation model. Inspired from existing literature in robustness inheritance (Kim et al., 2020), through theoretical investigation, we identify a close relationship between robust contrastive learning with the adversarial robustness of supervised learning. To further validate and utilize this theoretical insight, we design a simple-yet-effective robust auto-encoder as a data pre-processing method before feeding the data into the foundation model. The proposed approach has zero access to the foundation model when training the robust auto-encoder. Extensive experiments demonstrate the effectiveness of the proposed method in improving the robustness of downstream tasks, verifying the connection between the feature robustness (implied by small adversarial contrastive loss) and the robustness of the downstream task.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 6.3 -->
                    
                <!-- Medicine: 4.2 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Federated Learning: 3.3 -->
                    
                <!-- Math: 2.6 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Pathfinding: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10853" target="_blank" rel="noopener noreferrer">PT-Mark: Invisible Watermarking for Text-to-image Diffusion Models via Semantic-aware Pivotal Tuning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yaopeng Wang, Huiyu Xu, Zhibo Wang, Jiacheng Du, Zhichao Li, Yiming Li, Qiu Wang, Kui Ren
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Watermarking for diffusion images has drawn considerable attention due to the widespread use of text-to-image diffusion models and the increasing need for their copyright protection. Recently, advanced watermarking techniques, such as Tree Ring, integrate watermarks by embedding traceable patterns (</span>
                
                <span class="abstract-full" style="display: none;">Watermarking for diffusion images has drawn considerable attention due to the widespread use of text-to-image diffusion models and the increasing need for their copyright protection. Recently, advanced watermarking techniques, such as Tree Ring, integrate watermarks by embedding traceable patterns (e.g., Rings) into the latent distribution during the diffusion process. Such methods disrupt the original semantics of the generated images due to the inevitable distribution shift caused by the watermarks, thereby limiting their practicality, particularly in digital art creation. In this work, we present Semantic-aware Pivotal Tuning Watermarks (PT-Mark), a novel invisible watermarking method that preserves both the semantics of diffusion images and the traceability of the watermark. PT-Mark preserves the original semantics of the watermarked image by gradually aligning the generation trajectory with the original (pivotal) trajectory while maintaining the traceable watermarks during whole diffusion denoising process. To achieve this, we first compute the salient regions of the watermark at each diffusion denoising step as a spatial prior to identify areas that can be aligned without disrupting the watermark pattern. Guided by the region, we then introduce an additional pivotal tuning branch that optimizes the text embedding to align the semantics while preserving the watermarks. Extensive evaluations demonstrate that PT-Mark can preserve the original semantics of the diffusion images while integrating robust watermarks. It achieves a 10% improvement in the performance of semantic preservation (i.e., SSIM, PSNR, and LPIPS) compared to state-of-the-art watermarking methods, while also showing comparable robustness against real-world perturbations and four times greater efficiency.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.8 -->
                    
                <!-- Medicine: 4.3 -->
                    
                <!-- Reinforcement Learning: 4.1 -->
                    
                <!-- Math: 3.0 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10855" target="_blank" rel="noopener noreferrer">Virtual Contraction Approach to Decentralized Adaptive Stabilization of Nonlinear Time-Delayed Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yu Kawano, Zhiyong Sun
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we utilize a diagonally dominant structure for the decentralized stabilization of unknown nonlinear time-delayed networks. Generalizing the idea of virtual contraction analysis to time-delayed systems, we demonstrate that nonlinear time-delayed networks can be stabilized by diagonal h</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we utilize a diagonally dominant structure for the decentralized stabilization of unknown nonlinear time-delayed networks. Generalizing the idea of virtual contraction analysis to time-delayed systems, we demonstrate that nonlinear time-delayed networks can be stabilized by diagonal high-gains if the input matrices possess certain generalized (column/row) diagonally dominant properties. To achieve stabilization of unknown networks, we further propose a distributed adaptive tuning rule for each individual gain function, ensuring that all closed-loop trajectories converge to the origin. The effectiveness of the proposed decentralized adaptive control is verified in a case study on epidemic spreading control in SIS networks with transmission delays.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.9 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- GNN: 3.0 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- SpikingNN: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10859" target="_blank" rel="noopener noreferrer">A Sublinear Algorithm for Path Feasibility Among Rectangular Obstacles</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alex Fan, Alicia Li, Arul Kolla, Jason Gonzalez
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The problem of finding a path between two points while avoiding obstacles is critical in robotic path planning. We focus on the feasibility problem: determining whether such a path exists. We model the robot as a query-specific rectangular object capable of moving parallel to its sides. The obstacle</span>
                
                <span class="abstract-full" style="display: none;">The problem of finding a path between two points while avoiding obstacles is critical in robotic path planning. We focus on the feasibility problem: determining whether such a path exists. We model the robot as a query-specific rectangular object capable of moving parallel to its sides. The obstacles are axis-aligned, rectangular, and may overlap. Most previous works only consider nondisjoint rectangular objects and point-sized or statically sized robots. Our approach introduces a novel technique leveraging generalized Gabriel graphs and constructs a data structure to facilitate online queries regarding path feasibility with varying robot sizes in sublinear time. To efficiently handle feasibility queries, we propose an online algorithm utilizing sweep line to construct a generalized Gabriel graph under the $L_\infty$ norm, capturing key gap constraints between obstacles. We utilize a persistent disjoint-set union data structure to efficiently determine feasibility queries in $\mathcal{O}(\log n)$ time and $\mathcal{O}(n)$ total space.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.3 -->
                    
                <!-- Medicine: 6.0 -->
                    
                <!-- Networks: 3.5 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- GNN: 2.6 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Attention: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10864" target="_blank" rel="noopener noreferrer">Automata for the commutative closure of regular sets</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ver\'onica Becher, Simon Lew Deveali, Ignacio Mollo Cunningham
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Consider $ A^* $, the free monoid generated by the finite alphabet $A$ with the concatenation operation. Two words have the same commutative image when one is a permutation of the symbols of the other. The commutative closure of a set $ L \subseteq A^* $ is the set $ {C}(L) \subseteq A^* $ of words </span>
                
                <span class="abstract-full" style="display: none;">Consider $ A^* $, the free monoid generated by the finite alphabet $A$ with the concatenation operation. Two words have the same commutative image when one is a permutation of the symbols of the other. The commutative closure of a set $ L \subseteq A^* $ is the set $ {C}(L) \subseteq A^* $ of words whose commutative image coincides with that of some word in $ L $. We provide an algorithm that, given a regular set $ L $, produces a finite state automaton that accepts the commutative closure $ {C}(L) $, provided that this closure set is regular. The problem of deciding whether $ {C}(L) $ is regular was solved by Ginsburg and Spanier in 1966 using the decidability of Presburger sentences, and by Gohon in 1985 via formal power series. The problem of constructing an automaton that accepts $ {C}(L) $ has already been studied in the literature. We give a simpler algorithm using an algebraic approach.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.5 -->
                    
                <!-- Medicine: 5.7 -->
                    
                <!-- Math: 3.9 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Pathfinding: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10865" target="_blank" rel="noopener noreferrer">Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Han-Dong Lim, Donghwan Lee
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we study the theoretical properties of the projected Bellman equation (PBE) and two algorithms to solve this equation: linear Q-learning and approximate value iteration (AVI). We consider two sufficient conditions for the existence of a solution to PBE : strictly negatively row domina</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we study the theoretical properties of the projected Bellman equation (PBE) and two algorithms to solve this equation: linear Q-learning and approximate value iteration (AVI). We consider two sufficient conditions for the existence of a solution to PBE : strictly negatively row dominating diagonal (SNRDD) assumption and a condition motivated by the convergence of AVI. The SNRDD assumption also ensures the convergence of linear Q-learning, and its relationship with the convergence of AVI is examined. Lastly, several interesting observations on the solution of PBE are provided when using $\epsilon$-greedy policy.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.0 -->
                    
                <!-- LLMs: 8.8 -->
                    
                <!-- Quantum Computing: 4.1 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Math: 2.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- GNN: 1.0 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10877" target="_blank" rel="noopener noreferrer">Weather-Aware Object Detection Transformer for Domain Adaptation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Soheil Gharatappeh, Salimeh Sekeh, Vikas Dhiman
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">RT-DETRs have shown strong performance across various computer vision tasks but are known to degrade under challenging weather conditions such as fog. In this work, we investigate three novel approaches to enhance RT-DETR robustness in foggy environments: (1) Domain Adaptation via Perceptual Loss, w</span>
                
                <span class="abstract-full" style="display: none;">RT-DETRs have shown strong performance across various computer vision tasks but are known to degrade under challenging weather conditions such as fog. In this work, we investigate three novel approaches to enhance RT-DETR robustness in foggy environments: (1) Domain Adaptation via Perceptual Loss, which distills domain-invariant features from a teacher network to a student using perceptual supervision; (2) Weather Adaptive Attention, which augments the attention mechanism with fog-sensitive scaling by introducing an auxiliary foggy image stream; and (3) Weather Fusion Encoder, which integrates a dual-stream encoder architecture that fuses clear and foggy image features via multi-head self and cross-attention. Despite the architectural innovations, none of the proposed methods consistently outperform the baseline RT-DETR. We analyze the limitations and potential causes, offering insights for future research in weather-aware object detection.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.3 -->
                    
                <!-- Medicine: 6.7 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10898" target="_blank" rel="noopener noreferrer">Xpose: Bi-directional Engineering for Hidden Query Extraction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ahana Pradhan, Jayant Haritsa
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Query reverse engineering (QRE) aims to synthesize a SQL query to connect a given database and result instance. A recent variation of QRE is where an additional input, an opaque executable containing a ground-truth query, is provided, and the goal is to non-invasively extract this specific query thr</span>
                
                <span class="abstract-full" style="display: none;">Query reverse engineering (QRE) aims to synthesize a SQL query to connect a given database and result instance. A recent variation of QRE is where an additional input, an opaque executable containing a ground-truth query, is provided, and the goal is to non-invasively extract this specific query through only input-output examples. This variant, called Hidden Query Extraction (HQE), has a spectrum of industrial use-cases including query recovery, database security, and vendor migration. The reverse engineering (RE) tools developed for HQE, which are based on database mutation and generation techniques, can only extract flat queries with key-based equi joins and conjunctive arithmetic filter predicates, making them limited wrt both query structure and query operators. In this paper, we present Xpose, a HQE solution that elevates the extraction scope to realistic complex queries, such as those found in the TPCH benchmark. A two-pronged approach is taken: (1) The existing RE scope is substantially extended to incorporate union connectors, algebraic filter predicates, and disjunctions for both values and predicates. (2) The predictive power of LLMs is leveraged to convert business descriptions of the opaque application into extraction guidance, representing ``forward engineering" (FE). The FE module recognizes common constructs, such as nesting of sub-queries, outer joins, and scalar functions. In essence, FE establishes the broad query contours, while RE fleshes out the fine-grained details. We have evaluated Xpose on (a) E-TPCH, a query suite comprising the complete TPCH benchmark extended with queries featuring unions, diverse join types, and sub-queries; and (b) the real-world STACK benchmark. The experimental results demonstrate that its bi-directional engineering approach accurately extracts these complex queries, representing a significant step forward with regard to HQE coverage.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.3 -->
                    
                <!-- LLMs: 8.2 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10905" target="_blank" rel="noopener noreferrer">InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yukang Lin, Yan Hong, Zunnan Xu, Xindi Li, Chao Xu, Chuanbiao Song, Ronghui Li, Haoxing Chen, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang, Xiu Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent video generation research has focused heavily on isolated actions, leaving interactive motions-such as hand-face interactions-largely unexamined. These interactions are essential for emerging biometric authentication systems, which rely on interactive motion-based anti-spoofing approaches. Fr</span>
                
                <span class="abstract-full" style="display: none;">Recent video generation research has focused heavily on isolated actions, leaving interactive motions-such as hand-face interactions-largely unexamined. These interactions are essential for emerging biometric authentication systems, which rely on interactive motion-based anti-spoofing approaches. From a security perspective, there is a growing need for large-scale, high-quality interactive videos to train and strengthen authentication models. In this work, we introduce a novel paradigm for animating realistic hand-face interactions. Our approach simultaneously learns spatio-temporal contact dynamics and biomechanically plausible deformation effects, enabling natural interactions where hand movements induce anatomically accurate facial deformations while maintaining collision-free contact. To facilitate this research, we present InterHF, a large-scale hand-face interaction dataset featuring 18 interaction patterns and 90,000 annotated videos. Additionally, we propose InterAnimate, a region-aware diffusion model designed specifically for interaction animation. InterAnimate leverages learnable spatial and temporal latents to effectively capture dynamic interaction priors and integrates a region-aware interaction mechanism that injects these priors into the denoising process. To the best of our knowledge, this work represents the first large-scale effort to systematically study human hand-face interactions. Qualitative and quantitative results show InterAnimate produces highly realistic animations, setting a new benchmark. Code and data will be made public to advance research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.2 -->
                    
                <!-- LLMs: 8.3 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10920" target="_blank" rel="noopener noreferrer">Towards Efficient Partially Relevant Video Retrieval with Active Moment Discovering</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Peipei Song, Long Zhang, Long Lan, Weidong Chen, Dan Guo, Xun Yang, Meng Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Partially relevant video retrieval (PRVR) is a practical yet challenging task in text-to-video retrieval, where videos are untrimmed and contain much background content. The pursuit here is of both effective and efficient solutions to capture the partial correspondence between text queries and untri</span>
                
                <span class="abstract-full" style="display: none;">Partially relevant video retrieval (PRVR) is a practical yet challenging task in text-to-video retrieval, where videos are untrimmed and contain much background content. The pursuit here is of both effective and efficient solutions to capture the partial correspondence between text queries and untrimmed videos. Existing PRVR methods, which typically focus on modeling multi-scale clip representations, however, suffer from content independence and information redundancy, impairing retrieval performance. To overcome these limitations, we propose a simple yet effective approach with active moment discovering (AMDNet). We are committed to discovering video moments that are semantically consistent with their queries. By using learnable span anchors to capture distinct moments and applying masked multi-moment attention to emphasize salient moments while suppressing redundant backgrounds, we achieve more compact and informative video representations. To further enhance moment modeling, we introduce a moment diversity loss to encourage different moments of distinct regions and a moment relevance loss to promote semantically query-relevant moments, which cooperate with a partially relevant retrieval loss for end-to-end optimization. Extensive experiments on two large-scale video datasets (\ie, TVR and ActivityNet Captions) demonstrate the superiority and efficiency of our AMDNet. In particular, AMDNet is about 15.5 times smaller (\#parameters) while 6.0 points higher (SumR) than the up-to-date method GMMFormer on TVR.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.8 -->
                    
                <!-- Medicine: 7.6 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- T2I: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10925" target="_blank" rel="noopener noreferrer">Transfer Learning for Temporal Link Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ayan Chatterjee, Barbara Ikica, Babak Ravandi, John Palowitch
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Link prediction on graphs has applications spanning from recommender systems to drug discovery. Temporal link prediction (TLP) refers to predicting future links in a temporally evolving graph and adds additional complexity related to the dynamic nature of graphs. State-of-the-art TLP models incorpor</span>
                
                <span class="abstract-full" style="display: none;">Link prediction on graphs has applications spanning from recommender systems to drug discovery. Temporal link prediction (TLP) refers to predicting future links in a temporally evolving graph and adds additional complexity related to the dynamic nature of graphs. State-of-the-art TLP models incorporate memory modules alongside graph neural networks to learn both the temporal mechanisms of incoming nodes and the evolving graph topology. However, memory modules only store information about nodes seen at train time, and hence such models cannot be directly transferred to entirely new graphs at test time and deployment. In this work, we study a new transfer learning task for temporal link prediction, and develop transfer-effective methods for memory-laden models. Specifically, motivated by work showing the informativeness of structural signals for the TLP task, we augment a structural mapping module to the existing TLP model architectures, which learns a mapping from graph structural (topological) features to memory embeddings. Our work paves the way for a memory-free foundation model for TLP.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.2 -->
                    
                <!-- LLMs: 5.8 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10932" target="_blank" rel="noopener noreferrer">Multi-scale DeepOnet (Mscale-DeepOnet) for Mitigating Spectral Bias in Learning High Frequency Operators of Oscillatory Functions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bo Wang, Lizuo Liu, Wei Cai
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, a multi-scale DeepOnet (Mscale-DeepOnet) is proposed to reduce the spectral bias of the DeepOnet in learning high-frequency mapping between highly oscillatory functions, with an application to the nonlinear mapping between the coefficient of the Helmholtz equation and its solution. Th</span>
                
                <span class="abstract-full" style="display: none;">In this paper, a multi-scale DeepOnet (Mscale-DeepOnet) is proposed to reduce the spectral bias of the DeepOnet in learning high-frequency mapping between highly oscillatory functions, with an application to the nonlinear mapping between the coefficient of the Helmholtz equation and its solution. The Mscale-DeepOnet introduces the multiscale neural network in the branch and trunk networks of the original DeepOnet, the resulting Mscale-DeepOnet is shown to be able to capture various high-frequency components of the mapping itself and its image. Numerical results demonstrate the substantial improvement of the Mscale-DeepOnet for the problem of wave scattering in the high-frequency regime over the normal DeepOnet with a similar number of network parameters.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.9 -->
                    
                <!-- Reinforcement Learning: 3.8 -->
                    
                <!-- Math: 3.5 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Robotics: 2.3 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Pathfinding: 2.0 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10933" target="_blank" rel="noopener noreferrer">Towards Robust Trajectory Embedding for Similarity Computation: When Triangle Inequality Violations in Distance Metrics Matter</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jianing Si, Haitao Yuan, Nan Jiang, Minxiao Chen, Xiao Ma, Shangguang Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Trajectory similarity is a cornerstone of trajectory data management and analysis. Traditional similarity functions often suffer from high computational complexity and a reliance on specific distance metrics, prompting a shift towards deep representation learning in Euclidean space. However, existin</span>
                
                <span class="abstract-full" style="display: none;">Trajectory similarity is a cornerstone of trajectory data management and analysis. Traditional similarity functions often suffer from high computational complexity and a reliance on specific distance metrics, prompting a shift towards deep representation learning in Euclidean space. However, existing Euclidean-based trajectory embeddings often face challenges due to the triangle inequality constraints that do not universally hold for trajectory data. To address this issue, this paper introduces a novel approach by incorporating non-Euclidean geometry, specifically hyperbolic space, into trajectory representation learning. We present the first-ever integration of hyperbolic space to resolve the inherent limitations of the triangle inequality in Euclidean embeddings. In particular, we achieve it by designing a Lorentz distance measure, which is proven to overcome triangle inequality constraints. Additionally, we design a model-agnostic framework LH-plugin to seamlessly integrate hyperbolic embeddings into existing representation learning pipelines. This includes a novel projection method optimized with the Cosh function to prevent the diminishment of distances, supported by a theoretical foundation. Furthermore, we propose a dynamic fusion distance that intelligently adapts to variations in triangle inequality constraints across different trajectory pairs, blending Lorentzian and Euclidean distances for more robust similarity calculations. Comprehensive experimental evaluations demonstrate that our approach effectively enhances the accuracy of trajectory similarity measures in state-of-the-art models across multiple real-world datasets. The LH-plugin not only addresses the triangle inequality issues but also significantly refines the precision of trajectory similarity computations, marking a substantial advancement in the field of trajectory representation learning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.4 -->
                    
                <!-- Medicine: 4.2 -->
                    
                <!-- Reinforcement Learning: 3.8 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10937" target="_blank" rel="noopener noreferrer">Finding Locally Densest Subgraphs: Convex Programming with Edge and Triangle Density</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yi Yang, Chenhao Ma, Reynold Cheng, Laks V. S. Lakshmanan, Xiaolin Han
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Finding the densest subgraph (DS) from a graph is a fundamental problem in graph databases. The DS obtained, which reveals closely related entities, has been found to be useful in various application domains such as e-commerce, social science, and biology. However, in a big graph that contains billi</span>
                
                <span class="abstract-full" style="display: none;">Finding the densest subgraph (DS) from a graph is a fundamental problem in graph databases. The DS obtained, which reveals closely related entities, has been found to be useful in various application domains such as e-commerce, social science, and biology. However, in a big graph that contains billions of edges, it is desirable to find more than one subgraph cluster that is not necessarily the densest, yet they reveal closely related vertices. In this paper, we study the locally densest subgraph (LDS), a recently proposed variant of DS. An LDS is a subgraph which is the densest among the ``local neighbors''. Given a graph $G$, a number of LDSs can be returned, which reflect different dense regions of $G$ and thus give more information than DS. The existing LDS solution suffers from low efficiency. We thus develop a convex-programming-based solution that enables powerful pruning. We also extend our algorithm to triangle-based density to solve LTDS problem. Based on current algorithms, we propose a unified framework for the LDS and LTDS problems. Extensive experiments on thirteen real large graph datasets show that our proposed algorithm is up to four orders of magnitude faster than the state-of-the-art.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.1 -->
                    
                <!-- GNN: 3.5 -->
                    
                <!-- Networks: 3.5 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- Medicine: 2.9 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10947" target="_blank" rel="noopener noreferrer">Improved MST3 Encryption scheme based on small Ree groups</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gennady Khalimov, Yevgen Kotukh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This article presents an encryption scheme based on the small Ree groups. We propose utilizing the small Ree group structure to enhance the overall security parameters of the encryption scheme. By extending the logarithmic signature to encompass the entire group and modifying the encryption algorith</span>
                
                <span class="abstract-full" style="display: none;">This article presents an encryption scheme based on the small Ree groups. We propose utilizing the small Ree group structure to enhance the overall security parameters of the encryption scheme. By extending the logarithmic signature to encompass the entire group and modifying the encryption algorithm, we have developed robust protection against sequential key recovery attacks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.5 -->
                    
                <!-- Medicine: 5.1 -->
                    
                <!-- Reinforcement Learning: 4.4 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- Math: 3.1 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- SpikingNN: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10949" target="_blank" rel="noopener noreferrer">A Primer on Orthogonal Delay-Doppler Division Multiplexing (ODDM)</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hai Lin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As a new type of multicarrier (MC) scheme built upon the recently discovered delay-Doppler domain orthogonal pulse (DDOP), orthogonal delay-Doppler division multiplexing (ODDM) aims to address the challenges of waveform design in linear time-varying channels. In this paper, we explore the design pri</span>
                
                <span class="abstract-full" style="display: none;">As a new type of multicarrier (MC) scheme built upon the recently discovered delay-Doppler domain orthogonal pulse (DDOP), orthogonal delay-Doppler division multiplexing (ODDM) aims to address the challenges of waveform design in linear time-varying channels. In this paper, we explore the design principles of ODDM and clarify the key ideas underlying the DDOP. We then derive an alternative representation of the DDOP and highlight the fundamental differences between ODDM and conventional MC schemes. Finally, we discuss and compare two implementation methods for ODDM.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.5 -->
                    
                <!-- LLMs: 8.7 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- SpikingNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10954" target="_blank" rel="noopener noreferrer">Offset-free Nonlinear MPC with Koopman-based Surrogate Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Irene Schimperna, Lea Bold, Karl Worthmann
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we design offset-free nonlinear Model Predictive Control (MPC) for surrogate models based on Extended Dynamic Mode Decomposition (EDMD). The model used for prediction in MPC is augmented with a disturbance term, that is estimated by an observer. If the full information about the equil</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we design offset-free nonlinear Model Predictive Control (MPC) for surrogate models based on Extended Dynamic Mode Decomposition (EDMD). The model used for prediction in MPC is augmented with a disturbance term, that is estimated by an observer. If the full information about the equilibrium of the real system is not available, a reference calculator is introduced in the algorithm to compute the MPC state and input references. The control algorithm guarantees offset-free tracking of the controlled output under the assumption that the modeling errors are asymptotically constant. The effectiveness of the proposed approach is showcased with numerical simulations for two popular benchmark systems: the van-der-Pol oscillator and the four-tanks process.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.3 -->
                    
                <!-- Math: 3.8 -->
                    
                <!-- Reinforcement Learning: 3.8 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10957" target="_blank" rel="noopener noreferrer">When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hongkang Li, Yihua Zhang, Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model ed</span>
                
                <span class="abstract-full" style="display: none;">Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.8 -->
                    
                <!-- Medicine: 4.7 -->
                    
                <!-- Reinforcement Learning: 3.9 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Robotics: 2.3 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- SpikingNN: 1.7 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10958" target="_blank" rel="noopener noreferrer">Recognition of Geometrical Shapes by Dictionary Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alexander K\"ohler, Michael Breu{\ss}
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Dictionary learning is a versatile method to produce an overcomplete set of vectors, called atoms, to represent a given input with only a few atoms. In the literature, it has been used primarily for tasks that explore its powerful representation capabilities, such as for image reconstruction. In thi</span>
                
                <span class="abstract-full" style="display: none;">Dictionary learning is a versatile method to produce an overcomplete set of vectors, called atoms, to represent a given input with only a few atoms. In the literature, it has been used primarily for tasks that explore its powerful representation capabilities, such as for image reconstruction. In this work, we present a first approach to make dictionary learning work for shape recognition, considering specifically geometrical shapes. As we demonstrate, the choice of the underlying optimization method has a significant impact on recognition quality. Experimental results confirm that dictionary learning may be an interesting method for shape recognition tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.2 -->
                    
                <!-- Medicine: 5.2 -->
                    
                <!-- Quantum Computing: 4.5 -->
                    
                <!-- Reinforcement Learning: 3.2 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10959" target="_blank" rel="noopener noreferrer">Learning-Based User Association for MmWave Vehicular Networks With Kernelized Contextual Bandits</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiaoyang He, Xiaoxia Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Vehicles require timely channel conditions to determine the base station (BS) to communicate with, but it is costly to estimate the fast-fading mmWave channels frequently. Without additional channel estimations, the proposed Distributed Kernelized Upper Confidence Bound (DK-UCB) algorithm estimates </span>
                
                <span class="abstract-full" style="display: none;">Vehicles require timely channel conditions to determine the base station (BS) to communicate with, but it is costly to estimate the fast-fading mmWave channels frequently. Without additional channel estimations, the proposed Distributed Kernelized Upper Confidence Bound (DK-UCB) algorithm estimates the current instantaneous transmission rates utilizing past contexts, such as the vehicle's location and velocity, along with past instantaneous transmission rates. To capture the nonlinear mapping from a context to the instantaneous transmission rate, DK-UCB maps a context into the reproducing kernel Hilbert space (RKHS) where a linear mapping becomes observable. To improve estimation accuracy, we propose a novel kernel function in RKHS which incorporates the propagation characteristics of the mmWave signals. Moreover, DK-UCB encourages a vehicle to share necessary information when it has conducted significant explorations, which speeds up the learning process while maintaining affordable communication costs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 6.3 -->
                    
                <!-- Networks: 4.2 -->
                    
                <!-- Medicine: 3.5 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- LLMs: 2.8 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- SpikingNN: 1.3 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10960" target="_blank" rel="noopener noreferrer">A Linear Push-Pull Average Consensus Algorithm for Delay-Prone Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Evagoras Makridis, Themistoklis Charalambous
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we address the average consensus problem of multi-agent systems for possibly unbalanced and delay-prone networks with directional information flow. We propose a linear distributed algorithm (referred to as RPPAC) that handles asynchronous updates and time-varying heterogeneous informa</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we address the average consensus problem of multi-agent systems for possibly unbalanced and delay-prone networks with directional information flow. We propose a linear distributed algorithm (referred to as RPPAC) that handles asynchronous updates and time-varying heterogeneous information delays. Our proposed distributed algorithm utilizes a surplus-consensus mechanism and information regarding the number of incoming and outgoing links to guarantee state averaging, despite the imbalanced and delayed information flow in directional networks. The convergence of the RPPAC algorithm is examined using key properties of the backward product of time-varying matrices that correspond to different snapshots of the directional augmented network.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.5 -->
                    
                <!-- Medicine: 7.2 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- SpikingNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10962" target="_blank" rel="noopener noreferrer">$\pi$-MPPI: A Projection-based Model Predictive Path Integral Scheme for Smooth Optimal Control of Fixed-Wing Aerial Vehicles</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Edvin Martin Andrejev, Amith Manoharan, Karl-Eerik Unt, Arun Kumar Singh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Model Predictive Path Integral (MPPI) is a popular sampling-based Model Predictive Control (MPC) algorithm for nonlinear systems. It optimizes trajectories by sampling control sequences and averaging them. However, a key issue with MPPI is the non-smoothness of the optimal control sequence, leading </span>
                
                <span class="abstract-full" style="display: none;">Model Predictive Path Integral (MPPI) is a popular sampling-based Model Predictive Control (MPC) algorithm for nonlinear systems. It optimizes trajectories by sampling control sequences and averaging them. However, a key issue with MPPI is the non-smoothness of the optimal control sequence, leading to oscillations in systems like fixed-wing aerial vehicles (FWVs). Existing solutions use post-hoc smoothing, which fails to bound control derivatives. This paper introduces a new approach: we add a projection filter $\pi$ to minimally correct control samples, ensuring bounds on control magnitude and higher-order derivatives. The filtered samples are then averaged using MPPI, leading to our $\pi$-MPPI approach. We minimize computational overhead by using a neural accelerated custom optimizer for the projection filter. $\pi$-MPPI offers a simple way to achieve arbitrary smoothness in control sequences. While we focus on FWVs, this projection filter can be integrated into any MPPI pipeline. Applied to FWVs, $\pi$-MPPI is easier to tune than the baseline, resulting in smoother, more robust performance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.0 -->
                    
                <!-- Medicine: 4.0 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- Networks: 3.6 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- GNN: 2.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10964" target="_blank" rel="noopener noreferrer">Distributed Optimization with Gradient Tracking over Heterogeneous Delay-Prone Directed Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Evagoras Makridis, Gabriele Oliva, Kasagatta Ramesh Narahari, Mohammadreza Doostmohammadian, Usman A. Khan, Themistoklis Charalambous
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we address the distributed optimization problem over unidirectional networks with possibly time-invariant heterogeneous bounded transmission delays. In particular, we propose a modified version of the Accelerated Distributed Directed OPTimization (ADD-OPT) algorithm, herein called Rob</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we address the distributed optimization problem over unidirectional networks with possibly time-invariant heterogeneous bounded transmission delays. In particular, we propose a modified version of the Accelerated Distributed Directed OPTimization (ADD-OPT) algorithm, herein called Robustified ADD-OPT (R-ADD-OPT), which is able to solve the distributed optimization problem, even when the communication links suffer from heterogeneous but bounded transmission delays. We show that if the gradient step-size of the R-ADD-OPT algorithm is within a certain range, which also depends on the maximum time delay in the network, then the nodes are guaranteed to converge to the optimal solution of the distributed optimization problem. The range of the gradient step-size that guarantees convergence can be computed a priori based on the maximum time delay in the network.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Math: 5.5 -->
                    
                <!-- Reinforcement Learning: 5.5 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- GNN: 3.0 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Pathfinding: 1.7 -->
                    
                <!-- SpikingNN: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10971" target="_blank" rel="noopener noreferrer">The Effectiveness of Business Process Visualisations: a Systematic Literature Review</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: E. C. Overes, F. M. Santoro
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Business Process Visualisations (BPVs) have become indispensable tools for organisations seeking to enhance their operational efficiency, decision-making capabilities, and overall performance. The burgeoning interest in process modeling and tool development, coupled with the rise of data visualisati</span>
                
                <span class="abstract-full" style="display: none;">Business Process Visualisations (BPVs) have become indispensable tools for organisations seeking to enhance their operational efficiency, decision-making capabilities, and overall performance. The burgeoning interest in process modeling and tool development, coupled with the rise of data visualisation field, underscores the significant role of visual tools in leveraging human cognition. Unlike traditional models, data visualisation approaches graphics from a novel angle, emphasising the potency of visual representations. This review aims to integrate the domains of BPV and data visualisation to assess their combined influence on organisational effectiveness comprehensively. Through a meticulous analysis of existing literature, this study aims to amalgamate insights on BPVs impact from a data visualisation standpoint, advocating for a design philosophy that prioritises user engagement to bolster organisational outcomes. Additionally, our systematic review has unveiled promising avenues for future research, identifying underexplored variables that influence the efficacy of BPVs, thereby charting a path for forthcoming scholarly inquiries.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.7 -->
                    
                <!-- LLMs: 8.6 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.10972" target="_blank" rel="noopener noreferrer">AFiRe: Anatomy-Driven Self-Supervised Learning for Fine-Grained Representation in Radiographic Images</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yihang Liu, Lianghua He, Ying Wen, Longzhen Yang, Hongzhou Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Current self-supervised methods, such as contrastive learning, predominantly focus on global discrimination, neglecting the critical fine-grained anatomical details required for accurate radiographic analysis. To address this challenge, we propose an Anatomy-driven self-supervised framework for enha</span>
                
                <span class="abstract-full" style="display: none;">Current self-supervised methods, such as contrastive learning, predominantly focus on global discrimination, neglecting the critical fine-grained anatomical details required for accurate radiographic analysis. To address this challenge, we propose an Anatomy-driven self-supervised framework for enhancing Fine-grained Representation in radiographic image analysis (AFiRe). The core idea of AFiRe is to align the anatomical consistency with the unique token-processing characteristics of Vision Transformer. Specifically, AFiRe synergistically performs two self-supervised schemes: (i) Token-wise anatomy-guided contrastive learning, which aligns image tokens based on structural and categorical consistency, thereby enhancing fine-grained spatial-anatomical discrimination; (ii) Pixel-level anomaly-removal restoration, which particularly focuses on local anomalies, thereby refining the learned discrimination with detailed geometrical information. Additionally, we propose Synthetic Lesion Mask to enhance anatomical diversity while preserving intra-consistency, which is typically corrupted by traditional data augmentations, such as Cropping and Affine transformations. Experimental results show that AFiRe: (i) provides robust anatomical discrimination, achieving more cohesive feature clusters compared to state-of-the-art contrastive learning methods; (ii) demonstrates superior generalization, surpassing 7 radiography-specific self-supervised methods in multi-label classification tasks with limited labeling; and (iii) integrates fine-grained information, enabling precise anomaly detection using only image-level annotations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.5 -->
                    
                <!-- LLMs: 9.2 -->
                    
                <!-- Quantum Computing: 4.2 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- SpikingNN: 1.3 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1207
                </span>
                <a href="https://arxiv.org/abs/2206.02658" target="_blank" rel="noopener noreferrer">Longitudinal Analysis of Privacy Labels in the Apple App Store</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: David G. Balash, Mir Masood Ali, Monica Kodwani, Xiaoyuan Wu, Chris Kanich, Adam J. Aviv
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In December of 2020, Apple started to require app developers to self-report privacy label annotations on their apps indicating what data is collected and how it is used.To understand the adoption and shifts in privacy labels in the App Store, we collected nearly weekly snapshots of over 1.6 million </span>
                
                <span class="abstract-full" style="display: none;">In December of 2020, Apple started to require app developers to self-report privacy label annotations on their apps indicating what data is collected and how it is used.To understand the adoption and shifts in privacy labels in the App Store, we collected nearly weekly snapshots of over 1.6 million apps for over a year (July 15, 2021 -- October 25, 2022) to understand the dynamics of privacy label ecosystem. Nearly two years after privacy labels launched, only 70.1% of apps have privacy labels, but we observed an increase of 28% during the measurement period. Privacy label adoption rates are mostly driven by new apps rather than older apps coming into compliance. Of apps with labels, 18.1% collect data used to track users, 38.1% collect data that is linked to a user identity, and 42.0% collect data that is not linked. A surprisingly large share (41.8%) of apps with labels indicate that they do not collect any data, and while we do not perform direct analysis of the apps to verify this claim, we observe that it is likely that many of these apps are choosing a Does Not Collect label due to being forced to select a label, rather than this being the true behavior of the app. Moreover, for apps that have assigned labels during the measurement period nearly all do not change their labels, and when they do, the new labels indicate more data collection than less. This suggests that privacy labels may be a ``set once'' mechanism for developers that may not actually provide users with the clarity needed to make informed privacy decisions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.9 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1302
                </span>
                <a href="https://arxiv.org/abs/2504.11094" target="_blank" rel="noopener noreferrer">Evaluation Report on MCP Servers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhiling Luo, Xiaorong Shi, Xuanrui Lin, Jinyang Gao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">With the rise of LLMs, a large number of Model Context Protocol (MCP) services have emerged since the end of 2024. However, the effectiveness and efficiency of MCP servers have not been well studied. To study these questions, we propose an evaluation framework, called MCPBench. We selected several w</span>
                
                <span class="abstract-full" style="display: none;">With the rise of LLMs, a large number of Model Context Protocol (MCP) services have emerged since the end of 2024. However, the effectiveness and efficiency of MCP servers have not been well studied. To study these questions, we propose an evaluation framework, called MCPBench. We selected several widely used MCP server and conducted an experimental evaluation on their accuracy, time, and token usage. Our experiments showed that the most effective MCP, Bing Web Search, achieved an accuracy of 64%. Importantly, we found that the accuracy of MCP servers can be substantially enhanced by involving declarative interface. This research paves the way for further investigations into optimized MCP implementations, ultimately leading to better AI-driven applications and data retrieval solutions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 14.3 -->
                    
                <!-- Medicine: 9.2 -->
                    
                <!-- Quantum Computing: 4.9 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.2793
                </span>
                <a href="https://arxiv.org/abs/2504.10950" target="_blank" rel="noopener noreferrer">Unveiling Challenges for LLMs in Enterprise Data Engineering</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jan-Micha Bodensohn, Ulf Brackmann, Liane Vogel, Anupam Sanghi, Carsten Binnig
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large Language Models (LLMs) have demonstrated significant potential for automating data engineering tasks on tabular data, giving enterprises a valuable opportunity to reduce the high costs associated with manual data handling. However, the enterprise domain introduces unique challenges that existi</span>
                
                <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have demonstrated significant potential for automating data engineering tasks on tabular data, giving enterprises a valuable opportunity to reduce the high costs associated with manual data handling. However, the enterprise domain introduces unique challenges that existing LLM-based approaches for data engineering often overlook, such as large table sizes, more complex tasks, and the need for internal knowledge. To bridge these gaps, we identify key enterprise-specific challenges related to data, tasks, and background knowledge and conduct a comprehensive study of their impact on recent LLMs for data engineering. Our analysis reveals that LLMs face substantial limitations in real-world enterprise scenarios, resulting in significant accuracy drops. Our findings contribute to a systematic understanding of LLMs for enterprise data engineering to support their adoption in industry.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 27.9 -->
                    
                <!-- Medicine: 4.9 -->
                    
                <!-- RAG: 3.5 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4802
                </span>
                <a href="https://arxiv.org/abs/2504.10677" target="_blank" rel="noopener noreferrer">Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Muhammad Al-Zafar Khan, Jamal Al-Karaki
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we present a multi-agent reinforcement learning (MARL) framework for optimizing tissue repair processes using engineered biological agents. Our approach integrates: (1) stochastic reaction-diffusion systems modeling molecular signaling, (2) neural-like electrochemical communication wi</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we present a multi-agent reinforcement learning (MARL) framework for optimizing tissue repair processes using engineered biological agents. Our approach integrates: (1) stochastic reaction-diffusion systems modeling molecular signaling, (2) neural-like electrochemical communication with Hebbian plasticity, and (3) a biologically informed reward function combining chemical gradient tracking, neural synchronization, and robust penalties. A curriculum learning scheme guides the agent through progressively complex repair scenarios. In silico experiments demonstrate emergent repair strategies, including dynamic secretion control and spatial coordination.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.2 -->
                    
                <!-- LLMs: 10.8 -->
                    
                <!-- Quantum Computing: 4.4 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- T2I: 1.8 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.517
                </span>
                <a href="https://arxiv.org/abs/2501.17688" target="_blank" rel="noopener noreferrer">ContourFormer: Real-Time Contour-Based End-to-End Instance Segmentation Transformer</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Weiwei Yao, Chen Li, Minjun Xiong, Wenbo Dong, Hao Chen, Xiong Xiao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents Contourformer, a real-time contour-based instance segmentation algorithm. The method is fully based on the DETR paradigm and achieves end-to-end inference through iterative and progressive mechanisms to optimize contours. To improve efficiency and accuracy, we develop two novel t</span>
                
                <span class="abstract-full" style="display: none;">This paper presents Contourformer, a real-time contour-based instance segmentation algorithm. The method is fully based on the DETR paradigm and achieves end-to-end inference through iterative and progressive mechanisms to optimize contours. To improve efficiency and accuracy, we develop two novel techniques: sub-contour decoupling mechanisms and contour fine-grained distribution refinement. In the sub-contour decoupling mechanism, we propose a deformable attention-based module that adaptively selects sampling regions based on the current predicted contour, enabling more effective capturing of object boundary information. Additionally, we design a multi-stage optimization process to enhance segmentation precision by progressively refining sub-contours. The contour fine-grained distribution refinement technique aims to further improve the ability to express fine details of contours. These innovations enable Contourformer to achieve stable and precise segmentation for each instance while maintaining real-time performance. Extensive experiments demonstrate the superior performance of Contourformer on multiple benchmark datasets, including SBD, COCO, and KINS. We conduct comprehensive evaluations and comparisons with existing state-of-the-art methods, showing significant improvements in both accuracy and inference speed. This work provides a new solution for contour-based instance segmentation tasks and lays a foundation for future research, with the potential to become a strong baseline method in this field.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.1 -->
                    
                <!-- LLMs: 9.5 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.6
                </span>
                <a href="https://arxiv.org/abs/2504.11344" target="_blank" rel="noopener noreferrer">Interpretable Hybrid-Rule Temporal Point Processes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yunyang Cao, Juekai Lin, Hongye Wang, Wenhao Li, Bo Jin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Temporal Point Processes (TPPs) are widely used for modeling event sequences in various medical domains, such as disease onset prediction, progression analysis, and clinical decision support. Although TPPs effectively capture temporal dynamics, their lack of interpretability remains a critical chall</span>
                
                <span class="abstract-full" style="display: none;">Temporal Point Processes (TPPs) are widely used for modeling event sequences in various medical domains, such as disease onset prediction, progression analysis, and clinical decision support. Although TPPs effectively capture temporal dynamics, their lack of interpretability remains a critical challenge. Recent advancements have introduced interpretable TPPs. However, these methods fail to incorporate numerical features, thereby limiting their ability to generate precise predictions. To address this issue, we propose Hybrid-Rule Temporal Point Processes (HRTPP), a novel framework that integrates temporal logic rules with numerical features, improving both interpretability and predictive accuracy in event modeling. HRTPP comprises three key components: basic intensity for intrinsic event likelihood, rule-based intensity for structured temporal dependencies, and numerical feature intensity for dynamic probability modulation. To effectively discover valid rules, we introduce a two-phase rule mining strategy with Bayesian optimization. To evaluate our method, we establish a multi-criteria assessment framework, incorporating rule validity, model fitting, and temporal predictive accuracy. Experimental results on real-world medical datasets demonstrate that HRTPP outperforms state-of-the-art interpretable TPPs in terms of predictive performance and clinical interpretability. In case studies, the rules extracted by HRTPP explain the disease progression, offering valuable contributions to medical diagnosis.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.9 -->
                    
                <!-- LLMs: 10.9 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7554
                </span>
                <a href="https://arxiv.org/abs/2504.10739" target="_blank" rel="noopener noreferrer">HippoMM: Hippocampal-inspired Multimodal Memory for Long Audiovisual Event Understanding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yueqian Lin, Qinsi Wang, Hancheng Ye, Yuzhe Fu, Hai "Helen" Li, Yiran Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Comprehending extended audiovisual experiences remains a fundamental challenge for computational systems. Current approaches struggle with temporal integration and cross-modal associations that humans accomplish effortlessly through hippocampal-cortical networks. We introduce HippoMM, a biologically</span>
                
                <span class="abstract-full" style="display: none;">Comprehending extended audiovisual experiences remains a fundamental challenge for computational systems. Current approaches struggle with temporal integration and cross-modal associations that humans accomplish effortlessly through hippocampal-cortical networks. We introduce HippoMM, a biologically-inspired architecture that transforms hippocampal mechanisms into computational advantages for multimodal understanding. HippoMM implements three key innovations: (i) hippocampus-inspired pattern separation and completion specifically designed for continuous audiovisual streams, (ii) short-to-long term memory consolidation that transforms perceptual details into semantic abstractions, and (iii) cross-modal associative retrieval pathways enabling modality-crossing queries. Unlike existing retrieval systems with static indexing schemes, HippoMM dynamically forms integrated episodic representations through adaptive temporal segmentation and dual-process memory encoding. Evaluations on our challenging HippoVlog benchmark demonstrate that HippoMM significantly outperforms state-of-the-art approaches (78.2% vs. 64.2% accuracy) while providing substantially faster response times (20.4s vs. 112.5s). Our results demonstrate that translating neuroscientific memory principles into computational architectures provides a promising foundation for next-generation multimodal understanding systems. The code and benchmark dataset are publicly available at https://github.com/linyueqian/HippoMM.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.6 -->
                    
                <!-- LLMs: 11.1 -->
                    
                <!-- Quantum Computing: 6.2 -->
                    
                <!-- RAG: 2.7 -->
                    
                <!-- 3D: 2.5 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8241
                </span>
                <a href="https://arxiv.org/abs/2412.11758" target="_blank" rel="noopener noreferrer">Establishing a Foundation for Tetun Ad-Hoc Text Retrieval: Stemming, Indexing, Retrieval, and Ranking</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gabriel de Jesus, S\'ergio Nunes
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Searching for information on the internet and digital platforms to satisfy an information need requires effective retrieval solutions. However, such solutions are not yet available for Tetun, making it challenging to find relevant documents for text-based search queries in this language. To address </span>
                
                <span class="abstract-full" style="display: none;">Searching for information on the internet and digital platforms to satisfy an information need requires effective retrieval solutions. However, such solutions are not yet available for Tetun, making it challenging to find relevant documents for text-based search queries in this language. To address these challenges, this study investigates Tetun text retrieval with a focus on the ad-hoc retrieval task. It begins by developing essential language resources -- including a list of stopwords, a stemmer, and a test collection -- which serve as foundational components for solutions tailored to Tetun text retrieval. Various strategies are then explored using both document titles and content to evaluate retrieval effectiveness. The results show that retrieving document titles, after removing hyphens and apostrophes without applying stemming, significantly improves retrieval performance compared to the baseline. Efficiency increases by 31.37%, while effectiveness achieves an average gain of 9.40% in MAP@10 and 30.35% in nDCG@10 with DFR BM25. Beyond the top-10 cutoff point, Hiemstra LM demonstrates strong performance across various retrieval strategies and evaluation metrics. Contributions of this work include the development of Labadain-Stopwords (a list of 160 Tetun stopwords), Labadain-Stemmer (a Tetun stemmer with three variants), and Labadain-Avaliad\'or (a Tetun test collection containing 59 topics, 33,550 documents, and 5,900 qrels).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.4 -->
                    
                <!-- LLMs: 9.2 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8251
                </span>
                <a href="https://arxiv.org/abs/2409.17285" target="_blank" rel="noopener noreferrer">SpoofCeleb: Speech Deepfake Detection and SASV In The Wild</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jee-weon Jung, Yihan Wu, Xin Wang, Ji-Hoon Kim, Soumi Maiti, Yuta Matsunaga, Hye-jin Shim, Jinchuan Tian, Nicholas Evans, Joon Son Chung, Wangyou Zhang, Seyun Um, Shinnosuke Takamichi, Shinji Watanabe
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces SpoofCeleb, a dataset designed for Speech Deepfake Detection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV), utilizing source data from real-world conditions and spoofing attacks generated by Text-To-Speech (TTS) systems also trained on the same real-world data</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces SpoofCeleb, a dataset designed for Speech Deepfake Detection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV), utilizing source data from real-world conditions and spoofing attacks generated by Text-To-Speech (TTS) systems also trained on the same real-world data. Robust recognition systems require speech data recorded in varied acoustic environments with different levels of noise to be trained. However, current datasets typically include clean, high-quality recordings (bona fide data) due to the requirements for TTS training; studio-quality or well-recorded read speech is typically necessary to train TTS models. Current SDD datasets also have limited usefulness for training SASV models due to insufficient speaker diversity. SpoofCeleb leverages a fully automated pipeline we developed that processes the VoxCeleb1 dataset, transforming it into a suitable form for TTS training. We subsequently train 23 contemporary TTS systems. SpoofCeleb comprises over 2.5 million utterances from 1,251 unique speakers, collected under natural, real-world conditions. The dataset includes carefully partitioned training, validation, and evaluation sets with well-controlled experimental protocols. We present the baseline results for both SDD and SASV tasks. All data, protocols, and baselines are publicly available at https://jungjee.github.io/spoofceleb.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.2 -->
                    
                <!-- LLMs: 6.0 -->
                    
                <!-- Quantum Computing: 5.3 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.011
                </span>
                <a href="https://arxiv.org/abs/2504.10757" target="_blank" rel="noopener noreferrer">ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Amirhosein Chahe, Lifeng Zhou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Vision-language models (VLMs) show promise for autonomous driving but often lack transparent reasoning capabilities that are critical for safety. We investigate whether explicitly modeling reasoning during fine-tuning enhances VLM performance on driving decision tasks. Using GPT-4o, we generate stru</span>
                
                <span class="abstract-full" style="display: none;">Vision-language models (VLMs) show promise for autonomous driving but often lack transparent reasoning capabilities that are critical for safety. We investigate whether explicitly modeling reasoning during fine-tuning enhances VLM performance on driving decision tasks. Using GPT-4o, we generate structured reasoning chains for driving scenarios from the DriveLM benchmark with category-specific prompting strategies. We compare reasoning-based fine-tuning, answer-only fine-tuning, and baseline instruction-tuned models across multiple small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results demonstrate that reasoning-based fine-tuning consistently outperforms alternatives, with Llama3.2-11B-reason achieving the highest performance. Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions. These findings highlight the importance of transparent decision processes in safety-critical domains and offer a promising direction for developing more interpretable autonomous driving systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 18.2 -->
                    
                <!-- Medicine: 11.4 -->
                    
                <!-- Quantum Computing: 4.6 -->
                    
                <!-- RAG: 2.4 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0302
                </span>
                <a href="https://arxiv.org/abs/2504.10568" target="_blank" rel="noopener noreferrer">AgMMU: A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aruna Gauba, Irene Pi, Yunze Man, Ziqi Pang, Vikram S. Adve, Yu-Xiong Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We curate a dataset AgMMU for evaluating and developing vision-language models (VLMs) to produce factually accurate answers for knowledge-intensive expert domains. Our AgMMU concentrates on one of the most socially beneficial domains, agriculture, which requires connecting detailed visual observatio</span>
                
                <span class="abstract-full" style="display: none;">We curate a dataset AgMMU for evaluating and developing vision-language models (VLMs) to produce factually accurate answers for knowledge-intensive expert domains. Our AgMMU concentrates on one of the most socially beneficial domains, agriculture, which requires connecting detailed visual observation with precise knowledge to diagnose, e.g., pest identification, management instructions, etc. As a core uniqueness of our dataset, all facts, questions, and answers are extracted from 116,231 conversations between real-world users and authorized agricultural experts. After a three-step dataset curation pipeline with GPT-4o, LLaMA models, and human verification, AgMMU features an evaluation set of 5,460 multiple-choice questions (MCQs) and open-ended questions (OEQs). We also provide a development set that contains 205,399 pieces of agricultural knowledge information, including disease identification, symptoms descriptions, management instructions, insect and pest identification, and species identification. As a multimodal factual dataset, it reveals that existing VLMs face significant challenges with questions requiring both detailed perception and factual knowledge. Moreover, open-source VLMs still demonstrate a substantial performance gap compared to proprietary ones. To advance knowledge-intensive VLMs, we conduct fine-tuning experiments using our development set, which improves LLaVA-1.5 evaluation accuracy by up to 3.1%. We hope that AgMMU can serve both as an evaluation benchmark dedicated to agriculture and a development suite for incorporating knowledge-intensive expertise into general-purpose VLMs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.8 -->
                    
                <!-- LLMs: 10.4 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0355
                </span>
                <a href="https://arxiv.org/abs/2504.11022" target="_blank" rel="noopener noreferrer">Meta-learning For Few-Shot Time Series Crop Type Classification: A Benchmark On The EuroCropsML Dataset</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Joana Reuss, Jan Macdonald, Simon Becker, Konrad Schultka, Lorenz Richter, Marco K\"orner
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Spatial imbalances in crop type data pose significant challenges for accurate classification in remote sensing applications. Algorithms aiming at transferring knowledge from data-rich to data-scarce tasks have thus surged in popularity. However, despite their effectiveness in previous evaluations, t</span>
                
                <span class="abstract-full" style="display: none;">Spatial imbalances in crop type data pose significant challenges for accurate classification in remote sensing applications. Algorithms aiming at transferring knowledge from data-rich to data-scarce tasks have thus surged in popularity. However, despite their effectiveness in previous evaluations, their performance in challenging real-world applications is unclear and needs to be evaluated. This study benchmarks transfer learning and several meta-learning algorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML), Almost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the real-world EuroCropsML time series dataset, which combines farmer-reported crop data with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal. Our findings indicate that MAML-based meta-learning algorithms achieve slightly higher accuracy compared to simpler transfer learning methods when applied to crop type classification tasks in Estonia after pre-training on data from Latvia. However, this improvement comes at the cost of increased computational demands and training time. Moreover, we find that the transfer of knowledge between geographically disparate regions, such as Estonia and Portugal, poses significant challenges to all investigated algorithms. These insights underscore the trade-offs between accuracy and computational resource requirements in selecting machine learning methods for real-world crop type classification tasks and highlight the difficulties of transferring knowledge between different regions of the Earth. To facilitate future research in this domain, we present the first comprehensive benchmark for evaluating transfer and meta-learning methods for crop type classification under real-world conditions. The corresponding code is publicly available at https://github.com/dida-do/eurocrops-meta-learning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.8 -->
                    
                <!-- LLMs: 10.5 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1057
                </span>
                <a href="https://arxiv.org/abs/2504.10883" target="_blank" rel="noopener noreferrer">Bringing together invertible UNets with invertible attention modules for memory-efficient diffusion models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Karan Jain, Mohammad Nayeem Teli
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Diffusion models have recently gained state of the art performance on many image generation tasks. However, most models require significant computational resources to achieve this. This becomes apparent in the application of medical image synthesis due to the 3D nature of medical datasets like CT-sc</span>
                
                <span class="abstract-full" style="display: none;">Diffusion models have recently gained state of the art performance on many image generation tasks. However, most models require significant computational resources to achieve this. This becomes apparent in the application of medical image synthesis due to the 3D nature of medical datasets like CT-scans, MRIs, electron microscope, etc. In this paper we propose a novel architecture for a single GPU memory-efficient training for diffusion models for high dimensional medical datasets. The proposed model is built by using an invertible UNet architecture with invertible attention modules. This leads to the following two contributions: 1. denoising diffusion models and thus enabling memory usage to be independent of the dimensionality of the dataset, and 2. reducing the energy usage during training. While this new model can be applied to a multitude of image generation tasks, we showcase its memory-efficiency on the 3D BraTS2020 dataset leading to up to 15\% decrease in peak memory consumption during training with comparable results to SOTA while maintaining the image quality.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.4 -->
                    
                <!-- LLMs: 9.5 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.2656
                </span>
                <a href="https://arxiv.org/abs/2504.10506" target="_blank" rel="noopener noreferrer">WorldMove, a global open data for human mobility</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yuan Yuan, Yuheng Zhang, Jingtao Ding, Yong Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">High-quality human mobility data is crucial for applications such as urban planning, transportation management, and public health, yet its collection is often hindered by privacy concerns and data scarcity-particularly in less-developed regions. To address this challenge, we introduce WorldMove, a l</span>
                
                <span class="abstract-full" style="display: none;">High-quality human mobility data is crucial for applications such as urban planning, transportation management, and public health, yet its collection is often hindered by privacy concerns and data scarcity-particularly in less-developed regions. To address this challenge, we introduce WorldMove, a large-scale synthetic mobility dataset covering over 1,600 cities across 179 countries and 6 continents. Our method leverages publicly available multi-source data, including gridded population distribution, point-of-interest (POI) maps, and commuting origin-destination (OD) flows-to generate realistic city-scale mobility trajectories using a diffusion-based generative model. The generation process involves defining city boundaries, collecting multi-source input features, and simulating individual-level movements that reflect plausible daily mobility behavior. Comprehensive validation demonstrates that the generated data closely aligns with real-world observations, both in terms of fine-grained individual mobility behavior and city-scale population flows. Alongside the pre-generated datasets, we release the trained model and a complete open-source pipeline, enabling researchers and practitioners to generate custom synthetic mobility data for any city worldwide. This work not only fills critical data gaps, but also lays a global foundation for scalable, privacy-preserving, and inclusive mobility research-empowering data-scarce regions and enabling universal access to human mobility insights.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.2 -->
                    
                <!-- LLMs: 9.7 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.303
                </span>
                <a href="https://arxiv.org/abs/2504.10915" target="_blank" rel="noopener noreferrer">LOKA Protocol: A Decentralized Framework for Trustworthy and Ethical AI Agent Ecosystems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rajesh Ranjan, Shailja Gupta, Surya Narayan Singh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rise of autonomous AI agents, capable of perceiving, reasoning, and acting independently, signals a profound shift in how digital ecosystems operate, govern, and evolve. As these agents proliferate beyond centralized infrastructures, they expose foundational gaps in identity, accountability, and</span>
                
                <span class="abstract-full" style="display: none;">The rise of autonomous AI agents, capable of perceiving, reasoning, and acting independently, signals a profound shift in how digital ecosystems operate, govern, and evolve. As these agents proliferate beyond centralized infrastructures, they expose foundational gaps in identity, accountability, and ethical alignment. Three critical questions emerge: Identity: Who or what is the agent? Accountability: Can its actions be verified, audited, and trusted? Ethical Consensus: Can autonomous systems reliably align with human values and prevent harmful emergent behaviors? We present the novel LOKA Protocol (Layered Orchestration for Knowledgeful Agents), a unified, systems-level architecture for building ethically governed, interoperable AI agent ecosystems. LOKA introduces a proposed Universal Agent Identity Layer (UAIL) for decentralized, verifiable identity; intent-centric communication protocols for semantic coordination across diverse agents; and a Decentralized Ethical Consensus Protocol (DECP) that enables agents to make context-aware decisions grounded in shared ethical baselines. Anchored in emerging standards such as Decentralized Identifiers (DIDs), Verifiable Credentials (VCs), and post-quantum cryptography, LOKA offers a scalable, future-resilient blueprint for multi-agent AI governance. By embedding identity, trust, and ethics into the protocol layer itself, LOKA establishes the foundation for a new era of responsible, transparent, and autonomous AI ecosystems operating across digital and physical domains.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.1 -->
                    
                <!-- LLMs: 8.9 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3239
                </span>
                <a href="https://arxiv.org/abs/2504.11367" target="_blank" rel="noopener noreferrer">Network Alignment</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rui Tang, Ziyun Yong, Shuyu Jiang, Xingshu Chen, Yaofang Liu, Yi-Cheng Zhang, Gui-Quan Sun, Wei Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Complex networks are frequently employed to model physical or virtual complex systems. When certain entities exist across multiple systems simultaneously, unveiling their corresponding relationships across the networks becomes crucial. This problem, known as network alignment, holds significant impo</span>
                
                <span class="abstract-full" style="display: none;">Complex networks are frequently employed to model physical or virtual complex systems. When certain entities exist across multiple systems simultaneously, unveiling their corresponding relationships across the networks becomes crucial. This problem, known as network alignment, holds significant importance. It enhances our understanding of complex system structures and behaviours, facilitates the validation and extension of theoretical physics research about studying complex systems, and fosters diverse practical applications across various fields. However, due to variations in the structure, characteristics, and properties of complex networks across different fields, the study of network alignment is often isolated within each domain, with even the terminologies and concepts lacking uniformity. This review comprehensively summarizes the latest advancements in network alignment research, focusing on analyzing network alignment characteristics and progress in various domains such as social network analysis, bioinformatics, computational linguistics and privacy protection. It provides a detailed analysis of various methods' implementation principles, processes, and performance differences, including structure consistency-based methods, network embedding-based methods, and graph neural network-based (GNN-based) methods. Additionally, the methods for network alignment under different conditions, such as in attributed networks, heterogeneous networks, directed networks, and dynamic networks, are presented. Furthermore, the challenges and the open issues for future studies are also discussed.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 11.0 -->
                    
                <!-- Medicine: 10.7 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.4047
                </span>
                <a href="https://arxiv.org/abs/2411.19000" target="_blank" rel="noopener noreferrer">An AI-driven multimodal smart home platform for continuous monitoring and intelligent assistance in post-stroke patients</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chenyu Tang, Ruizhi Zhang, Shuo Gao, Zihe Zhao, Zibo Zhang, Jiaqi Wang, Cong Li, Junliang Chen, Yanning Dai, Shengbo Wang, Ruoyu Juan, Qiaoying Li, Ruimou Xie, Xuhang Chen, Xinkai Zhou, Yunjia Xia, Jianan Chen, Fanghao Lu, Xin Li, Ninglli Wang, Peter Smielewski, Yu Pan, Hubin Zhao, Luigi G. Occhipinti
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">At-home rehabilitation for post-stroke patients presents significant challenges, as continuous, personalized care is often limited outside clinical settings. Additionally, the absence of comprehensive solutions addressing diverse monitoring and assistance needs in home environments complicates recov</span>
                
                <span class="abstract-full" style="display: none;">At-home rehabilitation for post-stroke patients presents significant challenges, as continuous, personalized care is often limited outside clinical settings. Additionally, the absence of comprehensive solutions addressing diverse monitoring and assistance needs in home environments complicates recovery efforts. Here, we present a multimodal smart home platform designed for continuous, at-home rehabilitation of post-stroke patients, integrating wearable sensing, ambient monitoring, and adaptive automation. A plantar pressure insole equipped with a machine learning pipeline classifies users into motor recovery stages with up to 94% accuracy, enabling quantitative tracking of walking patterns. A head-mounted eye-tracking module supports cognitive assessments and hands-free control of household devices, while ambient sensors ensure sub-second response times for interaction. These data streams are fused locally via a hierarchical Internet of Things (IoT) architecture, protecting privacy and minimizing latency. An embedded large language model (LLM) agent, Auto-Care, continuously interprets multimodal data to provide real-time interventions-issuing personalized reminders, adjusting environmental conditions, and notifying caregivers. Implemented in a post-stroke context, this integrated smart home platform increases overall user satisfaction by an average of 115% (p<0.01) compared to traditional home environment. Beyond stroke, the system offers a scalable framework for patient-centered, long-term care in broader neurorehabilitation and aging-in-place applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.3 -->
                    
                <!-- LLMs: 10.6 -->
                    
                <!-- Quantum Computing: 4.1 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- RAG: 2.4 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.4965
                </span>
                <a href="https://arxiv.org/abs/2406.09211" target="_blank" rel="noopener noreferrer">WildlifeReID-10k: Wildlife re-identification dataset with 10k individual animals</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Luk\'a\v{s} Adam, Vojt\v{e}ch \v{C}erm\'ak, Kostas Papafitsoros, Lukas Picek
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces WildlifeReID-10k, a new large-scale re-identification benchmark with more than 10k animal identities of around 33 species across more than 140k images, re-sampled from 37 existing datasets. WildlifeReID-10k covers diverse animal species and poses significant challenges for SoTA</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces WildlifeReID-10k, a new large-scale re-identification benchmark with more than 10k animal identities of around 33 species across more than 140k images, re-sampled from 37 existing datasets. WildlifeReID-10k covers diverse animal species and poses significant challenges for SoTA methods, ensuring fair and robust evaluation through its time-aware and similarity-aware split protocol. The latter is designed to address the common issue of training-to-test data leakage caused by visually similar images appearing in both training and test sets. The WildlifeReID-10k dataset and benchmark are publicly available on Kaggle, along with strong baselines for both closed-set and open-set evaluation, enabling fair, transparent, and standardized evaluation of not just multi-species animal re-identification models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 15.7 -->
                    
                <!-- Medicine: 12.3 -->
                    
                <!-- RAG: 3.2 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Blockchain: 2.3 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.5586
                </span>
                <a href="https://arxiv.org/abs/2504.10944" target="_blank" rel="noopener noreferrer">Cartesian Merkle Tree</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Artem Chystiakov, Oleh Komendant, Kyrylo Riabov
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces the Cartesian Merkle Tree, a deterministic data structure that combines the properties of a Binary Search Tree, a Heap, and a Merkle tree. The Cartesian Merkle Tree supports insertions, updates, and removals of elements in $O(\log n)$ time, requires $n$ space, and enables membe</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces the Cartesian Merkle Tree, a deterministic data structure that combines the properties of a Binary Search Tree, a Heap, and a Merkle tree. The Cartesian Merkle Tree supports insertions, updates, and removals of elements in $O(\log n)$ time, requires $n$ space, and enables membership and non-membership proofs via Merkle-based authentication paths. This structure is particularly suitable for zero-knowledge applications, blockchain systems, and other protocols that require efficient and verifiable data structures.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.0 -->
                    
                <!-- LLMs: 9.9 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.603
                </span>
                <a href="https://arxiv.org/abs/2504.11380" target="_blank" rel="noopener noreferrer">Speak with Confidence: Designing an Augmented Reality Training Tool for Public Speaking</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mark Edison Jim, Jan Benjamin Yap, Gian Chill Laolao, Andrei Zachary Lim, Jordan Aiko Deja
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Public speaking anxiety affects many individuals, yet opportunities for real-world practice remain limited. This study explores how augmented reality (AR) can provide an accessible training environment for public speaking. Drawing from literature on public speaking, VR-based training, self-efficacy,</span>
                
                <span class="abstract-full" style="display: none;">Public speaking anxiety affects many individuals, yet opportunities for real-world practice remain limited. This study explores how augmented reality (AR) can provide an accessible training environment for public speaking. Drawing from literature on public speaking, VR-based training, self-efficacy, and behavioral feedback mechanisms, we designed SpeakAR, an AR-based tool that simulates audience interaction through virtual models. SpeakAR was evaluated with five participants of varying anxiety levels, each completing six speaking tasks. Results indicate that AR exposure can enhance confidence, with participants finding the system useful for practice. Feedback highlighted the importance of dynamic facial expressions and idle animations in virtual models to improve realism and engagement. Our findings contribute to the design of AR-based training tools for public speaking, offering insights into how immersive environments can support skill development and anxiety reduction.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.4 -->
                    
                <!-- LLMs: 12.5 -->
                    
                <!-- Quantum Computing: 6.8 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.6792
                </span>
                <a href="https://arxiv.org/abs/2504.11406" target="_blank" rel="noopener noreferrer">Multi-level Cellular Automata for FLIM networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Felipe Crispim Salvagnini, Jancarlo F. Gomes, Cid A. N. Santos, Silvio Jamil F. Guimar\~aes, Alexandre X. Falc\~ao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The necessity of abundant annotated data and complex network architectures presents a significant challenge in deep-learning Salient Object Detection (deep SOD) and across the broader deep-learning landscape. This challenge is particularly acute in medical applications in developing countries with l</span>
                
                <span class="abstract-full" style="display: none;">The necessity of abundant annotated data and complex network architectures presents a significant challenge in deep-learning Salient Object Detection (deep SOD) and across the broader deep-learning landscape. This challenge is particularly acute in medical applications in developing countries with limited computational resources. Combining modern and classical techniques offers a path to maintaining competitive performance while enabling practical applications. Feature Learning from Image Markers (FLIM) methodology empowers experts to design convolutional encoders through user-drawn markers, with filters learned directly from these annotations. Recent findings demonstrate that coupling a FLIM encoder with an adaptive decoder creates a flyweight network suitable for SOD, requiring significantly fewer parameters than lightweight models and eliminating the need for backpropagation. Cellular Automata (CA) methods have proven successful in data-scarce scenarios but require proper initialization -- typically through user input, priors, or randomness. We propose a practical intersection of these approaches: using FLIM networks to initialize CA states with expert knowledge without requiring user interaction for each image. By decoding features from each level of a FLIM network, we can initialize multiple CAs simultaneously, creating a multi-level framework. Our method leverages the hierarchical knowledge encoded across different network layers, merging multiple saliency maps into a high-quality final output that functions as a CA ensemble. Benchmarks across two challenging medical datasets demonstrate the competitiveness of our multi-level CA approach compared to established models in the deep SOD literature.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.0 -->
                    
                <!-- LLMs: 7.4 -->
                    
                <!-- Networks: 4.5 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.9675
                </span>
                <a href="https://arxiv.org/abs/2504.11307" target="_blank" rel="noopener noreferrer">Uncertainty Estimation for Trust Attribution to Speed-of-Sound Reconstruction with Variational Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sonia Laguna, Lin Zhang, Can Deniz Bezek, Monika Farkas, Dieter Schweizer, Rahel A. Kubik-Huch, Orcun Goksel
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Speed-of-sound (SoS) is a biomechanical characteristic of tissue, and its imaging can provide a promising biomarker for diagnosis. Reconstructing SoS images from ultrasound acquisitions can be cast as a limited-angle computed-tomography problem, with Variational Networks being a promising model-base</span>
                
                <span class="abstract-full" style="display: none;">Speed-of-sound (SoS) is a biomechanical characteristic of tissue, and its imaging can provide a promising biomarker for diagnosis. Reconstructing SoS images from ultrasound acquisitions can be cast as a limited-angle computed-tomography problem, with Variational Networks being a promising model-based deep learning solution. Some acquired data frames may, however, get corrupted by noise due to, e.g., motion, lack of contact, and acoustic shadows, which in turn negatively affects the resulting SoS reconstructions. We propose to use the uncertainty in SoS reconstructions to attribute trust to each individual acquired frame. Given multiple acquisitions, we then use an uncertainty based automatic selection among these retrospectively, to improve diagnostic decisions. We investigate uncertainty estimation based on Monte Carlo Dropout and Bayesian Variational Inference. We assess our automatic frame selection method for differential diagnosis of breast cancer, distinguishing between benign fibroadenoma and malignant carcinoma. We evaluate 21 lesions classified as BI-RADS~4, which represents suspicious cases for probable malignancy. The most trustworthy frame among four acquisitions of each lesion was identified using uncertainty based criteria. Selecting a frame informed by uncertainty achieved an area under curve of 76% and 80% for Monte Carlo Dropout and Bayesian Variational Inference, respectively, superior to any uncertainty-uninformed baselines with the best one achieving 64%. A novel use of uncertainty estimation is proposed for selecting one of multiple data acquisitions for further processing and decision making.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.2 -->
                    
                <!-- Quantum Computing: 5.7 -->
                    
                <!-- LLMs: 5.0 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.0882
                </span>
                <a href="https://arxiv.org/abs/2504.11186" target="_blank" rel="noopener noreferrer">Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Minjie Zou, Sahana Srinivasan, Thaddaeus Wai Soon Lo, Ke Zou, Gabriel Dawei Yang, Xuguang Ai, Hyunjae Kim, Maxwell Singer, Fares Antaki, Kelvin Li, Robert Chang, Marcus Tan, David Ziyou Chen, Dianbo Liu, Qingyu Chen, Yih Chung Tham
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advances in reasoning-focused large language models (LLMs) mark a shift from general LLMs toward models designed for complex decision-making, a crucial aspect in medicine. However, their performance in specialized domains like ophthalmology remains underexplored. This study comprehensively ev</span>
                
                <span class="abstract-full" style="display: none;">Recent advances in reasoning-focused large language models (LLMs) mark a shift from general LLMs toward models designed for complex decision-making, a crucial aspect in medicine. However, their performance in specialized domains like ophthalmology remains underexplored. This study comprehensively evaluated and compared the accuracy and reasoning capabilities of four newly developed reasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0 Flash-Thinking. Each model was assessed using 5,888 multiple-choice ophthalmology exam questions from the MedMCQA dataset in zero-shot setting. Quantitative evaluation included accuracy, Macro-F1, and five text-generation metrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed against ground-truth reasonings. Average inference time was recorded for a subset of 100 randomly selected questions. Additionally, two board-certified ophthalmologists qualitatively assessed clarity, completeness, and reasoning structure of responses to differential diagnosis questions.O1 (0.902) and DeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in Macro-F1 (0.900). The performance of models across the text-generation metrics varied: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1 and o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0 Flash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and o1 (0.176) led AlignScore. Inference time across the models varied, with DeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest (6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0 Flash-Thinking tended to provide detailed and comprehensive intermediate reasoning, whereas o1 and o3-mini displayed concise and summarized justifications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 28.3 -->
                    
                <!-- Medicine: 14.0 -->
                    
                <!-- RAG: 3.5 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.1751
                </span>
                <a href="https://arxiv.org/abs/1908.08652" target="_blank" rel="noopener noreferrer">MTCNET: Multi-task Learning Paradigm for Crowd Count Estimation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Abhay Kumar, Nishant Jain, Suraj Tripathi, Chirag Singh, Kamal Krishna
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a Multi-Task Learning (MTL) paradigm based deep neural network architecture, called MTCNet (Multi-Task Crowd Network) for crowd density and count estimation. Crowd count estimation is challenging due to the non-uniform scale variations and the arbitrary perspective of an individual image.</span>
                
                <span class="abstract-full" style="display: none;">We propose a Multi-Task Learning (MTL) paradigm based deep neural network architecture, called MTCNet (Multi-Task Crowd Network) for crowd density and count estimation. Crowd count estimation is challenging due to the non-uniform scale variations and the arbitrary perspective of an individual image. The proposed model has two related tasks, with Crowd Density Estimation as the main task and Crowd-Count Group Classification as the auxiliary task. The auxiliary task helps in capturing the relevant scale-related information to improve the performance of the main task. The main task model comprises two blocks: VGG-16 front-end for feature extraction and a dilated Convolutional Neural Network for density map generation. The auxiliary task model shares the same front-end as the main task, followed by a CNN classifier. Our proposed network achieves 5.8% and 14.9% lower Mean Absolute Error (MAE) than the state-of-the-art methods on ShanghaiTech dataset without using any data augmentation. Our model also outperforms with 10.5% lower MAE on UCF_CC_50 dataset.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.3 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.2466
                </span>
                <a href="https://arxiv.org/abs/2503.05447" target="_blank" rel="noopener noreferrer">Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Weigao Sun, Disen Lan, Tong Zhu, Xiaoye Qu, Yu Cheng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that </span>
                
                <span class="abstract-full" style="display: none;">Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides a unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for Linear-MoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as a next-generation foundational model architecture. Code: https://github.com/OpenSparseLLMs/Linear-MoE.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.6 -->
                    
                <!-- LLMs: 13.1 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- RAG: 2.2 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.3755
                </span>
                <a href="https://arxiv.org/abs/2504.11379" target="_blank" rel="noopener noreferrer">Omni$^2$: Unifying Omnidirectional Image Generation and Editing in an Omni Model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Liu Yang, Huiyu Duan, Yucheng Zhu, Xiaohong Liu, Lu Liu, Zitong Xu, Guangji Ma, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">$360^{\circ}$ omnidirectional images (ODIs) have gained considerable attention recently, and are widely used in various virtual reality (VR) and augmented reality (AR) applications. However, capturing such images is expensive and requires specialized equipment, making ODI synthesis increasingly impo</span>
                
                <span class="abstract-full" style="display: none;">$360^{\circ}$ omnidirectional images (ODIs) have gained considerable attention recently, and are widely used in various virtual reality (VR) and augmented reality (AR) applications. However, capturing such images is expensive and requires specialized equipment, making ODI synthesis increasingly important. While common 2D image generation and editing methods are rapidly advancing, these models struggle to deliver satisfactory results when generating or editing ODIs due to the unique format and broad 360$^{\circ}$ Field-of-View (FoV) of ODIs. To bridge this gap, we construct \textbf{\textit{Any2Omni}}, the first comprehensive ODI generation-editing dataset comprises 60,000+ training data covering diverse input conditions and up to 9 ODI generation and editing tasks. Built upon Any2Omni, we propose an \textbf{\underline{Omni}} model for \textbf{\underline{Omni}}-directional image generation and editing (\textbf{\textit{Omni$^2$}}), with the capability of handling various ODI generation and editing tasks under diverse input conditions using one model. Extensive experiments demonstrate the superiority and effectiveness of the proposed Omni$^2$ model for both the ODI generation and editing tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 17.5 -->
                    
                <!-- Medicine: 14.0 -->
                    
                <!-- RAG: 2.7 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- T2I: 2.1 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.382
                </span>
                <a href="https://arxiv.org/abs/2504.11305" target="_blank" rel="noopener noreferrer">CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable Wood Defect Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jincheng Kang, Yi Cen, Yigang Cen, Ke Wang, Yuhan Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Wood defect detection is critical for ensuring quality control in the wood processing industry. However, current industrial applications face two major challenges: traditional methods are costly, subjective, and labor-intensive, while mainstream deep learning models often struggle to balance detecti</span>
                
                <span class="abstract-full" style="display: none;">Wood defect detection is critical for ensuring quality control in the wood processing industry. However, current industrial applications face two major challenges: traditional methods are costly, subjective, and labor-intensive, while mainstream deep learning models often struggle to balance detection accuracy and computational efficiency for edge deployment. To address these issues, this study proposes CFIS-YOLO, a lightweight object detection model optimized for edge devices. The model introduces an enhanced C2f structure, a dynamic feature recombination module, and a novel loss function that incorporates auxiliary bounding boxes and angular constraints. These innovations improve multi-scale feature fusion and small object localization while significantly reducing computational overhead. Evaluated on a public wood defect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of 77.5\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON BM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to 17.3\% of the original implementation, and incurs only a 0.5 percentage point drop in mAP. These results demonstrate that CFIS-YOLO is a practical and effective solution for real-world wood defect detection in resource-constrained environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.6 -->
                    
                <!-- LLMs: 6.3 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Networks: 3.0 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- RAG: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.7642
                </span>
                <a href="https://arxiv.org/abs/2504.03699" target="_blank" rel="noopener noreferrer">Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ying-Jung Chen, Ahmad Albarqawi, Chi-Sheng Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advances in the data-driven medicine approach, which integrates ethically managed and explainable artificial intelligence into clinical decision support systems (CDSS), are critical to ensure reliable and effective patient care. This paper focuses on comparing novel agent system designs that </span>
                
                <span class="abstract-full" style="display: none;">Recent advances in the data-driven medicine approach, which integrates ethically managed and explainable artificial intelligence into clinical decision support systems (CDSS), are critical to ensure reliable and effective patient care. This paper focuses on comparing novel agent system designs that use modular agents to analyze laboratory results, vital signs, and clinical context, and to predict and validate results. We implement our agent system with the eICU database, including running lab analysis, vitals-only interpreters, and contextual reasoners agents first, then sharing the memory into the integration agent, prediction agent, transparency agent, and a validation agent. Our results suggest that the multi-agent system (MAS) performed better than the single-agent system (SAS) with mortality prediction accuracy (59%, 56%) and the mean error for length of stay (LOS)(4.37 days, 5.82 days), respectively. However, the transparency score for the SAS (86.21) is slightly better than the transparency score for MAS (85.5). Finally, this study suggests that our agent-based framework not only improves process transparency and prediction accuracy but also strengthens trustworthy AI-assisted decision support in an intensive care setting.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.3 -->
                    
                <!-- LLMs: 7.7 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.9754
                </span>
                <a href="https://arxiv.org/abs/2504.10750" target="_blank" rel="noopener noreferrer">Real-time Seafloor Segmentation and Mapping</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Michele Grimaldi, Nouf Alkaabi, Francesco Ruscio, Sebastian Realpe Rua, Rafael Garcia, Nuno Gracias
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Posidonia oceanica meadows are a species of seagrass highly dependent on rocks for their survival and conservation. In recent years, there has been a concerning global decline in this species, emphasizing the critical need for efficient monitoring and assessment tools. While deep learning-based sema</span>
                
                <span class="abstract-full" style="display: none;">Posidonia oceanica meadows are a species of seagrass highly dependent on rocks for their survival and conservation. In recent years, there has been a concerning global decline in this species, emphasizing the critical need for efficient monitoring and assessment tools. While deep learning-based semantic segmentation and visual automated monitoring systems have shown promise in a variety of applications, their performance in underwater environments remains challenging due to complex water conditions and limited datasets. This paper introduces a framework that combines machine learning and computer vision techniques to enable an autonomous underwater vehicle (AUV) to inspect the boundaries of Posidonia oceanica meadows autonomously. The framework incorporates an image segmentation module using an existing Mask R-CNN model and a strategy for Posidonia oceanica meadow boundary tracking. Furthermore, a new class dedicated to rocks is introduced to enhance the existing model, aiming to contribute to a comprehensive monitoring approach and provide a deeper understanding of the intricate interactions between the meadow and its surrounding environment. The image segmentation model is validated using real underwater images, while the overall inspection framework is evaluated in a realistic simulation environment, replicating actual monitoring scenarios with real underwater images. The results demonstrate that the proposed framework enables the AUV to autonomously accomplish the main tasks of underwater inspection and segmentation of rocks. Consequently, this work holds significant potential for the conservation and protection of marine environments, providing valuable insights into the status of Posidonia oceanica meadows and supporting targeted preservation efforts</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.3 -->
                    
                <!-- LLMs: 5.5 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Robotics: 2.3 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.7301
                </span>
                <a href="https://arxiv.org/abs/2504.04982" target="_blank" rel="noopener noreferrer">Transforming Future Data Center Operations and Management via Physical AI</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhiwei Cao, Minghao Li, Feng Lin, Jimin Jia, Yonggang Wen, Jianxiong Yin, Simon See
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Data centers (DCs) as mission-critical infrastructures are pivotal in powering the growth of artificial intelligence (AI) and the digital economy. The evolution from Internet DC to AI DC has introduced new challenges in operating and managing data centers for improved business resilience and reduced</span>
                
                <span class="abstract-full" style="display: none;">Data centers (DCs) as mission-critical infrastructures are pivotal in powering the growth of artificial intelligence (AI) and the digital economy. The evolution from Internet DC to AI DC has introduced new challenges in operating and managing data centers for improved business resilience and reduced total cost of ownership. As a result, new paradigms, beyond the traditional approaches based on best practices, must be in order for future data centers. In this research, we propose and develop a novel Physical AI (PhyAI) framework for advancing DC operations and management. Our system leverages the emerging capabilities of state-of-the-art industrial products and our in-house research and development. Specifically, it presents three core modules, namely: 1) an industry-grade in-house simulation engine to simulate DC operations in a highly accurate manner, 2) an AI engine built upon NVIDIA PhysicsNemo for the training and evaluation of physics-informed machine learning (PIML) models, and 3) a digital twin platform built upon NVIDIA Omniverse for our proposed 5-tier digital twin framework. This system presents a scalable and adaptable solution to digitalize, optimize, and automate future data center operations and management, by enabling real-time digital twins for future data centers. To illustrate its effectiveness, we present a compelling case study on building a surrogate model for predicting the thermal and airflow profiles of a large-scale DC in a real-time manner. Our results demonstrate its superior performance over traditional time-consuming Computational Fluid Dynamics/Heat Transfer (CFD/HT) simulation, with a median absolute temperature prediction error of 0.18 {\deg}C. This emerging approach would open doors to several potential research directions for advancing Physical AI in future DC operations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.2 -->
                    
                <!-- LLMs: 6.5 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.123
                </span>
                <a href="https://arxiv.org/abs/2504.10635" target="_blank" rel="noopener noreferrer">Skeleton-Based Intake Gesture Detection With Spatial-Temporal Graph Convolutional Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chunzhuo Wang, Zhewen Xue, T. Sunil Kumar, Guido Camps, Hans Hallez, Bart Vanrumste
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Overweight and obesity have emerged as widespread societal challenges, frequently linked to unhealthy eating patterns. A promising approach to enhance dietary monitoring in everyday life involves automated detection of food intake gestures. This study introduces a skeleton based approach using a mod</span>
                
                <span class="abstract-full" style="display: none;">Overweight and obesity have emerged as widespread societal challenges, frequently linked to unhealthy eating patterns. A promising approach to enhance dietary monitoring in everyday life involves automated detection of food intake gestures. This study introduces a skeleton based approach using a model that combines a dilated spatial-temporal graph convolutional network (ST-GCN) with a bidirectional long-short-term memory (BiLSTM) framework, as called ST-GCN-BiLSTM, to detect intake gestures. The skeleton-based method provides key benefits, including environmental robustness, reduced data dependency, and enhanced privacy preservation. Two datasets were employed for model validation. The OREBA dataset, which consists of laboratory-recorded videos, achieved segmental F1-scores of 86.18% and 74.84% for identifying eating and drinking gestures. Additionally, a self-collected dataset using smartphone recordings in more adaptable experimental conditions was evaluated with the model trained on OREBA, yielding F1-scores of 85.40% and 67.80% for detecting eating and drinking gestures. The results not only confirm the feasibility of utilizing skeleton data for intake gesture detection but also highlight the robustness of the proposed approach in cross-dataset validation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.3 -->
                    
                <!-- LLMs: 9.2 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.8051
                </span>
                <a href="https://arxiv.org/abs/2504.11318" target="_blank" rel="noopener noreferrer">Mildly-Interacting Fermionic Unitaries are Efficiently Learnable</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vishnu Iyer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent work has shown that one can efficiently learn fermionic Gaussian unitaries, also commonly known as nearest-neighbor matchcircuits or non-interacting fermionic unitaries. However, one could ask a similar question about unitaries that are near Gaussian: for example, unitaries prepared with a sm</span>
                
                <span class="abstract-full" style="display: none;">Recent work has shown that one can efficiently learn fermionic Gaussian unitaries, also commonly known as nearest-neighbor matchcircuits or non-interacting fermionic unitaries. However, one could ask a similar question about unitaries that are near Gaussian: for example, unitaries prepared with a small number of non-Gaussian circuit elements. These operators find significance in quantum chemistry and many-body physics, yet no algorithm exists to learn them.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 12.6 -->
                    
                <!-- LLMs: 10.1 -->
                    
                <!-- Medicine: 5.6 -->
                    
                <!-- GNN: 3.1 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -8.072
                </span>
                <a href="https://arxiv.org/abs/2410.13163" target="_blank" rel="noopener noreferrer">Revocable Encryption, Programs, and More: The Case of Multi-Copy Security</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Prabhanjan Ananth, Saachi Mutreja, Alexander Poremba
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Fundamental principles of quantum mechanics have inspired many new research directions, particularly in quantum cryptography. One such principle is quantum no-cloning which has led to the emerging field of revocable cryptography. Roughly speaking, in a revocable cryptographic primitive, a cryptograp</span>
                
                <span class="abstract-full" style="display: none;">Fundamental principles of quantum mechanics have inspired many new research directions, particularly in quantum cryptography. One such principle is quantum no-cloning which has led to the emerging field of revocable cryptography. Roughly speaking, in a revocable cryptographic primitive, a cryptographic object (such as a ciphertext or program) is represented as a quantum state in such a way that surrendering it effectively translates into losing the capability to use this cryptographic object. All of the revocable cryptographic systems studied so far have a major drawback: the recipient only receives one copy of the quantum state. Worse yet, the schemes become completely insecure if the recipient receives many identical copies of the same quantum state -- a property that is clearly much more desirable in practice. While multi-copy security has been extensively studied for a number of other quantum cryptographic primitives, it has so far received only little treatment in context of unclonable primitives. Our work, for the first time, shows the feasibility of revocable primitives, such as revocable encryption and revocable programs, which satisfy multi-copy security in oracle models. This suggest that the stronger notion of multi-copy security is within reach in unclonable cryptography more generally, and therefore could lead to a new research direction in the field.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 10.1 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- LLMs: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Math: 2.5 -->
                    
                <!-- Robotics: 2.3 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Pathfinding: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.0781
                </span>
                <a href="https://arxiv.org/abs/2504.11444" target="_blank" rel="noopener noreferrer">Fault Tolerant Quantum Simulation via Symplectic Transvections</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhuangzhuang Chen, Jack Owen Weinberg, Narayanan Rengaswamy
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Conventional approaches to fault-tolerant quantum computing realize logical circuits gate-by-gate, synthesizing each gate independently on one or more code blocks. This incurs excess overhead and doesn't leverage common structures in quantum algorithms. In contrast, we propose a framework that enabl</span>
                
                <span class="abstract-full" style="display: none;">Conventional approaches to fault-tolerant quantum computing realize logical circuits gate-by-gate, synthesizing each gate independently on one or more code blocks. This incurs excess overhead and doesn't leverage common structures in quantum algorithms. In contrast, we propose a framework that enables the execution of entire logical circuit blocks at once, preserving their global structure. This whole-block approach allows for the direct implementation of logical Trotter circuits - of arbitrary rotation angles - on any stabilizer code, providing a powerful new method for fault tolerant Hamiltonian simulation within a single code block. At the heart of our approach lies a deep structural correspondence between symplectic transvections and Trotter circuits. This connection enables both logical and physical circuits to share the Trotter structure while preserving stabilizer centralization and circuit symmetry even in the presence of non-Clifford rotations. We discuss potential approaches to fault tolerance via biased noise and code concatenation. While we illustrate the key principles using a $[[8,3,3]]$ code, our simulations show that the framework applies to Hamiltonian simulation on even good quantum LDPC codes. These results open the door to new algorithm-tailored, block-level strategies for fault tolerant circuit design, especially in quantum simulation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 11.3 -->
                    
                <!-- LLMs: 8.0 -->
                    
                <!-- Medicine: 4.3 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.387
                </span>
                <a href="https://arxiv.org/abs/2504.10692" target="_blank" rel="noopener noreferrer">PlantD: Performance, Latency ANalysis, and Testing for Data Pipelines -- An Open Source Measurement, Testing, and Simulation Framework</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christopher Bogart, Rajeev Chhajer, Baljit Singh, Tony Fontana, Majd Sakr
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As the volume of data available from sensor-enabled devices such as vehicles expands, it is increasingly hard for companies to make informed decisions about the cost of capturing, processing, and storing the data from every device. Business teams may forecast costs associated with deployments and us</span>
                
                <span class="abstract-full" style="display: none;">As the volume of data available from sensor-enabled devices such as vehicles expands, it is increasingly hard for companies to make informed decisions about the cost of capturing, processing, and storing the data from every device. Business teams may forecast costs associated with deployments and use patterns of devices that they sell, yet lack ways of forecasting the cost and performance of the data pipelines needed to support their devices. Without such forecasting, a company's safest choice is to make worst-case capacity estimates, and pay for overprovisioned infrastructure. Existing data pipeline benchmarking tools can measure latency, cost, and throughput as needed for development, but cannot easily close the gap in communicating the implications with business teams to inform cost forecasting. In this paper, we introduce an open-source tool, PlantD, a harness for measuring data pipelines as they are being developed, and for interpreting that data in a business context. PlantD collects a complete suite of metrics and visualizations, when developing or evaluating data pipeline architectures, configurations, and business use cases. It acts as a metaphorical data pipeline wind tunnel, enabling experiments with synthetic data to characterize and compare the performance of pipelines. It then uses those results to allow modeling of expected annual cost and performance under projected real-world loads. We describe the architecture of PlantD, walk through an example of using it to measure and compare three variants of a pipeline for processing automotive telemetry, and demonstrate how business and engineering teams can simulate scenarios together and answer "what-if" questions about the pipeline's performance under different business assumptions, allowing them to intelligently predict performance and cost measures of their critical, high-data generation infrastructure.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 18.8 -->
                    
                <!-- LLMs: 8.1 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -10.026
                </span>
                <a href="https://arxiv.org/abs/2504.10542" target="_blank" rel="noopener noreferrer">An Efficient Quantum Classifier Based on Hamiltonian Representations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Federico Tiblias, Anna Schroeder, Yue Zhang, Mariami Gachechiladze, Iryna Gurevych
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Quantum machine learning (QML) is a discipline that seeks to transfer the advantages of quantum computing to data-driven tasks. However, many studies rely on toy datasets or heavy feature reduction, raising concerns about their scalability. Progress is further hindered by hardware limitations and th</span>
                
                <span class="abstract-full" style="display: none;">Quantum machine learning (QML) is a discipline that seeks to transfer the advantages of quantum computing to data-driven tasks. However, many studies rely on toy datasets or heavy feature reduction, raising concerns about their scalability. Progress is further hindered by hardware limitations and the significant costs of encoding dense vector representations on quantum devices. To address these challenges, we propose an efficient approach called Hamiltonian classifier that circumvents the costs associated with data encoding by mapping inputs to a finite set of Pauli strings and computing predictions as their expectation values. In addition, we introduce two classifier variants with different scaling in terms of parameters and sample complexity. We evaluate our approach on text and image classification tasks, against well-established classical and quantum models. The Hamiltonian classifier delivers performance comparable to or better than these methods. Notably, our method achieves logarithmic complexity in both qubits and quantum gates, making it well-suited for large-scale, real-world applications. We make our implementation available on GitHub.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 16.3 -->
                    
                <!-- LLMs: 9.1 -->
                    
                <!-- Medicine: 6.2 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -10.9401
                </span>
                <a href="https://arxiv.org/abs/2504.11213" target="_blank" rel="noopener noreferrer">Characterizing High Schmidt Number Witnesses in Arbitrary Dimensions System</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Liang Xiong, Nung-sing Sze
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A profound comprehension of quantum entanglement is crucial for the progression of quantum technologies. The degree of entanglement can be assessed by enumerating the entangled degrees of freedom, leading to the determination of a parameter known as the Schmidt number. In this paper, we develop an e</span>
                
                <span class="abstract-full" style="display: none;">A profound comprehension of quantum entanglement is crucial for the progression of quantum technologies. The degree of entanglement can be assessed by enumerating the entangled degrees of freedom, leading to the determination of a parameter known as the Schmidt number. In this paper, we develop an efficient analytical tool for characterizing high Schmidt number witnesses for bipartite quantum states in arbitrary dimensions. Our methods not only offer viable mathematical methods for constructing high-dimensional Schmidt number witnesses in theory but also simplify the quantification of entanglement and dimensionality. Most notably, we develop high-dimensional Schmidt number witnesses within arbitrary-dimensional systems, with our Schmidt witness coefficients relying solely on the operator Schmidt coefficient. Subsequently, we demonstrate our theoretical advancements and computational superiority by constructing Schmidt number witnesses in arbitrary dimensional bipartite quantum systems with Schmidt numbers four and five.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 15.0 -->
                    
                <!-- Medicine: 8.2 -->
                    
                <!-- LLMs: 6.7 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -11.3637
                </span>
                <a href="https://arxiv.org/abs/2504.11109" target="_blank" rel="noopener noreferrer">Fine-Tuning Large Language Models on Quantum Optimization Problems for Circuit Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Linus Jern, Valter Uotila, Cong Yu, Bo Zhao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language models (LLM) have achieved remarkable outcomes in addressing complex problems, including math, coding, and analyzing large amounts of scientific reports. Yet few works have explored the potential of LLM in quantum computing. The most challenging problem is how to leverage LLMs to auto</span>
                
                <span class="abstract-full" style="display: none;">Large language models (LLM) have achieved remarkable outcomes in addressing complex problems, including math, coding, and analyzing large amounts of scientific reports. Yet few works have explored the potential of LLM in quantum computing. The most challenging problem is how to leverage LLMs to automatically generate quantum circuits at a large scale. In this paper, we address such a challenge by fine-tuning LLMs and injecting the domain-specific knowledge of quantum computing. In particular, we investigate the mechanisms to generate training data sets and construct the end-to-end pipeline to fine-tune pre-trained LLMs that produce parameterized quantum circuits for optimization problems. We have prepared 14,000 quantum circuits covering a substantial part of the quantum optimization landscape: 12 optimization problem instances and their optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can construct syntactically correct parametrized quantum circuits in the most recent OpenQASM 3.0. We have evaluated the quality of the parameters by comparing them to the optimized expectation values and distributions. Our evaluation shows that the fine-tuned LLM outperforms state-of-the-art models and that the parameters are better than random. The LLM-generated parametrized circuits and initial parameters can be used as a starting point for further optimization, \emph{e.g.,} templates in quantum machine learning and the benchmark for compilers and hardware.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 15.9 -->
                    
                <!-- Quantum Computing: 12.6 -->
                    
                <!-- Medicine: 4.3 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -11.9635
                </span>
                <a href="https://arxiv.org/abs/2410.17908" target="_blank" rel="noopener noreferrer">Fully smooth one shot multipartite soft covering of quantum states without pairwise independence</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pranab Sen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We provide a powerful machinery to prove fully smooth one shot multipartite covering, aka convex split, type results for quantum states. In the important case of smooth multipartite convex split for classical quantum states, aka smooth multipartite soft covering, our machinery works even when certai</span>
                
                <span class="abstract-full" style="display: none;">We provide a powerful machinery to prove fully smooth one shot multipartite covering, aka convex split, type results for quantum states. In the important case of smooth multipartite convex split for classical quantum states, aka smooth multipartite soft covering, our machinery works even when certain marginals of these states do not satisfy pairwise independence. The recent paper (arXiv:2410.17893) gave the first proof of fully smooth multipartite convex split by simplifying and extending a technique called telescoping, developed originally for convex split by (arXiv:2304.12056). However, that work as well as all earlier works on convex split assumed pairwise or even more independence amongst suitable marginals of the quantum states.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 17.9 -->
                    
                <!-- LLMs: 6.4 -->
                    
                <!-- Medicine: 5.5 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- SpikingNN: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Hardware: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -19.9057
                </span>
                <a href="https://arxiv.org/abs/2410.08010" target="_blank" rel="noopener noreferrer">Securing HHL Quantum Algorithm against Quantum Computer Attacks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yizhuo Tan, Hrvoje Kukina, Jakub Szefer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As the quantum research community expands and new quantum algorithms are created and implemented, it is essential to consider the security implications and potential threats that could lead to the compromise the information processed by them. This work focuses on securing the HHL quantum algorithm a</span>
                
                <span class="abstract-full" style="display: none;">As the quantum research community expands and new quantum algorithms are created and implemented, it is essential to consider the security implications and potential threats that could lead to the compromise the information processed by them. This work focuses on securing the HHL quantum algorithm against attacks while it executes on a quantum computer. Specifically, two types of potential attacks could be deployed on a cloud-based quantum computer by an attacker circuit attempting to interfere with the victim HHL circuit: the Improper Initialization Attack (IIA) and the Higher Energy Attack (HEA). To protect the HHL algorithm from IIA and HEA, this work proposes first-of-a-kind defense strategies against these attacks on the HHL quantum algorithm. Next, this work demonstrates an implementation of a new quantum circuit for the HHL quantum algorithm that incorporates these defenses. The redesigned quantum circuit is necessary to successfully apply and realize all proposed defense strategies. Finally, this work illustrates how these defense strategies function in practice in the redesigned circuit, specifically how they can protect the HHL quantum algorithm from both IIA and HEA across multiple qubits involving all three types of qubits used in the HHL algorithms: ancilla, clock, and b. The defense requires minimal modification to the circuit, and has only a very small effect on the fidelity of the circuits. The circuits have been tested and validated in both simulation, and also on real IBM quantum computer hardware. The work further analyzes how the modified HHL circuit with the defenses is affected by noise during quantum computation. This work in the end demonstrates that it is practical to add protections to quantum circuits so that they not only perform correct computation, but also self-detect if an attack has occured during the execution.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 24.9 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Math: 2.7 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- LLMs: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
    </div>
    
    <div class="date-section">
        <h2 class="date-header">2025-04-15</h2>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    17.2067
                </span>
                <a href="https://arxiv.org/abs/2504.09415" target="_blank" rel="noopener noreferrer">Nash Equilibrium Between Consumer Electronic Devices and DoS Attacker for Distributed IoT-enabled RSE Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gengcan Chen, Donghong Cai, Zahid Khan, Jawad Ahmad, Wadii Boulila
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between cons</span>
                
                <span class="abstract-full" style="display: none;">In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between consumer electronic devices and DoS attackers is critical. This paper focuses on the adversarial strategy between consumer electronic devices and DoS attackers in IoT-enabled RSE Systems. We first propose a remote joint estimation model for distributed measurements to effectively reduce consumer electronic device workload and minimize data leakage risks. The Kalman filter is deployed on the remote estimator, and the DoS attacks with open-loop as well as closed-loop are considered. We further introduce advanced reinforcement learning techniques, including centralized and distributed Minimax-DQN, to address high-dimensional decision-making challenges in both open-loop and closed-loop scenarios. Especially, the Q-network instead of the Q-table is used in the proposed approaches, which effectively solves the challenge of Q-learning. Moreover, the proposed distributed Minimax-DQN reduces the action space to expedite the search for Nash Equilibrium (NE). The experimental results validate that the proposed model can expeditiously restore the RSE error covariance to a stable state in the presence of DoS attacks, exhibiting notable attack robustness. The proposed centralized and distributed Minimax-DQN effectively resolves the NE in both open and closed-loop case, showcasing remarkable performance in terms of convergence. It reveals that substantial advantages in both efficiency and stability are achieved compared with the state-of-the-art methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 76.4%">
                            Reinforcement Learning
                        </span>
                <span class="tag-badge low-confidence" style="background-color: #546bc5" title="Confidence: 74.7%">
                            Game Theory
                        </span>
                <!-- Medicine: 5.9 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Robotics: 2.9 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    13.7429
                </span>
                <a href="https://arxiv.org/abs/2504.01332" target="_blank" rel="noopener noreferrer">When to Truncate the Archive? On the Effect of the Truncation Frequency in Multi-Objective Optimisation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhiji Cui, Zimin Liang, Lie Meng Pang, Hisao Ishibuchi, Miqing Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Using an archive to store nondominated solutions found during the search of a multi-objective evolutionary algorithm (MOEA) is a useful practice. However, as nondominated solutions of a multi-objective optimisation problem can be enormous or infinitely many, it is desirable to provide the decision-m</span>
                
                <span class="abstract-full" style="display: none;">Using an archive to store nondominated solutions found during the search of a multi-objective evolutionary algorithm (MOEA) is a useful practice. However, as nondominated solutions of a multi-objective optimisation problem can be enormous or infinitely many, it is desirable to provide the decision-maker with only a small, representative portion of all the nondominated solutions in the archive, thus entailing a truncation operation. Then, an important issue is when to truncate the archive. This can be done once a new solution generated, a batch of new solutions generated, or even using an unbounded archive to keep all nondominated solutions generated and truncate it later. Intuitively, the last approach may lead to a better result since we have all the information in hand before performing the truncation. In this paper, we study this issue and investigate the effect of the timing of truncating the archive. We apply well-established truncation criteria that are commonly used in the population maintenance procedure of MOEAs (e.g., crowding distance, hypervolume indicator, and decomposition). We show that, interestingly, truncating the archive once a new solution generated tends to be the best, whereas considering an unbounded archive is often the worst. We analyse and discuss this phenomenon. Our results highlight the importance of developing effective subset selection techniques (rather than employing the population maintenance methods in MOEAs) when using a large archive.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #ae668e" title="Confidence: 75.6%">
                            Evolutionary Algorithms
                        </span>
                <!-- Reinforcement Learning: 3.8 -->
                    
                <!-- Math: 3.2 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Pathfinding: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- GNN: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    10.8846
                </span>
                <a href="https://arxiv.org/abs/2409.11267" target="_blank" rel="noopener noreferrer">Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Caio Fabio Oliveira da Silva, Azita Dabiri, Bart De Schutter
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This work proposes an approach that integrates reinforcement learning and model predictive control (MPC) to solve finite-horizon optimal control problems in mixed-logical dynamical systems efficiently. Optimization-based control of such systems with discrete and continuous decision variables entails</span>
                
                <span class="abstract-full" style="display: none;">This work proposes an approach that integrates reinforcement learning and model predictive control (MPC) to solve finite-horizon optimal control problems in mixed-logical dynamical systems efficiently. Optimization-based control of such systems with discrete and continuous decision variables entails the online solution of mixed-integer linear programs, which suffer from the curse of dimensionality. Our approach aims to mitigate this issue by decoupling the decision on the discrete variables from the decision on the continuous variables. In the proposed approach, reinforcement learning determines the discrete decision variables and simplifies the online optimization problem of the MPC controller from a mixed-integer linear program to a linear program, significantly reducing the computational time. A fundamental contribution of this work is the definition of the decoupled Q-function, which plays a crucial role in making the learning problem tractable in a combinatorial action space. We motivate the use of recurrent neural networks to approximate the decoupled Q-function and show how they can be employed in a reinforcement learning setting. Simulation experiments on a microgrid system using real-world data demonstrate that the proposed method substantially reduces the online computation time of MPC while maintaining high feasibility and low suboptimality.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 79.8%">
                            Reinforcement Learning
                        </span>
                <!-- Medicine: 3.2 -->
                    
                <!-- Math: 2.6 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- LLMs: 2.0 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    10.4314
                </span>
                <a href="https://arxiv.org/abs/2504.09035" target="_blank" rel="noopener noreferrer">InterQ: A DQN Framework for Optimal Intermittent Control</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shubham Aggarwal, Dipankar Maity, Tamer Ba\c{s}ar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this letter, we explore the communication-control co-design of discrete-time stochastic linear systems through reinforcement learning. Specifically, we examine a closed-loop system involving two sequential decision-makers: a scheduler and a controller. The scheduler continuously monitors the syst</span>
                
                <span class="abstract-full" style="display: none;">In this letter, we explore the communication-control co-design of discrete-time stochastic linear systems through reinforcement learning. Specifically, we examine a closed-loop system involving two sequential decision-makers: a scheduler and a controller. The scheduler continuously monitors the system's state but transmits it to the controller intermittently to balance the communication cost and control performance. The controller, in turn, determines the control input based on the intermittently received information. Given the partially nested information structure, we show that the optimal control policy follows a certainty-equivalence form. Subsequently, we analyze the qualitative behavior of the scheduling policy. To develop the optimal scheduling policy, we propose InterQ, a deep reinforcement learning algorithm which uses a deep neural network to approximate the Q-function. Through extensive numerical evaluations, we analyze the scheduling landscape and further compare our approach against two baseline strategies: (a) a multi-period periodic scheduling policy, and (b) an event-triggered policy. The results demonstrate that our proposed method outperforms both baselines. The open source implementation can be found at https://github.com/AC-sh/InterQ.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 76.5%">
                            Reinforcement Learning
                        </span>
                <!-- Medicine: 4.2 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- LLMs: 2.3 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Multi-armed Bandit: 1.0 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    9.5515
                </span>
                <a href="https://arxiv.org/abs/2504.08943" target="_blank" rel="noopener noreferrer">Investigating the Treacherous Turn in Deep Reinforcement Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chace Ashcraft, Kiran Karra, Josh Carney, Nathan Drenkow
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by </span>
                
                <span class="abstract-full" style="display: none;">The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by the human supervisor, but when deployed to perform its task, it performs an alternate behavior without the supervisor there to prevent it. Initial experiments applying DRL to an implementation of the A Link to the Past example do not produce the treacherous turn effect naturally, despite various modifications to the environment intended to produce it. However, in this work, we find the treacherous behavior to be reproducible in a DRL agent when using other trojan injection strategies. This approach deviates from the prototypical treacherous turn behavior since the behavior is explicitly trained into the agent, rather than occurring as an emergent consequence of environmental complexity or poor objective specification. Nonetheless, these experiments provide new insights into the challenges of producing agents capable of true treacherous turn behavior.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 76.4%">
                            Reinforcement Learning
                        </span>
                <!-- LLMs: 4.4 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Medicine: 2.3 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    8.3912
                </span>
                <a href="https://arxiv.org/abs/2504.05108" target="_blank" rel="noopener noreferrer">Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, Caglar Gulcehre
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating t</span>
                
                <span class="abstract-full" style="display: none;">Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #ae668e" title="Confidence: 69.1%">
                            Evolutionary Algorithms
                        </span>
                <!-- LLMs: 38.8 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- Reinforcement Learning: 3.7 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    6.7451
                </span>
                <a href="https://arxiv.org/abs/2504.09716" target="_blank" rel="noopener noreferrer">Dominated Actions in Imperfect-Information Games</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sam Ganzfried
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Na</span>
                
                <span class="abstract-full" style="display: none;">Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Nash equilibrium. For imperfect-information games in extensive form, we could convert the game to strategic form and then iteratively remove dominated strategies in the same way; however, this conversion may cause an exponential blowup in game size. In this paper we define and study the concept of dominated actions in imperfect-information games. Our main result is a polynomial-time algorithm for determining whether an action is dominated (strictly or weakly) by any mixed strategy in n-player games, which can be extended to an algorithm for iteratively removing dominated actions. This allows us to efficiently reduce the size of the game tree as a preprocessing step for Nash equilibrium computation. We explore the role of dominated actions empirically in the "All In or Fold" No-Limit Texas Hold'em poker variant.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #546bc5" title="Confidence: 74.2%">
                            Game Theory
                        </span>
                <!-- LLMs: 3.8 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Math: 2.4 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Pathfinding: 2.2 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- GNN: 1.6 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    1.2103
                </span>
                <a href="https://arxiv.org/abs/2504.08801" target="_blank" rel="noopener noreferrer">Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andrew Kiruluta, Priscilla Burity, Samantha Williams
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Transformer architectures, underpinned by the self-attention mechanism, have achieved state-of-the-art results across numerous natural language processing (NLP) tasks by effectively modeling long-range dependencies. However, the computational complexity of self-attention, scaling quadratically with </span>
                
                <span class="abstract-full" style="display: none;">Transformer architectures, underpinned by the self-attention mechanism, have achieved state-of-the-art results across numerous natural language processing (NLP) tasks by effectively modeling long-range dependencies. However, the computational complexity of self-attention, scaling quadratically with input sequence length, presents significant challenges for processing very long sequences or operating under resource constraints. This paper introduces the Learnable Multi-Scale Wavelet Transformer (LMWT), a novel architecture that replaces the standard dot-product self-attention with a learnable multi-scale Haar wavelet transform module. Leveraging the intrinsic multi-resolution properties of wavelets, the LMWT efficiently captures both local details and global context. Crucially, the parameters of the wavelet transform, including scale-specific coefficients, are learned end-to-end during training, allowing the model to adapt its decomposition strategy to the data and task. We present the detailed mathematical formulation of the learnable Haar wavelet module and its integration into the transformer framework, supplemented by an architectural diagram. We conduct a comprehensive experimental evaluation on a standard machine translation benchmark (WMT16 En-De), comparing the LMWT against a baseline self-attention transformer using metrics like BLEU score, perplexity, and token accuracy. Furthermore, we analyze the computational complexity, highlighting the linear scaling of our approach, discuss its novelty in the context of related work, and explore the interpretability offered by visualizing the learned Haar coefficients. Our results indicate that the LMWT achieves competitive performance while offering substantial computational advantages, positioning it as a promising and novel alternative for efficient sequence modeling.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #2aa97e" title="Confidence: 72.6%">
                            Attention
                        </span>
                <!-- Medicine: 5.2 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- LLMs: 2.8 -->
                    
                <!-- Math: 2.4 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0935
                </span>
                <a href="https://arxiv.org/abs/2503.15199" target="_blank" rel="noopener noreferrer">Radon: a Programming Model and Platform for Computing Continuum Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Luca De Martini, Dario d'Abate, Alessandro Margara, Gianpaolo Cugola
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Emerging compute continuum environments pose new challenges that traditional cloud-centric architectures struggle to address. Latency, bandwidth constraints, and the heterogeneity of edge environments hinder the efficiency of centralized cloud solutions. While major cloud providers extend their plat</span>
                
                <span class="abstract-full" style="display: none;">Emerging compute continuum environments pose new challenges that traditional cloud-centric architectures struggle to address. Latency, bandwidth constraints, and the heterogeneity of edge environments hinder the efficiency of centralized cloud solutions. While major cloud providers extend their platforms to the edge, these approaches often overlook its unique characteristics, limiting its potential.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 14.0 -->
                    
                <!-- Medicine: 7.2 -->
                    
                <!-- Quantum Computing: 5.4 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1318
                </span>
                <a href="https://arxiv.org/abs/2504.09200" target="_blank" rel="noopener noreferrer">A Datagram Extension to DNS over QUIC: Proven Resource Conservation in the Internet of Things</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Darius Saif, Ashraf Matrawy
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we investigate the Domain Name System (DNS) over QUIC (DoQ) and propose a non-disruptive extension, which can greatly reduce DoQ's resource consumption. This extension can benefit all DNS clients - especially Internet of Things (IoT) devices. This is important because even resource-co</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we investigate the Domain Name System (DNS) over QUIC (DoQ) and propose a non-disruptive extension, which can greatly reduce DoQ's resource consumption. This extension can benefit all DNS clients - especially Internet of Things (IoT) devices. This is important because even resource-constrained IoT devices can generate dozens of DNS requests every hour. DNS is a crucial service that correlates IP addresses and domain names. It is traditionally sent as plain-text, favoring low-latency results over security and privacy. The repercussion of this can be eavesdropping and information leakage about IoT devices. To address these concerns, the newest and most promising solution is DoQ. QUIC offers features similar to TCP and TLS while also supporting early data delivery and stream multiplexing. DoQ's specification requires that DNS exchanges occur over independent streams in a long-lived QUIC connection. Our hypothesis is that due to DNS's typically high transaction volume, managing QUIC streams may be overly resource intensive for IoT devices. Therefore, we have designed and implemented a data delivery mode for DoQ using QUIC datagrams, which we believe to be more preferable than stream-based delivery. To test our theory, we analyzed the memory, CPU, signaling, power, and time of each DoQ delivery mode in a setup generating real queries and network traffic. Our novel datagram-based delivery mode proved to be decisively more resource-friendly with little compromise in terms of functionality or performance. Furthermore, our paper is the first to investigate multiple queries over DoQ, to our knowledge.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 11.9 -->
                    
                <!-- Medicine: 6.3 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1494
                </span>
                <a href="https://arxiv.org/abs/2504.09073" target="_blank" rel="noopener noreferrer">A Multi-view Discourse Framework for Integrating Semantic and Syntactic Features in Dialog Agents</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Akanksha Mehndiratta, Krishna Asawa
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multiturn dialogue models aim to generate human-like responses by leveraging conversational context, consisting of utterances from previous exchanges. Existing methods often neglect the interactions between these utterances or treat all of them as equally significant. This paper introduces a discour</span>
                
                <span class="abstract-full" style="display: none;">Multiturn dialogue models aim to generate human-like responses by leveraging conversational context, consisting of utterances from previous exchanges. Existing methods often neglect the interactions between these utterances or treat all of them as equally significant. This paper introduces a discourse-aware framework for response selection in retrieval-based dialogue systems. The proposed model first encodes each utterance and response with contextual, positional, and syntactic features using Multi-view Canonical Correlation Analysis (MCCA). It then learns discourse tokens that capture relationships between an utterance and its surrounding turns in a shared subspace via Canonical Correlation Analysis (CCA). This two-step approach effectively integrates semantic and syntactic features to build discourse-level understanding. Experiments on the Ubuntu Dialogue Corpus demonstrate that our model achieves significant improvements in automatic evaluation metrics, highlighting its effectiveness in response selection.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 16.4 -->
                    
                <!-- Medicine: 7.8 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- RAG: 2.3 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1816
                </span>
                <a href="https://arxiv.org/abs/2409.16395" target="_blank" rel="noopener noreferrer">HELIOT: LLM-Based CDSS for Adverse Drug Reaction Management</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gabriele De Vito, Filomena Ferrucci, Athanasios Angelakis
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Medication errors significantly threaten patient safety, leading to adverse drug events and substantial economic burdens on healthcare systems. Clinical Decision Support Systems (CDSSs) aimed at mitigating these errors often face limitations when processing unstructured clinical data, including reli</span>
                
                <span class="abstract-full" style="display: none;">Medication errors significantly threaten patient safety, leading to adverse drug events and substantial economic burdens on healthcare systems. Clinical Decision Support Systems (CDSSs) aimed at mitigating these errors often face limitations when processing unstructured clinical data, including reliance on static databases and rule-based algorithms, frequently generating excessive alerts that lead to alert fatigue among healthcare providers. This paper introduces HELIOT, an innovative CDSS for adverse drug reaction management that processes free-text clinical information using Large Language Models (LLMs) integrated with a comprehensive pharmaceutical data repository. HELIOT leverages advanced natural language processing capabilities to interpret medical narratives, extract relevant drug reaction information from unstructured clinical notes, and learn from past patient-specific medication tolerances to reduce false alerts, enabling more nuanced and contextual adverse drug event warnings across primary care, specialist consultations, and hospital settings. An initial evaluation using a synthetic dataset of clinical narratives and expert-verified ground truth shows promising results. HELIOT achieves high accuracy in a controlled setting. In addition, by intelligently analyzing previous medication tolerance documented in clinical notes and distinguishing between cases requiring different alert types, HELIOT can potentially reduce interruptive alerts by over 50% compared to traditional CDSSs. While these preliminary findings are encouraging, real-world validation will be essential to confirm these benefits in clinical practice.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 20.0 -->
                    
                <!-- Medicine: 8.2 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- RAG: 3.8 -->
                    
                <!-- 3D: 2.5 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1871
                </span>
                <a href="https://arxiv.org/abs/2410.05168" target="_blank" rel="noopener noreferrer">ReasoningRank: Teaching Student Models to Rank through Reasoning-Based Knowledge Distillation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yuelyu Ji, Zhuochun Li, Rui Meng, Daqing He
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reranking documents based on their relevance to a given query is a critical task in information retrieval. Traditional reranking methods often lack transparency and rely on proprietary models, hindering reproducibility and interpretability. We propose Reason-to-Rank (R2R), a novel open-source rerank</span>
                
                <span class="abstract-full" style="display: none;">Reranking documents based on their relevance to a given query is a critical task in information retrieval. Traditional reranking methods often lack transparency and rely on proprietary models, hindering reproducibility and interpretability. We propose Reason-to-Rank (R2R), a novel open-source reranking approach that enhances transparency by generating two types of reasoning: direct relevance reasoning, which explains how a document addresses the query, and comparison reasoning, which justifies the relevance of one document over another. We leverage large language models (LLMs) as teacher models to generate these explanations and distill this knowledge into smaller, openly available student models. Our student models are trained to generate meaningful reasoning and rerank documents, achieving competitive performance across multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate that R2R not only improves reranking accuracy but also provides valuable insights into the decision-making process. By offering a structured and interpretable solution with openly accessible resources, R2R aims to bridge the gap between effectiveness and transparency in information retrieval, fostering reproducibility and further research in the field.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 20.6 -->
                    
                <!-- Medicine: 5.1 -->
                    
                <!-- RAG: 2.5 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.2413
                </span>
                <a href="https://arxiv.org/abs/2503.02268" target="_blank" rel="noopener noreferrer">AppAgentX: Evolving GUI Agents as Proficient Smartphone Users</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wenjia Jiang, Yangyang Zhuang, Chenxi Song, Xu Yang, Joey Tianyi Zhou, Chi Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required</span>
                
                <span class="abstract-full" style="display: none;">Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required predefined rules. However, the reliance on step-by-step reasoning in LLM-based agents often results in inefficiencies, particularly for routine tasks. In contrast, traditional rule-based systems excel in efficiency but lack the intelligence and flexibility to adapt to novel scenarios. To address this challenge, we propose a novel evolutionary framework for GUI agents that enhances operational efficiency while retaining intelligence and flexibility. Our approach incorporates a memory mechanism that records the agent's task execution history. By analyzing this history, the agent identifies repetitive action sequences and evolves high-level actions that act as shortcuts, replacing these low-level operations and improving efficiency. This allows the agent to focus on tasks requiring more complex reasoning, while simplifying routine actions. Experimental results on multiple benchmark tasks demonstrate that our approach significantly outperforms existing methods in both efficiency and accuracy. The code will be open-sourced to support further research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 31.4 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- RAG: 2.4 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.2956
                </span>
                <a href="https://arxiv.org/abs/2503.12721" target="_blank" rel="noopener noreferrer">Can Reasoning Models Reason about Hardware? An Agentic HLS Perspective</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Luca Collini, Andrew Hennessee, Ramesh Karri, Siddharth Garg
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent Large Language Models (LLMs) such as OpenAI o3-mini and DeepSeek-R1 use enhanced reasoning through Chain-of-Thought (CoT). Their potential in hardware design, which relies on expert-driven iterative optimization, remains unexplored. This paper investigates whether reasoning LLMs can address c</span>
                
                <span class="abstract-full" style="display: none;">Recent Large Language Models (LLMs) such as OpenAI o3-mini and DeepSeek-R1 use enhanced reasoning through Chain-of-Thought (CoT). Their potential in hardware design, which relies on expert-driven iterative optimization, remains unexplored. This paper investigates whether reasoning LLMs can address challenges in High-Level Synthesis (HLS) design space exploration and optimization. During HLS, engineers manually define pragmas/directives to balance performance and resource constraints. We propose an LLM-based optimization agentic framework that automatically restructures code, inserts pragmas, and identifies optimal design points via feedback from HLs tools and access to integer-linear programming (ILP) solvers. Experiments compare reasoning models against conventional LLMs on benchmarks using success rate, efficiency, and design quality (area/latency) metrics, and provide the first-ever glimpse into the CoTs produced by a powerful open-source reasoning model like DeepSeek-R1.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 29.6 -->
                    
                <!-- Medicine: 5.9 -->
                    
                <!-- RAG: 3.5 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- 3D: 2.5 -->
                    
                <!-- Blockchain: 2.4 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.4657
                </span>
                <a href="https://arxiv.org/abs/2504.03786" target="_blank" rel="noopener noreferrer">Do "New Snow Tablets" Contain Snow? Large Language Models Over-Rely on Names to Identify Ingredients of Chinese Drugs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sifan Li, Yujun Cai, Bryan Hooi, Nanyun Peng, Yiwei Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Traditional Chinese Medicine (TCM) has seen increasing adoption in healthcare, with specialized Large Language Models (LLMs) emerging to support clinical applications. A fundamental requirement for these models is accurate identification of TCM drug ingredients. In this paper, we evaluate how genera</span>
                
                <span class="abstract-full" style="display: none;">Traditional Chinese Medicine (TCM) has seen increasing adoption in healthcare, with specialized Large Language Models (LLMs) emerging to support clinical applications. A fundamental requirement for these models is accurate identification of TCM drug ingredients. In this paper, we evaluate how general and TCM-specialized LLMs perform when identifying ingredients of Chinese drugs. Our systematic analysis reveals consistent failure patterns: models often interpret drug names literally, overuse common herbs regardless of relevance, and exhibit erratic behaviors when faced with unfamiliar formulations. LLMs also fail to understand the verification task. These findings demonstrate that current LLMs rely primarily on drug names rather than possessing systematic pharmacological knowledge. To address these limitations, we propose a Retrieval Augmented Generation (RAG) approach focused on ingredient names. Experiments across 220 TCM formulations show our method significantly improves accuracy from approximately 50% to 82% in ingredient verification tasks. Our work highlights critical weaknesses in current TCM-specific LLMs and offers a practical solution for enhancing their clinical reliability.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 41.9 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- RAG: 3.7 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.4724
                </span>
                <a href="https://arxiv.org/abs/2504.10430" target="_blank" rel="noopener noreferrer">LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Minqian Liu, Zhiyang Xu, Xinyi Zhang, Heajun An, Sarvech Qadir, Qi Zhang, Pamela J. Wisniewski, Jin-Hee Cho, Sang Won Lee, Ruoxi Jia, Lifu Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, e</span>
                
                <span class="abstract-full" style="display: none;">Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 42.5 -->
                    
                <!-- Medicine: 4.3 -->
                    
                <!-- RAG: 2.9 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2895
                </span>
                <a href="https://arxiv.org/abs/2504.09647" target="_blank" rel="noopener noreferrer">Building AI Service Repositories for On-Demand Service Orchestration in 6G AI-RAN</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yun Tang, Mengbang Zou, Udhaya Chandhar Srinivasan, Obumneme Umealor, Dennis Kevogo, Benjamin James Scott, Weisi Guo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Efficient orchestration of AI services in 6G AI-RAN requires well-structured, ready-to-deploy AI service repositories combined with orchestration methods adaptive to diverse runtime contexts across radio access, edge, and cloud layers. Current literature lacks comprehensive frameworks for constructi</span>
                
                <span class="abstract-full" style="display: none;">Efficient orchestration of AI services in 6G AI-RAN requires well-structured, ready-to-deploy AI service repositories combined with orchestration methods adaptive to diverse runtime contexts across radio access, edge, and cloud layers. Current literature lacks comprehensive frameworks for constructing such repositories and generally overlooks key practical orchestration factors. This paper systematically identifies and categorizes critical attributes influencing AI service orchestration in 6G networks and introduces an open-source, LLM-assisted toolchain that automates service packaging, deployment, and runtime profiling. We validate the proposed toolchain through the Cranfield AI Service repository case study, demonstrating significant automation benefits, reduced manual coding efforts, and the necessity of infrastructure-specific profiling, paving the way for more practical orchestration frameworks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 15.8 -->
                    
                <!-- Medicine: 10.2 -->
                    
                <!-- Quantum Computing: 4.1 -->
                    
                <!-- RAG: 2.5 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3623
                </span>
                <a href="https://arxiv.org/abs/2504.08807" target="_blank" rel="noopener noreferrer">The Exploratory Study on the Relationship Between the Failure of Distance Metrics in High-Dimensional Space and Emergent Phenomena</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: HongZheng Liu, YiNuo Tian, Zhiyue Wu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents a unified framework, integrating information theory and statistical mechanics, to connect metric failure in high-dimensional data with emergence in complex systems. We propose the "Information Dilution Theorem," demonstrating that as dimensionality ($d$) increases, the mutual inf</span>
                
                <span class="abstract-full" style="display: none;">This paper presents a unified framework, integrating information theory and statistical mechanics, to connect metric failure in high-dimensional data with emergence in complex systems. We propose the "Information Dilution Theorem," demonstrating that as dimensionality ($d$) increases, the mutual information efficiency between geometric metrics (e.g., Euclidean distance) and system states decays approximately as $O(1/d)$. This decay arises from the mismatch between linearly growing system entropy and sublinearly growing metric entropy, explaining the mechanism behind distance concentration. Building on this, we introduce information structural complexity ($C(S)$) based on the mutual information matrix spectrum and interaction encoding capacity ($C'$) derived from information bottleneck theory. The "Emergence Critical Theorem" states that when $C(S)$ exceeds $C'$, new global features inevitably emerge, satisfying a predefined mutual information threshold. This provides an operational criterion for self-organization and phase transitions. We discuss potential applications in physics, biology, and deep learning, suggesting potential directions like MI-based manifold learning (UMAP+) and offering a quantitative foundation for analyzing emergence across disciplines.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.2 -->
                    
                <!-- LLMs: 7.2 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4471
                </span>
                <a href="https://arxiv.org/abs/2504.09697" target="_blank" rel="noopener noreferrer">SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kenan Tang, Yanhong Li, Yao Qin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent prompt-based image editing models have demonstrated impressive prompt-following capability at structural editing tasks. However, existing models still fail to perform local edits, follow detailed editing prompts, or maintain global image quality beyond a single editing step. To address these </span>
                
                <span class="abstract-full" style="display: none;">Recent prompt-based image editing models have demonstrated impressive prompt-following capability at structural editing tasks. However, existing models still fail to perform local edits, follow detailed editing prompts, or maintain global image quality beyond a single editing step. To address these challenges, we introduce SPICE, a training-free workflow that accepts arbitrary resolutions and aspect ratios, accurately follows user requirements, and improves image quality consistently during more than 100 editing steps. By synergizing the strengths of a base diffusion model and a Canny edge ControlNet model, SPICE robustly handles free-form editing instructions from the user. SPICE outperforms state-of-the-art baselines on a challenging realistic image-editing dataset consisting of semantic editing (object addition, removal, replacement, and background change), stylistic editing (texture changes), and structural editing (action change) tasks. Not only does SPICE achieve the highest quantitative performance according to standard evaluation metrics, but it is also consistently preferred by users over existing image-editing methods. We release the workflow implementation for popular diffusion model Web UIs to support further research and artistic exploration.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.5 -->
                    
                <!-- Medicine: 10.3 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.5044
                </span>
                <a href="https://arxiv.org/abs/2504.08877" target="_blank" rel="noopener noreferrer">The SERENADE project: Sensor-Based Explainable Detection of Cognitive Decline</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gabriele Civitarese, Michele Fiori, Andrea Arighi, Daniela Galimberti, Graziana Florio, Claudio Bettini
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Mild Cognitive Impairment (MCI) affects 12-18% of individuals over 60. MCI patients exhibit cognitive dysfunctions without significant daily functional loss. While MCI may progress to dementia, predicting this transition remains a clinical challenge due to limited and unreliable indicators. Behavior</span>
                
                <span class="abstract-full" style="display: none;">Mild Cognitive Impairment (MCI) affects 12-18% of individuals over 60. MCI patients exhibit cognitive dysfunctions without significant daily functional loss. While MCI may progress to dementia, predicting this transition remains a clinical challenge due to limited and unreliable indicators. Behavioral changes, like in the execution of Activities of Daily Living (ADLs), can signal such progression. Sensorized smart homes and wearable devices offer an innovative solution for continuous, non-intrusive monitoring ADLs for MCI patients. However, current machine learning models for detecting behavioral changes lack transparency, hindering clinicians' trust. This paper introduces the SERENADE project, a European Union-funded initiative that aims to detect and explain behavioral changes associated with cognitive decline using explainable AI methods. SERENADE aims at collecting one year of data from 30 MCI patients living alone, leveraging AI to support clinical decision-making and offering a new approach to early dementia detection.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.3 -->
                    
                <!-- LLMs: 11.0 -->
                    
                <!-- Quantum Computing: 4.5 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.5975
                </span>
                <a href="https://arxiv.org/abs/2504.10070" target="_blank" rel="noopener noreferrer">DTFSal: Audio-Visual Dynamic Token Fusion for Video Saliency Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kiana Hoshanfar, Alireza Hosseini, Ahmad Kalhor, Babak Nadjar Araabi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Audio-visual saliency prediction aims to mimic human visual attention by identifying salient regions in videos through the integration of both visual and auditory information. Although visual-only approaches have significantly advanced, effectively incorporating auditory cues remains challenging due</span>
                
                <span class="abstract-full" style="display: none;">Audio-visual saliency prediction aims to mimic human visual attention by identifying salient regions in videos through the integration of both visual and auditory information. Although visual-only approaches have significantly advanced, effectively incorporating auditory cues remains challenging due to complex spatio-temporal interactions and high computational demands. To address these challenges, we propose Dynamic Token Fusion Saliency (DFTSal), a novel audio-visual saliency prediction framework designed to balance accuracy with computational efficiency. Our approach features a multi-scale visual encoder equipped with two novel modules: the Learnable Token Enhancement Block (LTEB), which adaptively weights tokens to emphasize crucial saliency cues, and the Dynamic Learnable Token Fusion Block (DLTFB), which employs a shifting operation to reorganize and merge features, effectively capturing long-range dependencies and detailed spatial information. In parallel, an audio branch processes raw audio signals to extract meaningful auditory features. Both visual and audio features are integrated using our Adaptive Multimodal Fusion Block (AMFB), which employs local, global, and adaptive fusion streams for precise cross-modal fusion. The resulting fused features are processed by a hierarchical multi-decoder structure, producing accurate saliency maps. Extensive evaluations on six audio-visual benchmarks demonstrate that DFTSal achieves SOTA performance while maintaining computational efficiency.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.3 -->
                    
                <!-- LLMs: 9.7 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.67
                </span>
                <a href="https://arxiv.org/abs/2504.09855" target="_blank" rel="noopener noreferrer">PestMA: LLM-based Multi-Agent System for Informed Pest Management</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hongrui Shi, Shunbao Li, Zhipeng Yuan, Po Yang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Effective pest management is complex due to the need for accurate, context-specific decisions. Recent advancements in large language models (LLMs) open new possibilities for addressing these challenges by providing sophisticated, adaptive knowledge acquisition and reasoning. However, existing LLM-ba</span>
                
                <span class="abstract-full" style="display: none;">Effective pest management is complex due to the need for accurate, context-specific decisions. Recent advancements in large language models (LLMs) open new possibilities for addressing these challenges by providing sophisticated, adaptive knowledge acquisition and reasoning. However, existing LLM-based pest management approaches often rely on a single-agent paradigm, which can limit their capacity to incorporate diverse external information, engage in systematic validation, and address complex, threshold-driven decisions. To overcome these limitations, we introduce PestMA, an LLM-based multi-agent system (MAS) designed to generate reliable and evidence-based pest management advice. Building on an editorial paradigm, PestMA features three specialized agents, an Editor for synthesizing pest management recommendations, a Retriever for gathering relevant external data, and a Validator for ensuring correctness. Evaluations on real-world pest scenarios demonstrate that PestMA achieves an initial accuracy of 86.8% for pest management decisions, which increases to 92.6% after validation. These results underscore the value of collaborative agent-based workflows in refining and validating decisions, highlighting the potential of LLM-based multi-agent systems to automate and enhance pest management processes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 12.3 -->
                    
                <!-- Medicine: 10.3 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- RAG: 2.5 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7065
                </span>
                <a href="https://arxiv.org/abs/2504.00698" target="_blank" rel="noopener noreferrer">Command A: An Enterprise-Ready Large Language Model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Team Cohere, :, Aakanksha, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Milad Alizadeh, Yazeed Alnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Rapha\"el Avalos, Zahara Aviv, Sammie Bae, Saurabh Baji, Alexandre Barbet, Max Bartolo, Bj\"orn Bebensee, Neeral Beladia, Walter Beller-Morales, Alexandre B\'erard, Andrew Berneshawi, Anna Bialas, Phil Blunsom, Matt Bobkin, Adi Bongale, Sam Braun, Maxime Brunet, Samuel Cahyawijaya, David Cairuz, Jon Ander Campos, Cassie Cao, Kris Cao, Roman Castagn\'e, Juli\'an Cendrero, Leila Chan Currie, Yash Chandak, Diane Chang, Giannis Chatziveroglou, Hongyu Chen, Claire Cheng, Alexis Chevalier, Justin T. Chiu, Eugene Cho, Eugene Choi, Eujeong Choi, Tim Chung, Volkan Cirik, Ana Cismaru, Pierre Clavier, Henry Conklin, Lucas Crawhall-Stein, Devon Crouse, Andres Felipe Cruz-Salinas, Ben Cyrus, Daniel D'souza, Hugo Dalla-Torre, John Dang, William Darling, Omar Darwiche Domingues, Saurabh Dash, Antoine Debugne, Th\'eo Dehaze, Shaan Desai, Joan Devassy, Rishit Dholakia, Kyle Duffy, Ali Edalati, Ace Eldeib, Abdullah Elkady, Sarah Elsharkawy, Irem Erg\"un, Beyza Ermis, Marzieh Fadaee, Boyu Fan, Lucas Fayoux, Yannis Flet-Berliac, Nick Frosst, Matthias Gall\'e, Wojciech Galuba, Utsav Garg, Matthieu Geist, Mohammad Gheshlaghi Azar, Ellen Gilsenan-McMahon, Seraphina Goldfarb-Tarrant, Tomas Goldsack, Aidan Gomez, Victor Machado Gonzaga, Nithya Govindarajan, Manoj Govindassamy, Nathan Grinsztajn, Nikolas Gritsch, Patrick Gu, Shangmin Guo, Kilian Haefeli, Rod Hajjar, Tim Hawes, Jingyi He, Sebastian Hofst\"atter, Sungjin Hong, Sara Hooker, Tom Hosking, Stephanie Howe, Eric Hu, Renjie Huang, Hemant Jain, Ritika Jain, Nick Jakobi, Madeline Jenkins, JJ Jordan, Dhruti Joshi, Jason Jung, Trushant Kalyanpur, Siddhartha Rao Kamalakara, Julia Kedrzycki, Gokce Keskin, Edward Kim, Joon Kim, Wei-Yin Ko, Tom Kocmi, Michael Kozakov, Wojciech Kry\'sci\'nski, Arnav Kumar Jain, Komal Kumar Teru, Sander Land, Michael Lasby, Olivia Lasche, Justin Lee, Patrick Lewis, Jeffrey Li, Jonathan Li, Hangyu Lin, Acyr Locatelli, Kevin Luong, Raymond Ma, Luk\'a\v{s} Mach, Marina Machado, Joanne Magbitang, Brenda Malacara Lopez, Aryan Mann, Kelly Marchisio, Olivia Markham, Alexandre Matton, Alex McKinney, Dominic McLoughlin, Jozef Mokry, Adrien Morisot, Autumn Moulder, Harry Moynehan, Maximilian Mozes, Vivek Muppalla, Lidiya Murakhovska, Hemangani Nagarajan, Alekhya Nandula, Hisham Nasir, Shauna Nehra, Josh Netto-Rosen, Daniel Ohashi, James Owers-Bardsley, Jason Ozuzu, Dennis Padilla, Gloria Park, Sam Passaglia, Jeremy Pekmez, Laura Penstone, Aleksandra Piktus, Case Ploeg, Andrew Poulton, Youran Qi, Shubha Raghvendra, Miguel Ramos, Ekagra Ranjan, Pierre Richemond, C\'ecile Robert-Michon, Aur\'elien Rodriguez, Sudip Roy, Sebastian Ruder, Laura Ruis, Louise Rust, Anubhav Sachan, Alejandro Salamanca, Kailash Karthik Saravanakumar, Isha Satyakam, Alice Schoenauer Sebag, Priyanka Sen, Sholeh Sepehri, Preethi Seshadri, Ye Shen, Tom Sherborne, Sylvie Shang Shi, Sanal Shivaprasad, Vladyslav Shmyhlo, Anirudh Shrinivason, Inna Shteinbuk, Amir Shukayev, Mathieu Simard, Ella Snyder, Ava Spataru, Victoria Spooner, Trisha Starostina, Florian Strub, Yixuan Su, Jimin Sun, Dwarak Talupuru, Eugene Tarassov, Elena Tommasone, Jennifer Tracey, Billy Trend, Evren Tumer, Ahmet \"Ust\"un, Bharat Venkitesh, David Venuto, Pat Verga, Maxime Voisin, Alex Wang, Donglu Wang, Shijian Wang, Edmond Wen, Naomi White, Jesse Willman, Marysia Winkels, Chen Xia, Jessica Xie, Minjie Xu, Bowen Yang, Tan Yi-Chern, Ivan Zhang, Zhenyu Zhao, Zhoujie Zhao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balanc</span>
                
                <span class="abstract-full" style="display: none;">In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 15.2 -->
                    
                <!-- Medicine: 12.1 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- RAG: 3.0 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7627
                </span>
                <a href="https://arxiv.org/abs/2504.09014" target="_blank" rel="noopener noreferrer">MSCCL++: Rethinking GPU Communication Abstractions for Cutting-edge AI Applications</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aashaka Shah, Abhinav Jangda, Binyang Li, Caio Rocha, Changho Hwang, Jithin Jose, Madan Musuvathi, Olli Saarikivi, Peng Cheng, Qinghua Zhou, Roshan Dathathri, Saeed Maleki, Ziyue Yang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modern cutting-edge AI applications are being developed over fast-evolving, heterogeneous, nascent hardware devices. This requires frequent reworking of the AI software stack to adopt bottom-up changes from new hardware, which takes time for general-purpose software libraries. Consequently, real app</span>
                
                <span class="abstract-full" style="display: none;">Modern cutting-edge AI applications are being developed over fast-evolving, heterogeneous, nascent hardware devices. This requires frequent reworking of the AI software stack to adopt bottom-up changes from new hardware, which takes time for general-purpose software libraries. Consequently, real applications often develop custom software stacks optimized for their specific workloads and hardware. Custom stacks help quick development and optimization, but incur a lot of redundant efforts across applications in writing non-portable code. This paper discusses an alternative communication library interface for AI applications that offers both portability and performance by reducing redundant efforts while maintaining flexibility for customization. We present MSCCL++, a novel abstraction of GPU communication based on separation of concerns: (1) a primitive interface provides a minimal hardware abstraction as a common ground for software and hardware developers to write custom communication, and (2) higher-level portable interfaces and specialized implementations enable optimization for different hardware environments. This approach makes the primitive interface reusable across applications while enabling highly flexible optimization. Compared to state-of-the-art baselines (NCCL, RCCL, and MSCCL), MSCCL++ achieves speedups of up to 3.8$\times$ for collective communication and up to 15\% for real-world AI inference workloads. MSCCL++ is in production of multiple AI services provided by Microsoft Azure, and is also adopted by RCCL, the GPU collective communication library maintained by AMD. MSCCL++ is open-source and available at https://github.com/microsoft/mscclpp.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.1 -->
                    
                <!-- LLMs: 9.3 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.767
                </span>
                <a href="https://arxiv.org/abs/2503.08915" target="_blank" rel="noopener noreferrer">Reconstruct Anything Model: a lightweight foundation model for computational imaging</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Matthieu Terris, Samuel Hurault, Maxime Song, Julian Tachella
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Most existing learning-based methods for solving imaging inverse problems can be roughly divided into two classes: iterative algorithms, such as plug-and-play and diffusion methods, that leverage pretrained denoisers, and unrolled architectures that are trained end-to-end for specific imaging proble</span>
                
                <span class="abstract-full" style="display: none;">Most existing learning-based methods for solving imaging inverse problems can be roughly divided into two classes: iterative algorithms, such as plug-and-play and diffusion methods, that leverage pretrained denoisers, and unrolled architectures that are trained end-to-end for specific imaging problems. Iterative methods in the first class are computationally costly and often provide suboptimal reconstruction performance, whereas unrolled architectures are generally specific to a single inverse problem and require expensive training. In this work, we propose a novel non-iterative, lightweight architecture that incorporates knowledge about the forward operator (acquisition physics and noise parameters) without relying on unrolling. Our model is trained to solve a wide range of inverse problems beyond denoising, including deblurring, magnetic resonance imaging, computed tomography, inpainting, and super-resolution. The proposed model can be easily adapted to unseen inverse problems or datasets with a few fine-tuning steps (up to a few images) in a self-supervised way, without ground-truth references. Throughout a series of experiments, we demonstrate state-of-the-art performance from medical imaging to low-photon imaging and microscopy.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.1 -->
                    
                <!-- LLMs: 7.0 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8123
                </span>
                <a href="https://arxiv.org/abs/2504.10117" target="_blank" rel="noopener noreferrer">AGO: Adaptive Grounding for Open World 3D Occupancy Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Peizheng Li, Shuxiao Ding, You Zhou, Qingwen Zhang, Onat Inak, Larissa Triess, Niklas Hanselmann, Marius Cordts, Andreas Zell
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Open-world 3D semantic occupancy prediction aims to generate a voxelized 3D representation from sensor inputs while recognizing both known and unknown objects. Transferring open-vocabulary knowledge from vision-language models (VLMs) offers a promising direction but remains challenging. However, met</span>
                
                <span class="abstract-full" style="display: none;">Open-world 3D semantic occupancy prediction aims to generate a voxelized 3D representation from sensor inputs while recognizing both known and unknown objects. Transferring open-vocabulary knowledge from vision-language models (VLMs) offers a promising direction but remains challenging. However, methods based on VLM-derived 2D pseudo-labels with traditional supervision are limited by a predefined label space and lack general prediction capabilities. Direct alignment with pretrained image embeddings, on the other hand, fails to achieve reliable performance due to often inconsistent image and text representations in VLMs. To address these challenges, we propose AGO, a novel 3D occupancy prediction framework with adaptive grounding to handle diverse open-world scenarios. AGO first encodes surrounding images and class prompts into 3D and text embeddings, respectively, leveraging similarity-based grounding training with 3D pseudo-labels. Additionally, a modality adapter maps 3D embeddings into a space aligned with VLM-derived image embeddings, reducing modality gaps. Experiments on Occ3D-nuScenes show that AGO improves unknown object prediction in zero-shot and few-shot transfer while achieving state-of-the-art closed-world self-supervised performance, surpassing prior methods by 4.09 mIoU.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.2 -->
                    
                <!-- LLMs: 8.4 -->
                    
                <!-- 3D: 5.2 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- RAG: 2.9 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8633
                </span>
                <a href="https://arxiv.org/abs/2504.10329" target="_blank" rel="noopener noreferrer">InstructEngine: Instruction-driven Text-to-Image Alignment</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xingyu Lu, Yuhang Hu, YiFan Zhang, Kaiyu Jiang, Changyi Liu, Tianke Zhang, Jinpeng Wang, Bin Wen, Chun Yuan, Fan Yang, Tingting Gao, Di Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been extensively utilized for preference alignment of text-to-image models. Existing methods face certain limitations in terms of both data and algorithm. For training data, most approaches rely on manual annotated preference data, eithe</span>
                
                <span class="abstract-full" style="display: none;">Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been extensively utilized for preference alignment of text-to-image models. Existing methods face certain limitations in terms of both data and algorithm. For training data, most approaches rely on manual annotated preference data, either by directly fine-tuning the generators or by training reward models to provide training signals. However, the high annotation cost makes them difficult to scale up, the reward model consumes extra computation and cannot guarantee accuracy. From an algorithmic perspective, most methods neglect the value of text and only take the image feedback as a comparative signal, which is inefficient and sparse. To alleviate these drawbacks, we propose the InstructEngine framework. Regarding annotation cost, we first construct a taxonomy for text-to-image generation, then develop an automated data construction pipeline based on it. Leveraging advanced large multimodal models and human-defined rules, we generate 25K text-image preference pairs. Finally, we introduce cross-validation alignment method, which refines data efficiency by organizing semantically analogous samples into mutually comparable pairs. Evaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and SDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art baselines, with ablation study confirming the benefits of InstructEngine's all components. A win rate of over 50% in human reviews also proves that InstructEngine better aligns with human preferences.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 11.3 -->
                    
                <!-- Medicine: 10.1 -->
                    
                <!-- Quantum Computing: 4.5 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.9055
                </span>
                <a href="https://arxiv.org/abs/2504.10148" target="_blank" rel="noopener noreferrer">Hierarchical and Step-Layer-Wise Tuning of Attention Specialty for Multi-Instance Synthesis in Diffusion Transformers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chunyang Zhang, Zhenhong Sun, Zhicheng Zhang, Junyan Wang, Yu Zhang, Dong Gong, Huadong Mo, Daoyi Dong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Text-to-image (T2I) generation models often struggle with multi-instance synthesis (MIS), where they must accurately depict multiple distinct instances in a single image based on complex prompts detailing individual features. Traditional MIS control methods for UNet architectures like SD v1.5/SDXL f</span>
                
                <span class="abstract-full" style="display: none;">Text-to-image (T2I) generation models often struggle with multi-instance synthesis (MIS), where they must accurately depict multiple distinct instances in a single image based on complex prompts detailing individual features. Traditional MIS control methods for UNet architectures like SD v1.5/SDXL fail to adapt to DiT-based models like FLUX and SD v3.5, which rely on integrated attention between image and text tokens rather than text-image cross-attention. To enhance MIS in DiT, we first analyze the mixed attention mechanism in DiT. Our token-wise and layer-wise analysis of attention maps reveals a hierarchical response structure: instance tokens dominate early layers, background tokens in middle layers, and attribute tokens in later layers. Building on this observation, we propose a training-free approach for enhancing MIS in DiT-based models with hierarchical and step-layer-wise attention specialty tuning (AST). AST amplifies key regions while suppressing irrelevant areas in distinct attention maps across layers and steps, guided by the hierarchical structure. This optimizes multimodal interactions by hierarchically decoupling the complex prompts with instance-based sketches. We evaluate our approach using upgraded sketch-based layouts for the T2I-CompBench and customized complex scenes. Both quantitative and qualitative results confirm our method enhances complex layout generation, ensuring precise instance placement and attribute representation in MIS.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 18.9 -->
                    
                <!-- Medicine: 10.0 -->
                    
                <!-- RAG: 2.8 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.9595
                </span>
                <a href="https://arxiv.org/abs/2504.09814" target="_blank" rel="noopener noreferrer">DUDA: Distilled Unsupervised Domain Adaptation for Lightweight Semantic Segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Beomseok Kang, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-Pang Chiu, Supun Samarasekera
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Unsupervised Domain Adaptation (UDA) is essential for enabling semantic segmentation in new domains without requiring costly pixel-wise annotations. State-of-the-art (SOTA) UDA methods primarily use self-training with architecturally identical teacher and student networks, relying on Exponential Mov</span>
                
                <span class="abstract-full" style="display: none;">Unsupervised Domain Adaptation (UDA) is essential for enabling semantic segmentation in new domains without requiring costly pixel-wise annotations. State-of-the-art (SOTA) UDA methods primarily use self-training with architecturally identical teacher and student networks, relying on Exponential Moving Average (EMA) updates. However, these approaches face substantial performance degradation with lightweight models due to inherent architectural inflexibility leading to low-quality pseudo-labels. To address this, we propose Distilled Unsupervised Domain Adaptation (DUDA), a novel framework that combines EMA-based self-training with knowledge distillation (KD). Our method employs an auxiliary student network to bridge the architectural gap between heavyweight and lightweight models for EMA-based updates, resulting in improved pseudo-label quality. DUDA employs a strategic fusion of UDA and KD, incorporating innovative elements such as gradual distillation from large to small networks, inconsistency loss prioritizing poorly adapted classes, and learning with multiple teachers. Extensive experiments across four UDA benchmarks demonstrate DUDA's superiority in achieving SOTA performance with lightweight models, often surpassing the performance of heavyweight models from other approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.4 -->
                    
                <!-- LLMs: 11.9 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- RAG: 2.5 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0421
                </span>
                <a href="https://arxiv.org/abs/2411.16219" target="_blank" rel="noopener noreferrer">Weakly Supervised Panoptic Segmentation for Defect-Based Grading of Fresh Produce</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Manuel Knott, Divinefavour Odion, Sameer Sontakke, Anup Karwa, Thijs Defraeye
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Visual inspection for defect grading in agricultural supply chains is crucial but traditionally labor-intensive and error-prone. Automated computer vision methods typically require extensively annotated datasets, which are often unavailable in decentralized supply chains. We address this challenge b</span>
                
                <span class="abstract-full" style="display: none;">Visual inspection for defect grading in agricultural supply chains is crucial but traditionally labor-intensive and error-prone. Automated computer vision methods typically require extensively annotated datasets, which are often unavailable in decentralized supply chains. We address this challenge by evaluating the Segment Anything Model (SAM) to generate dense panoptic segmentation masks from sparse annotations. These dense predictions are then used to train a supervised panoptic segmentation model. Focusing on banana surface defects (bruises and scars), we validate our approach using 476 field images annotated with 1440 defects. While SAM-generated masks generally align with human annotations, substantially reducing annotation effort, we explicitly identify failure cases associated with specific defect sizes and shapes. Despite these limitations, our approach offers practical estimates of defect number and relative size from panoptic masks, underscoring the potential and current boundaries of foundation models for defect quantification in low-data agricultural scenarios. GitHub: https://github.com/manuelknott/banana-defect-segmentation</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.8 -->
                    
                <!-- LLMs: 13.0 -->
                    
                <!-- Quantum Computing: 4.2 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0712
                </span>
                <a href="https://arxiv.org/abs/2504.10403" target="_blank" rel="noopener noreferrer">Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yan zhu, Jingyang zhu, Ting Wang, Yuanming Shi, Chunxiao Jiang, Khaled Ben Letaief
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Advancements in artificial intelligence (AI) and low-earth orbit (LEO) satellites have promoted the application of large remote sensing foundation models for various downstream tasks. However, direct downloading of these models for fine-tuning on the ground is impeded by privacy concerns and limited</span>
                
                <span class="abstract-full" style="display: none;">Advancements in artificial intelligence (AI) and low-earth orbit (LEO) satellites have promoted the application of large remote sensing foundation models for various downstream tasks. However, direct downloading of these models for fine-tuning on the ground is impeded by privacy concerns and limited bandwidth. Satellite federated learning (FL) offers a solution by enabling model fine-tuning directly on-board satellites and aggregating model updates without data downloading. Nevertheless, for large foundation models, the computational capacity of satellites is insufficient to support effective on-board fine-tuning in traditional satellite FL frameworks. To address these challenges, we propose a satellite-ground collaborative federated fine-tuning framework. The key of the framework lies in how to reasonably decompose and allocate model components to alleviate insufficient on-board computation capabilities. During fine-tuning, satellites exchange intermediate results with ground stations or other satellites for forward propagation and back propagation, which brings communication challenges due to the special communication topology of space transmission networks, such as intermittent satellite-ground communication, short duration of satellite-ground communication windows, and unstable inter-orbit inter-satellite links (ISLs). To reduce transmission delays, we further introduce tailored communication strategies that integrate both communication and computing resources. Specifically, we propose a parallel intra-orbit communication strategy, a topology-aware satellite-ground communication strategy, and a latency-minimalization inter-orbit communication strategy to reduce space communication costs. Simulation results demonstrate significant reductions in training time with improvements of approximately 33%.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.9 -->
                    
                <!-- LLMs: 10.1 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0931
                </span>
                <a href="https://arxiv.org/abs/2504.08907" target="_blank" rel="noopener noreferrer">Spatial Audio Processing with Large Language Model on Wearable Devices</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ayushi Mishra, Yang Bai, Priyadarshan Narayanasamy, Nakul Garg, Nirupam Roy
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Integrating spatial context into large language models (LLMs) has the potential to revolutionize human-computer interaction, particularly in wearable devices. In this work, we present a novel system architecture that incorporates spatial speech understanding into LLMs, enabling contextually aware an</span>
                
                <span class="abstract-full" style="display: none;">Integrating spatial context into large language models (LLMs) has the potential to revolutionize human-computer interaction, particularly in wearable devices. In this work, we present a novel system architecture that incorporates spatial speech understanding into LLMs, enabling contextually aware and adaptive applications for wearable technologies. Our approach leverages microstructure-based spatial sensing to extract precise Direction of Arrival (DoA) information using a monaural microphone. To address the lack of existing dataset for microstructure-assisted speech recordings, we synthetically create a dataset called OmniTalk by using the LibriSpeech dataset. This spatial information is fused with linguistic embeddings from OpenAI's Whisper model, allowing each modality to learn complementary contextual representations. The fused embeddings are aligned with the input space of LLaMA-3.2 3B model and fine-tuned with lightweight adaptation technique LoRA to optimize for on-device processing. SING supports spatially-aware automatic speech recognition (ASR), achieving a mean error of $25.72^\circ$-a substantial improvement compared to the 88.52$^\circ$ median error in existing work-with a word error rate (WER) of 5.3. SING also supports soundscaping, for example, inference how many people were talking and their directions, with up to 5 people and a median DoA error of 16$^\circ$. Our system demonstrates superior performance in spatial speech understanding while addressing the challenges of power efficiency, privacy, and hardware constraints, paving the way for advanced applications in augmented reality, accessibility, and immersive experiences.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 13.3 -->
                    
                <!-- Medicine: 10.9 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1017
                </span>
                <a href="https://arxiv.org/abs/2504.06106" target="_blank" rel="noopener noreferrer">A ROS2-based software library for inverse dynamics computation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vincenzo Petrone, Enrico Ferrentino, Pasquale Chiacchio
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Inverse dynamics computation is a critical component in robot control, planning and simulation, enabling the calculation of joint torques required to achieve a desired motion. This paper presents a ROS2-based software library designed to solve the inverse dynamics problem for robotic systems. The li</span>
                
                <span class="abstract-full" style="display: none;">Inverse dynamics computation is a critical component in robot control, planning and simulation, enabling the calculation of joint torques required to achieve a desired motion. This paper presents a ROS2-based software library designed to solve the inverse dynamics problem for robotic systems. The library is built around an abstract class with three concrete implementations: one for simulated robots and two for real UR10 and Franka robots. This contribution aims to provide a flexible, extensible, robot-agnostic solution to inverse dynamics, suitable for both simulation and real-world scenarios involving planning and control applications. The related software is available at https://github.com/unisa-acg/inverse-dynamics-solver/tree/rap.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.4 -->
                    
                <!-- LLMs: 6.2 -->
                    
                <!-- Quantum Computing: 4.2 -->
                    
                <!-- Networks: 3.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1519
                </span>
                <a href="https://arxiv.org/abs/2504.09628" target="_blank" rel="noopener noreferrer">Outage Probability Analysis for OTFS with Finite Blocklength</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xin Zhang, Wensheng Lin, Lixin Li, Zhu Han, Tad Matsumoto
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Orthogonal time frequency space (OTFS) modulation is widely acknowledged as a prospective waveform for future wireless communication networks.To provide insights for the practical system design, this paper analyzes the outage probability of OTFS modulation with finite blocklength.To begin with, we p</span>
                
                <span class="abstract-full" style="display: none;">Orthogonal time frequency space (OTFS) modulation is widely acknowledged as a prospective waveform for future wireless communication networks.To provide insights for the practical system design, this paper analyzes the outage probability of OTFS modulation with finite blocklength.To begin with, we present the system model and formulate the analysis of outage probability for OTFS with finite blocklength as an equivalent problem of calculating the outage probability with finite blocklength over parallel additive white Gaussian noise (AWGN) channels.Subsequently, we apply the equivalent noise approach to derive a lower bound on the outage probability of OTFS with finite blocklength under both average power allocation and water-filling power allocation strategies, respectively.Finally, the lower bounds of the outage probability are determined using the Monte-Carlo method for the two power allocation strategies.The impact of the number of resolvable paths and coding rates on the outage probability is analyzed, and the simulation results are compared with the theoretical lower bounds.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.6 -->
                    
                <!-- Math: 4.9 -->
                    
                <!-- Reinforcement Learning: 3.5 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- LLMs: 2.2 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1601
                </span>
                <a href="https://arxiv.org/abs/2504.03980" target="_blank" rel="noopener noreferrer">Virtual Reality Lensing for Surface Approximation in Feature-driven Volume Visualization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Roberta Mota, Ehud Sharlin, Usman Alim
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a novel lens technique to support the identification of heterogeneous features in direct volume rendering (DVR) visualizations. In contrast to data-centric transfer function (TF) design, our image-driven approach enables users to specify target features directly within the visualization u</span>
                
                <span class="abstract-full" style="display: none;">We present a novel lens technique to support the identification of heterogeneous features in direct volume rendering (DVR) visualizations. In contrast to data-centric transfer function (TF) design, our image-driven approach enables users to specify target features directly within the visualization using deformable quadric surfaces. The lens leverages quadrics for their expressive yet simple parametrization, enabling users to sculpt feature approximations by composing multiple quadric lenses. By doing so, the lens offers greater versatility than traditional rigid-shape lenses for selecting and bringing into focus features with irregular geometry. We discuss the lens visualization and interaction design, advocating for bimanual spatial virtual reality (VR) input for reducing cognitive and physical strain. We also report findings from a pilot qualitative evaluation with a domain specialist using a public asteroid impact dataset. These insights not only shed light on the benefits and pitfalls of using deformable lenses but also suggest directions for future research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.6 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Quantum Computing: 4.2 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- SpikingNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1674
                </span>
                <a href="https://arxiv.org/abs/2504.09206" target="_blank" rel="noopener noreferrer">Rethinking Remaining Useful Life Prediction with Scarce Time Series Data: Regression under Indirect Supervision</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jiaxiang Cheng, Yipeng Pang, Guoqiang Hu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Supervised time series prediction relies on directly measured target variables, but real-world use cases such as predicting remaining useful life (RUL) involve indirect supervision, where the target variable is labeled as a function of another dependent variable. Trending temporal regression techniq</span>
                
                <span class="abstract-full" style="display: none;">Supervised time series prediction relies on directly measured target variables, but real-world use cases such as predicting remaining useful life (RUL) involve indirect supervision, where the target variable is labeled as a function of another dependent variable. Trending temporal regression techniques rely on sequential time series inputs to capture temporal patterns, requiring interpolation when dealing with sparsely and irregularly sampled covariates along the timeline. However, interpolation can introduce significant biases, particularly with highly scarce data. In this paper, we address the RUL prediction problem with data scarcity as time series regression under indirect supervision. We introduce a unified framework called parameterized static regression, which takes single data points as inputs for regression of target values, inherently handling data scarcity without requiring interpolation. The time dependency under indirect supervision is captured via a parametrical rectification (PR) process, approximating a parametric function during inference with historical posteriori estimates, following the same underlying distribution used for labeling during training. Additionally, we propose a novel batch training technique for tasks in indirect supervision to prevent overfitting and enhance efficiency. We evaluate our model on public benchmarks for RUL prediction with simulated data scarcity. Our method demonstrates competitive performance in prediction accuracy when dealing with highly scarce time series data.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.6 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Quantum Computing: 4.1 -->
                    
                <!-- Networks: 3.6 -->
                    
                <!-- GNN: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- SpikingNN: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.2236
                </span>
                <a href="https://arxiv.org/abs/2504.10001" target="_blank" rel="noopener noreferrer">GaussVideoDreamer: 3D Scene Generation with Video Diffusion and Inconsistency-Aware Gaussian Splatting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Junlin Hao, Peiheng Wang, Haoyang Wang, Xinggong Zhang, Zongming Guo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Single-image 3D scene reconstruction presents significant challenges due to its inherently ill-posed nature and limited input constraints. Recent advances have explored two promising directions: multiview generative models that train on 3D consistent datasets but struggle with out-of-distribution ge</span>
                
                <span class="abstract-full" style="display: none;">Single-image 3D scene reconstruction presents significant challenges due to its inherently ill-posed nature and limited input constraints. Recent advances have explored two promising directions: multiview generative models that train on 3D consistent datasets but struggle with out-of-distribution generalization, and 3D scene inpainting and completion frameworks that suffer from cross-view inconsistency and suboptimal error handling, as they depend exclusively on depth data or 3D smoothness, which ultimately degrades output quality and computational performance. Building upon these approaches, we present GaussVideoDreamer, which advances generative multimedia approaches by bridging the gap between image, video, and 3D generation, integrating their strengths through two key innovations: (1) A progressive video inpainting strategy that harnesses temporal coherence for improved multiview consistency and faster convergence. (2) A 3D Gaussian Splatting consistency mask to guide the video diffusion with 3D consistent multiview evidence. Our pipeline combines three core components: a geometry-aware initialization protocol, Inconsistency-Aware Gaussian Splatting, and a progressive video inpainting strategy. Experimental results demonstrate that our approach achieves 32% higher LLaVA-IQA scores and at least 2x speedup compared to existing methods while maintaining robust performance across diverse scenes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 13.2 -->
                    
                <!-- Medicine: 10.3 -->
                    
                <!-- 3D: 8.2 -->
                    
                <!-- RAG: 3.7 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- T2I: 2.1 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.2536
                </span>
                <a href="https://arxiv.org/abs/2503.12419" target="_blank" rel="noopener noreferrer">EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Luming Wang, Hao Shi, Xiaoting Yin, Kailun Yang, Kaiwei Wang, Jian Bai
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Egocentric gesture recognition is a pivotal technology for enhancing natural human-computer interaction, yet traditional RGB-based solutions suffer from motion blur and illumination variations in dynamic scenarios. While event cameras show distinct advantages in handling high dynamic range with ultr</span>
                
                <span class="abstract-full" style="display: none;">Egocentric gesture recognition is a pivotal technology for enhancing natural human-computer interaction, yet traditional RGB-based solutions suffer from motion blur and illumination variations in dynamic scenarios. While event cameras show distinct advantages in handling high dynamic range with ultra-low power consumption, existing RGB-based architectures face inherent limitations in processing asynchronous event streams due to their synchronous frame-based nature. Moreover, from an egocentric perspective, event cameras record data that includes events generated by both head movements and hand gestures, thereby increasing the complexity of gesture recognition. To address this, we propose a novel network architecture specifically designed for event data processing, incorporating (1) a lightweight CNN with asymmetric depthwise convolutions to reduce parameters while preserving spatiotemporal features, (2) a plug-and-play state-space model as context block that decouples head movement noise from gesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM) that shifts features along bins and temporal dimensions to fuse sparse events efficiently. We further establish the EgoEvGesture dataset, the first large-scale dataset for egocentric gesture recognition using event cameras. Experimental results demonstrate that our method achieves 62.7% accuracy tested on unseen subjects with only 7M parameters, 3.1% higher than state-of-the-art approaches. Notable misclassifications in freestyle motions stem from high inter-personal variability and unseen test patterns differing from training data. Moreover, our approach achieved a remarkable accuracy of 97.0% on the DVS128 Gesture, demonstrating the effectiveness and generalization capability of our method on public datasets. The dataset and models are made available at https://github.com/3190105222/EgoEv_Gesture.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.9 -->
                    
                <!-- LLMs: 8.1 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.384
                </span>
                <a href="https://arxiv.org/abs/2504.10014" target="_blank" rel="noopener noreferrer">Air Quality Prediction with A Meteorology-Guided Modality-Decoupled Spatio-Temporal Network</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hang Yin, Yan-Ming Zhang, Jian Xu, Jian-Long Chang, Yin Li, Cheng-Lin Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Air quality prediction plays a crucial role in public health and environmental protection. Accurate air quality prediction is a complex multivariate spatiotemporal problem, that involves interactions across temporal patterns, pollutant correlations, spatial station dependencies, and particularly met</span>
                
                <span class="abstract-full" style="display: none;">Air quality prediction plays a crucial role in public health and environmental protection. Accurate air quality prediction is a complex multivariate spatiotemporal problem, that involves interactions across temporal patterns, pollutant correlations, spatial station dependencies, and particularly meteorological influences that govern pollutant dispersion and chemical transformations. Existing works underestimate the critical role of atmospheric conditions in air quality prediction and neglect comprehensive meteorological data utilization, thereby impairing the modeling of dynamic interdependencies between air quality and meteorological data. To overcome this, we propose MDSTNet, an encoder-decoder framework that explicitly models air quality observations and atmospheric conditions as distinct modalities, integrating multi-pressure-level meteorological data and weather forecasts to capture atmosphere-pollution dependencies for prediction. Meantime, we construct ChinaAirNet, the first nationwide dataset combining air quality records with multi-pressure-level meteorological observations. Experimental results on ChinaAirNet demonstrate MDSTNet's superiority, substantially reducing 48-hour prediction errors by 17.54\% compared to the state-of-the-art model. The source code and dataset will be available on github.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.1 -->
                    
                <!-- LLMs: 12.9 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3917
                </span>
                <a href="https://arxiv.org/abs/2110.13694" target="_blank" rel="noopener noreferrer">A Fast Horizon Detector and a New Annotated Dataset for Maritime Video Processing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yassir Zardoua, Boulaala Mohammed, Mhamed El Mrabet, Astito Abdelali
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate and fast sea horizon detection is vital for tasks in autonomous navigation and maritime security, such as video stabilization, target region reduction, precise tracking, and obstacle avoidance. This paper introduces a novel sea horizon detector from RGB videos, focusing on rapid and effecti</span>
                
                <span class="abstract-full" style="display: none;">Accurate and fast sea horizon detection is vital for tasks in autonomous navigation and maritime security, such as video stabilization, target region reduction, precise tracking, and obstacle avoidance. This paper introduces a novel sea horizon detector from RGB videos, focusing on rapid and effective sea noise suppression while preserving weak horizon edges. Line fitting methods are subsequently employed on filtered edges for horizon detection. We address the filtering problem by extracting line segments with a very low edge threshold, ensuring the detection of line segments even in low-contrast horizon conditions. We show that horizon line segments have simple and relevant properties in RGB images, which we exploit to suppress noisy segments. Then we use the surviving segments to construct a filtered edge map and infer the horizon from the filtered edges. We propose a careful incorporation of temporal information for horizon inference and experimentally show its effectiveness. We address the computational constraint by providing a vectorized implementation for efficient CPU execution, and leveraging image downsizing with minimal loss of accuracy on the original size. Moreover, we contribute a public horizon line dataset to enrich existing data resources. Our algorithm's performance is rigorously evaluated against state-of-the-art methods, and its components are validated through ablation experiments. Source code and dataset files are available at: https://github.com/Zardoua-Yassir/A_fast_horizon_detector_and_a_new_annotated_dataset_for_maritime_video_processing</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.3 -->
                    
                <!-- LLMs: 6.0 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.4586
                </span>
                <a href="https://arxiv.org/abs/2504.09685" target="_blank" rel="noopener noreferrer">Can LLMs Revolutionize the Design of Explainable and Efficient TinyML Models?</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christophe El Zeinaty, Wassim Hamidouche, Glenn Herrou, Daniel Menard, Merouane Debbah
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces a novel framework for designing efficient neural network architectures specifically tailored to tiny machine learning (TinyML) platforms. By leveraging large language models (LLMs) for neural architecture search (NAS), a vision transformer (ViT)-based knowledge distillation (KD</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces a novel framework for designing efficient neural network architectures specifically tailored to tiny machine learning (TinyML) platforms. By leveraging large language models (LLMs) for neural architecture search (NAS), a vision transformer (ViT)-based knowledge distillation (KD) strategy, and an explainability module, the approach strikes an optimal balance between accuracy, computational efficiency, and memory usage. The LLM-guided search explores a hierarchical search space, refining candidate architectures through Pareto optimization based on accuracy, multiply-accumulate operations (MACs), and memory metrics. The best-performing architectures are further fine-tuned using logits-based KD with a pre-trained ViT-B/16 model, which enhances generalization without increasing model size. Evaluated on the CIFAR-100 dataset and deployed on an STM32H7 microcontroller (MCU), the three proposed models, LMaNet-Elite, LMaNet-Core, and QwNet-Core, achieve accuracy scores of 74.50%, 74.20% and 73.00%, respectively. All three models surpass current state-of-the-art (SOTA) models, such as MCUNet-in3/in4 (69.62% / 72.86%) and XiNet (72.27%), while maintaining a low computational cost of less than 100 million MACs and adhering to the stringent 320 KB static random-access memory (SRAM) constraint. These results demonstrate the efficiency and performance of the proposed framework for TinyML platforms, underscoring the potential of combining LLM-driven search, Pareto optimization, KD, and explainability to develop accurate, efficient, and interpretable models. This approach opens new possibilities in NAS, enabling the design of efficient architectures specifically suited for TinyML.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.3 -->
                    
                <!-- LLMs: 10.3 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.5281
                </span>
                <a href="https://arxiv.org/abs/2504.09149" target="_blank" rel="noopener noreferrer">MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Changhao Li, Yu Xin, Xiaowei Zhou, Ariel Shamir, Hao Zhang, Ligang Liu, Ruizhen Hu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view and parametrized representation of 3D shapes. Inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3D shapes, MASH represents a 3D shape as a collection of observable</span>
                
                <span class="abstract-full" style="display: none;">We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view and parametrized representation of 3D shapes. Inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3D shapes, MASH represents a 3D shape as a collection of observable local surface patches, each defined by a spherical distance function emanating from an anchor point. We further leverage the compactness of spherical harmonics to encode the MASH functions, combined with a generalized view cone with a parameterized base that masks the spatial extent of the spherical function to attain locality. We develop a differentiable optimization algorithm capable of converting any point cloud into a MASH representation accurately approximating ground-truth surfaces with arbitrary geometry and topology. Extensive experiments demonstrate that MASH is versatile for multiple applications including surface reconstruction, shape generation, completion, and blending, achieving superior performance thanks to its unique representation encompassing both implicit and explicit features.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.8 -->
                    
                <!-- LLMs: 5.5 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- Networks: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.5622
                </span>
                <a href="https://arxiv.org/abs/2412.16003" target="_blank" rel="noopener noreferrer">Choose Your Explanation: A Comparison of SHAP and GradCAM in Human Activity Recognition</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Felix Tempel, Daniel Groos, Espen Alexander F. Ihlen, Lars Adde, Inga Str\"umke
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Explaining machine learning (ML) models using eXplainable AI (XAI) techniques has become essential to make them more transparent and trustworthy. This is especially important in high-stakes domains like healthcare, where understanding model decisions is critical to ensure ethical, sound, and trustwo</span>
                
                <span class="abstract-full" style="display: none;">Explaining machine learning (ML) models using eXplainable AI (XAI) techniques has become essential to make them more transparent and trustworthy. This is especially important in high-stakes domains like healthcare, where understanding model decisions is critical to ensure ethical, sound, and trustworthy outcome predictions. However, users are often confused about which explanability method to choose for their specific use case. We present a comparative analysis of widely used explainability methods, Shapley Additive Explanations (SHAP) and Gradient-weighted Class Activation Mapping (Grad-CAM), within the domain of human activity recognition (HAR) utilizing graph convolutional networks (GCNs). By evaluating these methods on skeleton-based data from two real-world datasets, including a healthcare-critical cerebral palsy (CP) case, this study provides vital insights into both approaches' strengths, limitations, and differences, offering a roadmap for selecting the most appropriate explanation method based on specific models and applications. We quantitatively and quantitatively compare these methods, focusing on feature importance ranking, interpretability, and model sensitivity through perturbation experiments. While SHAP provides detailed input feature attribution, Grad-CAM delivers faster, spatially oriented explanations, making both methods complementary depending on the application's requirements. Given the importance of XAI in enhancing trust and transparency in ML models, particularly in sensitive environments like healthcare, our research demonstrates how SHAP and Grad-CAM could complement each other to provide more interpretable and actionable model explanations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 14.9 -->
                    
                <!-- Medicine: 11.4 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.6243
                </span>
                <a href="https://arxiv.org/abs/2503.18982" target="_blank" rel="noopener noreferrer">Generative Data Imputation for Sparse Learner Performance Data Using Generative Adversarial Imputation Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Liang Zhang, Jionghao Lin, John Sabatini, Diego Zapata-Rivera, Carol Forsyth, Yang Jiang, John Hollander, Xiangen Hu, Arthur C. Graesser
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Learner performance data collected by Intelligent Tutoring Systems (ITSs), such as responses to questions, is essential for modeling and predicting learners' knowledge states. However, missing responses due to skips or incomplete attempts create data sparsity, challenging accurate assessment and per</span>
                
                <span class="abstract-full" style="display: none;">Learner performance data collected by Intelligent Tutoring Systems (ITSs), such as responses to questions, is essential for modeling and predicting learners' knowledge states. However, missing responses due to skips or incomplete attempts create data sparsity, challenging accurate assessment and personalized instruction. To address this, we propose a generative imputation approach using Generative Adversarial Imputation Networks (GAIN). Our method features a three-dimensional (3D) framework (learners, questions, and attempts), flexibly accommodating various sparsity levels. Enhanced by convolutional neural networks and optimized with a least squares loss function, the GAIN-based method aligns input and output dimensions to question-attempt matrices along the learners' dimension. Extensive experiments using datasets from AutoTutor Adult Reading Comprehension (ARC), ASSISTments, and MATHia demonstrate that our approach significantly outperforms tensor factorization and alternative GAN methods in imputation accuracy across different attempt scenarios. Bayesian Knowledge Tracing (BKT) further validates the effectiveness of the imputed data by estimating learning parameters: initial knowledge (P(L0)), learning rate (P(T)), guess rate (P(G)), and slip rate (P(S)). Results indicate the imputed data enhances model fit and closely mirrors original distributions, capturing underlying learning behaviors reliably. Kullback-Leibler (KL) divergence assessments confirm minimal divergence, showing the imputed data preserves essential learning characteristics effectively. These findings underscore GAIN's capability as a robust imputation tool in ITSs, alleviating data sparsity and supporting adaptive, individualized instruction, ultimately leading to more precise and responsive learner assessments and improved educational outcomes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.5 -->
                    
                <!-- LLMs: 9.4 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.6336
                </span>
                <a href="https://arxiv.org/abs/2504.09156" target="_blank" rel="noopener noreferrer">LEREL: Lipschitz Continuity-Constrained Emotion Recognition Ensemble Learning For Electroencephalography</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shengyu Gong, Yueyang Li, Zijian Kang, Weiming Zeng, Hongjie Yan, Wai Ting Siok, Nizhuan Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate and efficient perception of emotional states in oneself and others is crucial, as emotion-related disorders are associated with severe psychosocial impairments. While electroencephalography (EEG) offers a powerful tool for emotion detection, current EEG-based emotion recognition (EER) metho</span>
                
                <span class="abstract-full" style="display: none;">Accurate and efficient perception of emotional states in oneself and others is crucial, as emotion-related disorders are associated with severe psychosocial impairments. While electroencephalography (EEG) offers a powerful tool for emotion detection, current EEG-based emotion recognition (EER) methods face key limitations: insufficient model stability, limited accuracy in processing high-dimensional nonlinear EEG signals, and poor robustness against intra-subject variability and signal noise. To address these challenges, we propose LEREL (Lipschitz continuity-constrained Emotion Recognition Ensemble Learning), a novel framework that significantly enhances both the accuracy and robustness of emotion recognition performance. The LEREL framework employs Lipschitz continuity constraints to enhance model stability and generalization in EEG emotion recognition, reducing signal variability and noise susceptibility while maintaining strong performance on small-sample datasets. The ensemble learning strategy reduces single-model bias and variance through multi-classifier decision fusion, further optimizing overall performance. Experimental results on three public benchmark datasets (EAV, FACED and SEED) demonstrate LEREL's effectiveness, achieving average recognition accuracies of 76.43%, 83.00% and 89.22%, respectively.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.1 -->
                    
                <!-- LLMs: 12.4 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- RAG: 2.8 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.6352
                </span>
                <a href="https://arxiv.org/abs/2504.10065" target="_blank" rel="noopener noreferrer">A Computational Cognitive Model for Processing Repetitions of Hierarchical Relations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zeng Ren, Xinyi Guan, Martin Rohrmeier
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Patterns are fundamental to human cognition, enabling the recognition of structure and regularity across diverse domains. In this work, we focus on structural repeats, patterns that arise from the repetition of hierarchical relations within sequential data, and develop a candidate computational mode</span>
                
                <span class="abstract-full" style="display: none;">Patterns are fundamental to human cognition, enabling the recognition of structure and regularity across diverse domains. In this work, we focus on structural repeats, patterns that arise from the repetition of hierarchical relations within sequential data, and develop a candidate computational model of how humans detect and understand such structural repeats. Based on a weighted deduction system, our model infers the minimal generative process of a given sequence in the form of a Template program, a formalism that enriches the context-free grammar with repetition combinators. Such representation efficiently encodes the repetition of sub-computations in a recursive manner. As a proof of concept, we demonstrate the expressiveness of our model on short sequences from music and action planning. The proposed model offers broader insights into the mental representations and cognitive mechanisms underlying human pattern recognition.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.1 -->
                    
                <!-- LLMs: 9.2 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.6672
                </span>
                <a href="https://arxiv.org/abs/2405.01614" target="_blank" rel="noopener noreferrer">RULSurv: A probabilistic survival-based method for early censoring-aware prediction of remaining useful life in ball bearings</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christian Marius Lillelund, Fernando Pannullo, Morten Opprud Jakobsen, Manuel Morante, Christian Fischer Pedersen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Predicting the remaining useful life (RUL) of ball bearings is an active area of research, where novel machine learning techniques are continuously being applied to predict degradation trends and anticipate failures before they occur. However, few studies have explicitly addressed the challenge of h</span>
                
                <span class="abstract-full" style="display: none;">Predicting the remaining useful life (RUL) of ball bearings is an active area of research, where novel machine learning techniques are continuously being applied to predict degradation trends and anticipate failures before they occur. However, few studies have explicitly addressed the challenge of handling censored data, where information about a specific event (\eg mechanical failure) is incomplete or only partially observed. To address this issue, we introduce a novel and flexible method for early fault detection using Kullback-Leibler (KL) divergence and RUL estimation using survival analysis that naturally supports censored data. We demonstrate our approach in the XJTU-SY dataset using a 5-fold cross-validation strategy across three different operating conditions. When predicting the time to failure for bearings under the highest load (C1, 12.0 kN and 2100 RPM) with 25% random censoring, our approach achieves a mean absolute error (MAE) of 14.7 minutes (95% CI = 13.6-15.8) using a linear CoxPH model, and an MAE of 12.6 minutes (95% CI = 11.8-13.4) using a nonlinear Random Survival Forests model, compared to an MAE of 18.5 minutes (95% CI = 17.4-19.6) using a linear LASSO model that does not support censoring. Moreover, our approach achieves a mean cumulative relative accuracy (CRA) of 0.7586 over 5 bearings under the highest load, which improves over several state-of-the-art baselines. Our work highlights the importance of considering censored data as part of the model design when building predictive models for early fault detection and RUL estimation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.2 -->
                    
                <!-- LLMs: 7.9 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.7345
                </span>
                <a href="https://arxiv.org/abs/2504.10350" target="_blank" rel="noopener noreferrer">Benchmarking 3D Human Pose Estimation Models Under Occlusions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Filipa Lino, Carlos Santiago, Manuel Marques
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper addresses critical challenges in 3D Human Pose Estimation (HPE) by analyzing the robustness and sensitivity of existing models to occlusions, camera position, and action variability. Using a novel synthetic dataset, BlendMimic3D, which includes diverse scenarios with multi-camera setups a</span>
                
                <span class="abstract-full" style="display: none;">This paper addresses critical challenges in 3D Human Pose Estimation (HPE) by analyzing the robustness and sensitivity of existing models to occlusions, camera position, and action variability. Using a novel synthetic dataset, BlendMimic3D, which includes diverse scenarios with multi-camera setups and several occlusion types, we conduct specific tests on several state-of-the-art models. Our study focuses on the discrepancy in keypoint formats between common datasets such as Human3.6M, and 2D datasets such as COCO, commonly used for 2D detection models and frequently input of 3D HPE models. Our work explores the impact of occlusions on model performance and the generality of models trained exclusively under standard conditions. The findings suggest significant sensitivity to occlusions and camera settings, revealing a need for models that better adapt to real-world variability and occlusion scenarios. This research contributed to ongoing efforts to improve the fidelity and applicability of 3D HPE systems in complex environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 15.6 -->
                    
                <!-- Medicine: 10.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8289
                </span>
                <a href="https://arxiv.org/abs/2504.08952" target="_blank" rel="noopener noreferrer">RiskRAG: A Data-Driven Solution for Improved AI Model Risk Reporting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pooja S. B. Rao, Sanja \v{S}\'cepanovi\'c, Ke Zhou, Edyta Paulina Bogucka, Daniele Quercia
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Risk reporting is essential for documenting AI models, yet only 14% of model cards mention risks, out of which 96% copying content from a small set of cards, leading to a lack of actionable insights. Existing proposals for improving model cards do not resolve these issues. To address this, we introd</span>
                
                <span class="abstract-full" style="display: none;">Risk reporting is essential for documenting AI models, yet only 14% of model cards mention risks, out of which 96% copying content from a small set of cards, leading to a lack of actionable insights. Existing proposals for improving model cards do not resolve these issues. To address this, we introduce RiskRAG, a Retrieval Augmented Generation based risk reporting solution guided by five design requirements we identified from literature, and co-design with 16 developers: identifying diverse model-specific risks, clearly presenting and prioritizing them, contextualizing for real-world uses, and offering actionable mitigation strategies. Drawing from 450K model cards and 600 real-world incidents, RiskRAG pre-populates contextualized risk reports. A preliminary study with 50 developers showed that they preferred RiskRAG over standard model cards, as it better met all the design requirements. A final study with 38 developers, 40 designers, and 37 media professionals showed that RiskRAG improved their way of selecting the AI model for a specific application, encouraging a more careful and deliberative decision-making. The RiskRAG project page is accessible at: https://social-dynamics.net/ai-risks/card.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.6 -->
                    
                <!-- LLMs: 9.7 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.846
                </span>
                <a href="https://arxiv.org/abs/2311.16780" target="_blank" rel="noopener noreferrer">Enhancing autonomousnvehicle acceptance with age and education sensitive simulation interventions: An experimental trial</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Celina Kacperski, Roberto Ulloa, Jeremy Wautelet, Tobias Vogel, Florian Kutzner
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The familiarity principle posits that acceptance increases with exposure, which has previously been shown with in vivo and simulated experiences with connected and autonomous vehicles (CAVs). We investigate the impact of a simulated video-based first-person drive on CAV acceptance, as well as the im</span>
                
                <span class="abstract-full" style="display: none;">The familiarity principle posits that acceptance increases with exposure, which has previously been shown with in vivo and simulated experiences with connected and autonomous vehicles (CAVs). We investigate the impact of a simulated video-based first-person drive on CAV acceptance, as well as the impact of information customization, with a particular focus on acceptance by older individuals and those with lower education. Findings from an online experiment with N=799 German residents reveal that the simulated experience improved acceptance across response variables such as intention to use and ease of use, particularly among older individuals. However, the opportunity to customize navigation information decreased acceptance of older individuals and those with university degrees and increased acceptance for younger individuals and those with lower educational levels.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.4 -->
                    
                <!-- LLMs: 9.6 -->
                    
                <!-- Quantum Computing: 5.2 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8505
                </span>
                <a href="https://arxiv.org/abs/2504.08854" target="_blank" rel="noopener noreferrer">Hardware Design and Security Needs Attention: From Survey to Path Forward</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sujan Ghimire, Muhtasim Alam Chowdhury, Banafsheh Saber Latibari, Muntasir Mamun, Jaeden Wolf Carpenter, Benjamin Tan, Hammond Pearce, Pratik Satam, Soheil Salehi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advances in attention-based artificial intelligence (AI) models have unlocked vast potential to automate digital hardware design while enhancing and strengthening security measures against various threats. This rapidly emerging field leverages Large Language Models (LLMs) to generate HDL code</span>
                
                <span class="abstract-full" style="display: none;">Recent advances in attention-based artificial intelligence (AI) models have unlocked vast potential to automate digital hardware design while enhancing and strengthening security measures against various threats. This rapidly emerging field leverages Large Language Models (LLMs) to generate HDL code, identify vulnerabilities, and sometimes mitigate them. The state of the art in this design automation space utilizes optimized LLMs with HDL datasets, creating automated systems for register-transfer level (RTL) generation, verification, and debugging, and establishing LLM-driven design environments for streamlined logic designs. Additionally, attention-based models like graph attention have shown promise in chip design applications, including floorplanning. This survey investigates the integration of these models into hardware-related domains, emphasizing logic design and hardware security, with or without the use of IP libraries. This study explores the commercial and academic landscape, highlighting technical hurdles and future prospects for automating hardware design and security. Moreover, it provides new insights into the study of LLM-driven design systems, advances in hardware security mechanisms, and the impact of influential works on industry practices. Through the examination of 30 representative approaches and illustrative case studies, this paper underscores the transformative potential of attention-based models in revolutionizing hardware design while addressing the challenges that lie ahead in this interdisciplinary domain.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 24.8 -->
                    
                <!-- Medicine: 10.7 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8798
                </span>
                <a href="https://arxiv.org/abs/2504.01243" target="_blank" rel="noopener noreferrer">FUSION: Frequency-guided Underwater Spatial Image recOnstructioN</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jaskaran Singh Walia, Shravan Venkatraman, Pavithra LK
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Underwater images suffer from severe degradations, including color distortions, reduced visibility, and loss of structural details due to wavelength-dependent attenuation and scattering. Existing enhancement methods primarily focus on spatial-domain processing, neglecting the frequency domain's pote</span>
                
                <span class="abstract-full" style="display: none;">Underwater images suffer from severe degradations, including color distortions, reduced visibility, and loss of structural details due to wavelength-dependent attenuation and scattering. Existing enhancement methods primarily focus on spatial-domain processing, neglecting the frequency domain's potential to capture global color distributions and long-range dependencies. To address these limitations, we propose FUSION, a dual-domain deep learning framework that jointly leverages spatial and frequency domain information. FUSION independently processes each RGB channel through multi-scale convolutional kernels and adaptive attention mechanisms in the spatial domain, while simultaneously extracting global structural information via FFT-based frequency attention. A Frequency Guided Fusion module integrates complementary features from both domains, followed by inter-channel fusion and adaptive channel recalibration to ensure balanced color distributions. Extensive experiments on benchmark datasets (UIEB, EUVP, SUIM-E) demonstrate that FUSION achieves state-of-the-art performance, consistently outperforming existing methods in reconstruction fidelity (highest PSNR of 23.717 dB and SSIM of 0.883 on UIEB), perceptual quality (lowest LPIPS of 0.112 on UIEB), and visual enhancement metrics (best UIQM of 3.414 on UIEB), while requiring significantly fewer parameters (0.28M) and lower computational complexity, demonstrating its suitability for real-time underwater imaging applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.1 -->
                    
                <!-- LLMs: 12.3 -->
                    
                <!-- Quantum Computing: 4.4 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8959
                </span>
                <a href="https://arxiv.org/abs/2504.09674" target="_blank" rel="noopener noreferrer">On Stochastic Performance Analysis of Secure Integrated Sensing and Communication Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Marziyeh Soltani, Mahtab Mirmohseni, Rahim Tafazolli
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper analyzes the stochastic security performance of a multiple-input multiple-output (MIMO) integrated sensing and communication (ISAC) system in a downlink scenario. A base station (BS) transmits a multi-functional signal to simultaneously communicate with a user, sense a target angular loca</span>
                
                <span class="abstract-full" style="display: none;">This paper analyzes the stochastic security performance of a multiple-input multiple-output (MIMO) integrated sensing and communication (ISAC) system in a downlink scenario. A base station (BS) transmits a multi-functional signal to simultaneously communicate with a user, sense a target angular location, and counteract eavesdropping threats. The system includes a passive single-antenna communication eavesdropper and a multi-antenna sensing eavesdropper attempting to infer the target location. The BS-user and BS-eavesdroppers channels follow Rayleigh fading, while the target azimuth angle is uniformly distributed. To evaluate the performance, we derive exact expressions for the secrecy ergodic rate and the ergodic Cramer-Rao lower bound (CRB) for target localization at both the BS and the sensing eavesdropper. This involves computing the probability density functions (PDFs) of the signal-to-noise ratio (SNR) and CRB, leveraging the central limit theorem for tractability. Numerical results validate our findings.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.0 -->
                    
                <!-- Networks: 3.6 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.9411
                </span>
                <a href="https://arxiv.org/abs/2504.09203" target="_blank" rel="noopener noreferrer">AerOSeg: Harnessing SAM for Open-Vocabulary Segmentation in Remote Sensing Images</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Saikat Dutta, Akhil Vasim, Siddhant Gole, Hamid Rezatofighi, Biplab Banerjee
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Image segmentation beyond predefined categories is a key challenge in remote sensing, where novel and unseen classes often emerge during inference. Open-vocabulary image Segmentation addresses these generalization issues in traditional supervised segmentation models while reducing reliance on extens</span>
                
                <span class="abstract-full" style="display: none;">Image segmentation beyond predefined categories is a key challenge in remote sensing, where novel and unseen classes often emerge during inference. Open-vocabulary image Segmentation addresses these generalization issues in traditional supervised segmentation models while reducing reliance on extensive per-pixel annotations, which are both expensive and labor-intensive to obtain. Most Open-Vocabulary Segmentation (OVS) methods are designed for natural images but struggle with remote sensing data due to scale variations, orientation changes, and complex scene compositions. This necessitates the development of OVS approaches specifically tailored for remote sensing. In this context, we propose AerOSeg, a novel OVS approach for remote sensing data. First, we compute robust image-text correlation features using multiple rotated versions of the input image and domain-specific prompts. These features are then refined through spatial and class refinement blocks. Inspired by the success of the Segment Anything Model (SAM) in diverse domains, we leverage SAM features to guide the spatial refinement of correlation features. Additionally, we introduce a semantic back-projection module and loss to ensure the seamless propagation of SAM's semantic information throughout the segmentation pipeline. Finally, we enhance the refined correlation features using a multi-scale attention-aware decoder to produce the final segmentation map. We validate our SAM-guided Open-Vocabulary Remote Sensing Segmentation model on three benchmark remote sensing datasets: iSAID, DLRSD, and OpenEarthMap. Our model outperforms state-of-the-art open-vocabulary segmentation methods, achieving an average improvement of 2.54 h-mIoU.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.1 -->
                    
                <!-- LLMs: 8.5 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.9554
                </span>
                <a href="https://arxiv.org/abs/2504.09170" target="_blank" rel="noopener noreferrer">Langformers: Unified NLP Pipelines for Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Transformer-based language models have revolutionized the field of natural language processing (NLP). However, using these models often involves navigating multiple frameworks and tools, as well as writing repetitive boilerplate code. This complexity can discourage non-programmers and beginners, and</span>
                
                <span class="abstract-full" style="display: none;">Transformer-based language models have revolutionized the field of natural language processing (NLP). However, using these models often involves navigating multiple frameworks and tools, as well as writing repetitive boilerplate code. This complexity can discourage non-programmers and beginners, and even slow down prototyping for experienced developers. To address these challenges, we introduce Langformers, an open-source Python library designed to streamline NLP pipelines through a unified, factory-based interface for large language model (LLM) and masked language model (MLM) tasks. Langformers integrates conversational AI, MLM pretraining, text classification, sentence embedding/reranking, data labelling, semantic search, and knowledge distillation into a cohesive API, supporting popular platforms such as Hugging Face and Ollama. Key innovations include: (1) task-specific factories that abstract training, inference, and deployment complexities; (2) built-in memory and streaming for conversational agents; and (3) lightweight, modular design that prioritizes ease of use. Documentation: https://langformers.com</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 23.1 -->
                    
                <!-- Medicine: 11.1 -->
                    
                <!-- RAG: 3.6 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Networks: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.0084
                </span>
                <a href="https://arxiv.org/abs/2408.14358" target="_blank" rel="noopener noreferrer">An Embedding is Worth a Thousand Noisy Labels</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Francesco Di Salvo, Sebastian Doerrich, Ines Rieger, Christian Ledig
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The performance of deep neural networks scales with dataset size and label quality, rendering the efficient mitigation of low-quality data annotations crucial for building robust and cost-effective systems. Existing strategies to address label noise exhibit severe limitations due to computational co</span>
                
                <span class="abstract-full" style="display: none;">The performance of deep neural networks scales with dataset size and label quality, rendering the efficient mitigation of low-quality data annotations crucial for building robust and cost-effective systems. Existing strategies to address label noise exhibit severe limitations due to computational complexity and application dependency. In this work, we propose WANN, a Weighted Adaptive Nearest Neighbor approach that builds on self-supervised feature representations obtained from foundation models. To guide the weighted voting scheme, we introduce a reliability score $\eta$, which measures the likelihood of a data label being correct. WANN outperforms reference methods, including a linear layer trained with robust loss functions, on diverse datasets of varying size and under various noise types and severities. WANN also exhibits superior generalization on imbalanced data compared to both Adaptive-NNs (ANN) and fixed k-NNs. Furthermore, the proposed weighting scheme enhances supervised dimensionality reduction under noisy labels. This yields a significant boost in classification performance with 10x and 100x smaller image embeddings, minimizing latency and storage requirements. Our approach, emphasizing efficiency and explainability, emerges as a simple, robust solution to overcome inherent limitations of deep neural network training. The code is available at https://github.com/francescodisalvo05/wann-noisy-labels .</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.7 -->
                    
                <!-- LLMs: 6.9 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.2485
                </span>
                <a href="https://arxiv.org/abs/2504.08764" target="_blank" rel="noopener noreferrer">Evaluation of the phi-3-mini SLM for identification of texts related to medicine, health, and sports injuries</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chris Brogly, Saif Rjaibi, Charlotte Liang, Erica Lam, Edward Wang, Adam Levitan, Sarah Paleczny, Michael Cusimano
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Small Language Models (SLMs) have potential to be used for automatically labelling and identifying aspects of text data for medicine/health-related purposes from documents and the web. As their resource requirements are significantly lower than Large Language Models (LLMs), these can be deployed pot</span>
                
                <span class="abstract-full" style="display: none;">Small Language Models (SLMs) have potential to be used for automatically labelling and identifying aspects of text data for medicine/health-related purposes from documents and the web. As their resource requirements are significantly lower than Large Language Models (LLMs), these can be deployed potentially on more types of devices. SLMs often are benchmarked on health/medicine-related tasks, such as MedQA, although performance on these can vary especially depending on the size of the model in terms of number of parameters. Furthermore, these test results may not necessarily reflect real-world performance regarding the automatic labelling or identification of texts in documents and the web. As a result, we compared topic-relatedness scores from Microsofts phi-3-mini-4k-instruct SLM to the topic-relatedness scores from 7 human evaluators on 1144 samples of medical/health-related texts and 1117 samples of sports injury-related texts. These texts were from a larger dataset of about 9 million news headlines, each of which were processed and assigned scores by phi-3-mini-4k-instruct. Our sample was selected (filtered) based on 1 (low filtering) or more (high filtering) Boolean conditions on the phi-3 SLM scores. We found low-moderate significant correlations between the scores from the SLM and human evaluators for sports injury texts with low filtering (\r{ho} = 0.3413, p < 0.001) and medicine/health texts with high filtering (\r{ho} = 0.3854, p < 0.001), and low significant correlation for medicine/health texts with low filtering (\r{ho} = 0.2255, p < 0.001). There was negligible, insignificant correlation for sports injury-related texts with high filtering (\r{ho} = 0.0318, p = 0.4466).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 13.0 -->
                    
                <!-- Medicine: 12.3 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.2573
                </span>
                <a href="https://arxiv.org/abs/2504.08777" target="_blank" rel="noopener noreferrer">The Lyme Disease Controversy: An AI-Driven Discourse Analysis of a Quarter Century of Academic Debate and Divides</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Teo Susnjak, Cole Palffy, Tatiana Zimina, Nazgul Altynbekova, Kunal Garg, Leona Gilbert
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The scientific discourse surrounding Chronic Lyme Disease (CLD) and Post-Treatment Lyme Disease Syndrome (PTLDS) has evolved over the past twenty-five years into a complex and polarised debate, shaped by shifting research priorities, institutional influences, and competing explanatory models. This s</span>
                
                <span class="abstract-full" style="display: none;">The scientific discourse surrounding Chronic Lyme Disease (CLD) and Post-Treatment Lyme Disease Syndrome (PTLDS) has evolved over the past twenty-five years into a complex and polarised debate, shaped by shifting research priorities, institutional influences, and competing explanatory models. This study presents the first large-scale, systematic examination of this discourse using an innovative hybrid AI-driven methodology, combining large language models with structured human validation to analyse thousands of scholarly abstracts spanning 25 years. By integrating Large Language Models (LLMs) with expert oversight, we developed a quantitative framework for tracking epistemic shifts in contested medical fields, with applications to other content analysis domains. Our analysis revealed a progressive transition from infection-based models of Lyme disease to immune-mediated explanations for persistent symptoms. This study offers new empirical insights into the structural and epistemic forces shaping Lyme disease research, providing a scalable and replicable methodology for analysing discourse, while underscoring the value of AI-assisted methodologies in social science and medical research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 19.5 -->
                    
                <!-- Medicine: 13.5 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.5052
                </span>
                <a href="https://arxiv.org/abs/2504.10165" target="_blank" rel="noopener noreferrer">WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nguyen Ngoc Dat, Tom Richardson, Matthew Watson, Kilian Meier, Jenna Kline, Sid Reid, Guy Maalouf, Duncan Hine, Majid Mirmehdi, Tilo Burghardt
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Live tracking of wildlife via high-resolution video processing directly onboard drones is widely unexplored and most existing solutions rely on streaming video to ground stations to support navigation. Yet, both autonomous animal-reactive flight control beyond visual line of sight and/or mission-spe</span>
                
                <span class="abstract-full" style="display: none;">Live tracking of wildlife via high-resolution video processing directly onboard drones is widely unexplored and most existing solutions rely on streaming video to ground stations to support navigation. Yet, both autonomous animal-reactive flight control beyond visual line of sight and/or mission-specific individual and behaviour recognition tasks rely to some degree on this capability. In response, we introduce WildLive -- a near real-time animal detection and tracking framework for high-resolution imagery running directly onboard uncrewed aerial vehicles (UAVs). The system performs multi-animal detection and tracking at 17fps+ for HD and 7fps+ on 4K video streams suitable for operation during higher altitude flights to minimise animal disturbance. Our system is optimised for Jetson Orin AGX onboard hardware. It integrates the efficiency of sparse optical flow tracking and mission-specific sampling with device-optimised and proven YOLO-driven object detection and segmentation techniques. Essentially, computational resource is focused onto spatio-temporal regions of high uncertainty to significantly improve UAV processing speeds without domain-specific loss of accuracy. Alongside, we introduce our WildLive dataset, which comprises 200k+ annotated animal instances across 19k+ frames from 4K UAV videos collected at the Ol Pejeta Conservancy in Kenya. All frames contain ground truth bounding boxes, segmentation masks, as well as individual tracklets and tracking point trajectories. We compare our system against current object tracking approaches including OC-SORT, ByteTrack, and SORT. Our multi-animal tracking experiments with onboard hardware confirm that near real-time high-resolution wildlife tracking is possible on UAVs whilst maintaining high accuracy levels as needed for future navigational and mission-specific animal-centric operational autonomy.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.7 -->
                    
                <!-- LLMs: 10.6 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.5511
                </span>
                <a href="https://arxiv.org/abs/2501.06019" target="_blank" rel="noopener noreferrer">BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, Weihao Xuan, Junjue Wang, Xinlei Shao, Yimin Wei, Junshi Xia, Cuiling Lan, Konrad Schindler, Naoto Yokoya
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Disaster events occur around the world and cause significant damage to human life and property. Earth observation (EO) data enables rapid and comprehensive building damage assessment (BDA), an essential capability in the aftermath of a disaster to reduce human casualties and to inform disaster relie</span>
                
                <span class="abstract-full" style="display: none;">Disaster events occur around the world and cause significant damage to human life and property. Earth observation (EO) data enables rapid and comprehensive building damage assessment (BDA), an essential capability in the aftermath of a disaster to reduce human casualties and to inform disaster relief efforts. Recent research focuses on the development of AI models to achieve accurate mapping of unseen disaster events, mostly using optical EO data. However, solutions based on optical data are limited to clear skies and daylight hours, preventing a prompt response to disasters. Integrating multimodal (MM) EO data, particularly the combination of optical and SAR imagery, makes it possible to provide all-weather, day-and-night disaster responses. Despite this potential, the development of robust multimodal AI models has been constrained by the lack of suitable benchmark datasets. In this paper, we present a BDA dataset using veRy-hIGH-resoluTion optical and SAR imagery (BRIGHT) to support AI-based all-weather disaster response. To the best of our knowledge, BRIGHT is the first open-access, globally distributed, event-diverse MM dataset specifically curated to support AI-based disaster response. It covers five types of natural disasters and two types of man-made disasters across 14 regions worldwide, with a particular focus on developing countries where external assistance is most needed. The optical and SAR imagery in BRIGHT, with a spatial resolution between 0.3-1 meters, provides detailed representations of individual buildings, making it ideal for precise BDA. In our experiments, we have tested seven advanced AI models trained with our BRIGHT to validate the transferability and robustness. The dataset and code are available at https://github.com/ChenHongruixuan/BRIGHT. BRIGHT also serves as the official dataset for the 2025 IEEE GRSS Data Fusion Contest.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.3 -->
                    
                <!-- LLMs: 11.5 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.5705
                </span>
                <a href="https://arxiv.org/abs/2409.17091" target="_blank" rel="noopener noreferrer">Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence Classification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xinrui Zhou, Yuhao Huang, Haoran Dou, Shijing Chen, Ao Chang, Jia Liu, Weiran Long, Jian Zheng, Erjiao Xu, Jie Ren, Ruobing Huang, Jun Cheng, Wufeng Xue, Dong Ni
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In the medical field, the limited availability of large-scale datasets and labor-intensive annotation processes hinder the performance of deep models. Diffusion-based generative augmentation approaches present a promising solution to this issue, having been proven effective in advancing downstream m</span>
                
                <span class="abstract-full" style="display: none;">In the medical field, the limited availability of large-scale datasets and labor-intensive annotation processes hinder the performance of deep models. Diffusion-based generative augmentation approaches present a promising solution to this issue, having been proven effective in advancing downstream medical recognition tasks. Nevertheless, existing works lack sufficient semantic and sequential steerability for challenging video/3D sequence generation, and neglect quality control of noisy synthesized samples, resulting in unreliable synthetic databases and severely limiting the performance of downstream tasks. In this work, we present Ctrl-GenAug, a novel and general generative augmentation framework that enables highly semantic- and sequential-customized sequence synthesis and suppresses incorrectly synthesized samples, to aid medical sequence classification. Specifically, we first design a multimodal conditions-guided sequence generator for controllably synthesizing diagnosis-promotive samples. A sequential augmentation module is integrated to enhance the temporal/stereoscopic coherence of generated samples. Then, we propose a noisy synthetic data filter to suppress unreliable cases at semantic and sequential levels. Extensive experiments on 3 medical datasets, using 11 networks trained on 3 paradigms, comprehensively analyze the effectiveness and generality of Ctrl-GenAug, particularly in underrepresented high-risk populations and out-domain conditions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.2 -->
                    
                <!-- LLMs: 9.9 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.612
                </span>
                <a href="https://arxiv.org/abs/2504.09249" target="_blank" rel="noopener noreferrer">NoTeS-Bank: Benchmarking Neural Transcription and Search for Scientific Notes Understanding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aniket Pal, Sanket Biswas, Alloy Das, Ayush Lodh, Priyanka Banerjee, Soumitri Chattopadhyay, Dimosthenis Karatzas, Josep Llados, C. V. Jawahar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Understanding and reasoning over academic handwritten notes remains a challenge in document AI, particularly for mathematical equations, diagrams, and scientific notations. Existing visual question answering (VQA) benchmarks focus on printed or structured handwritten text, limiting generalization to</span>
                
                <span class="abstract-full" style="display: none;">Understanding and reasoning over academic handwritten notes remains a challenge in document AI, particularly for mathematical equations, diagrams, and scientific notations. Existing visual question answering (VQA) benchmarks focus on printed or structured handwritten text, limiting generalization to real-world note-taking. To address this, we introduce NoTeS-Bank, an evaluation benchmark for Neural Transcription and Search in note-based question answering. NoTeS-Bank comprises complex notes across multiple domains, requiring models to process unstructured and multimodal content. The benchmark defines two tasks: (1) Evidence-Based VQA, where models retrieve localized answers with bounding-box evidence, and (2) Open-Domain VQA, where models classify the domain before retrieving relevant documents and answers. Unlike classical Document VQA datasets relying on optical character recognition (OCR) and structured data, NoTeS-BANK demands vision-language fusion, retrieval, and multimodal reasoning. We benchmark state-of-the-art Vision-Language Models (VLMs) and retrieval frameworks, exposing structured transcription and reasoning limitations. NoTeS-Bank provides a rigorous evaluation with NDCG@5, MRR, Recall@K, IoU, and ANLS, establishing a new standard for visual document understanding and reasoning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.9 -->
                    
                <!-- LLMs: 14.1 -->
                    
                <!-- RAG: 3.7 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- T2I: 1.8 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.7304
                </span>
                <a href="https://arxiv.org/abs/2410.09880" target="_blank" rel="noopener noreferrer">Improving Colorectal Cancer Screening and Risk Assessment through Predictive Modeling on Medical Images and Records</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shuai Jiang, Christina Robinson, Joseph Anderson, William Hisey, Lynn Butterly, Arief Suriawinata, Saeed Hassanpour
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Colonoscopy screening effectively identifies and removes polyps before they progress to colorectal cancer (CRC), but current follow-up guidelines rely primarily on histopathological features, overlooking other important CRC risk factors. Variability in polyp characterization among pathologists also </span>
                
                <span class="abstract-full" style="display: none;">Colonoscopy screening effectively identifies and removes polyps before they progress to colorectal cancer (CRC), but current follow-up guidelines rely primarily on histopathological features, overlooking other important CRC risk factors. Variability in polyp characterization among pathologists also hinders consistent surveillance decisions. Advances in digital pathology and deep learning enable the integration of pathology slides and medical records for more accurate CRC risk prediction. Using data from the New Hampshire Colonoscopy Registry, including longitudinal follow-up, we adapted a transformer-based model for histopathology image analysis to predict 5-year CRC risk. We further explored multi-modal fusion strategies to combine clinical records with deep learning-derived image features. Training the model to predict intermediate clinical variables improved 5-year CRC risk prediction (AUC = 0.630) compared to direct prediction (AUC = 0.615, p = 0.013). Incorporating both imaging and non-imaging data, without requiring manual slide review, further improved performance (AUC = 0.674) compared to traditional features from colonoscopy and microscopy reports (AUC = 0.655, p = 0.001). These results highlight the value of integrating diverse data modalities with computational methods to enhance CRC risk stratification.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 18.9 -->
                    
                <!-- LLMs: 9.8 -->
                    
                <!-- Quantum Computing: 4.7 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- SpikingNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.8129
                </span>
                <a href="https://arxiv.org/abs/2504.09016" target="_blank" rel="noopener noreferrer">VIBES: Exploring Viewer Spatial Interactions as Direct Input for Livestreamed Content</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Michael Yin, Robert Xiao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Livestreaming has rapidly become a popular online pastime, with real-time interaction between streamer and viewer being a key motivating feature. However, viewers have traditionally had limited opportunity to directly influence the streamed content; even when such interactions are possible, it has b</span>
                
                <span class="abstract-full" style="display: none;">Livestreaming has rapidly become a popular online pastime, with real-time interaction between streamer and viewer being a key motivating feature. However, viewers have traditionally had limited opportunity to directly influence the streamed content; even when such interactions are possible, it has been reliant on text-based chat. We investigate the potential of spatial interaction on the livestreamed video content as a form of direct, real-time input for livestreamed applications. We developed VIBES, a flexible digital system that registers viewers' mouse interactions on the streamed video, i.e., clicks or movements, and transmits it directly into the streamed application. We used VIBES as a technology probe; first designing possible demonstrative interactions and using these interactions to explore streamers' perception of viewer influence and possible challenges and opportunities. We then deployed applications built using VIBES in two livestreams to explore its effects on audience engagement and investigate their relationships with the stream, the streamer, and fellow audience members. The use of spatial interactions enhances engagement and participation and opens up new avenues for both streamer-viewer and viewer-viewer participation. We contextualize our findings around a broader understanding of motivations and engagement in livestreaming, and we propose design guidelines and extensions for future research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.8 -->
                    
                <!-- LLMs: 7.1 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.0081
                </span>
                <a href="https://arxiv.org/abs/2504.09328" target="_blank" rel="noopener noreferrer">Text To 3D Object Generation For Scalable Room Assembly</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sonia Laguna, Alberto Garcia-Garcia, Marie-Julie Rakotosaona, Stylianos Moschoglou, Leonhard Helminger, Sergio Orts-Escolano
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modern machine learning models for scene understanding, such as depth estimation and object tracking, rely on large, high-quality datasets that mimic real-world deployment scenarios. To address data scarcity, we propose an end-to-end system for synthetic data generation for scalable, high-quality, a</span>
                
                <span class="abstract-full" style="display: none;">Modern machine learning models for scene understanding, such as depth estimation and object tracking, rely on large, high-quality datasets that mimic real-world deployment scenarios. To address data scarcity, we propose an end-to-end system for synthetic data generation for scalable, high-quality, and customizable 3D indoor scenes. By integrating and adapting text-to-image and multi-view diffusion models with Neural Radiance Field-based meshing, this system generates highfidelity 3D object assets from text prompts and incorporates them into pre-defined floor plans using a rendering tool. By introducing novel loss functions and training strategies into existing methods, the system supports on-demand scene generation, aiming to alleviate the scarcity of current available data, generally manually crafted by artists. This system advances the role of synthetic data in addressing machine learning training limitations, enabling more robust and generalizable models for real-world applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 21.7 -->
                    
                <!-- LLMs: 9.6 -->
                    
                <!-- Quantum Computing: 4.2 -->
                    
                <!-- RAG: 2.4 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.4599
                </span>
                <a href="https://arxiv.org/abs/2504.09899" target="_blank" rel="noopener noreferrer">Digital Staining with Knowledge Distillation: A Unified Framework for Unpaired and Paired-But-Misaligned Data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ziwang Xu, Lanqing Guo, Satoshi Tsutsui, Shuyan Zhang, Alex C. Kot, Bihan Wen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Staining is essential in cell imaging and medical diagnostics but poses significant challenges, including high cost, time consumption, labor intensity, and irreversible tissue alterations. Recent advances in deep learning have enabled digital staining through supervised model training. However, coll</span>
                
                <span class="abstract-full" style="display: none;">Staining is essential in cell imaging and medical diagnostics but poses significant challenges, including high cost, time consumption, labor intensity, and irreversible tissue alterations. Recent advances in deep learning have enabled digital staining through supervised model training. However, collecting large-scale, perfectly aligned pairs of stained and unstained images remains difficult. In this work, we propose a novel unsupervised deep learning framework for digital cell staining that reduces the need for extensive paired data using knowledge distillation. We explore two training schemes: (1) unpaired and (2) paired-but-misaligned settings. For the unpaired case, we introduce a two-stage pipeline, comprising light enhancement followed by colorization, as a teacher model. Subsequently, we obtain a student staining generator through knowledge distillation with hybrid non-reference losses. To leverage the pixel-wise information between adjacent sections, we further extend to the paired-but-misaligned setting, adding the Learning to Align module to utilize pixel-level information. Experiment results on our dataset demonstrate that our proposed unsupervised deep staining method can generate stained images with more accurate positions and shapes of the cell targets in both settings. Compared with competing methods, our method achieves improved results both qualitatively and quantitatively (e.g., NIQE and PSNR).We applied our digital staining method to the White Blood Cell (WBC) dataset, investigating its potential for medical applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.4 -->
                    
                <!-- LLMs: 8.3 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.4679
                </span>
                <a href="https://arxiv.org/abs/2407.16682" target="_blank" rel="noopener noreferrer">SAM-CP: Marrying SAM with Composable Prompts for Versatile Segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pengfei Chen, Lingxi Xie, Xinyue Huo, Xuehui Yu, Xiaopeng Zhang, Yingfei Sun, Zhenjun Han, Qi Tian
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The Segment Anything model (SAM) has shown a generalized ability to group image pixels into patches, but applying it to semantic-aware segmentation still faces major challenges. This paper presents SAM-CP, a simple approach that establishes two types of composable prompts beyond SAM and composes the</span>
                
                <span class="abstract-full" style="display: none;">The Segment Anything model (SAM) has shown a generalized ability to group image pixels into patches, but applying it to semantic-aware segmentation still faces major challenges. This paper presents SAM-CP, a simple approach that establishes two types of composable prompts beyond SAM and composes them for versatile segmentation. Specifically, given a set of classes (in texts) and a set of SAM patches, the Type-I prompt judges whether a SAM patch aligns with a text label, and the Type-II prompt judges whether two SAM patches with the same text label also belong to the same instance. To decrease the complexity in dealing with a large number of semantic classes and patches, we establish a unified framework that calculates the affinity between (semantic and instance) queries and SAM patches and merges patches with high affinity to the query. Experiments show that SAM-CP achieves semantic, instance, and panoptic segmentation in both open and closed domains. In particular, it achieves state-of-the-art performance in open-vocabulary segmentation. Our research offers a novel and generalized methodology for equipping vision foundation models like SAM with multi-grained semantic perception abilities.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.6 -->
                    
                <!-- LLMs: 6.9 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.8137
                </span>
                <a href="https://arxiv.org/abs/2504.09182" target="_blank" rel="noopener noreferrer">seg2med: a segmentation-based medical image generation framework using denoising diffusion probabilistic models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zeyu Yang, Zhilin Chen, Yipeng Sun, Anika Strittmatter, Anish Raj, Ahmad Allababidi, Johann S. Rink, Frank G. Z\"ollner
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this study, we present seg2med, an advanced medical image synthesis framework that uses Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality synthetic medical images conditioned on anatomical masks from TotalSegmentator. The framework synthesizes CT and MR images from segmenta</span>
                
                <span class="abstract-full" style="display: none;">In this study, we present seg2med, an advanced medical image synthesis framework that uses Denoising Diffusion Probabilistic Models (DDPM) to generate high-quality synthetic medical images conditioned on anatomical masks from TotalSegmentator. The framework synthesizes CT and MR images from segmentation masks derived from real patient data and XCAT digital phantoms, achieving a Structural Similarity Index Measure (SSIM) of 0.94 +/- 0.02 for CT and 0.89 +/- 0.04 for MR images compared to ground-truth images of real patients. It also achieves a Feature Similarity Index Measure (FSIM) of 0.78 +/- 0.04 for CT images from XCAT. The generative quality is further supported by a Fr\'echet Inception Distance (FID) of 3.62 for CT image generation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 25.2 -->
                    
                <!-- LLMs: 8.5 -->
                    
                <!-- Quantum Computing: 4.8 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.847
                </span>
                <a href="https://arxiv.org/abs/2503.14911" target="_blank" rel="noopener noreferrer">Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Siyuan Yan, Ming Hu, Yiwen Jiang, Xieji Li, Hao Fei, Philipp Tschandl, Harald Kittler, Zongyuan Ge
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datas</span>
                
                <span class="abstract-full" style="display: none;">The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datasets are limited in both scale and depth, offering only single-label annotations across a narrow range of diseases instead of rich textual descriptions, and lacking the crucial clinical context needed for real-world applications. To address these limitations, we present Derm1M, the first large-scale vision-language dataset for dermatology, comprising 1,029,761 image-text pairs. Built from diverse educational resources and structured around a standard ontology collaboratively developed by experts, Derm1M provides comprehensive coverage for over 390 skin conditions across four hierarchical levels and 130 clinical concepts with rich contextual information such as medical history, symptoms, and skin tone. To demonstrate Derm1M potential in advancing both AI research and clinical application, we pretrained a series of CLIP-like models, collectively called DermLIP, on this dataset. The DermLIP family significantly outperforms state-of-the-art foundation models on eight diverse datasets across multiple tasks, including zero-shot skin disease classification, clinical and artifacts concept identification, few-shot/full-shot learning, and cross-modal retrieval. Our dataset and code will be publicly available at https://github.com/SiyuanYan1/Derm1M upon acceptance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 21.1 -->
                    
                <!-- LLMs: 11.0 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- RAG: 2.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.8683
                </span>
                <a href="https://arxiv.org/abs/2504.09047" target="_blank" rel="noopener noreferrer">Multi-Robot Coordination with Adversarial Perception</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rayan Bahrami, Hamidreza Jafarnejadsani
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper investigates the resilience of perception-based multi-robot coordination with wireless communication to online adversarial perception. A systematic study of this problem is essential for many safety-critical robotic applications that rely on the measurements from learned perception module</span>
                
                <span class="abstract-full" style="display: none;">This paper investigates the resilience of perception-based multi-robot coordination with wireless communication to online adversarial perception. A systematic study of this problem is essential for many safety-critical robotic applications that rely on the measurements from learned perception modules. We consider a (small) team of quadrotor robots that rely only on an Inertial Measurement Unit (IMU) and the visual data measurements obtained from a learned multi-task perception module (e.g., object detection) for downstream tasks, including relative localization and coordination. We focus on a class of adversarial perception attacks that cause misclassification, mislocalization, and latency. We propose that the effects of adversarial misclassification and mislocalization can be modeled as sporadic (intermittent) and spurious measurement data for the downstream tasks. To address this, we present a framework for resilience analysis of multi-robot coordination with adversarial measurements. The framework integrates data from Visual-Inertial Odometry (VIO) and the learned perception model for robust relative localization and state estimation in the presence of adversarially sporadic and spurious measurements. The framework allows for quantifying the degradation in system observability and stability in relation to the success rate of adversarial perception. Finally, experimental results on a multi-robot platform demonstrate the real-world applicability of our methodology for resource-constrained robotic platforms.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.6 -->
                    
                <!-- LLMs: 5.6 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.3483
                </span>
                <a href="https://arxiv.org/abs/2407.06165" target="_blank" rel="noopener noreferrer">Tumor likelihood estimation on MRI prostate data by utilizing k-Space information</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: M. Rempe, F. H\"orst, C. Seibold, B. Hadaschik, M. Schlimbach, J. Egger, K. Kr\"oninger, F. Breuer, M. Blaimer, J. Kleesiek
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a novel preprocessing and prediction pipeline for the classification of magnetic resonance imaging (MRI) that takes advantage of the information rich complex valued k-Space. Using a publicly available MRI raw dataset with 312 subject and a total of 9508 slices, we show the advantage of ut</span>
                
                <span class="abstract-full" style="display: none;">We present a novel preprocessing and prediction pipeline for the classification of magnetic resonance imaging (MRI) that takes advantage of the information rich complex valued k-Space. Using a publicly available MRI raw dataset with 312 subject and a total of 9508 slices, we show the advantage of utilizing the k-Space for better prostate cancer likelihood estimation in comparison to just using the magnitudinal information in the image domain, with an AUROC of $86.1\%\pm1.8\%$. Additionally, by using high undersampling rates and a simple principal component analysis (PCA) for coil compression, we reduce the time needed for reconstruction by avoiding the time intensive GRAPPA reconstruction algorithm. By using digital undersampling for our experiments, we show that scanning and reconstruction time could be reduced. Even with an undersampling factor of 16, our approach achieves meaningful results, with an AUROC of $71.4\%\pm2.9\%$, using the PCA coil combination and taking into account the k-Space information. With this study, we were able to show the feasibility of preserving phase and k-Space information, with consistent results. Besides preserving valuable information for further diagnostics, this approach can work without the time intensive ADC and reconstruction calculations, greatly reducing the post processing, as well as potential scanning time, increasing patient comfort and allowing a close to real-time prediction.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 19.0 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.1116
                </span>
                <a href="https://arxiv.org/abs/2504.10193" target="_blank" rel="noopener noreferrer">A Piece of QAICCC: Towards a Countermeasure Against Crosstalk Attacks in Quantum Servers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yoann Marquer, Domenico Bianculli
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Quantum computing, while allowing for processing information exponentially faster than classical computing, requires computations to be delegated to quantum servers, which makes security threats possible. For instance, previous studies demonstrated that crosstalk between attacker and victim's qubits</span>
                
                <span class="abstract-full" style="display: none;">Quantum computing, while allowing for processing information exponentially faster than classical computing, requires computations to be delegated to quantum servers, which makes security threats possible. For instance, previous studies demonstrated that crosstalk between attacker and victim's qubits can be exploited to mount security attacks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 14.2 -->
                    
                <!-- LLMs: 9.6 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- GNN: 3.1 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- RAG: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- SpikingNN: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.3417
                </span>
                <a href="https://arxiv.org/abs/2404.07594" target="_blank" rel="noopener noreferrer">Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Tool Segmentation in Robot-Assisted Cardiovascular Catheterization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Olatunji Mumini Omisore, Toluwanimi Akinyemi, Anh Nguyen, Lei Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Robot-assisted catheterization has garnered a good attention for its potentials in treating cardiovascular diseases. However, advancing surgeon-robot collaboration still requires further research, particularly on task-specific automation. For instance, automated tool segmentation can assist surgeons</span>
                
                <span class="abstract-full" style="display: none;">Robot-assisted catheterization has garnered a good attention for its potentials in treating cardiovascular diseases. However, advancing surgeon-robot collaboration still requires further research, particularly on task-specific automation. For instance, automated tool segmentation can assist surgeons in visualizing and tracking of endovascular tools during cardiac procedures. While learning-based models have demonstrated state-of-the-art segmentation performances, generating ground-truth labels for fully-supervised methods is both labor-intensive time consuming, and costly. In this study, we propose a weakly-supervised learning method with multi-lateral pseudo labeling for tool segmentation in cardiovascular angiogram datasets. The method utilizes a modified U-Net architecture featuring one encoder and multiple laterally branched decoders. The decoders generate diverse pseudo labels under different perturbations, augmenting available partial labels. The pseudo labels are self-generated using a mixed loss function with shared consistency across the decoders. The weakly-supervised model was trained end-to-end and validated using partially annotated angiogram data from three cardiovascular catheterization procedures. Validation results show that the model could perform closer to fully-supervised models. Also, the proposed weakly-supervised multi-lateral method outperforms three well known methods used for weakly-supervised learning, offering the highest segmentation performance across the three angiogram datasets. Furthermore, numerous ablation studies confirmed the model's consistent performance under different parameters. Finally, the model was applied for tool segmentation in a robot-assisted catheterization experiments. The model enhanced visualization with high connectivity indices for guidewire and catheter, and a mean processing time of 35 ms per frame.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 24.5 -->
                    
                <!-- LLMs: 6.3 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.2318
                </span>
                <a href="https://arxiv.org/abs/2504.09088" target="_blank" rel="noopener noreferrer">Multi-Modal Brain Tumor Segmentation via 3D Multi-Scale Self-attention and Cross-attention</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yonghao Huang, Leiting Chen, Chuan Zhou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Due to the success of CNN-based and Transformer-based models in various computer vision tasks, recent works study the applicability of CNN-Transformer hybrid architecture models in 3D multi-modality medical segmentation tasks. Introducing Transformer brings long-range dependent information modeling </span>
                
                <span class="abstract-full" style="display: none;">Due to the success of CNN-based and Transformer-based models in various computer vision tasks, recent works study the applicability of CNN-Transformer hybrid architecture models in 3D multi-modality medical segmentation tasks. Introducing Transformer brings long-range dependent information modeling ability in 3D medical images to hybrid models via the self-attention mechanism. However, these models usually employ fixed receptive fields of 3D volumetric features within each self-attention layer, ignoring the multi-scale volumetric lesion features. To address this issue, we propose a CNN-Transformer hybrid 3D medical image segmentation model, named TMA-TransBTS, based on an encoder-decoder structure. TMA-TransBTS realizes simultaneous extraction of multi-scale 3D features and modeling of long-distance dependencies by multi-scale division and aggregation of 3D tokens in a self-attention layer. Furthermore, TMA-TransBTS proposes a 3D multi-scale cross-attention module to establish a link between the encoder and the decoder for extracting rich volume representations by exploiting the mutual attention mechanism of cross-attention and multi-scale aggregation of 3D tokens. Extensive experimental results on three public 3D medical segmentation datasets show that TMA-TransBTS achieves higher averaged segmentation results than previous state-of-the-art CNN-based 3D methods and CNN-Transform hybrid 3D methods for the segmentation of 3D multi-modality brain tumors.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 30.4 -->
                    
                <!-- LLMs: 11.9 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.4732
                </span>
                <a href="https://arxiv.org/abs/2504.10363" target="_blank" rel="noopener noreferrer">Fine-Grained Complexity via Quantum Natural Proofs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yanlin Chen, Yilei Chen, Rajendra Kumar, Subhasree Patro, Florian Speelman
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Buhrman, Patro, and Speelman presented a framework of conjectures that together form a quantum analogue of the strong exponential-time hypothesis and its variants. They called it the QSETH framework. In this paper, using a notion of quantum natural proofs (built from natural proofs introduced by Raz</span>
                
                <span class="abstract-full" style="display: none;">Buhrman, Patro, and Speelman presented a framework of conjectures that together form a quantum analogue of the strong exponential-time hypothesis and its variants. They called it the QSETH framework. In this paper, using a notion of quantum natural proofs (built from natural proofs introduced by Razborov and Rudich), we show how part of the QSETH conjecture that requires properties to be `compression oblivious' can in many cases be replaced by assuming the existence of quantum-secure pseudorandom functions, a standard hardness assumption. Combined with techniques from Fourier analysis of Boolean functions, we show that properties such as PARITY and MAJORITY are compression oblivious for certain circuit class $\Lambda$ if subexponentially secure quantum pseudorandom functions exist in $\Lambda$, answering an open question in [Buhrman-Patro-Speelman 2021].</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 10.5 -->
                    
                <!-- LLMs: 10.4 -->
                    
                <!-- Medicine: 4.9 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -10.0299
                </span>
                <a href="https://arxiv.org/abs/2504.10452" target="_blank" rel="noopener noreferrer">Integrating Vision and Location with Transformers: A Multimodal Deep Learning Framework for Medical Wound Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ramin Mousa, Hadis Taherinia, Khabiba Abdiyeva, Amir Ali Bengari, Mohammadmahdi Vahediahmar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Effective recognition of acute and difficult-to-heal wounds is a necessary step in wound diagnosis. An efficient classification model can help wound specialists classify wound types with less financial and time costs and also help in deciding on the optimal treatment method. Traditional machine lear</span>
                
                <span class="abstract-full" style="display: none;">Effective recognition of acute and difficult-to-heal wounds is a necessary step in wound diagnosis. An efficient classification model can help wound specialists classify wound types with less financial and time costs and also help in deciding on the optimal treatment method. Traditional machine learning models suffer from feature selection and are usually cumbersome models for accurate recognition. Recently, deep learning (DL) has emerged as a powerful tool in wound diagnosis. Although DL seems promising for wound type recognition, there is still a large scope for improving the efficiency and accuracy of the model. In this study, a DL-based multimodal classifier was developed using wound images and their corresponding locations to classify them into multiple classes, including diabetic, pressure, surgical, and venous ulcers. A body map was also created to provide location data, which can help wound specialists label wound locations more effectively. The model uses a Vision Transformer to extract hierarchical features from input images, a Discrete Wavelet Transform (DWT) layer to capture low and high frequency components, and a Transformer to extract spatial features. The number of neurons and weight vector optimization were performed using three swarm-based optimization techniques (Monster Gorilla Toner (MGTO), Improved Gray Wolf Optimization (IGWO), and Fox Optimization Algorithm). The evaluation results show that weight vector optimization using optimization algorithms can increase diagnostic accuracy and make it a very effective approach for wound detection. In the classification using the original body map, the proposed model was able to achieve an accuracy of 0.8123 using image data and an accuracy of 0.8007 using a combination of image data and wound location. Also, the accuracy of the model in combination with the optimization models varied from 0.7801 to 0.8342.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 26.7 -->
                    
                <!-- LLMs: 5.0 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -13.4789
                </span>
                <a href="https://arxiv.org/abs/2504.09498" target="_blank" rel="noopener noreferrer">EasyREG: Easy Depth-Based Markerless Registration and Tracking using Augmented Reality Device for Surgical Guidance</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yue Yang, Christoph Leuze, Brian Hargreaves, Bruce Daniel, Fred Baik
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The use of Augmented Reality (AR) devices for surgical guidance has gained increasing traction in the medical field. Traditional registration methods often rely on external fiducial markers to achieve high accuracy and real-time performance. However, these markers introduce cumbersome calibration pr</span>
                
                <span class="abstract-full" style="display: none;">The use of Augmented Reality (AR) devices for surgical guidance has gained increasing traction in the medical field. Traditional registration methods often rely on external fiducial markers to achieve high accuracy and real-time performance. However, these markers introduce cumbersome calibration procedures and can be challenging to deploy in clinical settings. While commercial solutions have attempted real-time markerless tracking using the native RGB cameras of AR devices, their accuracy remains questionable for medical guidance, primarily due to occlusions and significant outliers between the live sensor data and the preoperative target anatomy point cloud derived from MRI or CT scans. In this work, we present a markerless framework that relies only on the depth sensor of AR devices and consists of two modules: a registration module for high-precision, outlier-robust target anatomy localization, and a tracking module for real-time pose estimation. The registration module integrates depth sensor error correction, a human-in-the-loop region filtering technique, and a robust global alignment with curvature-aware feature sampling, followed by local ICP refinement, for markerless alignment of preoperative models with patient anatomy. The tracking module employs a fast and robust registration algorithm that uses the initial pose from the registration module to estimate the target pose in real-time. We comprehensively evaluated the performance of both modules through simulation and real-world measurements. The results indicate that our markerless system achieves superior performance for registration and comparable performance for tracking to industrial solutions. The two-module design makes our system a one-stop solution for surgical procedures where the target anatomy moves or stays static during surgery.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 44.9 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -14.3575
                </span>
                <a href="https://arxiv.org/abs/2504.10436" target="_blank" rel="noopener noreferrer">Capacities of highly Markovian divisible quantum channels</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Satvik Singh, Nilanjana Datta
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We analyze information transmission capacities of quantum channels acting on $d$-dimensional quantum systems that are highly Markovian divisible, i.e., channels of the form \begin{equation*}</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 25.1 -->
                    
                <!-- LLMs: 8.7 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- Math: 3.1 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- SpikingNN: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -14.5115
                </span>
                <a href="https://arxiv.org/abs/2504.08844" target="_blank" rel="noopener noreferrer">Artificial Intelligence Augmented Medical Imaging Reconstruction in Radiation Therapy</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Di Xu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Efficiently acquired and precisely reconstructed imaging are crucial to the success of modern radiation therapy (RT). Computed tomography (CT) and magnetic resonance imaging (MRI) are two common modalities for providing RT treatment planning and delivery guidance/monitoring. In recent decades, artif</span>
                
                <span class="abstract-full" style="display: none;">Efficiently acquired and precisely reconstructed imaging are crucial to the success of modern radiation therapy (RT). Computed tomography (CT) and magnetic resonance imaging (MRI) are two common modalities for providing RT treatment planning and delivery guidance/monitoring. In recent decades, artificial intelligence (AI) has emerged as a powerful and widely adopted technique across various fields, valued for its efficiency and convenience enabled by implicit function definition and data-driven feature representation learning. Here, we present a series of AI-driven medical imaging reconstruction frameworks for enhanced radiotherapy, designed to improve CT image reconstruction quality and speed, refine dual-energy CT (DECT) multi-material decomposition (MMD), and significantly accelerate 4D MRI acquisition.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 62.9%">
                            Medicine
                        </span>
                <!-- LLMs: 6.8 -->
                    
                <!-- Quantum Computing: 4.8 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -14.7055
                </span>
                <a href="https://arxiv.org/abs/2502.04493" target="_blank" rel="noopener noreferrer">LUND-PROBE -- LUND Prostate Radiotherapy Open Benchmarking and Evaluation dataset</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Viktor Rogowski, Lars E Olsson, Jonas Scherman, Emilia Persson, Mustafa Kadhim, Sacha af Wetterstedt, Adalsteinn Gunnlaugsson, Martin P. Nilsson, Nandor Vass, Mathieu Moreau, Maria Gebre Medhin, Sven B\"ack, Per Munck af Rosensch\"old, Silke Engelholm, Christian Jamtheim Gustafsson
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Radiotherapy treatment for prostate cancer relies on computed tomography (CT) and/or magnetic resonance imaging (MRI) for segmentation of target volumes and organs at risk (OARs). Manual segmentation of these volumes is regarded as the gold standard for ground truth in machine learning applications </span>
                
                <span class="abstract-full" style="display: none;">Radiotherapy treatment for prostate cancer relies on computed tomography (CT) and/or magnetic resonance imaging (MRI) for segmentation of target volumes and organs at risk (OARs). Manual segmentation of these volumes is regarded as the gold standard for ground truth in machine learning applications but to acquire such data is tedious and time-consuming. A publicly available clinical dataset is presented, comprising MRI- and synthetic CT (sCT) images, target and OARs segmentations, and radiotherapy dose distributions for 432 prostate cancer patients treated with MRI-guided radiotherapy. An extended dataset with 35 patients is also included, with the addition of deep learning (DL)-generated segmentations, DL segmentation uncertainty maps, and DL segmentations manually adjusted by four radiation oncologists. The publication of these resources aims to aid research within the fields of automated radiotherapy treatment planning, segmentation, inter-observer analyses, and DL model uncertainty investigation. The dataset is hosted on the AIDA Data Hub and offers a free-to-use resource for the scientific community, valuable for the advancement of medical imaging and prostate cancer radiotherapy research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 49.0 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -16.3839
                </span>
                <a href="https://arxiv.org/abs/2504.09902" target="_blank" rel="noopener noreferrer">Quantum Image Visualizer: Visual Debugging of Quantum Image Processing Circuits</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Anja Heim, Thomas Lang, Alexander Gall, Eduard Gr\"oller, Christoph Heinzl
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Quantum computing is an emerging field that utilizes the unique principles of quantum mechanics to offer significant advantages in algorithm execution over classical approaches. This potential is particularly promising in the domain of quantum image processing, which aims to manipulate all pixels si</span>
                
                <span class="abstract-full" style="display: none;">Quantum computing is an emerging field that utilizes the unique principles of quantum mechanics to offer significant advantages in algorithm execution over classical approaches. This potential is particularly promising in the domain of quantum image processing, which aims to manipulate all pixels simultaneously. However, the process of designing and verifying these algorithms remains a complex and error-prone task. To address this challenge, new methods are needed to support effective debugging of quantum circuits. The Quantum Image Visualizer is an interactive visual analysis tool that allows for the examination of quantum images and their transformation throughout quantum circuits. The framework incorporates two overview visualizations that trace image evolution across a sequence of gates based on the most probable outcomes. Interactive exploration allows users to focus on relevant gates, and select pixels of interest. Upon selection, detailed visualizations enable in-depth inspection of individual pixels and their probability distributions, revealing how specific gates influence the likelihood of pixel color values and the magnitude of these changes. An evaluation of the Quantum Image Visualizer was conducted through in-depth interviews with eight domain experts. The findings demonstrate the effectiveness and practical value of our approach in supporting visual debugging of quantum image processing circuits.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 18.3 -->
                    
                <!-- LLMs: 10.2 -->
                    
                <!-- Medicine: 7.0 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -19.9215
                </span>
                <a href="https://arxiv.org/abs/2504.08876" target="_blank" rel="noopener noreferrer">Is Productivity in Quantum Programming Equivalent to Expressiveness?</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Francini Corrales-Garro, Danny Valerio-Ram\'irez, Santiago N\'u\~ez-Corrales
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The expressiveness of quantum programming languages plays a crucial role in the efficient and comprehensible representation of quantum algorithms. Unlike classical programming languages, which offer mature and well-defined abstraction mechanisms, quantum languages must integrate cognitively challeng</span>
                
                <span class="abstract-full" style="display: none;">The expressiveness of quantum programming languages plays a crucial role in the efficient and comprehensible representation of quantum algorithms. Unlike classical programming languages, which offer mature and well-defined abstraction mechanisms, quantum languages must integrate cognitively challenging concepts such as superposition, interference and entanglement while maintaining clarity and usability. However, identifying and characterizing differences in expressiveness between quantum programming paradigms remains an open area of study. Our work investigates the landscape of expressiveness through a comparative analysis of hosted quantum programming languages such as Qiskit, Cirq, Qrisp, and quAPL, and standalone languages including Q# and Qmod. We focused on evaluating how different quantum programming languages support the implementation of core quantum algorithms -- Deutsch-Jozsa, Simon, Bernstein-Vazirani, and Grover -- using expressiveness metrics: Lines of Code (LOC), Cyclomatic Complexity (CC), and Halstead Complexity (HC) metrics as proxies for developer productivity. Our findings suggest that different quantum programming paradigms offer distinct trade-offs between expressiveness and productivity, highlighting the importance of language design in quantum software development.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 24.8 -->
                    
                <!-- LLMs: 11.5 -->
                    
                <!-- Medicine: 8.4 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -22.1519
                </span>
                <a href="https://arxiv.org/abs/2504.09213" target="_blank" rel="noopener noreferrer">Spiking Neural Network for Intra-cortical Brain Signal Decoding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Song Yang, Haotian Fu, Herui Zhang, Peng Zhang, Wei Li, Dongrui Wu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Decoding brain signals accurately and efficiently is crucial for intra-cortical brain-computer interfaces. Traditional decoding approaches based on neural activity vector features suffer from low accuracy, whereas deep learning based approaches have high computational cost. To improve both the decod</span>
                
                <span class="abstract-full" style="display: none;">Decoding brain signals accurately and efficiently is crucial for intra-cortical brain-computer interfaces. Traditional decoding approaches based on neural activity vector features suffer from low accuracy, whereas deep learning based approaches have high computational cost. To improve both the decoding accuracy and efficiency, this paper proposes a spiking neural network (SNN) for effective and energy-efficient intra-cortical brain signal decoding. We also propose a feature fusion approach, which integrates the manually extracted neural activity vector features with those extracted by a deep neural network, to further improve the decoding accuracy. Experiments in decoding motor-related intra-cortical brain signals of two rhesus macaques demonstrated that our SNN model achieved higher accuracy than traditional artificial neural networks; more importantly, it was tens or hundreds of times more efficient. The SNN model is very suitable for high precision and low power applications like intra-cortical brain-computer interfaces.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #e2e4bd" title="Confidence: 70.3%">
                            SpikingNN
                        </span>
                <!-- Medicine: 10.9 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- LLMs: 3.6 -->
                    
                <!-- Networks: 3.0 -->
                    
                <!-- GNN: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -27.3459
                </span>
                <a href="https://arxiv.org/abs/2106.00571" target="_blank" rel="noopener noreferrer">A reduced 3D-0D FSI model of the aortic valve including leaflet curvature</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ivan Fumagalli, Luca Dede', Alfio Quarteroni
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce an innovative lumped-parameter model of the aortic valve, designed to efficiently simulate the impact of valve dynamics on blood flow. Our reduced model includes the elastic effects associated with the leaflets' curvature and the stress exchanged with the blood flow. The introduction of</span>
                
                <span class="abstract-full" style="display: none;">We introduce an innovative lumped-parameter model of the aortic valve, designed to efficiently simulate the impact of valve dynamics on blood flow. Our reduced model includes the elastic effects associated with the leaflets' curvature and the stress exchanged with the blood flow. The introduction of a lumped-parameter model based on momentum balance entails an easier calibration of the model parameters: phenomenological-based models, on the other hand, typically have numerous parameters. This model is coupled to 3D Navier-Stokes equations describing the blood flow, where the moving valve leaflets are immersed in the fluid domain by a resistive method. A stabilized finite element method with a BDF time scheme is adopted for the discretization of the coupled problem, and the computational results show the suitability of the system in representing the leaflet motion, the blood flow in the ascending aorta, and the pressure jump across the leaflets. Both physiological and stenotic configurations are investigated, and we analyze the effects of different treatments for the leaflet velocity on the blood flow.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #4ff278" title="Confidence: 82.0%">
                            Medicine
                        </span>
                <!-- Math: 4.4 -->
                    
                <!-- Reinforcement Learning: 3.9 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Robotics: 2.4 -->
                    
                <!-- Pathfinding: 1.6 -->
                    
                <!-- LLMs: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -28.5933
                </span>
                <a href="https://arxiv.org/abs/2504.07822" target="_blank" rel="noopener noreferrer">DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wanna Cui, Peizheng Wang, Faliang Yin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with stati</span>
                
                <span class="abstract-full" style="display: none;">Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 68.6%">
                            GNN
                        </span>
                <!-- LLMs: 7.1 -->
                    
                <!-- Quantum Computing: 4.8 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- SpikingNN: 1.3 -->
                    
                <!-- Attention: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -29.844
                </span>
                <a href="https://arxiv.org/abs/2504.03175" target="_blank" rel="noopener noreferrer">Mathematical Modeling of Option Pricing with an Extended Black-Scholes Framework</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nikhil Shivakumar Nayak
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study investigates enhancing option pricing by extending the Black-Scholes model to include stochastic volatility and interest rate variability within the Partial Differential Equation (PDE). The PDE is solved using the finite difference method. The extended Black-Scholes model and a machine le</span>
                
                <span class="abstract-full" style="display: none;">This study investigates enhancing option pricing by extending the Black-Scholes model to include stochastic volatility and interest rate variability within the Partial Differential Equation (PDE). The PDE is solved using the finite difference method. The extended Black-Scholes model and a machine learning-based LSTM model are developed and evaluated for pricing Google stock options. Both models were backtested using historical market data. While the LSTM model exhibited higher predictive accuracy, the finite difference method demonstrated superior computational efficiency. This work provides insights into model performance under varying market conditions and emphasizes the potential of hybrid approaches for robust financial modeling.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #c48298" title="Confidence: 72.6%">
                            Finance
                        </span>
                <!-- LLMs: 16.8 -->
                    
                <!-- Medicine: 3.6 -->
                    
                <!-- Blockchain: 2.7 -->
                    
                <!-- RAG: 2.6 -->
                    
                <!-- 3D: 2.4 -->
                    
                <!-- T2I: 2.2 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- Fuzzy Logic: 1.5 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -31.0766
                </span>
                <a href="https://arxiv.org/abs/2504.09909" target="_blank" rel="noopener noreferrer">Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Farha Nausheen, Khandakar Ahmed, M Imad Khan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In recent developments, deep learning methodologies applied to Natural Language Processing (NLP) have revealed a paradox: They improve performance but demand considerable data and resources for their training. Alternatively, quantum computing exploits the principles of quantum mechanics to overcome </span>
                
                <span class="abstract-full" style="display: none;">In recent developments, deep learning methodologies applied to Natural Language Processing (NLP) have revealed a paradox: They improve performance but demand considerable data and resources for their training. Alternatively, quantum computing exploits the principles of quantum mechanics to overcome the computational limitations of current methodologies, thereby establishing an emerging field known as quantum natural language processing (QNLP). This domain holds the potential to attain a quantum advantage in the processing of linguistic structures, surpassing classical models in both efficiency and accuracy. In this paper, it is proposed to categorise QNLP models based on quantum computing principles, architecture, and computational approaches. This paper attempts to provide a survey on how quantum meets language by mapping state-of-the-art in this area, embracing quantum encoding techniques for classical data, QNLP models for prevalent NLP tasks, and quantum optimisation techniques for hyper parameter tuning. The landscape of quantum computing approaches applied to various NLP tasks is summarised by showcasing the specific QNLP methods used, and the popularity of these methods is indicated by their count. From the findings, it is observed that QNLP approaches are still limited to small data sets, with only a few models explored extensively, and there is increasing interest in the application of quantum computing to natural language processing tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 38.8 -->
                    
                <!-- LLMs: 4.6 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -36.8592
                </span>
                <a href="https://arxiv.org/abs/2504.07048" target="_blank" rel="noopener noreferrer">Context Switching for Secure Multi-programming of Near-Term Quantum Computers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Avinash Kumar, Meng Wang, Chenxu Liu, Ang Li, Prashant J. Nair, Poulami Das
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multi-programming quantum computers improve device utilization and throughput. However, crosstalk from concurrent two-qubit CNOT gates poses security risks, compromising the fidelity and output of co-running victim programs. We design Zero Knowledge Tampering Attacks (ZKTAs), using which attackers c</span>
                
                <span class="abstract-full" style="display: none;">Multi-programming quantum computers improve device utilization and throughput. However, crosstalk from concurrent two-qubit CNOT gates poses security risks, compromising the fidelity and output of co-running victim programs. We design Zero Knowledge Tampering Attacks (ZKTAs), using which attackers can exploit crosstalk without knowledge of the hardware error profile. ZKTAs can alter victim program outputs in 40% of cases on commercial systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 69.1%">
                            Quantum Computing
                        </span>
                <!-- Medicine: 8.2 -->
                    
                <!-- LLMs: 8.2 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- SpikingNN: 1.6 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -55.1667
                </span>
                <a href="https://arxiv.org/abs/2504.06951" target="_blank" rel="noopener noreferrer">GLT hidden structures in mean-field quantum spin systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christiaan J. F. van de Ven, Muhammad Faisal Khan, S. Serra-Capizzano
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This work explores structured matrix sequences arising in mean-field quantum spin systems. We express these sequences within the framework of generalized locally Toeplitz (GLT) $*$-algebras, leveraging the fact that each GLT matrix sequence has a unique GLT symbol. This symbol characterizes both the</span>
                
                <span class="abstract-full" style="display: none;">This work explores structured matrix sequences arising in mean-field quantum spin systems. We express these sequences within the framework of generalized locally Toeplitz (GLT) $*$-algebras, leveraging the fact that each GLT matrix sequence has a unique GLT symbol. This symbol characterizes both the asymptotic singular value distribution and, for Hermitian or quasi-Hermitian sequences, the asymptotic spectral distribution. Specifically, we analyze two cases of real symmetric matrix sequences stemming from mean-field quantum spin systems and determine their associated distributions using GLT theory. Our study concludes with visualizations and numerical tests that validate the theoretical findings, followed by a discussion of open problems and future directions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #d37d97" title="Confidence: 75.9%">
                            Quantum Computing
                        </span>
                <!-- Medicine: 6.1 -->
                    
                <!-- LLMs: 4.8 -->
                    
                <!-- Math: 2.5 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- SpikingNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -57.2116
                </span>
                <a href="https://arxiv.org/abs/2402.14696" target="_blank" rel="noopener noreferrer">On Schr\"odingerization based quantum algorithms for linear dynamical systems with inhomogeneous terms</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shi Jin, Nana Liu, Chuwen Ma
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We analyze the Schr\"odingerization method for quantum simulation of a general class of non-unitary dynamics with inhomogeneous source terms. The Schr\"odingerization technique, introduced in [31], transforms any linear ordinary and partial differential equations with non-unitary dynamics into a sys</span>
                
                <span class="abstract-full" style="display: none;">We analyze the Schr\"odingerization method for quantum simulation of a general class of non-unitary dynamics with inhomogeneous source terms. The Schr\"odingerization technique, introduced in [31], transforms any linear ordinary and partial differential equations with non-unitary dynamics into a system under unitary dynamics via a warped phase transition that maps the equations into a higher dimension, making them suitable for quantum simulation. This technique can also be applied to these equations with inhomogeneous terms modeling source or forcing terms, or boundary and interface conditions, and discrete dynamical systems such as iterative methods in numerical linear algebra, through extra equations in the system. Difficulty arises with the presence of inhomogeneous terms since they can change the stability of the original system. In this paper, we systematically study-both theoretically and numerically-the important issue of recovering the original variables from the Schr\"odingerized equations, even when the evolution operator contains unstable modes. We show that, even with unstable modes, one can still construct a stable scheme; however, to recover the original variable, one needs to use suitable data in the extended space. We analyze and compare both the discrete and continuous Fourier transforms used in the extended dimension and derive corresponding error estimates, which allow one to use the more appropriate transform for specific equations. We also provide a smoother initialization for the Schr\"odingerized system to gain higher-order accuracy in the extended space. We homogenize the inhomogeneous terms with a stretch transformation, making it easier to recover the original variable. Our recovery technique also provides a simple and generic framework to solve general ill-posed problems in a computationally stable way.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #d37d97" title="Confidence: 78.7%">
                            Quantum Computing
                        </span>
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- LLMs: 2.0 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -94.3761
                </span>
                <a href="https://arxiv.org/abs/2305.14046" target="_blank" rel="noopener noreferrer">Enhancing Smart Contract Security Analysis with Execution Property Graphs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kaihua Qin, Zhe Ye, Zhun Wang, Weilin Li, Liyi Zhou, Chao Zhang, Dawn Song, Arthur Gervais
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Smart contract vulnerabilities have led to significant financial losses, with their increasing complexity rendering outright prevention of hacks increasingly challenging. This trend highlights the crucial need for advanced forensic analysis and real-time intrusion detection, where dynamic analysis p</span>
                
                <span class="abstract-full" style="display: none;">Smart contract vulnerabilities have led to significant financial losses, with their increasing complexity rendering outright prevention of hacks increasingly challenging. This trend highlights the crucial need for advanced forensic analysis and real-time intrusion detection, where dynamic analysis plays a key role in dissecting smart contract executions. Therefore, there is a pressing need for a unified and generic representation of smart contract executions, complemented by an efficient methodology that enables the modeling and identification of a broad spectrum of emerging attacks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #3cc377" title="Confidence: 75.4%">
                            Blockchain
                        </span>
                <!-- LLMs: 16.3 -->
                    
                <!-- RAG: 3.4 -->
                    
                <!-- 3D: 2.9 -->
                    
                <!-- T2I: 2.2 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- Medicine: 1.6 -->
                    
                <!-- Fuzzy Logic: 1.3 -->
                    
                <!-- Finance: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Quality Diversity: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
    </div>
    
    
    <div id="jsonPopup" class="json-popup">
        <pre id="jsonContent"></pre>
        <button onclick="copyJson()">Copy to Clipboard</button>
        <button onclick="closePopup()">Close</button>
    </div>

    <script>
        function extractPaperData(paperElement) {
            const titleElement = paperElement.querySelector('.paper-title a');
            const metaElement = paperElement.querySelector('.paper-meta');
            const abstractElement = paperElement.querySelector('.paper-abstract');
            const tagsElement = paperElement.querySelector('.paper-tags');
            
            // Get the date from the parent date-section header
            const dateSection = paperElement.closest('.date-section');
            const dateText = dateSection.querySelector('.date-header').textContent.trim();
            
            const authorsText = metaElement.textContent.replace('Authors:', '').trim();
            
            const paperData = {
                title: titleElement.textContent,
                url: titleElement.href,
                authors: authorsText.split(',').map(author => author.trim()),
                created: dateText,
                abstract: abstractElement.querySelector('.abstract-full').textContent
            };
            
            return paperData;
        }

        function showJson(paperElement) {
            const popup = document.getElementById('jsonPopup');
            const content = document.getElementById('jsonContent');
            const paperData = extractPaperData(paperElement);
            content.textContent = JSON.stringify(paperData, null, null);
            popup.style.display = 'block';
            document.addEventListener('click', function closePopupOnClick(event) {
                if (!popup.contains(event.target)) {
                    popup.style.display = 'none';
                    document.removeEventListener('click', closePopupOnClick);
                }
            });
        }
        function toggleAbstract(element) {
            const abstract = element.parentElement;
            const short = abstract.querySelector('.abstract-short');
            const full = abstract.querySelector('.abstract-full');
            const lowConfidenceTags = abstract.parentElement.querySelectorAll('.tag-badge.low-confidence');
            
            if (element.textContent === '... more') {
                short.style.display = 'none';
                full.style.display = 'inline';
                element.textContent = ' less';
                lowConfidenceTags.forEach(tag => tag.style.display = 'inline-block');
            } else {
                short.style.display = 'inline';
                full.style.display = 'none';
                element.textContent = '... more';
                lowConfidenceTags.forEach(tag => tag.style.display = 'none');
            }
        }

        function closePopup() {
            document.getElementById('jsonPopup').style.display = 'none';
        }

        function copyJson() {
            const content = document.getElementById('jsonContent').textContent;
            navigator.clipboard.writeText(content).catch(() => {
                // If clipboard API is not available, just show the popup
                alert('Could not copy to clipboard. JSON is displayed in the popup.');
            });
        }

        // Close popup when clicking outside
        window.onclick = function(event) {
            const popup = document.getElementById('jsonPopup');
            if (event.target === popup) {
                popup.style.display = 'none';
            }
        }
    </script>
</body>
</html> 