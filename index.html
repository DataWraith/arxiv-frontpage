<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Frontpage</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .paper {
            margin-bottom: 30px;
            margin-top: 30px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .paper-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .paper-meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        .paper-abstract {
            margin-bottom: 10px;
        }
        .abstract-short {
            display: inline;
        }
        .abstract-full {
            display: none;
        }
        .more-link {
            color: blue;
            cursor: pointer;
            text-decoration: underline;
        }
        .tag-badge {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 5px;
            margin-bottom: 5px;
            border-radius: 3px;
            font-size: 0.8em;
            color: white;
        }
        .tag-badge.high-confidence {
            opacity: 1;
        }
        .tag-badge.low-confidence {
            opacity: 0.6;
            display: none;
        }
        .interestingness-score {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 10px;
            color: white;
            border-radius: 3px;
            font-weight: bold;
        }
        .interestingness-positive {
            background-color: #4CAF50;
        }
        .interestingness-negative {
            background-color: #f44336;
        }
        .interestingness-neutral {
            background-color: #9e9e9e;
        }
        .last-updated {
            text-align: right;
            color: #666;
            font-size: 0.9em;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .intro {
            text-align: center;
            max-width: 60em;
            margin: 0 auto;
            color: #888;
        }
        .copy-icon {
            display: inline-block;
            width: 16px;
            height: 16px;
            cursor: pointer;
            margin-left: 5px;
            opacity: 0.5;
        }
        .copy-icon:hover {
            opacity: 1;
        }
        .json-popup {
            display: none;
            position: fixed;
            top: 20px;
            right: 20px;
            background: white;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            max-width: 500px;
            max-height: 300px;
            overflow: auto;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        a {
            color: inherit;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h1 {
            text-align: center;
        }
        h1 a {
            text-decoration: underline;
        }
        .date-section {
            margin-bottom: 40px;
        }
        .date-header {
            color: #666;
            font-size: 1.5em;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #eee;
        }
    </style>
</head>
<body>
    <h1>
        <a href="https://github.com/DataWraith/arxiv-frontpage">DataWraith's</a> ArXiv Frontpage
    </h1>

    <div class="last-updated">
        Last updated: 2025-04-28
    </div>

    <p class="intro">
        This frontpage is made by scraping ArXiv's computer science RSS feed and tagging papers with a classifier.
    </p>

    <p class="intro">
        Each tag is weighted according to my preferences to compute a paper's <i>interestingness</i> score.
    </p>
    
    
    <div class="date-section">
        <h2 class="date-header">2025-04-28</h2>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    1.3799
                </span>
                <a href="https://arxiv.org/abs/2504.18208" target="_blank" rel="noopener noreferrer">Ultra-fast feature learning for the training of two-layer neural networks in the two-timescale regime</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rapha\"el Barboni (\'ENS-PSL), Gabriel Peyr\'e (CNRS, \'ENS-PSL), Fran\c{c}ois-Xavier Vialard (LIGM)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the convergence of gradient methods for the training of mean-field single hidden layer neural networks with square loss. Observing this is a separable non-linear least-square problem which is linear w.r.t. the outer layer's weights, we consider a Variable Projection (VarPro) or two-timescal</span>
                
                <span class="abstract-full" style="display: none;">We study the convergence of gradient methods for the training of mean-field single hidden layer neural networks with square loss. Observing this is a separable non-linear least-square problem which is linear w.r.t. the outer layer's weights, we consider a Variable Projection (VarPro) or two-timescale learning algorithm, thereby eliminating the linear variables and reducing the learning problem to the training of the feature distribution. Whereas most convergence rates or the training of neural networks rely on a neural tangent kernel analysis where features are fixed, we show such a strategy enables provable convergence rates for the sampling of a teacher feature distribution. Precisely, in the limit where the regularization strength vanishes, we show that the dynamic of the feature distribution corresponds to a weighted ultra-fast diffusion equation. Relying on recent results on the asymptotic behavior of such PDEs, we obtain guarantees for the convergence of the trained feature distribution towards the teacher feature distribution in a teacher-student setup.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 8.3 -->
                    
                <!-- Medicine: 4.9 -->
                    
                <!-- Math: 4.8 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Pathfinding: 1.8 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- LLMs: 1.3 -->
                    
                <!-- SpikingNN: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    1.375
                </span>
                <a href="https://arxiv.org/abs/2503.14976" target="_blank" rel="noopener noreferrer">Application of linear regression and quasi-Newton methods to the deep reinforcement learning in continuous action cases</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hisato Komatsu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The linear regression (LR) method offers the advantage that optimal parameters can be calculated relatively easily, although its representation capability is limited than that of the deep learning technique. To improve deep reinforcement learning, the Least Squares Deep Q Network (LS-DQN) method was</span>
                
                <span class="abstract-full" style="display: none;">The linear regression (LR) method offers the advantage that optimal parameters can be calculated relatively easily, although its representation capability is limited than that of the deep learning technique. To improve deep reinforcement learning, the Least Squares Deep Q Network (LS-DQN) method was proposed by Levine et al., which combines Deep Q Network (DQN) with LR method. However, the LS-DQN method assumes that the actions are discrete. In this study, we propose the Double Least Squares Deep Deterministic Policy Gradient (DLS-DDPG) method to address this limitation. This method combines the LR method with the Deep Deterministic Policy Gradient (DDPG) technique, one of the representative deep reinforcement learning algorithms for continuous action cases. For the LR update of the critic network, DLS-DDPG uses an algorithm similar to the Fitted Q iteration, the method which LS-DQN adopted. In addition, we calculated the optimal action using the quasi-Newton method and used it as both the agent's action and the training data for the LR update of the actor network. Numerical experiments conducted in MuJoCo environments showed that the proposed method improved performance at least in some tasks, although there are difficulties such as the inability to make the regularization terms small.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 10.1 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- Math: 3.0 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Pathfinding: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Multi-armed Bandit: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.898
                </span>
                <a href="https://arxiv.org/abs/2405.00495" target="_blank" rel="noopener noreferrer">On the Loewner framework, the Kolmogorov superposition theorem, and the curse of dimensionality</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Athanasios C. Antoulas, Ion Victor Gosea, Charles Poussot-Vassal
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The Loewner framework is an interpolatory approach for the approximation of linear and nonlinear systems. The purpose here is to extend this framework to linear parametric systems with an arbitrary number n of parameters. To achieve this, a new generalized multivariate rational function realization </span>
                
                <span class="abstract-full" style="display: none;">The Loewner framework is an interpolatory approach for the approximation of linear and nonlinear systems. The purpose here is to extend this framework to linear parametric systems with an arbitrary number n of parameters. To achieve this, a new generalized multivariate rational function realization is proposed. Then, we introduce the n-dimensional multivariate Loewner matrices and show that they can be computed by solving a set of coupled Sylvester equations. The null space of these Loewner matrices allows the construction of the multivariate barycentric rational function. The principal result of this work is to show how the null space of the n-dimensional Loewner matrix can be computed using a sequence of 1-dimensional Loewner matrices, leading to a drastic reduction of the computational burden. Equally importantly, this burden is alleviated by avoiding the explicit construction of large-scale n-dimensional Loewner matrices of size $N \times N$. Instead, the proposed methodology achieves decoupling of variables, leading to (i) a complexity reduction from $O(N^3)$ to below $O(N^{1.5})$ when $n > 5$ and (ii) to memory storage bounded by the largest variable dimension rather than their product, thus taming the curse of dimensionality and making the solution scalable to very large data sets. This decoupling of the variables leads to a result similar to the Kolmogorov superposition theorem for rational functions. Thus, making use of barycentric representations, every multivariate rational function can be computed using the composition and superposition of single-variable functions. Finally, we suggest two algorithms (one direct and one iterative) to construct, directly from data, multivariate (or parametric) realizations ensuring (approximate) interpolation. Numerical examples highlight the effectiveness and scalability of the method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 5.4 -->
                    
                <!-- Medicine: 4.9 -->
                    
                <!-- LLMs: 3.6 -->
                    
                <!-- Math: 3.0 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.8655
                </span>
                <a href="https://arxiv.org/abs/2403.18055" target="_blank" rel="noopener noreferrer">Adaptive Boundary Control of the Kuramoto-Sivashinsky Equation Under Intermittent Sensing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mohamed Camil Belhadjoudja, Mohamed Maghenem, Emmanuel Witrant, Christophe Prieur
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study in this paper boundary stabilization, in the L2 sense, of the perturbed Kuramoto-Sivashinsky (KS) equation subject to intermittent sensing. We assume that we measure the state on a given spatial subdomain during certain time intervals, while we measure the state on the remaining spatial sub</span>
                
                <span class="abstract-full" style="display: none;">We study in this paper boundary stabilization, in the L2 sense, of the perturbed Kuramoto-Sivashinsky (KS) equation subject to intermittent sensing. We assume that we measure the state on a given spatial subdomain during certain time intervals, while we measure the state on the remaining spatial subdomain during the remaining time intervals. We assign a feedback law at the boundary of the spatial domain and force to zero the value of the state at the junction of the two subdomains. Throughout the study, the equation's destabilizing coefficient is assumed to be unknown and possibly space dependent but bounded. As a result, adaptive boundary controllers are designed under different assumptions on the perturbation. In particular, we guarantee input-to-state stability (ISS) when an upperbound on the perturbation's size is known. Otherwise, only global uniform ultimate boundedness (GUUB) is guaranteed. In contrast, when the state is measured at every spatial point all the time (full state measurement), convergence to an arbitrarily-small neighborhood of the origin is guaranteed, even if the perturbation's maximal size is unknown. Numerical simulations are performed to illustrate our results.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 6.9 -->
                    
                <!-- Math: 4.9 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Pathfinding: 2.2 -->
                    
                <!-- LLMs: 2.0 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Multi-armed Bandit: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.8421
                </span>
                <a href="https://arxiv.org/abs/2504.18075" target="_blank" rel="noopener noreferrer">Fictitious Play in Extensive-Form Games of Imperfect Information</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jason Castiglione, G\"urdal Arslan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the long-term behavior of the fictitious play process in repeated extensive-form games of imperfect information with perfect recall. Each player maintains incorrect beliefs that the moves at all information sets, except the one at which the player is about to make a move, are made according</span>
                
                <span class="abstract-full" style="display: none;">We study the long-term behavior of the fictitious play process in repeated extensive-form games of imperfect information with perfect recall. Each player maintains incorrect beliefs that the moves at all information sets, except the one at which the player is about to make a move, are made according to fixed random strategies, independently across all information sets. Accordingly, each player makes his moves at any of his information sets to maximize his expected payoff assuming that, at any other information set, the moves are made according to the empirical frequencies of the past moves. We extend the well-known Monderer-Shapley result [1] on the convergence of the empirical frequencies to the set of Nash equilibria to a certain class of extensive-form games with identical interests. We then strengthen this result by the use of inertia and fading memory, and prove the convergence of the realized play-paths to an essentially pure Nash equilibrium in all extensive-form games of imperfect information with identical interests.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 6.2 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Math: 3.5 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.8236
                </span>
                <a href="https://arxiv.org/abs/2504.18022" target="_blank" rel="noopener noreferrer">Iterative Joint Detection of Kalman Filter and Channel Decoder for Sensor-to-Controller Link in Wireless Networked Control Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jinnan Piao, Dong Li, Yiming Sun, Zhibo Li, Ming Yang, Xueting Yu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this letter, we propose an iterative joint detection algorithm of Kalman filter (KF) and channel decoder for the sensor-to-controller link of wireless networked control systems, which utilizes the prior information of control system to improve the control and communication performance. In the alg</span>
                
                <span class="abstract-full" style="display: none;">In this letter, we propose an iterative joint detection algorithm of Kalman filter (KF) and channel decoder for the sensor-to-controller link of wireless networked control systems, which utilizes the prior information of control system to improve the control and communication performance. In the algorithm, we first use the KF to estimate the probability density of the control system outputs and calculate the prior probability of received signals to assist decoding. Then, the possible outputs of the control system are traversed to update the prior probability in order to implement iterative detection. The simulation results show that the prior information can reduce the block error rate performance of communications to improve the root mean square error performance of controls.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 5.5 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.7613
                </span>
                <a href="https://arxiv.org/abs/2504.08937" target="_blank" rel="noopener noreferrer">Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable General-Purpose Deep Fusion</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Minjie Deng, Yan Wei, Hao Zhai, An Wu, Yuncan Ouyang, Qianyao Peng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In image fusion tasks, the absence of real fused images as priors presents a fundamental challenge. Most deep learning-based fusion methods rely on large-scale paired datasets to extract global weighting features from raw images, thereby generating fused outputs that approximate real fused images. I</span>
                
                <span class="abstract-full" style="display: none;">In image fusion tasks, the absence of real fused images as priors presents a fundamental challenge. Most deep learning-based fusion methods rely on large-scale paired datasets to extract global weighting features from raw images, thereby generating fused outputs that approximate real fused images. In contrast to previous studies, this paper explores few-shot training of neural networks under the condition of having prior knowledge. We propose a novel fusion framework named GBFF, and a Granular Ball Significant Extraction algorithm specifically designed for the few-shot prior setting. All pixel pairs involved in the fusion process are initially modeled as a Coarse-Grained Granular Ball. At the local level, Fine-Grained Granular Balls are used to slide through the brightness space to extract Non-Salient Pixel Pairs, and perform splitting operations to obtain Salient Pixel Pairs. Pixel-wise weights are then computed to generate a pseudo-supervised image. At the global level, pixel pairs with significant contributions to the fusion process are categorized into the Positive Region, while those whose contributions cannot be accurately determined are assigned to the Boundary Region. The Granular Ball performs modality-aware adaptation based on the proportion of the positive region, thereby adjusting the neural network's loss function and enabling it to complement the information of the boundary region. Extensive experiments demonstrate the effectiveness of both the proposed algorithm and the underlying theory. Compared with state-of-the-art (SOTA) methods, our approach shows strong competitiveness in terms of both fusion time and image expressiveness. Our code is publicly available at:</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 5.1 -->
                    
                <!-- Medicine: 4.7 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.6984
                </span>
                <a href="https://arxiv.org/abs/2504.18437" target="_blank" rel="noopener noreferrer">Enhancing Pre-Trained Model-Based Class-Incremental Learning through Neural Collapse</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kun He, Zijian Song, Shuoxi Zhang, John E. Hopcroft
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Class-Incremental Learning (CIL) is a critical capability for real-world applications, enabling learning systems to adapt to new tasks while retaining knowledge from previous ones. Recent advancements in pre-trained models (PTMs) have significantly advanced the field of CIL, demonstrating superior p</span>
                
                <span class="abstract-full" style="display: none;">Class-Incremental Learning (CIL) is a critical capability for real-world applications, enabling learning systems to adapt to new tasks while retaining knowledge from previous ones. Recent advancements in pre-trained models (PTMs) have significantly advanced the field of CIL, demonstrating superior performance over traditional methods. However, understanding how features evolve and are distributed across incremental tasks remains an open challenge. In this paper, we propose a novel approach to modeling feature evolution in PTM-based CIL through the lens of neural collapse (NC), a striking phenomenon observed in the final phase of training, which leads to a well-separated, equiangular feature space. We explore the connection between NC and CIL effectiveness, showing that aligning feature distributions with the NC geometry enhances the ability to capture the dynamic behavior of continual learning. Based on this insight, we introduce Neural Collapse-inspired Pre-Trained Model-based CIL (NCPTM-CIL), a method that dynamically adjusts the feature space to conform to the elegant NC structure, thereby enhancing the continual learning process. Extensive experiments demonstrate that NCPTM-CIL outperforms state-of-the-art methods across four benchmark datasets. Notably, when initialized with ViT-B/16-IN1K, NCPTM-CIL surpasses the runner-up method by 6.73% on VTAB, 1.25% on CIFAR-100, and 2.5% on OmniBenchmark.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.2 -->
                    
                <!-- Reinforcement Learning: 5.0 -->
                    
                <!-- Medicine: 3.1 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.5187
                </span>
                <a href="https://arxiv.org/abs/2504.17924" target="_blank" rel="noopener noreferrer">Learning Attentive Neural Processes for Planning with Pushing Actions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Atharv Jain, Seiji Shaw, Nicholas Roy
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Our goal is to enable robots to plan sequences of tabletop actions to push a block with unknown physical properties to a desired goal pose on the table. We approach this problem by learning the constituent models of a Partially-Observable Markov Decision Process (POMDP), where the robot can observe </span>
                
                <span class="abstract-full" style="display: none;">Our goal is to enable robots to plan sequences of tabletop actions to push a block with unknown physical properties to a desired goal pose on the table. We approach this problem by learning the constituent models of a Partially-Observable Markov Decision Process (POMDP), where the robot can observe the outcome of a push, but the physical properties of the block that govern the dynamics remain unknown. The pushing problem is a difficult POMDP to solve due to the challenge of state estimation. The physical properties have a nonlinear relationship with the outcomes, requiring computationally expensive methods, such as particle filters, to represent beliefs. Leveraging the Attentive Neural Process architecture, we propose to replace the particle filter with a neural network that learns the inference computation over the physical properties given a history of actions. This Neural Process is integrated into planning as the Neural Process Tree with Double Progressive Widening (NPT-DPW). Simulation results indicate that NPT-DPW generates more effective plans faster than traditional particle filter methods, even in complex pushing scenarios.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 5.1 -->
                    
                <!-- Networks: 5.1 -->
                    
                <!-- Medicine: 2.9 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- LLMs: 1.9 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17799" target="_blank" rel="noopener noreferrer">Subfunction Structure Matters: A New Perspective on Local Optima Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: S. L. Thomson, M. W. Przewozniczek
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Local optima networks (LONs) capture fitness landscape information. They are typically constructed in a black-box manner; information about the problem structure is not utilised. This also applies to the analysis of LONs: knowledge about the problem, such as interaction between variables, is not con</span>
                
                <span class="abstract-full" style="display: none;">Local optima networks (LONs) capture fitness landscape information. They are typically constructed in a black-box manner; information about the problem structure is not utilised. This also applies to the analysis of LONs: knowledge about the problem, such as interaction between variables, is not considered. We challenge this status-quo with an alternative approach: we consider how LON analysis can be improved by incorporating subfunction-based information - this can either be known a-priori or learned during search. To this end, LONs are constructed for several benchmark pseudo-boolean problems using three approaches: firstly, the standard algorithm; a second algorithm which uses deterministic grey-box crossover; and a third algorithm which selects perturbations based on learned information about variable interactions. Metrics related to subfunction changes in a LON are proposed and compared with metrics from previous literature which capture other aspects of a LON. Incorporating problem structure in LON construction and analysing it can bring enriched insight into optimisation dynamics. Such information may be crucial to understanding the difficulty of solving a given problem with state-of-the-art linkage learning optimisers. In light of the results, we suggest incorporation of problem structure as an alternative paradigm in landscape analysis for problems with known or suspected subfunction structure.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.7 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- 3D: 1.0 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17817" target="_blank" rel="noopener noreferrer">Learning Underwater Active Perception in Simulation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alexandre Cardaillac, Donald G. Dansereau
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">When employing underwater vehicles for the autonomous inspection of assets, it is crucial to consider and assess the water conditions. Indeed, they have a significant impact on the visibility, which also affects robotic operations. Turbidity can jeopardise the whole mission as it may prevent correct</span>
                
                <span class="abstract-full" style="display: none;">When employing underwater vehicles for the autonomous inspection of assets, it is crucial to consider and assess the water conditions. Indeed, they have a significant impact on the visibility, which also affects robotic operations. Turbidity can jeopardise the whole mission as it may prevent correct visual documentation of the inspected structures. Previous works have introduced methods to adapt to turbidity and backscattering, however, they also include manoeuvring and setup constraints. We propose a simple yet efficient approach to enable high-quality image acquisition of assets in a broad range of water conditions. This active perception framework includes a multi-layer perceptron (MLP) trained to predict image quality given a distance to a target and artificial light intensity. We generated a large synthetic dataset including ten water types with different levels of turbidity and backscattering. For this, we modified the modelling software Blender to better account for the underwater light propagation properties. We validated the approach in simulation and showed significant improvements in visual coverage and quality of imagery compared to traditional approaches. The project code is available on our project page at https://roboticimaging.org/Projects/ActiveUW/.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.9 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Robotics: 2.6 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17820" target="_blank" rel="noopener noreferrer">How fake news can turn against its spreader</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dorje C. Brody, Tomooki Yuasa
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">When different information sources on a given topic are combined, they interact in a nontrivial manner for a rational receiver of these information sources. Suppose that there are two information sources, one is genuine and the other contains disinformation. It is shown that under the conditions tha</span>
                
                <span class="abstract-full" style="display: none;">When different information sources on a given topic are combined, they interact in a nontrivial manner for a rational receiver of these information sources. Suppose that there are two information sources, one is genuine and the other contains disinformation. It is shown that under the conditions that the signal-to-noise ratio of the genuine information source is sufficiently large, and that the noise terms in the two information sources are positively correlated, the effect of disinformation is reversed from its original intent. That is, the effect of disinformation on a receiver of both information sources, who is unaware of the existence of disinformation, is to generate an opposite interpretation. While the condition in which this phenomenon occurs cannot always be ensured, when it is satisfied, the effect provides an effective way of countering the impacts of disinformation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.1 -->
                    
                <!-- Math: 3.9 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- Reinforcement Learning: 3.2 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Pathfinding: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17838" target="_blank" rel="noopener noreferrer">CaRL: Learning Scalable Planning Policies with Simple Rewards</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bernhard Jaeger, Daniel Dauner, Jens Bei{\ss}wenger, Simon Gerstenecker, Kashyap Chitta, Andreas Geiger
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We investigate reinforcement learning (RL) for privileged planning in autonomous driving. State-of-the-art approaches for this task are rule-based, but these methods do not scale to the long tail. RL, on the other hand, is scalable and does not suffer from compounding errors like imitation learning.</span>
                
                <span class="abstract-full" style="display: none;">We investigate reinforcement learning (RL) for privileged planning in autonomous driving. State-of-the-art approaches for this task are rule-based, but these methods do not scale to the long tail. RL, on the other hand, is scalable and does not suffer from compounding errors like imitation learning. Contemporary RL approaches for driving use complex shaped rewards that sum multiple individual rewards, \eg~progress, position, or orientation rewards. We show that PPO fails to optimize a popular version of these rewards when the mini-batch size is increased, which limits the scalability of these approaches. Instead, we propose a new reward design based primarily on optimizing a single intuitive reward term: route completion. Infractions are penalized by terminating the episode or multiplicatively reducing route completion. We find that PPO scales well with higher mini-batch sizes when trained with our simple reward, even improving performance. Training with large mini-batch sizes enables efficient scaling via distributed data parallelism. We scale PPO to 300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The resulting model achieves 64 DS on the CARLA longest6 v2 benchmark, outperforming other RL methods with more complex rewards by a large margin. Requiring only minimal adaptations from its use in CARLA, the same method is the best learning-based approach on nuPlan. It scores 91.3 in non-reactive and 90.6 in reactive traffic on the Val14 benchmark while being an order of magnitude faster than prior work.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.2 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Reinforcement Learning: 3.7 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17862" target="_blank" rel="noopener noreferrer">Geodetic Set on Graphs of Constant Pathwidth and Feedback Vertex Set Number</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Prafullkumar Tale
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In the \textsc{Geodetic Set} problem, the input consists of a graph $G$ and a positive integer $k$. The goal is to determine whether there exists a subset $S$ of vertices of size $k$ such that every vertex in the graph is included in a shortest path between two vertices in $S$. Kellerhals and Koana </span>
                
                <span class="abstract-full" style="display: none;">In the \textsc{Geodetic Set} problem, the input consists of a graph $G$ and a positive integer $k$. The goal is to determine whether there exists a subset $S$ of vertices of size $k$ such that every vertex in the graph is included in a shortest path between two vertices in $S$. Kellerhals and Koana [IPEC 2020; J. Graph Algorithms Appl 2022] proved that the problem is $\W[1]$-hard when parameterized by the pathwidth and the feedback vertex set number of the input graph. They posed the question of whether the problem admits an $\XP$ algorithm when parameterized by the combination of these two parameters. We answer this in negative by proving that the problem remains \NP-hard on graphs of constant pathwidth and feedback vertex set number.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.0 -->
                    
                <!-- Medicine: 3.8 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Math: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Pathfinding: 2.1 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17868" target="_blank" rel="noopener noreferrer">Preserving Distances in Faulty Colored Graphs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Merav Parter, Asaf Petruschka
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study color fault-tolerant (CFT) network design problems: Given an $n$-vertex graph $G$ whose edges are arbitrarily colored (with no ``legality'' restrictions), the goal is to find a sparse subgraph $H$ such that, when any color fault causes all edges of some color $c$ to crash, the surviving sub</span>
                
                <span class="abstract-full" style="display: none;">We study color fault-tolerant (CFT) network design problems: Given an $n$-vertex graph $G$ whose edges are arbitrarily colored (with no ``legality'' restrictions), the goal is to find a sparse subgraph $H$ such that, when any color fault causes all edges of some color $c$ to crash, the surviving subgraph $H-c$ remains ``similar'' to the surviving graph $G-c$. The similarity is problem-dependent, usually pertaining to distance preserving. If each color class has size $\Delta$ or less, a brute-force approach can disregard the colors and take $H$ to be $\Delta$-edge fault-tolerant ($\Delta$-EFT), so that $H-F$ is similar to $G-F$ for every set $F$ of $\leq \Delta$ edges. We ask if the colors can be utilized to provide a sparser $H$.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.8 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- Networks: 3.7 -->
                    
                <!-- GNN: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17884" target="_blank" rel="noopener noreferrer">Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yongkang Li, Panagiotis Eustratiadis, Simon Lupart, Evangelos Kanoulas
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper concerns corpus poisoning attacks in dense information retrieval, where an adversary attempts to compromise the ranking performance of a search algorithm by injecting a small number of maliciously generated documents into the corpus. Our work addresses two limitations in the current liter</span>
                
                <span class="abstract-full" style="display: none;">This paper concerns corpus poisoning attacks in dense information retrieval, where an adversary attempts to compromise the ranking performance of a search algorithm by injecting a small number of maliciously generated documents into the corpus. Our work addresses two limitations in the current literature. First, attacks that perform adversarial gradient-based word substitution search do so in the discrete lexical space, while retrieval itself happens in the continuous embedding space. We thus propose an optimization method that operates in the embedding space directly. Specifically, we train a perturbation model with the objective of maintaining the geometric distance between the original and adversarial document embeddings, while also maximizing the token-level dissimilarity between the original and adversarial documents. Second, it is common for related work to have a strong assumption that the adversary has prior knowledge about the queries. In this paper, we focus on a more challenging variant of the problem where the adversary assumes no prior knowledge about the query distribution (hence, unsupervised). Our core contribution is an adversarial corpus attack that is fast and effective. We present comprehensive experimental results on both in- and out-of-domain datasets, focusing on two related tasks: a top-1 attack and a corpus poisoning attack. We consider attacks under both a white-box and a black-box setting. Notably, our method can generate successful adversarial examples in under two minutes per target document; four times faster compared to the fastest gradient-based word substitution methods in the literature with the same hardware. Furthermore, our adversarial generation method generates text that is more likely to occur under the distribution of natural text (low perplexity), and is therefore more difficult to detect.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.1 -->
                    
                <!-- Reinforcement Learning: 3.5 -->
                    
                <!-- Medicine: 2.6 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Pathfinding: 1.8 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17887" target="_blank" rel="noopener noreferrer">Searching in trees with heavy group sets of fixed size</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Micha{\l} Szyfelbein
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We consider the following generalization of the binary search problem: A searcher is required to find a hidden element $x$ in a tree $T$. To do so, they iteratively perform queries to an oracle about a chosen vertex $v$. After each such call, the oracle responds whether the target was found and if n</span>
                
                <span class="abstract-full" style="display: none;">We consider the following generalization of the binary search problem: A searcher is required to find a hidden element $x$ in a tree $T$. To do so, they iteratively perform queries to an oracle about a chosen vertex $v$. After each such call, the oracle responds whether the target was found and if not, the searcher receives as a reply the neighbor of $v$ that lays on the shortest path towards $x$. Additionally, each vertex $v$ may have a different query cost $w(v)$. The goal is to find the optimal querying strategy for the searcher which minimizes the worst case query cost required to find $x$. The problem is known to be NP-hard even in restricted classes of trees such as bounded diameter spiders [Cicalese et al. 2016] and no constant factor approximation algorithm is known for the general case. Inspired by recent studies [Dereniowski et al. 2022, Dereniowski et al. 2024], instead of restricted classes of trees, we explore restrictions on the weight function. We introduce the concept of a heavy group set of a vertex $HG(v,w)$. We show that if for every $v\in T$: $|HG(v,w)|\leq k$ an $O(\log\log n)$-approximation can be found within $2^{O(\log^2k)}\cdot\text{poly}(n)$ time.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 3.2 -->
                    
                <!-- Reinforcement Learning: 3.2 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Medicine: 3.1 -->
                    
                <!-- Math: 2.6 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Pathfinding: 1.6 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17889" target="_blank" rel="noopener noreferrer">Terrain-Aware Kinodynamic Planning with Efficiently Adaptive State Lattices for Mobile Robot Navigation in Off-Road Environments</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Eric R. Damm, Jason M. Gregory, Eli S. Lancaster, Felix A. Sanchez, Daniel M. Sahu, Thomas M. Howard
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">To safely traverse non-flat terrain, robots must account for the influence of terrain shape in their planned motions. Terrain-aware motion planners use an estimate of the vehicle roll and pitch as a function of pose, vehicle suspension, and ground elevation map to weigh the cost of edges in the sear</span>
                
                <span class="abstract-full" style="display: none;">To safely traverse non-flat terrain, robots must account for the influence of terrain shape in their planned motions. Terrain-aware motion planners use an estimate of the vehicle roll and pitch as a function of pose, vehicle suspension, and ground elevation map to weigh the cost of edges in the search space. Encoding such information in a traditional two-dimensional cost map is limiting because it is unable to capture the influence of orientation on the roll and pitch estimates from sloped terrain. The research presented herein addresses this problem by encoding kinodynamic information in the edges of a recombinant motion planning search space based on the Efficiently Adaptive State Lattice (EASL). This approach, which we describe as a Kinodynamic Efficiently Adaptive State Lattice (KEASL), differs from the prior representation in two ways. First, this method uses a novel encoding of velocity and acceleration constraints and vehicle direction at expanded nodes in the motion planning graph. Second, this approach describes additional steps for evaluating the roll, pitch, constraints, and velocities associated with poses along each edge during search in a manner that still enables the graph to remain recombinant. Velocities are computed using an iterative bidirectional method using Eulerian integration that more accurately estimates the duration of edges that are subject to terrain-dependent velocity limits. Real-world experiments on a Clearpath Robotics Warthog Unmanned Ground Vehicle were performed in a non-flat, unstructured environment. Results from 2093 planning queries from these experiments showed that KEASL provided a more efficient route than EASL in 83.72% of cases when EASL plans were adjusted to satisfy terrain-dependent velocity constraints. An analysis of relative runtimes and differences between planned routes is additionally presented.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.9 -->
                    
                <!-- Medicine: 4.2 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Pathfinding: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17895" target="_blank" rel="noopener noreferrer">POD-ROM methods: error analysis for continuous parametrized approximations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bosco Garc\'ia-Arcilla, Alicia Garc\'ia-Mascaraque, Julia Novo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper studies the numerical approximation of parametric time-dependent partial differential equations (PDEs) by proper orthogonal decomposition reduced order models (POD-ROMs). Although many papers in the literature consider reduced order models for parametric equations, a complete error analys</span>
                
                <span class="abstract-full" style="display: none;">This paper studies the numerical approximation of parametric time-dependent partial differential equations (PDEs) by proper orthogonal decomposition reduced order models (POD-ROMs). Although many papers in the literature consider reduced order models for parametric equations, a complete error analysis of the methods is still a challenge. We introduce and analyze in this paper a new POD method based on finite differences (respect to time and respect to the parameters that may be considered). We obtain a priori bounds for the new method valid for any value of time in a given time interval and any value of the parameter in a given parameter interval. Our design of the new POD method allow us to prove pointwise-in-time error estimates as opposed to average error bounds obtained typically in POD methods. Most of the papers concerning POD methods for parametric equations are just based on the snapshots computed at different times and parameter values instead of their difference quotients. We show that the error analysis of the present paper can also cover the error analysis of that case (that we call standard). Some numerical experiments compare our new approach with the standard one and support the error analysis.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.0 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Reinforcement Learning: 3.2 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- Math: 2.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17916" target="_blank" rel="noopener noreferrer">Non-distributive Lattices, Stable Matchings, and Linear Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christopher En, Yuri Faenza
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We show that all finite lattices, including non-distributive lattices, arise as stable matching lattices under standard assumptions on choice functions. In the process, we introduce new tools to reason on general lattices for optimization purposes: the partial representation of a lattice, which part</span>
                
                <span class="abstract-full" style="display: none;">We show that all finite lattices, including non-distributive lattices, arise as stable matching lattices under standard assumptions on choice functions. In the process, we introduce new tools to reason on general lattices for optimization purposes: the partial representation of a lattice, which partially extends Birkhoff's representation theorem to non-distributive lattices; the distributive closure of a lattice, which gives such a partial representation; and join constraints, which can be added to the distributive closure to obtain a representation for the original lattice. Then, we use these techniques to show that the minimum cost stable matching problem under the same standard assumptions on choice functions is NP-hard, by establishing a connection with antimatroid theory.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 3.9 -->
                    
                <!-- Networks: 3.3 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17919" target="_blank" rel="noopener noreferrer">Secured Encryption scheme based on the Ree groups</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gennady Khalimov, Yevgen Kotukh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">An improved design of a cryptosystem based on small Ree groups is proposed. We have changed the encryption algorithm and propose to use a logarithmic signature for the entire Ree group. This approach improves security against sequential key recovery attacks. Hence, the complexity of the key recovery</span>
                
                <span class="abstract-full" style="display: none;">An improved design of a cryptosystem based on small Ree groups is proposed. We have changed the encryption algorithm and propose to use a logarithmic signature for the entire Ree group. This approach improves security against sequential key recovery attacks. Hence, the complexity of the key recovery attack will be defined by a brute-force attack over the entire group. In this paper, we have proved that to construct secure cryptosystems with group computations over a small finite field, it is needed to use a 3-parametric small Ree group.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.6 -->
                    
                <!-- Networks: 3.2 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- Medicine: 2.9 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17955" target="_blank" rel="noopener noreferrer">Combinatorial Drone Searching</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ofek Gila (University of California, Irvine), Michael T. Goodrich (University of California, Irvine), Zahra Hadizadeh (University of Rochester), Daniel S. Hirschberg (University of California, Irvine), Shayan Taherijam (University of California, Irvine)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce and study the combinatorial drone searching problem, which we describe in terms of search strategies for finding one or more hikers lost in a forest. An aerial drone can issue a probe to send a signal a given distance such that if there is a lost hiker within this distance, then the dro</span>
                
                <span class="abstract-full" style="display: none;">We introduce and study the combinatorial drone searching problem, which we describe in terms of search strategies for finding one or more hikers lost in a forest. An aerial drone can issue a probe to send a signal a given distance such that if there is a lost hiker within this distance, then the drone will learn this. But the drone does not learn the direction or distance to the lost hiker. The optimization problem is to minimize the number of probes and/or hiker responses, as well as possibly minimizing the flight distance for the drone. We describe a number of efficient combinatorial drone searching strategies and we analyze each one in terms of the size, $n$, of the search domain. Moreover, we derive strong bounds for the constant factors for the search costs for our algorithms, which in some cases involve computer-assisted proofs. We also show how to extend these strategies to find all lost hikers using a simple, memoryless drone search, traveling a distance that is $\mathcal{O}(\log{k})$-competitive with the optimal traveling salesperson (TSP) tour for $k$ lost hikers.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.8 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- Networks: 3.2 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Pathfinding: 1.5 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18027" target="_blank" rel="noopener noreferrer">A Large Vision-Language Model based Environment Perception System for Visually Impaired People</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zezhou Chen, Zhaoxiang Liu, Kai Wang, Kohou Wang, Shiguo Lian
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">It is a challenging task for visually impaired people to perceive their surrounding environment due to the complexity of the natural scenes. Their personal and social activities are thus highly limited. This paper introduces a Large Vision-Language Model(LVLM) based environment perception system whi</span>
                
                <span class="abstract-full" style="display: none;">It is a challenging task for visually impaired people to perceive their surrounding environment due to the complexity of the natural scenes. Their personal and social activities are thus highly limited. This paper introduces a Large Vision-Language Model(LVLM) based environment perception system which helps them to better understand the surrounding environment, by capturing the current scene they face with a wearable device, and then letting them retrieve the analysis results through the device. The visually impaired people could acquire a global description of the scene by long pressing the screen to activate the LVLM output, retrieve the categories of the objects in the scene resulting from a segmentation model by tapping or swiping the screen, and get a detailed description of the objects they are interested in by double-tapping the screen. To help visually impaired people more accurately perceive the world, this paper proposes incorporating the segmentation result of the RGB image as external knowledge into the input of LVLM to reduce the LVLM's hallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the system could provide a more accurate description of the scene compared to Qwen-VL-Chat, exploratory experiments show that the system helps visually impaired people to perceive the surrounding environment effectively.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.8 -->
                    
                <!-- Medicine: 3.3 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- Math: 2.5 -->
                    
                <!-- LLMs: 2.4 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Pathfinding: 1.8 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18038" target="_blank" rel="noopener noreferrer">Optimal Secure Coded Distributed Computation over all Fields</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pedro Soto
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We construct optimal secure coded distributed schemes that extend the known optimal constructions over fields of characteristic 0 to all fields. A serendipitous result is that we can encode \emph{all} functions over finite fields with a recovery threshold proportional to the complexity (tensor rank </span>
                
                <span class="abstract-full" style="display: none;">We construct optimal secure coded distributed schemes that extend the known optimal constructions over fields of characteristic 0 to all fields. A serendipitous result is that we can encode \emph{all} functions over finite fields with a recovery threshold proportional to the complexity (tensor rank or multiplicative); this is due to the well-known result that all functions over a finite field can be represented as multivariate polynomials (or symmetric tensors). We get that a tensor of order $\ell$ (or a multivariate polynomial of degree $\ell$) can be computed in the faulty network of $N$ nodes setting within a factor of $\ell$ and an additive term depending on the genus of a code with $N$ rational points and distance covering the number of faulty servers; in particular, we present a coding scheme for general matrix multiplication of two $m \times m $ matrices with a recovery threshold of $2 m^{\omega } -1+g$ where $\omega $ is the exponent of matrix multiplication which is optimal for coding schemes using AG codes. Moreover, we give sufficient conditions for which the Hadamard-Shur product of general linear codes gives a similar recovery threshold, which we call \textit{log-additive codes}. Finally, we show that evaluation codes with a \textit{curve degree} function (first defined in [Ben-Sasson et al. (STOC '13)]) that have well-behaved zero sets are log-additive.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 4.6 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- Math: 3.5 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- GNN: 2.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18048" target="_blank" rel="noopener noreferrer">Modes of Sequence Models and Learning Coefficients</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhongtian Chen, Daniel Murfet
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We develop a geometric account of sequence modelling that links patterns in the data to measurable properties of the loss landscape in transformer networks. First, we cast conditional sequence distributions into a Hilbert-space framework and apply tensor decompositions to identify their principal mo</span>
                
                <span class="abstract-full" style="display: none;">We develop a geometric account of sequence modelling that links patterns in the data to measurable properties of the loss landscape in transformer networks. First, we cast conditional sequence distributions into a Hilbert-space framework and apply tensor decompositions to identify their principal modes. Truncating the small-amplitude modes yields an effective data distribution that preserves dominant structure while discarding statistical detail. Second, we show theoretically that Local Learning Coefficient (LLC) estimates are insensitive to modes below a data-dependent threshold. Consequently, the LLC calculated in practice characterises the geometry of the effective rather than the true distribution. This insight clarifies why reliable LLC estimates can be obtained even when a network parameter is not a strict minimiser of the population loss, and it highlights how the inverse temperature in SGLD acts as a resolution dial on the landscape structure.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.7 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Medicine: 2.9 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18050" target="_blank" rel="noopener noreferrer">Validating Network Protocol Parsers with Traceable RFC Document Interpretation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mingwei Zheng, Danning Xie, Qingkai Shi, Chengpeng Wang, Xiangyu Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Validating the correctness of network protocol implementations is highly challenging due to the oracle and traceability problems. The former determines when a protocol implementation can be considered buggy, especially when the bugs do not cause any observable symptoms. The latter allows developers </span>
                
                <span class="abstract-full" style="display: none;">Validating the correctness of network protocol implementations is highly challenging due to the oracle and traceability problems. The former determines when a protocol implementation can be considered buggy, especially when the bugs do not cause any observable symptoms. The latter allows developers to understand how an implementation violates the protocol specification, thereby facilitating bug fixes. Unlike existing works that rarely take both problems into account, this work considers both and provides an effective solution using recent advances in large language models (LLMs). Our key observation is that network protocols are often released with structured specification documents, a.k.a. RFC documents, which can be systematically translated to formal protocol message specifications via LLMs. Such specifications, which may contain errors due to the hallucination of LLMs, are used as a quasi-oracle to validate protocol parsers, while the validation results in return gradually refine the oracle. Since the oracle is derived from the document, any bugs we find in a protocol implementation can be traced back to the document, thus addressing the traceability problem. We have extensively evaluated our approach using nine network protocols and their implementations written in C, Python, and Go. The results show that our approach outperforms the state-of-the-art and has detected 69 bugs, with 36 confirmed. The project also demonstrates the potential for fully automating software validation based on natural language specifications, a process previously considered predominantly manual due to the need to understand specification documents and derive expected outputs for test inputs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.5 -->
                    
                <!-- LLMs: 4.3 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18056" target="_blank" rel="noopener noreferrer">Range-based 6-DoF Monte Carlo SLAM with Gradient-guided Particle Filter on GPU</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Takumi Nakao, Kenji Koide, Aoki Takanose, Shuji Oishi, Masashi Yokozuka, Hisashi Date
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents range-based 6-DoF Monte Carlo SLAM with a gradient-guided particle update strategy. While non-parametric state estimation methods, such as particle filters, are robust in situations with high ambiguity, they are known to be unsuitable for high-dimensional problems due to the curs</span>
                
                <span class="abstract-full" style="display: none;">This paper presents range-based 6-DoF Monte Carlo SLAM with a gradient-guided particle update strategy. While non-parametric state estimation methods, such as particle filters, are robust in situations with high ambiguity, they are known to be unsuitable for high-dimensional problems due to the curse of dimensionality. To address this issue, we propose a particle update strategy that improves the sampling efficiency by using the gradient information of the likelihood function to guide particles toward its mode. Additionally, we introduce a keyframe-based map representation that represents the global map as a set of past frames (i.e., keyframes) to mitigate memory consumption. The keyframe poses for each particle are corrected using a simple loop closure method to maintain trajectory consistency. The combination of gradient information and keyframe-based map representation significantly enhances sampling efficiency and reduces memory usage compared to traditional RBPF approaches. To process a large number of particles (e.g., 100,000 particles) in real-time, the proposed framework is designed to fully exploit GPU parallel processing. Experimental results demonstrate that the proposed method exhibits extreme robustness to state ambiguity and can even deal with kidnapping situations, such as when the sensor moves to different floors via an elevator, with minimal heuristics.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.9 -->
                    
                <!-- Networks: 3.9 -->
                    
                <!-- Medicine: 3.5 -->
                    
                <!-- Reinforcement Learning: 3.3 -->
                    
                <!-- GNN: 2.6 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18177" target="_blank" rel="noopener noreferrer">On the approximation of the von Neumann equation in the semi-classical limit. Part II : numerical analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fran\c{c}ois Golse (X), Francis Filbet (IMT)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper is devoted to the numerical analysis of the Hermite spectral method proposed in [14], which provides, in the semi-classical limit, an asymptotic preserving approximation of the von Neumann equation. More precisely, it relies on the use of so-called Weyl's variables to effectively address </span>
                
                <span class="abstract-full" style="display: none;">This paper is devoted to the numerical analysis of the Hermite spectral method proposed in [14], which provides, in the semi-classical limit, an asymptotic preserving approximation of the von Neumann equation. More precisely, it relies on the use of so-called Weyl's variables to effectively address the stiffness associated to the equation. Then by employing a truncated Hermite expansion of the density operator, we successfully manage this stiffness and provide error estimates by leveraging the propagation of regularity in the exact solution.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.3 -->
                    
                <!-- Reinforcement Learning: 4.0 -->
                    
                <!-- Math: 3.8 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Pathfinding: 2.4 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18206" target="_blank" rel="noopener noreferrer">A Machine Learning Approach For Bitcoin Forecasting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Stefano Sossi-Rojas, Gissel Velarde, Damian Zieba
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Bitcoin is one of the cryptocurrencies that is gaining more popularity in recent years. Previous studies have shown that closing price alone is not enough to forecast stock market series. We introduce a new set of time series and demonstrate that a subset is necessary to improve directional accuracy</span>
                
                <span class="abstract-full" style="display: none;">Bitcoin is one of the cryptocurrencies that is gaining more popularity in recent years. Previous studies have shown that closing price alone is not enough to forecast stock market series. We introduce a new set of time series and demonstrate that a subset is necessary to improve directional accuracy based on a machine learning ensemble. In our experiments, we study which time series and machine learning algorithms deliver the best results. We found that the most relevant time series that contribute to improving directional accuracy are Open, High and Low, with the largest contribution of Low in combination with an ensemble of Gated Recurrent Unit network and a baseline forecast. The relevance of other Bitcoin-related features that are not price-related is negligible. The proposed method delivers similar performance to the state-of-the-art when observing directional accuracy.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.9 -->
                    
                <!-- LLMs: 4.7 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- GNN: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18211" target="_blank" rel="noopener noreferrer">Dynamic Memory Management on GPUs with SYCL</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Russell K. Standish
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Dynamic memory allocation is not traditionally available in kernels running on GPUs. This work aims to build on Ouroboros, an efficient dynamic memory management library for CUDA applications, by porting the code to SYCL, a cross-platform accelerator API. Since SYCL can be compiled to a CUDA backend</span>
                
                <span class="abstract-full" style="display: none;">Dynamic memory allocation is not traditionally available in kernels running on GPUs. This work aims to build on Ouroboros, an efficient dynamic memory management library for CUDA applications, by porting the code to SYCL, a cross-platform accelerator API. Since SYCL can be compiled to a CUDA backend, it is possible to compare the performance of the SYCL implementation with that of the original CUDA implementation, as well as test it on non-CUDA platforms such as Intel's Xe graphics.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.4 -->
                    
                <!-- Quantum Computing: 4.4 -->
                    
                <!-- Networks: 3.0 -->
                    
                <!-- GNN: 2.7 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18247" target="_blank" rel="noopener noreferrer">Efficient Matching of Some Fundamental Regular Expressions with Backreferences</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Taisei Nogami, Tachio Terauchi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Regular expression matching is of practical importance due to its widespread use in real-world applications. In practical use, regular expressions are often used with real-world extensions. Accordingly, the matching problem of regular expressions with real-world extensions has been actively studied </span>
                
                <span class="abstract-full" style="display: none;">Regular expression matching is of practical importance due to its widespread use in real-world applications. In practical use, regular expressions are often used with real-world extensions. Accordingly, the matching problem of regular expressions with real-world extensions has been actively studied in recent years, yielding steady progress. However, backreference, a popular extension supported by most modern programming languages such as Java, Python, JavaScript and others in their standard libraries for string processing, is an exception to this positive trend. In fact, it is known that the matching problem of regular expressions with backreferences (rewbs) is theoretically hard and the existence of an asymptotically fast matching algorithm for arbitrary rewbs seems unlikely. Even among currently known partial solutions, the balance between efficiency and generality remains unsatisfactory. To bridge this gap, we present an efficient matching algorithm for rewbs of the form $e_0 (e)_1 e_1 \backslash 1 e_2$ where $e_0, e, e_1, e_2$ are pure regular expressions, which are fundamental and frequently used in practical applications. It runs in quadratic time with respect to the input string length, substantially improving the best-known cubic time complexity for these rewbs. Our algorithm combines ideas from both stringology and automata theory in a novel way. We leverage two techniques from automata theory, injection and summarization, to simultaneously examine matches whose backreferenced substrings are either a fixed right-maximal repeat or its extendable prefixes, which are concepts from stringology. By further utilizing a subtle property of extendable prefixes, our algorithm correctly decides the matching problem while achieving the quadratic-time complexity.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18262" target="_blank" rel="noopener noreferrer">Local Statistical Parity for the Estimation of Fair Decision Trees</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andrea Quintanilla, Johan Van Horebeek
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Given the high computational complexity of decision tree estimation, classical methods construct a tree by adding one node at a time in a recursive way. To facilitate promoting fairness, we propose a fairness criterion local to the tree nodes. We prove how it is related to the Statistical Parity cri</span>
                
                <span class="abstract-full" style="display: none;">Given the high computational complexity of decision tree estimation, classical methods construct a tree by adding one node at a time in a recursive way. To facilitate promoting fairness, we propose a fairness criterion local to the tree nodes. We prove how it is related to the Statistical Parity criterion, popular in the Algorithmic Fairness literature, and show how to incorporate it into standard recursive tree estimation algorithms.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.4 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Networks: 3.3 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Attention: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18277" target="_blank" rel="noopener noreferrer">Multiplicative Rewards in Markovian Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christel Baier, Krishnendu Chatterjee, Tobias Meggendorfer, Jakob Piribauer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper studies the expected value of multiplicative rewards, where rewards obtained in each step are multiplied (instead of the usual addition), in Markov chains (MCs) and Markov decision processes (MDPs). One of the key differences to additive rewards is that the expected value may diverge to i</span>
                
                <span class="abstract-full" style="display: none;">This paper studies the expected value of multiplicative rewards, where rewards obtained in each step are multiplied (instead of the usual addition), in Markov chains (MCs) and Markov decision processes (MDPs). One of the key differences to additive rewards is that the expected value may diverge to infinity not only due to recurrent, but also due to transient states. For MCs, computing the value is shown to be possible in polynomial time given an oracle for the comparison of succinctly represented integers (CSRI), which is only known to be solvable in polynomial time subject to number-theoretic conjectures. Interestingly, distinguishing whether the value is infinite or 0 is at least as hard as CSRI, while determining if it is one of these two can be done in polynomial time. In MDPs, the optimal value can be computed in polynomial space. Further refined complexity results and results on the complexity of optimal schedulers are presented. The techniques developed for MDPs additionally allow to solve the multiplicative variant of the stochastic shortest path problem. Finally, for MCs and MDPs where an absorbing state is reached almost surely, all considered problems are solvable in polynomial time.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.5 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- Math: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Robotics: 2.3 -->
                    
                <!-- Medicine: 2.2 -->
                    
                <!-- Pathfinding: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Hardware: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18322" target="_blank" rel="noopener noreferrer">Stable localized orthogonal decomposition in Raviart-Thomas spaces</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Patrick Henning, Hao Li, Timo Sprekeler
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This work proposes a computational multiscale method for the mixed formulation of a second-order linear elliptic equation subject to a homogeneous Neumann boundary condition, based on a stable localized orthogonal decomposition (LOD) in Raviart-Thomas finite element spaces. In the spirit of numerica</span>
                
                <span class="abstract-full" style="display: none;">This work proposes a computational multiscale method for the mixed formulation of a second-order linear elliptic equation subject to a homogeneous Neumann boundary condition, based on a stable localized orthogonal decomposition (LOD) in Raviart-Thomas finite element spaces. In the spirit of numerical homogenization, the construction provides low-dimensional coarse approximation spaces that incorporate fine-scale information from the heterogeneous coefficients by solving local patch problems on a fine mesh. The resulting numerical scheme is accompanied by a rigorous error analysis, and it is applicable beyond periodicity and scale-separation in spatial dimensions two and three. In particular, this novel realization circumvents the presence of pollution terms observed in a previous LOD construction for elliptic problems in mixed formulation. Finally, various numerical experiments are provided that demonstrate the performance of the method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.3 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18338" target="_blank" rel="noopener noreferrer">Computing Distances on Graph Associahedra is Fixed-parameter Tractable</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lu\'is Felipe I. Cunha, Ignasi Sau, U\'everton S. Souza, Mario Valencia-Pabon
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">An elimination tree of a connected graph $G$ is a rooted tree on the vertices of $G$ obtained by choosing a root $v$ and recursing on the connected components of $G-v$ to obtain the subtrees of $v$. The graph associahedron of $G$ is a polytope whose vertices correspond to elimination trees of $G$ an</span>
                
                <span class="abstract-full" style="display: none;">An elimination tree of a connected graph $G$ is a rooted tree on the vertices of $G$ obtained by choosing a root $v$ and recursing on the connected components of $G-v$ to obtain the subtrees of $v$. The graph associahedron of $G$ is a polytope whose vertices correspond to elimination trees of $G$ and whose edges correspond to tree rotations, a natural operation between elimination trees. These objects generalize associahedra, which correspond to the case where $G$ is a path. Ito et al. [ICALP 2023] recently proved that the problem of computing distances on graph associahedra is NP-hard. In this paper we prove that the problem, for a general graph $G$, is fixed-parameter tractable parameterized by the distance $k$. Prior to our work, only the case where $G$ is a path was known to be fixed-parameter tractable. To prove our result, we use a novel approach based on a marking scheme that restricts the search to a set of vertices whose size is bounded by a (large) function of $k$.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 3.3 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- GNN: 2.8 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18339" target="_blank" rel="noopener noreferrer">Optimal Control of Sensor-Induced Illusions on Robotic Agents</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lorenzo Medici, Steven M. LaValle, Basak Sakcak
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents a novel problem of creating and regulating localization and navigation illusions considering two agents: a receiver and a producer. A receiver is moving on a plane localizing itself using the intensity of signals from three known towers observed at its position. Based on this pos</span>
                
                <span class="abstract-full" style="display: none;">This paper presents a novel problem of creating and regulating localization and navigation illusions considering two agents: a receiver and a producer. A receiver is moving on a plane localizing itself using the intensity of signals from three known towers observed at its position. Based on this position estimate, it follows a simple policy to reach its goal. The key idea is that a producer alters the signal intensities to alter the position estimate of the receiver while ensuring it reaches a different destination with the belief that it reached its goal. We provide a precise mathematical formulation of this problem and show that it allows standard techniques from control theory to be applied to generate localization and navigation illusions that result in a desired receiver behavior.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.8 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Networks: 3.4 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18358" target="_blank" rel="noopener noreferrer">Convergence analysis of Lie and Strang splitting for operator-valued differential Riccati equations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Eskil Hansen, Tony Stillfjord, Teodor {\AA}berg
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Differential Riccati equations (DREs) are semilinear matrix- or operator-valued differential equations with quadratic non-linearities. They arise in many different areas, and are particularly important in optimal control of linear quadratic regulators, where they provide the optimal feedback control</span>
                
                <span class="abstract-full" style="display: none;">Differential Riccati equations (DREs) are semilinear matrix- or operator-valued differential equations with quadratic non-linearities. They arise in many different areas, and are particularly important in optimal control of linear quadratic regulators, where they provide the optimal feedback control laws. In the context of control of partial differential equations, these Riccati equations are operator-valued. To approximate their solutions, both spatial and temporal discretizations are needed. While the former have been well analyzed in the literature, there are very few rigorous convergence analyses of time stepping methods applied to DREs, particularly in the infinite-dimensional, operator-valued setting. In view of this, we analyze two numerical time-stepping schemes, the Lie and Strang splitting methods, in such a setting. The analysis relies on the assumption that the uncontrolled system evolves via an operator that generates an analytic semigroup, and that either the initial condition is sufficiently smooth, or the nonlinearity in the DRE is sufficiently smoothing. These assumptions are mild, in the sense that they are not enough to even guarantee continuity in operator-norm of the exact solution to the DRE. However, they imply certain regularity in a pointwise sense, which can be leveraged to prove convergence in operator-norm with the classical orders. The results are illustrated by four numerical experiments, where convergence with the expected order is correlated with the relevant assumptions being fulfilled. The experiments also demonstrate that matrix-valued DREs which arise as spatial discretizations of operator-valued DREs behave similarly, unless the discretization is coarse.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.1 -->
                    
                <!-- Reinforcement Learning: 4.0 -->
                    
                <!-- Math: 3.9 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Pathfinding: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18364" target="_blank" rel="noopener noreferrer">Achievable Rates and Error Probability Bounds of Frequency-based Channels of Unlimited Input Resolution</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ran Tamir, Nir Weinberger
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We consider a molecular channel, in which messages are encoded to the frequency of objects in a pool, and whose output during reading time is a noisy version of the input frequencies, as obtained by sampling with replacement from the pool. Motivated by recent DNA storage techniques, we focus on the </span>
                
                <span class="abstract-full" style="display: none;">We consider a molecular channel, in which messages are encoded to the frequency of objects in a pool, and whose output during reading time is a noisy version of the input frequencies, as obtained by sampling with replacement from the pool. Motivated by recent DNA storage techniques, we focus on the regime in which the input resolution is unlimited. We propose two error probability bounds for this channel; the first bound is based on random coding analysis of the error probability of the maximum likelihood decoder and the second bound is derived by code expurgation techniques. We deduce an achievable bound on the capacity of this channel, and compare it to both the achievable bounds under limited input resolution, as well as to a converse bound.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.2 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Math: 3.0 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- GNN: 1.0 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18394" target="_blank" rel="noopener noreferrer">Maximum Coverage in Turnstile Streams with Applications to Fingerprinting Measures</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alina Ene, Alessandro Epasto, Vahab Mirrokni, Hoai-An Nguyen, Huy L. Nguyen, David P. Woodruff, Peilin Zhong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In the maximum coverage problem we are given $d$ subsets from a universe $[n]$, and the goal is to output $k$ subsets such that their union covers the largest possible number of distinct items. We present the first algorithm for maximum coverage in the turnstile streaming model, where updates which </span>
                
                <span class="abstract-full" style="display: none;">In the maximum coverage problem we are given $d$ subsets from a universe $[n]$, and the goal is to output $k$ subsets such that their union covers the largest possible number of distinct items. We present the first algorithm for maximum coverage in the turnstile streaming model, where updates which insert or delete an item from a subset come one-by-one. Notably our algorithm only uses $poly\log n$ update time. We also present turnstile streaming algorithms for targeted and general fingerprinting for risk management where the goal is to determine which features pose the greatest re-identification risk in a dataset. As part of our work, we give a result of independent interest: an algorithm to estimate the complement of the $p^{\text{th}}$ frequency moment of a vector for $p \geq 2$. Empirical evaluation confirms the practicality of our fingerprinting algorithms demonstrating a speedup of up to $210$x over prior work.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.9 -->
                    
                <!-- Reinforcement Learning: 3.7 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Pathfinding: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18504" target="_blank" rel="noopener noreferrer">Information Freshness in Dynamic Gossip Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Arunabh Srivastava, Thomas Jacob Maranzatto, Sennur Ulukus
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We consider a source that shares updates with a network of $n$ gossiping nodes. The network's topology switches between two arbitrary topologies, with switching governed by a two-state continuous time Markov chain (CTMC) process. Information freshness is well-understood for static networks. This wor</span>
                
                <span class="abstract-full" style="display: none;">We consider a source that shares updates with a network of $n$ gossiping nodes. The network's topology switches between two arbitrary topologies, with switching governed by a two-state continuous time Markov chain (CTMC) process. Information freshness is well-understood for static networks. This work evaluates the impact of time-varying connections on information freshness. In order to quantify the freshness of information, we use the version age of information metric. If the two networks have static long-term average version ages of $f_1(n)$ and $f_2(n)$ with $f_1(n) \ll f_2(n)$, then the version age of the varying-topologies network is related to $f_1(n)$, $f_2(n)$, and the transition rates in the CTMC. If the transition rates in the CTMC are faster than $f_1(n)$, the average version age of the varying-topologies network is $f_1(n)$. Further, we observe that the behavior of a vanishingly small fraction of nodes can severely impact the long-term average version age of a network in a negative way. This motivates the definition of a typical set of nodes in the network. We evaluate the impact of fast and slow CTMC transition rates on the typical set of nodes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.0 -->
                    
                <!-- Math: 4.1 -->
                    
                <!-- Reinforcement Learning: 3.5 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Pathfinding: 2.1 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.18530" target="_blank" rel="noopener noreferrer">Scaling Laws For Scalable Oversight</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Joshua Engels, David D. Baek, Subhash Kantamneni, Max Tegmark
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Scalable oversight, the process by which weaker AI systems supervise stronger ones, has been proposed as a key strategy to control future superintelligent systems. However, it is still unclear how scalable oversight itself scales. To address this gap, we propose a framework that quantifies the proba</span>
                
                <span class="abstract-full" style="display: none;">Scalable oversight, the process by which weaker AI systems supervise stronger ones, has been proposed as a key strategy to control future superintelligent systems. However, it is still unclear how scalable oversight itself scales. To address this gap, we propose a framework that quantifies the probability of successful oversight as a function of the capabilities of the overseer and the system being overseen. Specifically, our framework models oversight as a game between capability-mismatched players; the players have oversight-specific and deception-specific Elo scores that are a piecewise-linear function of their general intelligence, with two plateaus corresponding to task incompetence and task saturation. We validate our framework with a modified version of the game Nim and then apply it to four oversight games: "Mafia", "Debate", "Backdoor Code" and "Wargames". For each game, we find scaling laws that approximate how domain performance depends on general AI system capability (using Chatbot Arena Elo as a proxy for general capability). We then build on our findings in a theoretical study of Nested Scalable Oversight (NSO), a process in which trusted models oversee untrusted stronger models, which then become the trusted models in the next step. We identify conditions under which NSO succeeds and derive numerically (and in some cases analytically) the optimal number of oversight levels to maximize the probability of oversight success. In our numerical examples, the NSO success rate is below 52% when overseeing systems that are 400 Elo points stronger than the baseline overseer, and it declines further for overseeing even stronger systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.9 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.17890" target="_blank" rel="noopener noreferrer">Quaternion Domain Super MDS for 3D Localization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Keigo Masuoka, Takumi Takahashi, Giuseppe Thadeu Freitas de Abreu, Hideki Ochiai
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a novel low-complexity three-dimensional (3D) localization algorithm for wireless sensor networks, termed quaternion-domain super multidimensional scaling (QD-SMDS). This algorithm reformulates the conventional SMDS, which was originally developed in the real domain, into the quaternion d</span>
                
                <span class="abstract-full" style="display: none;">We propose a novel low-complexity three-dimensional (3D) localization algorithm for wireless sensor networks, termed quaternion-domain super multidimensional scaling (QD-SMDS). This algorithm reformulates the conventional SMDS, which was originally developed in the real domain, into the quaternion domain. By representing 3D coordinates as quaternions, the method enables the construction of a rank-1 Gram edge kernel (GEK) matrix that integrates both relative distance and angular (phase) information between nodes, maximizing the noise reduction effect achieved through low-rank truncation via singular value decomposition (SVD). The simulation results indicate that the proposed method demonstrates a notable enhancement in localization accuracy relative to the conventional SMDS algorithm, particularly in scenarios characterized by substantial measurement errors.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- Networks: 4.0 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Pathfinding: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2211.05950" target="_blank" rel="noopener noreferrer">CR-LSO: Convex Neural Architecture Optimization in the Latent Space of Graph Variational Autoencoder with Input Convex Neural Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xuan Rao, Bo Zhao, Derong Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In neural architecture search (NAS) methods based on latent space optimization (LSO), a deep generative model is trained to embed discrete neural architectures into a continuous latent space. In this case, different optimization algorithms that operate in the continuous space can be implemented to s</span>
                
                <span class="abstract-full" style="display: none;">In neural architecture search (NAS) methods based on latent space optimization (LSO), a deep generative model is trained to embed discrete neural architectures into a continuous latent space. In this case, different optimization algorithms that operate in the continuous space can be implemented to search neural architectures. However, the optimization of latent variables is challenging for gradient-based LSO since the mapping from the latent space to the architecture performance is generally non-convex. To tackle this problem, this paper develops a convexity regularized latent space optimization (CR-LSO) method, which aims to regularize the learning process of latent space in order to obtain a convex architecture performance mapping. Specifically, CR-LSO trains a graph variational autoencoder (G-VAE) to learn the continuous representations of discrete architectures. Simultaneously, the learning process of latent space is regularized by the guaranteed convexity of input convex neural networks (ICNNs). In this way, the G-VAE is forced to learn a convex mapping from the architecture representation to the architecture performance. Hereafter, the CR-LSO approximates the performance mapping using the ICNN and leverages the estimated gradient to optimize neural architecture representations. Experimental results on three popular NAS benchmarks show that CR-LSO achieves competitive evaluation results in terms of both computational complexity and architecture performance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.8 -->
                    
                <!-- Networks: 3.4 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- LLMs: 1.9 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Pathfinding: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- SpikingNN: 1.3 -->
                    
                <!-- Multi-armed Bandit: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2307.16178" target="_blank" rel="noopener noreferrer">Quick Updates for the Perturbed Static Output Feedback Control Problem in Linear Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: MirSaleh Bahavarnia, Ahmad F. Taha
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces a method for efficiently updating a nominal stabilizing static output feedback (SOF) controller in perturbed linear systems. As operating points and state-space matrices change in dynamic systems, accommodating updates to the SOF controller are necessary. Traditional methods ad</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces a method for efficiently updating a nominal stabilizing static output feedback (SOF) controller in perturbed linear systems. As operating points and state-space matrices change in dynamic systems, accommodating updates to the SOF controller are necessary. Traditional methods address such changes by re-solving for the updated SOF gain, which is often \textit{(i)} computationally expensive due to the NP-hard nature of the problem or \textit{(ii)} infeasible due the limitations of its semidefinite programming relaxations. To overcome this, we leverage the concept of \textit{minimum destabilizing real perturbation} to formulate a norm minimization problem that yields fast, reliable controller updates. This approach accommodates a variety of known perturbations, including abrupt changes, model inaccuracies, and equilibrium-dependent linearizations. We also introduce geometric metrics to quantify the proximity to instability and rigorously define stability-guaranteed regions. Extensive numerical simulations validate the efficiency and robustness of the proposed method. We demonstrate the results on the SOF control of mutlti-machine power networks with changing operating points, and demonstrate that the computed quick updates produce comparable solutions to the SOF ones, while requiring orders of magnitude less time.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.0 -->
                    
                <!-- Medicine: 3.8 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2311.02050" target="_blank" rel="noopener noreferrer">Fast Approximation Algorithms for Piercing Boxes by Points</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pankaj K. Agarwal, Sariel Har-Peled, Rahul Raychaudhury, Stavros Sintos
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">$\newcommand{\popt}{{\mathcal{p}}} \newcommand{\Re}{\mathbb{R}}\newcommand{\N}{{\mathcal{N}}} \newcommand{\BX}{\mathcal{B}} \newcommand{\bb}{\mathsf{b}} \newcommand{\eps}{\varepsilon} \newcommand{\polylog}{\mathrm{polylog}} $ Let $\mathcal{B}=\{\mathsf{b}_1, \ldots ,\mathsf{b}_n\}$ be a set of $n$ a</span>
                
                <span class="abstract-full" style="display: none;">$\newcommand{\popt}{{\mathcal{p}}} \newcommand{\Re}{\mathbb{R}}\newcommand{\N}{{\mathcal{N}}} \newcommand{\BX}{\mathcal{B}} \newcommand{\bb}{\mathsf{b}} \newcommand{\eps}{\varepsilon} \newcommand{\polylog}{\mathrm{polylog}} $ Let $\mathcal{B}=\{\mathsf{b}_1, \ldots ,\mathsf{b}_n\}$ be a set of $n$ axis-aligned boxes in $\Re^d$ where $d\geq2$ is a constant. The \emph{piercing problem} is to compute a smallest set of points $\N \subset \Re^d$ that hits every box in $\mathcal{B}$, i.e., $\N\cap \mathsf{b}_i\neq \emptyset$, for $i=1,\ldots, n$. Let $\popt=\popt(\mathcal{B})$, the \emph{piercing number} be the minimum size of a piercing set of $\mathcal{B}$. We present a randomized $O(d^2\log\log \popt)$-approximation algorithm with expected running time $O(n^{d/2}\polylog n)$. Next, we present a faster $O(n^{\log d+1})$-time algorithm but with a slightly inferior approximation factor of $O(2^{4d}\log\log\popt)$. The running time of both algorithms can be improved to near-linear using a sampling-based technique, if $\popt = O(n^{1/d})$.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.9 -->
                    
                <!-- LLMs: 4.2 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- Networks: 3.5 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2401.05100" target="_blank" rel="noopener noreferrer">Sampled-Data Primal-Dual Gradient Dynamics in Model Predictive Control</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ryuta Moriyasu, Sho Kawaguchi, Kenji Kashima
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Model Predictive Control (MPC) is a versatile approach capable of accommodating diverse control requirements that holds significant promise for a broad spectrum of industrial applications. Noteworthy challenges associated with MPC include the substantial computational burden, which is sometimes cons</span>
                
                <span class="abstract-full" style="display: none;">Model Predictive Control (MPC) is a versatile approach capable of accommodating diverse control requirements that holds significant promise for a broad spectrum of industrial applications. Noteworthy challenges associated with MPC include the substantial computational burden, which is sometimes considered excessive even for linear systems. Recently, a rapid computation method that guides the input toward convergence with the optimal control problem solution by employing primal-dual gradient (PDG) dynamics as a controller has been proposed for linear MPCs. However, stability has been ensured under the assumption that the controller is a continuous-time system, leading to potential instability when the controller undergoes discretization and is implemented as a sampled-data system. In this paper, we propose a discrete-time dynamical controller, incorporating specific modifications to the PDG approach, and present stability conditions relevant to the resulting sampled-data system. Additionally, we introduce an extension designed to enhance control performance, that was traded off in the original. Numerical examples substantiate that our proposed method, which can be executed in only 1 $\mu$s in a standard laptop, not only ensures stability with considering sampled-data implementation but also effectively enhances control performance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.0 -->
                    
                <!-- GNN: 3.9 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Networks: 3.5 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2405.16874" target="_blank" rel="noopener noreferrer">CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xingqun Qi, Hengyuan Zhang, Yatian Wang, Jiahao Pan, Chen Liu, Peng Li, Xiaowei Chi, Mengfei Li, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data. In this paper, we propose CoCoGesture, a novel framework enablin</span>
                
                <span class="abstract-full" style="display: none;">Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data. In this paper, we propose CoCoGesture, a novel framework enabling vivid and diverse gesture synthesis from unseen human speech prompts. Our key insight is built upon the custom-designed pretrain-fintune training paradigm. At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold. Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-speech 3D gesture dataset containing more than 40M meshed posture instances across 4.3K speakers, dubbed GES-X. Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts. At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation. Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model. Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human speech and the gesture features from the pre-trained gesture experts with a routing mechanism. Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation. Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot speech-to-gesture generation. The dataset will be publicly available at: https://mattie-e.github.io/GES-X/</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.8 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- LLMs: 2.7 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2406.02105" target="_blank" rel="noopener noreferrer">Can Kernel Methods Explain How the Data Affects Neural Collapse?</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vignesh Kothapalli, Tom Tirer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A vast amount of literature has recently focused on the "Neural Collapse" (NC) phenomenon, which emerges when training neural network (NN) classifiers beyond the zero training error point. The core component of NC is the decrease in the within-class variability of the network's deepest features, dub</span>
                
                <span class="abstract-full" style="display: none;">A vast amount of literature has recently focused on the "Neural Collapse" (NC) phenomenon, which emerges when training neural network (NN) classifiers beyond the zero training error point. The core component of NC is the decrease in the within-class variability of the network's deepest features, dubbed as NC1. The theoretical works that study NC are typically based on simplified unconstrained features models (UFMs) that mask any effect of the data on the extent of collapse. To address this limitation of UFMs, this paper explores the possibility of analyzing NC1 using kernels associated with shallow NNs. We begin by formulating an NC1 metric as a function of the kernel. Then, we specialize it to the NN Gaussian Process kernel (NNGP) and the Neural Tangent Kernel (NTK), associated with wide networks at initialization and during gradient-based training with a small learning rate, respectively. As a key result, we show that the NTK does not represent more collapsed features than the NNGP for Gaussian data of arbitrary dimensions. This showcases the limitations of data-independent kernels such as NTK in approximating the NC behavior of NNs. As an alternative to NTK, we then empirically explore a recently proposed data-aware Gaussian Process kernel, which generalizes NNGP to model feature learning. We show that this kernel yields lower NC1 than NNGP but may not follow the trends of the shallow NN. Our study demonstrates that adaptivity to data may allow kernel-based analysis of NC, though further advancements in this area are still needed. A nice byproduct of our study is showing both theoretically and empirically that the choice of nonlinear activation function affects NC1 (with ERF yielding lower values than ReLU). The code is available at: https://github.com/kvignesh1420/shallow_nc1</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.5 -->
                    
                <!-- Reinforcement Learning: 4.2 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Math: 3.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2409.05685" target="_blank" rel="noopener noreferrer">On the role of the signature transform in nonlinear systems and data-driven control</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Anna Scampicchio, Melanie N. Zeilinger
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Classic control techniques typically rely on a model of the system's response to external inputs, which is difficult to obtain from first principles especially if the unknown dynamics are nonlinear. In this paper, we address this issue by presenting an approach based on the so-called signature trans</span>
                
                <span class="abstract-full" style="display: none;">Classic control techniques typically rely on a model of the system's response to external inputs, which is difficult to obtain from first principles especially if the unknown dynamics are nonlinear. In this paper, we address this issue by presenting an approach based on the so-called signature transform, a tool that is still largely unexplored in data-driven control. We first show that the signature provides rigorous and practically effective features to represent and predict system trajectories. Furthermore, we propose a novel use of this tool on an output-matching problem, paving the way for signature-based, data-driven predictive control.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.9 -->
                    
                <!-- LLMs: 4.2 -->
                    
                <!-- Reinforcement Learning: 3.7 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2409.11779" target="_blank" rel="noopener noreferrer">Evolving Distributions Under Local Motion</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aditya Acharya, David M. Mount
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Geometric data sets arising in modern applications are often very large and change dynamically over time. A popular framework for dealing with such data sets is the evolving data framework, where a discrete structure continuously varies over time due to the unseen actions of an evolver, which makes </span>
                
                <span class="abstract-full" style="display: none;">Geometric data sets arising in modern applications are often very large and change dynamically over time. A popular framework for dealing with such data sets is the evolving data framework, where a discrete structure continuously varies over time due to the unseen actions of an evolver, which makes small changes to the data. An algorithm probes the current state through an oracle, and the objective is to maintain a hypothesis of the data set's current state that is close to its actual state at all times. In this paper, we apply this framework to maintaining a set of $n$ point objects in motion in $d$-dimensional Euclidean space. To model the uncertainty in the object locations, both the ground truth and hypothesis are based on spatial probability distributions, and the distance between them is measured by the Kullback-Leibler divergence (relative entropy). We introduce a simple and intuitive motion model where with each time step, the distance that any object can move is a fraction of the distance to its nearest neighbor. We present an algorithm that, in steady state, guarantees a distance of $O(n)$ between the true and hypothesized placements. We also show that for any algorithm in this model, there is an evolver that can generate a distance of $\Omega(n)$, implying that our algorithm is asymptotically optimal.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.5 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- Medicine: 2.9 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Pathfinding: 1.9 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2409.12007" target="_blank" rel="noopener noreferrer">Real-Time-Feasible Collision-Free Motion Planning For Ellipsoidal Objects</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yunfan Gao, Florian Messerer, Niels van Duijkeren, Boris Houska, Moritz Diehl
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Online planning of collision-free trajectories is a fundamental task for robotics and self-driving car applications. This paper revisits collision avoidance between ellipsoidal objects using differentiable constraints. Two ellipsoids do not overlap if and only if the endpoint of the vector between t</span>
                
                <span class="abstract-full" style="display: none;">Online planning of collision-free trajectories is a fundamental task for robotics and self-driving car applications. This paper revisits collision avoidance between ellipsoidal objects using differentiable constraints. Two ellipsoids do not overlap if and only if the endpoint of the vector between the center points of the ellipsoids does not lie in the interior of the Minkowski sum of the ellipsoids. This condition is formulated using a parametric over-approximation of the Minkowski sum, which can be made tight in any given direction. The resulting collision avoidance constraint is included in an optimal control problem (OCP) and evaluated in comparison to the separating-hyperplane approach. Not only do we observe that the Minkowski-sum formulation is computationally more efficient in our experiments, but also that using pre-determined over-approximation parameters based on warm-start trajectories leads to a very limited increase in suboptimality. This gives rise to a novel real-time scheme for collision-free motion planning with model predictive control (MPC). Both the real-time feasibility and the effectiveness of the constraint formulation are demonstrated in challenging real-world experiments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- LLMs: 4.3 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Robotics: 2.4 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Pathfinding: 2.0 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2409.16048" target="_blank" rel="noopener noreferrer">Whole-body End-Effector Pose Tracking</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tifanny Portela, Andrei Cramariuc, Mayank Mittal, Marco Hutter
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Combining manipulation with the mobility of legged robots is essential for a wide range of robotic applications. However, integrating an arm with a mobile base significantly increases the system's complexity, making precise end-effector control challenging. Existing model-based approaches are often </span>
                
                <span class="abstract-full" style="display: none;">Combining manipulation with the mobility of legged robots is essential for a wide range of robotic applications. However, integrating an arm with a mobile base significantly increases the system's complexity, making precise end-effector control challenging. Existing model-based approaches are often constrained by their modeling assumptions, leading to limited robustness. Meanwhile, recent Reinforcement Learning (RL) implementations restrict the arm's workspace to be in front of the robot or track only the position to obtain decent tracking accuracy. In this work, we address these limitations by introducing a whole-body RL formulation for end-effector pose tracking in a large workspace on rough, unstructured terrains. Our proposed method involves a terrain-aware sampling strategy for the robot's initial configuration and end-effector pose commands, as well as a game-based curriculum to extend the robot's operating range. We validate our approach on the ANYmal quadrupedal robot with a six DoF robotic arm. Through our experiments, we show that the learned controller achieves precise command tracking over a large workspace and adapts across varying terrains such as stairs and slopes. On deployment, it achieves a pose-tracking error of 2.64 cm and 3.64 degrees, outperforming existing competitive baselines.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.5 -->
                    
                <!-- Reinforcement Learning: 3.3 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2410.15546" target="_blank" rel="noopener noreferrer">Improved Contact Graph Routing in Delay Tolerant Networks with Capacity and Buffer Constraints</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tania Alhajj, Vincent Corlay
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Satellite communications present challenging characteristics. Continuous end-to-end connectivity may not be available due to the large distances between satellites. Moreover, resources such as link capacity and buffer memory may be limited. Routing in satellite networks is therefore both complex and</span>
                
                <span class="abstract-full" style="display: none;">Satellite communications present challenging characteristics. Continuous end-to-end connectivity may not be available due to the large distances between satellites. Moreover, resources such as link capacity and buffer memory may be limited. Routing in satellite networks is therefore both complex and crucial to avoid packet losses and long delays. The Delay Tolerant Network (DTN) paradigm has emerged as an efficient solution for managing these challenging networks. Contact Graph Routing (CGR), a deterministic routing algorithm, is one of the most popular DTN algorithms. CGR is compatible with the ``store, carry, and forward" principle, whereby a node receives a message and stores it in its buffer until a transmission opportunity becomes available. However, CGR relies on simplified models to incorporate potential constraints in the route search. For instance, the linear volume assumption is often used to consider capacity constraints. Moreover, capacity management and buffer management are mostly performed during the forwarding phase, once an issue has occurred. In this paper, we propose taking measures before or during the route search in order to find routes that respect both contact-capacity and node-buffer limits. We introduce the contact splitting and edge pruning operations to effectively account for the routing constraints. This ensures that CGR outputs the optimal solution among the subset of valid solutions. The proposed approach can also be used to book resources to be used in case of issues during the forwarding phase.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.8 -->
                    
                <!-- Medicine: 3.8 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2412.08988" target="_blank" rel="noopener noreferrer">EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gaoxiang Cong, Jiadong Pan, Liang Li, Yuankai Qi, Yuxin Peng, Anton van den Hengel, Jian Yang, Qingming Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Given a piece of text, a video clip, and a reference audio, the movie dubbing task aims to generate speech that aligns with the video while cloning the desired voice. The existing methods have two primary deficiencies: (1) They struggle to simultaneously hold audio-visual sync and achieve clear pron</span>
                
                <span class="abstract-full" style="display: none;">Given a piece of text, a video clip, and a reference audio, the movie dubbing task aims to generate speech that aligns with the video while cloning the desired voice. The existing methods have two primary deficiencies: (1) They struggle to simultaneously hold audio-visual sync and achieve clear pronunciation; (2) They lack the capacity to express user-defined emotions. To address these problems, we propose EmoDubber, an emotion-controllable dubbing architecture that allows users to specify emotion type and emotional intensity while satisfying high-quality lip sync and pronunciation. Specifically, we first design Lip-related Prosody Aligning (LPA), which focuses on learning the inherent consistency between lip motion and prosody variation by duration level contrastive learning to incorporate reasonable alignment. Then, we design Pronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences by efficient conformer to improve speech intelligibility. Next, the speaker identity adapting module aims to decode acoustics prior and inject the speaker style embedding. After that, the proposed Flow-based User Emotion Controlling (FUEC) is used to synthesize waveform by flow matching prediction network conditioned on acoustics prior. In this process, the FUEC determines the gradient direction and guidance scale based on the user's emotion instructions by the positive and negative guidance mechanism, which focuses on amplifying the desired emotion while suppressing others. Extensive experimental results on three benchmark datasets demonstrate favorable performance compared to several state-of-the-art methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- Reinforcement Learning: 4.2 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- 3D: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2501.18308" target="_blank" rel="noopener noreferrer">Zero Estimation Cost Strategy for Witsenhausen Counterexample with Causal Encoder</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mengyuan Zhao, Tobias J. Oechtering, Ma\"el Le Treust
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a zero estimation cost (ZEC) scheme for causal-encoding noncausal-decoding vector-valued Witsenhausen counterexample based on the coordination coding result. In contrast to source coding, our goal is to communicate a controlled system state. The introduced ZEC scheme is a joint control-co</span>
                
                <span class="abstract-full" style="display: none;">We propose a zero estimation cost (ZEC) scheme for causal-encoding noncausal-decoding vector-valued Witsenhausen counterexample based on the coordination coding result. In contrast to source coding, our goal is to communicate a controlled system state. The introduced ZEC scheme is a joint control-communication approach that transforms the system state into a sequence that can be efficiently communicated using block coding. Numerical results show that our approach significantly reduces the power required for achieving zero-estimation-cost state reconstruction at the decoder. In the second part, we introduce a more general non-zero estimation cost (Non-ZEC) scheme. We observe numerically that the Non-ZEC scheme operates as a time-sharing mechanism between the two-point strategy and the ZEC scheme. Overall, by leveraging block-coding gain, our proposed methods substantially improve the power-estimation trade-off for Witsenhausen counterexample.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- Networks: 4.6 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Multi-armed Bandit: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2502.17432" target="_blank" rel="noopener noreferrer">FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jason Jingzhou Liu, Yulong Li, Kenneth Shaw, Tony Tao, Ruslan Salakhutdinov, Deepak Pathak
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Many contact-rich tasks humans perform, such as box pickup or rolling dough, rely on force feedback for reliable execution. However, this force information, which is readily available in most robot arms, is not commonly used in teleoperation and policy learning. Consequently, robot behavior is often</span>
                
                <span class="abstract-full" style="display: none;">Many contact-rich tasks humans perform, such as box pickup or rolling dough, rely on force feedback for reliable execution. However, this force information, which is readily available in most robot arms, is not commonly used in teleoperation and policy learning. Consequently, robot behavior is often limited to quasi-static kinematic tasks that do not require intricate force-feedback. In this paper, we first present a low-cost, intuitive, bilateral teleoperation setup that relays external forces of the follower arm back to the teacher arm, facilitating data collection for complex, contact-rich tasks. We then introduce FACTR, a policy learning method that employs a curriculum which corrupts the visual input with decreasing intensity throughout training. The curriculum prevents our transformer-based policy from over-fitting to the visual input and guides the policy to properly attend to the force modality. We demonstrate that by fully utilizing the force information, our method significantly improves generalization to unseen objects by 43\% compared to baseline approaches without a curriculum. Video results, codebases, and instructions at https://jasonjzliu.com/factr/</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.7 -->
                    
                <!-- Reinforcement Learning: 3.9 -->
                    
                <!-- Medicine: 3.3 -->
                    
                <!-- GNN: 3.2 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2211.14708" target="_blank" rel="noopener noreferrer">Identifying Chemicals Through Dimensionality Reduction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Emile Anand, Charles Steinhardt, Martin Hansen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Civilizations have tried to make drinking water safe to consume for thousands of years. The process of determining water contaminants has evolved with the complexity of the contaminants due to pesticides and heavy metals. The routine procedure to determine water safety is to use targeted analysis wh</span>
                
                <span class="abstract-full" style="display: none;">Civilizations have tried to make drinking water safe to consume for thousands of years. The process of determining water contaminants has evolved with the complexity of the contaminants due to pesticides and heavy metals. The routine procedure to determine water safety is to use targeted analysis which searches for specific substances from some known list; however, we do not explicitly know which substances should be on this list. Before experimentally determining which substances are contaminants, how do we answer the sampling problem of identifying all the substances in the water? Here, we present an approach that builds on the work of Jaanus Liigand et al., which used non-targeted analysis that conducts a broader search on the sample to develop a random-forest regression model, to predict the names of all the substances in a sample, as well as their respective concentrations[1]. This work utilizes techniques from dimensionality reduction and linear decompositions to present a more accurate model using data from the European Massbank Metabolome Library to produce a global list of chemicals that researchers can then identify and test for when purifying water.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.1 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2408.08990" target="_blank" rel="noopener noreferrer">Adaptive Uncertainty Quantification for Generative AI</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jungeum Kim, Sean O'Hagan, Veronika Rockova
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This work is concerned with conformal prediction in contemporary applications (including generative AI) where a black-box model has been trained on data that are not accessible to the user. Mirroring split-conformal inference, we design a wrapper around a black-box algorithm which calibrates conform</span>
                
                <span class="abstract-full" style="display: none;">This work is concerned with conformal prediction in contemporary applications (including generative AI) where a black-box model has been trained on data that are not accessible to the user. Mirroring split-conformal inference, we design a wrapper around a black-box algorithm which calibrates conformity scores. This calibration is local and proceeds in two stages by first adaptively partitioning the predictor space into groups and then calibrating sectionally group by group. Adaptive partitioning (self-grouping) is achieved by fitting a robust regression tree to the conformity scores on the calibration set. This new tree variant is designed in such a way that adding a single new observation does not change the tree fit with overwhelmingly large probability. This add-one-in robustness property allows us to conclude a finite sample group-conditional coverage guarantee, a refinement of the marginal guarantee. In addition, unlike traditional split-conformal inference, adaptive splitting and within-group calibration yields adaptive bands which can stretch and shrink locally. We demonstrate benefits of local tightening on several simulated as well as real examples using non-parametric regression. Finally, we consider two contemporary classification applications for obtaining uncertainty quantification around GPT-4o predictions. We conformalize skin disease diagnoses based on self-reported symptoms as well as predicted states of U.S. legislators based on summaries of their ideology. We demonstrate substantial local tightening of the uncertainty sets while attaining similar marginal coverage.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.6 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0414
                </span>
                <a href="https://arxiv.org/abs/2504.18459" target="_blank" rel="noopener noreferrer">Probabilistic Shaping in MIMO: Going Beyond 1.53dB AWGN Gain With the Non-Linear Demapper</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kirill Ivanov, Wei Yang, Jing Jiang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Constellation shaping is a well-established method to improve upon a regular quadrature amplitude modulation (QAM). It is known that the gain achieved by any shaping method for an additive white Gaussian noise (AWGN) channel is upper-bounded by 1.53dB. However, the situation becomes less clear in th</span>
                
                <span class="abstract-full" style="display: none;">Constellation shaping is a well-established method to improve upon a regular quadrature amplitude modulation (QAM). It is known that the gain achieved by any shaping method for an additive white Gaussian noise (AWGN) channel is upper-bounded by 1.53dB. However, the situation becomes less clear in the multiple-input and multiple-output (MIMO) setting.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.8 -->
                    
                <!-- Medicine: 4.2 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Networks: 3.2 -->
                    
                <!-- GNN: 2.7 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0419
                </span>
                <a href="https://arxiv.org/abs/2402.03145" target="_blank" rel="noopener noreferrer">SafEDMD: A Koopman-based data-driven controller design framework for nonlinear dynamical systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Robin Str\"asser, Manuel Schaller, Karl Worthmann, Julian Berberich, Frank Allg\"ower
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD). In this paper, we propose SafEDMD, a novel stability- and certificate-oriented EDMD-based control</span>
                
                <span class="abstract-full" style="display: none;">The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD). In this paper, we propose SafEDMD, a novel stability- and certificate-oriented EDMD-based controller design framework. Our approach leverages a reliable surrogate model generated in a data-driven fashion in order to provide closed-loop guarantees. In particular, we establish a controller design based on semi-definite programming with guaranteed stabilization of the underlying nonlinear system. As central ingredient, we derive proportional error bounds that vanish at the origin and are tailored to control tasks. We illustrate the developed method by means of several benchmark examples and highlight the advantages over state-of-the-art methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.0 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.047
                </span>
                <a href="https://arxiv.org/abs/2504.17891" target="_blank" rel="noopener noreferrer">Do We Need Transformers to Play FPS Video Games?</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Karmanbir Batth, Krish Sethi, Aly Shariff, Leo Shi, Hetul Patel
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we explore the Transformer based architectures for reinforcement learning in both online and offline settings within the Doom game environment. Our investigation focuses on two primary approaches: Deep Transformer Q- learning Networks (DTQN) for online learning and Decision Transforme</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we explore the Transformer based architectures for reinforcement learning in both online and offline settings within the Doom game environment. Our investigation focuses on two primary approaches: Deep Transformer Q- learning Networks (DTQN) for online learning and Decision Transformers (DT) for offline reinforcement learning. DTQN leverages the sequential modelling capabilities of Transformers to enhance Q-learning in partially observable environments,while Decision Transformers repurpose sequence modelling techniques to enable offline agents to learn from past trajectories without direct interaction with the environment. We conclude that while Transformers might have performed well in Atari games, more traditional methods perform better than Transformer based method in both the settings in the VizDoom environment.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.1 -->
                    
                <!-- Reinforcement Learning: 4.2 -->
                    
                <!-- Medicine: 4.0 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Attention: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0479
                </span>
                <a href="https://arxiv.org/abs/2504.18082" target="_blank" rel="noopener noreferrer">Efficient GNN Training Through Structure-Aware Randomized Mini-Batching</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vignesh Balaji, Christos Kozyrakis, Gal Chechik, Haggai Maron
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph Neural Networks (GNNs) enable learning on realworld graphs and mini-batch training has emerged as the de facto standard for training GNNs because it can scale to very large graphs and improve convergence. Current mini-batch construction policies largely ignore efficiency considerations of GNN </span>
                
                <span class="abstract-full" style="display: none;">Graph Neural Networks (GNNs) enable learning on realworld graphs and mini-batch training has emerged as the de facto standard for training GNNs because it can scale to very large graphs and improve convergence. Current mini-batch construction policies largely ignore efficiency considerations of GNN training. Specifically, existing mini-batching techniques employ randomization schemes to improve accuracy and convergence. However, these randomization schemes are often agnostic to the structural properties of the graph (for eg. community structure), resulting in highly irregular memory access patterns during GNN training that make suboptimal use of on-chip GPU caches. On the other hand, while deterministic mini-batching based solely on graph structure delivers fast runtime performance, the lack of randomness compromises both the final model accuracy and training convergence speed. In this paper, we present Community-structure-aware Randomized Mini-batching (COMM-RAND), a novel methodology that bridges the gap between the above extremes. COMM-RAND allows practitioners to explore the space between pure randomness and pure graph structural awareness during mini-batch construction, leading to significantly more efficient GNN training with similar accuracy. We evaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND cuts down GNN training time by up to 2.76x (1.8x on average) while achieving an accuracy that is within 1.79% points (0.42% on average) compared to popular random mini-batching approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.8 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- GNN: 4.0 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0483
                </span>
                <a href="https://arxiv.org/abs/2504.17809" target="_blank" rel="noopener noreferrer">Monero Peer-to-peer Network Topology Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yu Gao, Yu Zhang, Matija Pi\v{s}korec, Claudio J. Tessone
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Monero, a privacy-focused cryptocurrency, employs a decentralized peer-to-peer (P2P) network that plays a critical role in transaction propagation and consensus formation. While much research has explored Monero's privacy transaction mechanisms, its underlying P2P network architecture has remained r</span>
                
                <span class="abstract-full" style="display: none;">Monero, a privacy-focused cryptocurrency, employs a decentralized peer-to-peer (P2P) network that plays a critical role in transaction propagation and consensus formation. While much research has explored Monero's privacy transaction mechanisms, its underlying P2P network architecture has remained relatively underexplored. In this study, building on our recent work on Monero network detection, we further investigate the network topology of Monero's P2P structure, which has evolved following recent protocol updates that enhanced security by obscuring peer information. Using k-core decomposition, we confirm that the Monero network exhibits a core-periphery structure, where a tightly interconnected core of supernodes is crucial for maintaining network cohesion, while peripheral nodes rely on these core nodes for connectivity. This structure explains why targeting central nodes does not easily lead to the rapid disintegration of the network's largest connected component while also providing a deeper understanding of the true architecture of Monero's peer protocol.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.3 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- GNN: 2.9 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0484
                </span>
                <a href="https://arxiv.org/abs/2504.17973" target="_blank" rel="noopener noreferrer">Toward Low-Latency Services over PON using OCDMA Private Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Steevy J. Cordette
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">An low-latency service scheme is proposed over Passive Optical Network (PON). The Optical Code Division Multiplexing Access (OCDMA) technique is used to define multiple private networks serving as Virtual GE-PON that mimic the service-based VLAN (S-VLAN) in the optical domain.</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.3 -->
                    
                <!-- Quantum Computing: 4.6 -->
                    
                <!-- Medicine: 4.2 -->
                    
                <!-- GNN: 3.8 -->
                    
                <!-- Networks: 3.6 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0485
                </span>
                <a href="https://arxiv.org/abs/2504.18150" target="_blank" rel="noopener noreferrer">Toward Automated Test Generation for Dockerfiles Based on Analysis of Docker Image Layers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yuki Goto, Shinsuke Matsumoto, Shinji Kusumoto
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Docker has gained attention as a lightweight container-based virtualization platform. The process for building a Docker image is defined in a text file called a Dockerfile. A Dockerfile can be considered as a kind of source code that contains instructions on how to build a Docker image. Its behavior</span>
                
                <span class="abstract-full" style="display: none;">Docker has gained attention as a lightweight container-based virtualization platform. The process for building a Docker image is defined in a text file called a Dockerfile. A Dockerfile can be considered as a kind of source code that contains instructions on how to build a Docker image. Its behavior should be verified through testing, as is done for source code in a general programming language. For source code in languages such as Java, search-based test generation techniques have been proposed. However, existing automated test generation techniques cannot be applied to Dockerfiles. Since a Dockerfile does not contain branches, the coverage metric, typically used as an objective function in existing methods, becomes meaningless. In this study, we propose an automated test generation method for Dockerfiles based on processing results rather than processing steps. The proposed method determines which files should be tested and generates the corresponding tests based on an analysis of Dockerfile instructions and Docker image layers. The experimental results show that the proposed method can reproduce over 80% of the tests created by developers.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.3 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0489
                </span>
                <a href="https://arxiv.org/abs/2504.18127" target="_blank" rel="noopener noreferrer">Salient Region-Guided Spacecraft Image Arbitrary-Scale Super-Resolution Network</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jingfan Yang, Hu Gao, Ying Zhang, Depeng Dang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large </span>
                
                <span class="abstract-full" style="display: none;">Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large black space background, introducing irrelevant noise. In this paper, we propose a salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR), which uses features from the spacecraft core salient regions to guide latent modulation and achieve arbitrary-scale super-resolution. Specifically, we design a spacecraft core region recognition block (SCRRB) that identifies the core salient regions in spacecraft images using a pre-trained saliency detection model. Furthermore, we present an adaptive-weighted feature fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft core region features with general image features by dynamic weight parameter to enhance the response of the core salient regions. Experimental results demonstrate that the proposed SGSASR outperforms state-of-the-art approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.4 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0514
                </span>
                <a href="https://arxiv.org/abs/2504.18161" target="_blank" rel="noopener noreferrer">Fully Dynamic Algorithms for Transitive Reduction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gramoz Goranci, Adam Karczmarz, Ali Momeni, Nikos Parotsidis
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Given a directed graph $G$, a transitive reduction $G^t$ of $G$ (first studied by Aho, Garey, Ullman [SICOMP `72]) is a minimal subgraph of $G$ that preserves the reachability relation between every two vertices in $G$.</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.2 -->
                    
                <!-- Medicine: 4.5 -->
                    
                <!-- Quantum Computing: 4.2 -->
                    
                <!-- GNN: 3.1 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0519
                </span>
                <a href="https://arxiv.org/abs/2406.10060" target="_blank" rel="noopener noreferrer">PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory Planner</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kota Kondo, Claudius T. Tewari, Andrea Tagliabue, Jesus Tordesillas, Parker C. Lusk, Mason B. Peterson, Jonathan P. How
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In decentralized multiagent trajectory planners, agents need to communicate and exchange their positions to generate collision-free trajectories. However, due to localization errors/uncertainties, trajectory deconfliction can fail even if trajectories are perfectly shared between agents. To address </span>
                
                <span class="abstract-full" style="display: none;">In decentralized multiagent trajectory planners, agents need to communicate and exchange their positions to generate collision-free trajectories. However, due to localization errors/uncertainties, trajectory deconfliction can fail even if trajectories are perfectly shared between agents. To address this issue, we first present PARM and PARM*, perception-aware, decentralized, asynchronous multiagent trajectory planners that enable a team of agents to navigate uncertain environments while deconflicting trajectories and avoiding obstacles using perception information. PARM* differs from PARM as it is less conservative, using more computation to find closer-to-optimal solutions. While these methods achieve state-of-the-art performance, they suffer from high computational costs as they need to solve large optimization problems onboard, making it difficult for agents to replan at high rates. To overcome this challenge, we present our second key contribution, PRIMER, a learning-based planner trained with imitation learning (IL) using PARM* as the expert demonstrator. PRIMER leverages the low computational requirements at deployment of neural networks and achieves a computation speed up to 5500 times faster than optimization-based approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.2 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- GNN: 4.0 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- 3D: 2.4 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.052
                </span>
                <a href="https://arxiv.org/abs/2504.18139" target="_blank" rel="noopener noreferrer">Kalman-Langevin dynamics : exponential convergence, particle approximation and numerical approximation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Axel Ringh, Akash Sharma
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Langevin dynamics has found a large number of applications in sampling, optimization and estimation. Preconditioning the gradient in the dynamics with the covariance - an idea that originated in literature related to solving estimation and inverse problems using Kalman techniques - results in a mean</span>
                
                <span class="abstract-full" style="display: none;">Langevin dynamics has found a large number of applications in sampling, optimization and estimation. Preconditioning the gradient in the dynamics with the covariance - an idea that originated in literature related to solving estimation and inverse problems using Kalman techniques - results in a mean-field (McKean-Vlasov) SDE. We demonstrate exponential convergence of the time marginal law of the mean-field SDE to the Gibbs measure with non-Gaussian potentials. This extends previous results, obtained in the Gaussian setting, to a broader class of potential functions. We also establish uniform in time bounds on all moments and convergence in $p$-Wasserstein distance. Furthermore, we show convergence of a weak particle approximation, that avoids computing the square root of the empirical covariance matrix, to the mean-field limit. Finally, we prove that an explicit numerical scheme for approximating the particle dynamics converges, uniformly in number of particles, to its continuous-time limit, addressing non-global Lipschitzness in the measure.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.2 -->
                    
                <!-- Reinforcement Learning: 3.3 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Robotics: 2.3 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Pathfinding: 1.9 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0522
                </span>
                <a href="https://arxiv.org/abs/2409.16915" target="_blank" rel="noopener noreferrer">Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized Gaussian Splat</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jonathan Michaux, Seth Isaacson, Challen Enninful Adu, Adam Li, Rahul Kashyap Swayampakula, Parker Ewen, Sean Rice, Katherine A. Skinner, Ram Vasudevan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Neural Radiance Fields and Gaussian Splatting have recently transformed computer vision by enabling photo-realistic representations of complex scenes. However, they have seen limited application in real-world robotics tasks such as trajectory optimization. This is due to the difficulty in reasoning </span>
                
                <span class="abstract-full" style="display: none;">Neural Radiance Fields and Gaussian Splatting have recently transformed computer vision by enabling photo-realistic representations of complex scenes. However, they have seen limited application in real-world robotics tasks such as trajectory optimization. This is due to the difficulty in reasoning about collisions in radiance models and the computational complexity associated with operating in dense models. This paper addresses these challenges by proposing SPLANNING, a risk-aware trajectory optimizer operating in a Gaussian Splatting model. This paper first derives a method to rigorously upper-bound the probability of collision between a robot and a radiance field. Then, this paper introduces a normalized reformulation of Gaussian Splatting that enables efficient computation of this collision bound. Finally, this paper presents a method to optimize trajectories that avoid collisions in a Gaussian Splat. Experiments show that SPLANNING outperforms state-of-the-art methods in generating collision-free trajectories in cluttered environments. The proposed system is also tested on a real-world robot manipulator. A project page is available at https://roahmlab.github.io/splanning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.3 -->
                    
                <!-- GNN: 3.3 -->
                    
                <!-- Networks: 3.2 -->
                    
                <!-- 3D: 2.7 -->
                    
                <!-- Robotics: 2.6 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Attention: 1.1 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0523
                </span>
                <a href="https://arxiv.org/abs/2412.01114" target="_blank" rel="noopener noreferrer">Dense Dynamics-Aware Reward Synthesis: Integrating Prior Experience with Demonstrations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Cevahir Koprulu, Po-han Li, Tianyu Qiu, Ruihan Zhao, Tyler Westenbroek, David Fridovich-Keil, Sandeep Chinchali, Ufuk Topcu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Many continuous control problems can be formulated as sparse-reward reinforcement learning (RL) tasks. In principle, online RL methods can automatically explore the state space to solve each new task. However, discovering sequences of actions that lead to a non-zero reward becomes exponentially more</span>
                
                <span class="abstract-full" style="display: none;">Many continuous control problems can be formulated as sparse-reward reinforcement learning (RL) tasks. In principle, online RL methods can automatically explore the state space to solve each new task. However, discovering sequences of actions that lead to a non-zero reward becomes exponentially more difficult as the task horizon increases. Manually shaping rewards can accelerate learning for a fixed task, but it is an arduous process that must be repeated for each new environment. We introduce a systematic reward-shaping framework that distills the information contained in 1) a task-agnostic prior data set and 2) a small number of task-specific expert demonstrations, and then uses these priors to synthesize dense dynamics-aware rewards for the given task. This supervision substantially accelerates learning in our experiments, and we provide analysis demonstrating how the approach can effectively guide online learning agents to faraway goals.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.8 -->
                    
                <!-- Quantum Computing: 4.2 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- GNN: 2.6 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0535
                </span>
                <a href="https://arxiv.org/abs/2504.15204" target="_blank" rel="noopener noreferrer">Soft-Output from Covered Space Decoding of Product Codes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tim Janz, Simon Oberm\"uller, Andreas Zunker, Stephan ten Brink
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this work, we propose a new soft-in soft-out decoder called soft-output from covered space (SOCS) decoder. It estimates the a posteriori reliability based on the space explored by a list decoder, i.e., the set of vectors for which the list decoder knows whether they are codewords. This approach e</span>
                
                <span class="abstract-full" style="display: none;">In this work, we propose a new soft-in soft-out decoder called soft-output from covered space (SOCS) decoder. It estimates the a posteriori reliability based on the space explored by a list decoder, i.e., the set of vectors for which the list decoder knows whether they are codewords. This approach enables a more accurate calculation of the a posteriori reliability and results in gains of up to 0.25$\,$dB for turbo product decoding with SOCS compared to Chase-Pyndiah decoding.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.4 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0537
                </span>
                <a href="https://arxiv.org/abs/2504.17818" target="_blank" rel="noopener noreferrer">Fast Multichannel Topology Discovery in Cognitive Radio Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yung-Li Wang, Yiwei Liu, Cheng-Shang Chang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In Cognitive Radio Networks (CRNs), secondary users (SUs) must efficiently discover each other across multiple communication channels while avoiding interference from primary users (PUs). Traditional multichannel rendezvous algorithms primarily focus on enabling pairs of SUs to find common channels </span>
                
                <span class="abstract-full" style="display: none;">In Cognitive Radio Networks (CRNs), secondary users (SUs) must efficiently discover each other across multiple communication channels while avoiding interference from primary users (PUs). Traditional multichannel rendezvous algorithms primarily focus on enabling pairs of SUs to find common channels without explicitly considering the underlying network topology. In this paper, we extend the rendezvous framework to explicitly incorporate network topology, introducing the \emph{multichannel topology discovery problem}. We propose a novel \emph{pseudo-random sweep algorithm with forward replacement}, designed to minimize correlation between consecutive unsuccessful rendezvous attempts, thereby significantly reducing the expected time-to-discovery (ETTD). Additionally, we introduce a \emph{threshold-based stick-together strategy} that dynamically synchronizes user hopping sequences based on partially known information, further enhancing discovery efficiency. Extensive simulation results validate our theoretical analysis, demonstrating that the proposed algorithms substantially outperform conventional (sequential) sweep methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.0 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- GNN: 3.6 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0538
                </span>
                <a href="https://arxiv.org/abs/2504.18026" target="_blank" rel="noopener noreferrer">Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Emiliano Penaloza, Tianyue H. Zhan, Laurent Charlin, Mateo Espinosa Zarlenga
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI systems by constraining their decisions on a set of human understandable concepts. However, CBMs typically assume that datasets contains accurate concept labels an assumption often violated in practice, which we show can s</span>
                
                <span class="abstract-full" style="display: none;">Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI systems by constraining their decisions on a set of human understandable concepts. However, CBMs typically assume that datasets contains accurate concept labels an assumption often violated in practice, which we show can significantly degrade performance (by 25% in some cases). To address this, we introduce the Concept Preference Optimization (CPO) objective, a new loss function based on Direct Preference Optimization, which effectively mitigates the negative impact of concept mislabeling on CBM performance. We provide an analysis on some key properties of the CPO objective showing it directly optimizes for the concept's posterior distribution, and contrast it against Binary Cross Entropy (BCE) where we show CPO is inherently less sensitive to concept noise. We empirically confirm our analysis finding that CPO consistently outperforms BCE in three real world datasets with and without added label noise.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.5 -->
                    
                <!-- Medicine: 3.6 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0545
                </span>
                <a href="https://arxiv.org/abs/2412.18594" target="_blank" rel="noopener noreferrer">Structure Learning in Gaussian Graphical Models from Glauber Dynamics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vignesh Tirukkonda, Anirudh Rayas, Gautam Dasarathy
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Gaussian graphical model selection is an important paradigm with numerous applications, including biological network modeling, financial network modeling, and social network analysis. Traditional approaches assume access to independent and identically distributed (i.i.d) samples, which is often impr</span>
                
                <span class="abstract-full" style="display: none;">Gaussian graphical model selection is an important paradigm with numerous applications, including biological network modeling, financial network modeling, and social network analysis. Traditional approaches assume access to independent and identically distributed (i.i.d) samples, which is often impractical in real-world scenarios. In this paper, we address Gaussian graphical model selection under observations from a more realistic dependent stochastic process known as Glauber dynamics. Glauber dynamics, also called the Gibbs sampler, is a Markov chain that sequentially updates the variables of the underlying model based on the statistics of the remaining model. Such models, aside from frequently being employed to generate samples from complex multivariate distributions, naturally arise in various settings, such as opinion consensus in social networks and clearing/stock-price dynamics in financial networks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.1 -->
                    
                <!-- Medicine: 4.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.2413
                </span>
                <a href="https://arxiv.org/abs/2504.18085" target="_blank" rel="noopener noreferrer">Random-Set Large Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Muhammad Mubashar, Shireen Kudukkil Manchingal, Fabio Cuzzolin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large Language Models (LLMs) are known to produce very high-quality tests and responses to our queries. But how much can we trust this generated text? In this paper, we study the problem of uncertainty quantification in LLMs. We propose a novel Random-Set Large Language Model (RSLLM) approach which </span>
                
                <span class="abstract-full" style="display: none;">Large Language Models (LLMs) are known to produce very high-quality tests and responses to our queries. But how much can we trust this generated text? In this paper, we study the problem of uncertainty quantification in LLMs. We propose a novel Random-Set Large Language Model (RSLLM) approach which predicts finite random sets (belief functions) over the token space, rather than probability vectors as in classical LLMs. In order to allow so efficiently, we also present a methodology based on hierarchical clustering to extract and use a budget of "focal" subsets of tokens upon which the belief prediction is defined, rather than using all possible collections of tokens, making the method scalable yet effective. RS-LLMs encode the epistemic uncertainty induced in their generation process by the size and diversity of its training set via the size of the credal sets associated with the predicted belief functions. The proposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b, Mistral-7b and Phi-2 models and is shown to outperform the standard model in both datasets in terms of correctness of answer while also showing potential in estimating the second level uncertainty in its predictions and providing the capability to detect when its hallucinating.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 24.1 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3844
                </span>
                <a href="https://arxiv.org/abs/2503.11795" target="_blank" rel="noopener noreferrer">Set-based and Dynamical Feedback-augmented Hands-off Control</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andrei Speril\u{a}, Sorin Olaru, St\'ephane Drobot
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A novel set-theoretical approach to hands-off control is proposed, which focuses on spatial arguments for command limitation, rather than temporal ones. By employing dynamical feedback alongside invariant set-based constraints, actuation is employed only to drive the system's state inside a "hands-o</span>
                
                <span class="abstract-full" style="display: none;">A novel set-theoretical approach to hands-off control is proposed, which focuses on spatial arguments for command limitation, rather than temporal ones. By employing dynamical feedback alongside invariant set-based constraints, actuation is employed only to drive the system's state inside a "hands-off region" of its state-space, where the plant may freely evolve in open-loop configuration. A computationally-efficient procedure with strong theoretical guarantees is devised, and its effectiveness is showcased via an intuitive practical example.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.5 -->
                    
                <!-- Medicine: 5.4 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Networks: 3.0 -->
                    
                <!-- GNN: 2.7 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Math: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4205
                </span>
                <a href="https://arxiv.org/abs/2504.18253" target="_blank" rel="noopener noreferrer">Depth-Constrained ASV Navigation with Deep RL and Limited Sensing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Amirhossein Zhalehmehrabi, Daniele Meli, Francesco Dal Santo, Francesco Trotti, Alessandro Farinelli
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Autonomous Surface Vehicles (ASVs) play a crucial role in maritime operations, yet their navigation in shallow-water environments remains challenging due to dynamic disturbances and depth constraints. Traditional navigation strategies struggle with limited sensor information, making safe and efficie</span>
                
                <span class="abstract-full" style="display: none;">Autonomous Surface Vehicles (ASVs) play a crucial role in maritime operations, yet their navigation in shallow-water environments remains challenging due to dynamic disturbances and depth constraints. Traditional navigation strategies struggle with limited sensor information, making safe and efficient operation difficult. In this paper, we propose a reinforcement learning (RL) framework for ASV navigation under depth constraints, where the vehicle must reach a target while avoiding unsafe areas with only a single depth measurement per timestep from a downward-facing Single Beam Echosounder (SBES). To enhance environmental awareness, we integrate Gaussian Process (GP) regression into the RL framework, enabling the agent to progressively estimate a bathymetric depth map from sparse sonar readings. This approach improves decision-making by providing a richer representation of the environment. Furthermore, we demonstrate effective sim-to-real transfer, ensuring that trained policies generalize well to real-world aquatic conditions. Experimental results validate our method's capability to improve ASV navigation performance while maintaining safety in challenging shallow-water environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.7 -->
                    
                <!-- Medicine: 5.5 -->
                    
                <!-- Networks: 3.2 -->
                    
                <!-- Reinforcement Learning: 3.2 -->
                    
                <!-- GNN: 3.0 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Attention: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4312
                </span>
                <a href="https://arxiv.org/abs/2504.18168" target="_blank" rel="noopener noreferrer">Revolutionizing Symbiotic Radio: Exploiting Tradeoffs in Hybrid Active-Passive Communications</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rui Xu, Yinghui Ye, Haijian Sun, Liqin Shi, Guangyue Lu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Symbiotic radio (SR), a novel energy- and spectrum-sharing paradigm of backscatter communications (BC), has been deemed a promising solution for ambient Internet of Things (A-IoT), enabling ultra-low power consumption and massive connectivity. However, A-IoT nodes utilizing BC suffer from low transm</span>
                
                <span class="abstract-full" style="display: none;">Symbiotic radio (SR), a novel energy- and spectrum-sharing paradigm of backscatter communications (BC), has been deemed a promising solution for ambient Internet of Things (A-IoT), enabling ultra-low power consumption and massive connectivity. However, A-IoT nodes utilizing BC suffer from low transmission rates, which may limit the applications of SR in A-IoT scenarios with data transmission requirements. To address this issue, in this article, we introduce hybrid active-passive communications (HAPC) into SR by exploiting tradeoffs between transmission rate and power consumption. We first present an overview of novel BC paradigms including ambient BC and SR. Then, a novel HAPC-enabled SR is proposed to enhance the transmission rate of A-IoT nodes. Furthermore, within this paradigm, we investigate the resource allocation scheme and present preliminary research results. Simulation results show that the transmission rate of A-IoT nodes in the proposed HAPC-enabled SR surpasses that in traditional SR. Finally, we discuss open issues related to HAPC-enabled SR.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.5 -->
                    
                <!-- Medicine: 5.9 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4841
                </span>
                <a href="https://arxiv.org/abs/2504.18524" target="_blank" rel="noopener noreferrer">Augmenting Perceptual Super-Resolution via Image Quality Predictors</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fengjia Zhang, Samrudhdhi B. Rangrej, Tristan Aumentado-Armstrong, Afsaneh Fazly, Alex Levinshtein
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Super-resolution (SR), a classical inverse problem in computer vision, is inherently ill-posed, inducing a distribution of plausible solutions for every input. However, the desired result is not simply the expectation of this distribution, which is the blurry image obtained by minimizing pixelwise e</span>
                
                <span class="abstract-full" style="display: none;">Super-resolution (SR), a classical inverse problem in computer vision, is inherently ill-posed, inducing a distribution of plausible solutions for every input. However, the desired result is not simply the expectation of this distribution, which is the blurry image obtained by minimizing pixelwise error, but rather the sample with the highest image quality. A variety of techniques, from perceptual metrics to adversarial losses, are employed to this end. In this work, we explore an alternative: utilizing powerful non-reference image quality assessment (NR-IQA) models in the SR context. We begin with a comprehensive analysis of NR-IQA metrics on human-derived SR data, identifying both the accuracy (human alignment) and complementarity of different metrics. Then, we explore two methods of applying NR-IQA models to SR learning: (i) altering data sampling, by building on an existing multi-ground-truth SR framework, and (ii) directly optimizing a differentiable quality score. Our results demonstrate a more human-centric perception-distortion tradeoff, focusing less on non-perceptual pixel-wise distortion, instead improving the balance between perceptual fidelity and human-tuned NR-IQA measures.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.9 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5087
                </span>
                <a href="https://arxiv.org/abs/2504.18154" target="_blank" rel="noopener noreferrer">EcoServe: Enabling Cost-effective LLM Serving with Proactive Intra- and Inter-Instance Orchestration</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jiangsu Du, Hongbin Zhang, Taosheng Wei, Zhenyi Zheng, Kaiyi Wu, Zhiguang Chen, Yutong Lu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Existing LLM serving strategies can be categorized based on whether prefill and decode phases are disaggregated: non-disaggregated (NoDG) or fully disaggregated (FuDG). However, the NoDG strategy leads to strong prefill-decode interference and the FuDG strategy highly relies on high-performance inte</span>
                
                <span class="abstract-full" style="display: none;">Existing LLM serving strategies can be categorized based on whether prefill and decode phases are disaggregated: non-disaggregated (NoDG) or fully disaggregated (FuDG). However, the NoDG strategy leads to strong prefill-decode interference and the FuDG strategy highly relies on high-performance interconnects, making them less cost-effective.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.7 -->
                    
                <!-- Medicine: 6.2 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- 3D: 2.5 -->
                    
                <!-- Blockchain: 2.4 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5118
                </span>
                <a href="https://arxiv.org/abs/2504.18432" target="_blank" rel="noopener noreferrer">FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xuzheng Chen, Jie Zhang, Baolin Zhu, Xueying Zhu, Zhongqing Chen, Shu Ma, Lingjun Zhu, Chao Shi, Yin Zhang, Zeke Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As the gap between network and CPU speeds rapidly increases, the CPU-centric network stack proves inadequate due to excessive CPU and memory overhead. While hardware-offloaded network stacks alleviate these issues, they suffer from limited flexibility in both control and data planes. Offloading netw</span>
                
                <span class="abstract-full" style="display: none;">As the gap between network and CPU speeds rapidly increases, the CPU-centric network stack proves inadequate due to excessive CPU and memory overhead. While hardware-offloaded network stacks alleviate these issues, they suffer from limited flexibility in both control and data planes. Offloading network stack to off-path SmartNIC seems promising to provide high flexibility; however, throughput remains constrained by inherent SmartNIC architectural limitations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.3 -->
                    
                <!-- Medicine: 6.3 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- 3D: 3.0 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- RAG: 2.5 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Hardware: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5606
                </span>
                <a href="https://arxiv.org/abs/2502.19546" target="_blank" rel="noopener noreferrer">Repurposing the scientific literature with vision-language models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Anton Alyakin, Jaden Stryker, Daniel Alexander Alber, Karl L. Sangwon, Jin Vivian Lee, Brandon Duderstadt, Akshay Save, David Kurland, Spencer Frome, Shrutika Singh, Jeff Zhang, Eunice Yang, Ki Yun Park, Cordelia Orillac, Aly A. Valliani, Sean Neifert, Albert Liu, Aneek Patel, Christopher Livia, Darryl Lau, Ilya Laufer, Peter A. Rozman, Eveline Teresa Hidalgo, Howard Riina, Rui Feng, Todd Hollon, Yindalon Aphinyanaphongs, John G. Golfinos, Laura Snyder, Eric Leuthardt, Douglas Kondziolka, Eric Karl Oermann
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Leading vision-language models (VLMs) are trained on general Internet content, overlooking scientific journals' rich, domain-specific knowledge. Training on specialty-specific literature could yield high-performance, task-specific tools, enabling generative AI to match generalist models in specialty</span>
                
                <span class="abstract-full" style="display: none;">Leading vision-language models (VLMs) are trained on general Internet content, overlooking scientific journals' rich, domain-specific knowledge. Training on specialty-specific literature could yield high-performance, task-specific tools, enabling generative AI to match generalist models in specialty publishing, educational, and clinical tasks. We created NeuroPubs, a multimodal dataset of 23,000 Neurosurgery Publications articles (134M words, 78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready graphical abstracts (70% of 100 abstracts) and board-style questions indistinguishable from human-written ones (54% of 89,587 questions). We used these questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded, randomized controlled trial, our model demonstrated non-inferiority to then state-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical utility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%, p=0.3797). Our pilot study demonstrates how training generative AI models on specialty-specific journal content - without large-scale internet data - results in high-performance academic and clinical tools, enabling domain-tailored AI across diverse fields.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 12.5 -->
                    
                <!-- Medicine: 6.3 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- 3D: 2.4 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- T2I: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.575
                </span>
                <a href="https://arxiv.org/abs/2504.18513" target="_blank" rel="noopener noreferrer">PODNO: Proper Orthogonal Decomposition Neural Operators</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zilan Cheng, Zhongjian Wang, Li-Lian Wang, Mejdi Azaiez
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we introduce Proper Orthogonal Decomposition Neural Operators (PODNO) for solving partial differential equations (PDEs) dominated by high-frequency components. Building on the structure of Fourier Neural Operators (FNO), PODNO replaces the Fourier transform with (inverse) orthonormal </span>
                
                <span class="abstract-full" style="display: none;">In this paper, we introduce Proper Orthogonal Decomposition Neural Operators (PODNO) for solving partial differential equations (PDEs) dominated by high-frequency components. Building on the structure of Fourier Neural Operators (FNO), PODNO replaces the Fourier transform with (inverse) orthonormal transforms derived from the Proper Orthogonal Decomposition (POD) method to construct the integral kernel. Due to the optimality of POD basis, the PODNO has potential to outperform FNO in both accuracy and computational efficiency for high-frequency problems. From analysis point of view, we established the universality of a generalization of PODNO, termed as Generalized Spectral Operator (GSO). In addition, we evaluate PODNO's performance numerically on dispersive equations such as the Nonlinear Schrodinger (NLS) equation and the Kadomtsev-Petviashvili (KP) equation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.0 -->
                    
                <!-- Medicine: 5.6 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5776
                </span>
                <a href="https://arxiv.org/abs/2504.17921" target="_blank" rel="noopener noreferrer">Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mateo Espinosa Zarlenga, Gabriele Dominici, Pietro Barbiero, Zohreh Shams, Mateja Jamnik
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we investigate how concept-based models (CMs) respond to out-of-distribution (OOD) inputs. CMs are interpretable neural architectures that first predict a set of high-level concepts (e.g., stripes, black) and then predict a task label from those concepts. In particular, we study the i</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we investigate how concept-based models (CMs) respond to out-of-distribution (OOD) inputs. CMs are interpretable neural architectures that first predict a set of high-level concepts (e.g., stripes, black) and then predict a task label from those concepts. In particular, we study the impact of concept interventions (i.e., operations where a human expert corrects a CM's mispredicted concepts at test time) on CMs' task predictions when inputs are OOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we term leakage poisoning, that prevents them from properly improving their accuracy when intervened on for OOD inputs. To address this, we introduce MixCEM, a new CM that learns to dynamically exploit leaked information missing from its concepts only when this information is in-distribution. Our results across tasks with and without complete sets of concept annotations demonstrate that MixCEMs outperform strong baselines by significantly improving their accuracy for both in-distribution and OOD samples in the presence and absence of concept interventions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.3 -->
                    
                <!-- Medicine: 6.0 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- GNN: 2.9 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.597
                </span>
                <a href="https://arxiv.org/abs/2504.18391" target="_blank" rel="noopener noreferrer">Fast Autoregressive Models for Continuous Latent Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tiankai Hang, Jianmin Bao, Fangyun Wei, Dong Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Autoregressive models have demonstrated remarkable success in sequential data generation, particularly in NLP, but their extension to continuous-domain image generation presents significant challenges. Recent work, the masked autoregressive model (MAR), bypasses quantization by modeling per-token di</span>
                
                <span class="abstract-full" style="display: none;">Autoregressive models have demonstrated remarkable success in sequential data generation, particularly in NLP, but their extension to continuous-domain image generation presents significant challenges. Recent work, the masked autoregressive model (MAR), bypasses quantization by modeling per-token distributions in continuous spaces using a diffusion head but suffers from slow inference due to the high computational cost of the iterative denoising process. To address this, we propose the Fast AutoRegressive model (FAR), a novel framework that replaces MAR's diffusion head with a lightweight shortcut head, enabling efficient few-step sampling while preserving autoregressive principles. Additionally, FAR seamlessly integrates with causal Transformers, extending them from discrete to continuous token generation without requiring architectural modifications. Experiments demonstrate that FAR achieves $2.3\times$ faster inference than MAR while maintaining competitive FID and IS scores. This work establishes the first efficient autoregressive paradigm for high-fidelity continuous-space image generation, bridging the critical gap between quality and scalability in visual autoregressive modeling.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.9 -->
                    
                <!-- Medicine: 6.7 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- T2I: 2.1 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Attention: 1.0 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.615
                </span>
                <a href="https://arxiv.org/abs/2404.00146" target="_blank" rel="noopener noreferrer">Fast Orthogonal Matching Pursuit through Successive Regression</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Huiyuan Yu, Jia He, Maggie Cheng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Orthogonal Matching Pursuit (OMP) has been a powerful method in sparse signal recovery and approximation. However, OMP suffers computational issues when the signal has a large number of non-zeros. This paper advances OMP and its extension called generalized OMP (gOMP) by offering fast algorithms for</span>
                
                <span class="abstract-full" style="display: none;">Orthogonal Matching Pursuit (OMP) has been a powerful method in sparse signal recovery and approximation. However, OMP suffers computational issues when the signal has a large number of non-zeros. This paper advances OMP and its extension called generalized OMP (gOMP) by offering fast algorithms for the orthogonal projection of the input signal at each iteration. The proposed modifications directly reduce the computational complexity of OMP and gOMP. Experiment results verified the improvement in computation time. This paper also provides sufficient conditions for exact signal recovery. For general signals with additive noise, the approximation error is at the same order as OMP (gOMP), but is obtained within much less time.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.8 -->
                    
                <!-- LLMs: 5.0 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.638
                </span>
                <a href="https://arxiv.org/abs/2503.18339" target="_blank" rel="noopener noreferrer">GranQ: Granular Zero-Shot Quantization with Unified Layer-Channel Awareness</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Zero-shot quantization (ZSQ) enables neural network compression without training data, which is crucial in restricted data access environments. However, existing ZSQ methods suffer from significant activation loss in low-bit environments owing to their coarse-grained scaling strategy. To address thi</span>
                
                <span class="abstract-full" style="display: none;">Zero-shot quantization (ZSQ) enables neural network compression without training data, which is crucial in restricted data access environments. However, existing ZSQ methods suffer from significant activation loss in low-bit environments owing to their coarse-grained scaling strategy. To address this issue, we propose GranQ, a novel ZSQ approach that leverages layer-channel awareness to minimize the quantization error. Unlike conventional layer- or channel-wise quantization, GranQ dynamically adjusts quantization granularity by considering both layer- and channel-level activation distributions. This enables fine-grained quantization while minimizing activation distortion. Additionally, we introduce vectorized activation quantization, which enables efficient parallel computation and reduces computational overhead while preserving accuracy. GranQ achieves superior performance compared with those of state-of-the-art ZSQ methods that employ quantization-aware training. With these findings, we anticipate that GranQ will inspire novel research directions beyond conventional ZSQ approaches focused on data generation and model training.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.5 -->
                    
                <!-- Medicine: 6.2 -->
                    
                <!-- GNN: 4.3 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- 3D: 2.9 -->
                    
                <!-- T2I: 2.3 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.702
                </span>
                <a href="https://arxiv.org/abs/2504.18317" target="_blank" rel="noopener noreferrer">Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhengru Fang, Zhenghao Liu, Jingjing Wang, Senkang Hu, Yu Guo, Yiqin Deng, Yuguang Fang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles (UAVs) localization in urban areas where global positioning system (GPS) signals are unavailable. Vision-based methods offer a viable alternative but face severe bandwidth, memory and processing constraints on lightweight UA</span>
                
                <span class="abstract-full" style="display: none;">To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles (UAVs) localization in urban areas where global positioning system (GPS) signals are unavailable. Vision-based methods offer a viable alternative but face severe bandwidth, memory and processing constraints on lightweight UAVs. Inspired by mammalian spatial cognition, we propose a task-oriented communication framework, where UAVs equipped with multi-camera systems extract compact multi-view features and offload localization tasks to edge servers. We introduce the Orthogonally-constrained Variational Information Bottleneck encoder (O-VIB), which incorporates automatic relevance determination (ARD) to prune non-informative features while enforcing orthogonality to minimize redundancy. This enables efficient and accurate localization with minimal transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows that O-VIB achieves high-precision localization under stringent bandwidth budgets. Code and dataset will be made publicly available: github.com/fangzr/TOC-Edge-Aerial.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.2 -->
                    
                <!-- LLMs: 5.6 -->
                    
                <!-- 3D: 3.4 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- GNN: 2.9 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- T2I: 1.8 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Attention: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7138
                </span>
                <a href="https://arxiv.org/abs/2504.18036" target="_blank" rel="noopener noreferrer">Direct sampling method to retrieve small objects from two-dimensional limited-aperture scattered field data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Won-Kwang Park
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this study, we investigated the application of the direct sampling method (DSM) to identify small dielectric objects in a limited-aperture inverse scattering problem. Unlike previous studies, we consider the bistatic measurement configuration corresponding to the transmitter location and design i</span>
                
                <span class="abstract-full" style="display: none;">In this study, we investigated the application of the direct sampling method (DSM) to identify small dielectric objects in a limited-aperture inverse scattering problem. Unlike previous studies, we consider the bistatic measurement configuration corresponding to the transmitter location and design indicator functions for both a single source and multiple sources, and we convert the unknown measurement data to a fixed nonzero constant. To explain the applicability and limitation of object detection, we demonstrate that the indicator functions can be expressed by an infinite series of Bessel functions, the material properties of the objects, the bistatic angle, and the converted constant. Based on the theoretical results, we explain how the imaging performance of the DSM is influenced by the bistatic angle and the converted constant. In addition, the results of our analyses demonstrate that a smaller bistatic angle enhances the imaging accuracy and that optimal selection of the converted constant is crucial to realize reliable object detection. The results of the numerical simulations obtained using a two-dimensional Fresnel dataset validated the theoretical findings and illustrate the effectiveness and limitations of the designed indicator functions for small objects.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.5 -->
                    
                <!-- Reinforcement Learning: 6.7 -->
                    
                <!-- Math: 2.9 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- LLMs: 2.0 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Pathfinding: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7193
                </span>
                <a href="https://arxiv.org/abs/2504.17861" target="_blank" rel="noopener noreferrer">Efficient iterative techniques for solving tensor problems with the T-product</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Malihe Nobakht Kooshkghazi, Salman Ahmadi-Asl, Hamidreza Afshin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents iterative methods for solving tensor equations involving the T-product. The proposed approaches apply tensor computations without matrix construction. For each initial tensor, these algorithms solve related problems in a finite number of iterations, with negligible errors. The th</span>
                
                <span class="abstract-full" style="display: none;">This paper presents iterative methods for solving tensor equations involving the T-product. The proposed approaches apply tensor computations without matrix construction. For each initial tensor, these algorithms solve related problems in a finite number of iterations, with negligible errors. The theoretical analysis is validated by numerical examples that demonstrate the practicality and effectiveness of these algorithms.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.5 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Quantum Computing: 4.6 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.735
                </span>
                <a href="https://arxiv.org/abs/2504.18352" target="_blank" rel="noopener noreferrer">A Linear Time Algorithm for the Maximum Overlap of Two Convex Polygons Under Translation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Timothy M. Chan, Isaac M. Hair
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Given two convex polygons $P$ and $Q$ with $n$ and $m$ edges, the maximum overlap problem is to find a translation of $P$ that maximizes the area of its intersection with $Q$. We give the first randomized algorithm for this problem with linear running time. Our result improves the previous two-and-a</span>
                
                <span class="abstract-full" style="display: none;">Given two convex polygons $P$ and $Q$ with $n$ and $m$ edges, the maximum overlap problem is to find a translation of $P$ that maximizes the area of its intersection with $Q$. We give the first randomized algorithm for this problem with linear running time. Our result improves the previous two-and-a-half-decades-old algorithm by de Berg, Cheong, Devillers, van Kreveld, and Teillaud (1998), which ran in $O((n+m)\log(n+m))$ time, as well as multiple recent algorithms given for special cases of the problem.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.8 -->
                    
                <!-- LLMs: 5.0 -->
                    
                <!-- Quantum Computing: 4.1 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7481
                </span>
                <a href="https://arxiv.org/abs/2407.09709" target="_blank" rel="noopener noreferrer">GOFA: A Generative One-For-All Model for Joint Graph Language Modeling</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Foundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation</span>
                
                <span class="abstract-full" style="display: none;">Foundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM). For example, current attempts at designing general graph models either transform graph data into a language format for LLM-based prediction or still train a GNN model with LLM as an assistant. The former can handle unlimited tasks, while the latter captures graph structure much better -- yet, no existing work can achieve both simultaneously. In this paper, we identify three key desirable properties of a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To account for these properties, we extend the conventional language modeling to the graph domain and propose a novel generative graph language model GOFA to solve the problem. The model interleaves randomly initialized GNN layers into a frozen pre-trained LLM so that the semantic and structural modeling abilities are organically combined. GOFA is pre-trained on newly proposed graph-level next-word prediction, question-answering, and structural tasks to obtain the above GFM properties. The pre-trained model is further fine-tuned on downstream tasks to obtain task-solving ability. The fine-tuned model is evaluated on various downstream tasks, demonstrating a strong ability to solve structural and contextual problems in zero-shot scenarios. The code is available at https://github.com/JiaruiFeng/GOFA.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.5 -->
                    
                <!-- Medicine: 5.5 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7817
                </span>
                <a href="https://arxiv.org/abs/2504.18084" target="_blank" rel="noopener noreferrer">RL-Driven Data Generation for Robust Vision-Based Dexterous Grasping</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Atsushi Kanehira, Naoki Wake, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This work presents reinforcement learning (RL)-driven data augmentation to improve the generalization of vision-action (VA) models for dexterous grasping. While real-to-sim-to-real frameworks, where a few real demonstrations seed large-scale simulated data, have proven effective for VA models, apply</span>
                
                <span class="abstract-full" style="display: none;">This work presents reinforcement learning (RL)-driven data augmentation to improve the generalization of vision-action (VA) models for dexterous grasping. While real-to-sim-to-real frameworks, where a few real demonstrations seed large-scale simulated data, have proven effective for VA models, applying them to dexterous settings remains challenging: obtaining stable multi-finger contacts is nontrivial across diverse object shapes. To address this, we leverage RL to generate contact-rich grasping data across varied geometries. In line with the real-to-sim-to-real paradigm, the grasp skill is formulated as a parameterized and tunable reference trajectory refined by a residual policy learned via RL. This modular design enables trajectory-level control that is both consistent with real demonstrations and adaptable to diverse object geometries. A vision-conditioned policy trained on simulation-augmented data demonstrates strong generalization to unseen objects, highlighting the potential of our approach to alleviate the data bottleneck in training VA models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.5 -->
                    
                <!-- LLMs: 4.8 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7874
                </span>
                <a href="https://arxiv.org/abs/2504.18529" target="_blank" rel="noopener noreferrer">Practical Type-Based Taint Checking and Inference (Extended Version)</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nima Karimipour, Kanak Das, Manu Sridharan, Behnaz Hassanshahi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Many important security properties can be formulated in terms of flows of tainted data, and improved taint analysis tools to prevent such flows are of critical need. Most existing taint analyses use whole-program static analysis, leading to scalability challenges. Type-based checking is a promising </span>
                
                <span class="abstract-full" style="display: none;">Many important security properties can be formulated in terms of flows of tainted data, and improved taint analysis tools to prevent such flows are of critical need. Most existing taint analyses use whole-program static analysis, leading to scalability challenges. Type-based checking is a promising alternative, as it enables modular and incremental checking for fast performance. However, type-based approaches have not been widely adopted in practice, due to challenges with false positives and annotating existing codebases. In this paper, we present a new approach to type-based checking of taint properties that addresses these challenges, based on two key techniques. First, we present a new type-based tainting checker with significantly reduced false positives, via more practical handling of third-party libraries and other language constructs. Second, we present a novel technique to automatically infer tainting type qualifiers for existing code. Our technique supports inference of generic type argument annotations, crucial for tainting properties. We implemented our techniques in a tool TaintTyper and evaluated it on real-world benchmarks. TaintTyper exceeds the recall of a state-of-the-art whole-program taint analyzer, with comparable precision, and 2.93X-22.9X faster checking time. Further, TaintTyper infers annotations comparable to those written by hand, suitable for insertion into source code. TaintTyper is a promising new approach to efficient and practical taint checking.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.8 -->
                    
                <!-- Medicine: 5.7 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8181
                </span>
                <a href="https://arxiv.org/abs/2308.10459" target="_blank" rel="noopener noreferrer">Implicit Bonded Discrete Element Method with Manifold Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jia-Ming Lu, Geng-Chen Cao, Chen-Feng Li, Shi-Min Hu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper proposes a novel approach that combines variational integration with the bonded discrete element method (BDEM) to achieve faster and more accurate fracture simulations. The approach leverages the efficiency of implicit integration and the accuracy of BDEM in modeling fracture phenomena. W</span>
                
                <span class="abstract-full" style="display: none;">This paper proposes a novel approach that combines variational integration with the bonded discrete element method (BDEM) to achieve faster and more accurate fracture simulations. The approach leverages the efficiency of implicit integration and the accuracy of BDEM in modeling fracture phenomena. We introduce a variational integrator and a manifold optimization approach utilizing a nullspace operator to speed up the solving of quaternion-constrained systems. Additionally, the paper presents an element packing and surface reconstruction method specifically designed for bonded discrete element methods. Results from the experiments prove that the proposed method offers 2.8 to 12 times faster state-of-the-art methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.1 -->
                    
                <!-- Medicine: 5.9 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8359
                </span>
                <a href="https://arxiv.org/abs/2504.18200" target="_blank" rel="noopener noreferrer">Implementation Analysis of Collaborative Robot Digital Twins in Physics Engines</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christian K\"onig, Jan Petershans, Jan Herbst, Matthias R\"ub, Dennis Krummacker, Eric Mittag, Hand D. Schooten
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents a Digital Twin (DT) of a 6G communications system testbed that integrates two robotic manipulators with a high-precision optical infrared tracking system in Unreal Engine 5. Practical details of the setup and implementation insights provide valuable guidance for users aiming to r</span>
                
                <span class="abstract-full" style="display: none;">This paper presents a Digital Twin (DT) of a 6G communications system testbed that integrates two robotic manipulators with a high-precision optical infrared tracking system in Unreal Engine 5. Practical details of the setup and implementation insights provide valuable guidance for users aiming to replicate such systems, an endeavor that is crucial to advancing DT applications within the scientific community. Key topics discussed include video streaming, integration within the Robot Operating System 2 (ROS 2), and bidirectional communication. The insights provided are intended to support the development and deployment of DTs in robotics and automation research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.7 -->
                    
                <!-- Medicine: 6.5 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8654
                </span>
                <a href="https://arxiv.org/abs/2504.18398" target="_blank" rel="noopener noreferrer">Partition Map-Based Fast Block Partitioning for VVC Inter Coding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xinmin Feng, Zhuoyuan Li, Li Li, Dong Liu, Feng Wu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Among the new techniques of Versatile Video Coding (VVC), the quadtree with nested multi-type tree (QT+MTT) block structure yields significant coding gains by providing more flexible block partitioning patterns. However, the recursive partition search in the VVC encoder increases the encoder complex</span>
                
                <span class="abstract-full" style="display: none;">Among the new techniques of Versatile Video Coding (VVC), the quadtree with nested multi-type tree (QT+MTT) block structure yields significant coding gains by providing more flexible block partitioning patterns. However, the recursive partition search in the VVC encoder increases the encoder complexity substantially. To address this issue, we propose a partition map-based algorithm to pursue fast block partitioning in inter coding. Based on our previous work on partition map-based methods for intra coding, we analyze the characteristics of VVC inter coding, and thus improve the partition map by incorporating an MTT mask for early termination. Next, we develop a neural network that uses both spatial and temporal features to predict the partition map. It consists of several special designs including stacked top-down and bottom-up processing, quantization parameter modulation layers, and partitioning-adaptive warping. Furthermore, we present a dual-threshold decision scheme to achieve a fine-grained trade-off between complexity reduction and rate-distortion (RD) performance loss. The experimental results demonstrate that the proposed method achieves an average 51.30% encoding time saving with a 2.12% Bjontegaard Delta Bit Rate (BDBR) under the random access configuration.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.8 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8679
                </span>
                <a href="https://arxiv.org/abs/2410.17390" target="_blank" rel="noopener noreferrer">Revealing The Secret Power: How Algorithms Can Influence Content Visibility on Social Media</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alessandro Galeazzi, Pujan Paudel, Mauro Conti, Emiliano De Cristofaro, Gianluca Stringhini
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In recent years, the opaque design and the limited public understanding of social networks' recommendation algorithms have raised concerns about potential manipulation of information exposure. While reducing content visibility, aka shadow banning, may help limit harmful content, it can also be used </span>
                
                <span class="abstract-full" style="display: none;">In recent years, the opaque design and the limited public understanding of social networks' recommendation algorithms have raised concerns about potential manipulation of information exposure. While reducing content visibility, aka shadow banning, may help limit harmful content, it can also be used to suppress dissenting voices. This prompts the need for greater transparency and a better understanding of this practice.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.5 -->
                    
                <!-- LLMs: 6.9 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- Blockchain: 2.5 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.868
                </span>
                <a href="https://arxiv.org/abs/2504.18335" target="_blank" rel="noopener noreferrer">Rack-Aware Minimum Storage Partially Cooperative Regenerating Codes with Small Sub-Packetization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hengming Zhao, Dianhua Wu, Minquan Cheng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In the rack-aware model, there are $\bar{n}$ racks each of which has $u$ nodes with the same storage capacity. Assume that there are $h$ failed nodes uniformly distributed in $\bar{h}$ host racks ( defined as racks containing failed nodes), each rack containing $h/\bar{h}$ failed nodes where $h$ is </span>
                
                <span class="abstract-full" style="display: none;">In the rack-aware model, there are $\bar{n}$ racks each of which has $u$ nodes with the same storage capacity. Assume that there are $h$ failed nodes uniformly distributed in $\bar{h}$ host racks ( defined as racks containing failed nodes), each rack containing $h/\bar{h}$ failed nodes where $h$ is divisible by $\bar{h}$. Then together with its internal helper nodes, each host rack downloads recovery data from $\bar{d}$ helper racks and repairs its failed nodes. The repair bandwidth is defined as the total inter-rack data transfer required for failures recovery, as the intra-rack communication does not contribute to this cost. The full cooperative repair model requires that each host rack must exchange the data with all the other $\bar{h}$ host racks during the cooperative repair phase. However, in the partial cooperative repair model, each host rack only needs to exchange data with $\bar{h}-\delta\ (1\leq\delta\leq\bar{h}-1)$ other host racks, during the cooperative repair phase. In this paper, we focus on the rack-aware minimum storage partially cooperative regenerating (MSPCR) codes for repairing the $h$ node failures. We first derive the lower bound on the repair bandwidth for rack-aware MSPCR codes using extremal combinatorics, and then construct two classes of optimal repair schemes for rack-aware MSPCR codes with small sub-packetization level. In particular, when $\delta=1$, our second codes reduce to rack-aware minimum-storage cooperative regenerating (MSCR) codes, while achieving an $(\bar{h}+1)$-fold reduction in sub-packetization level compared to known rack-aware MSCR codes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.8 -->
                    
                <!-- LLMs: 4.6 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9001
                </span>
                <a href="https://arxiv.org/abs/2404.03818" target="_blank" rel="noopener noreferrer">PRobELM: Plausibility Ranking Evaluation for Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhangdie Yuan, Eric Chamoun, Rami Aly, Chenxi Whitehouse, Andreas Vlachos
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a benchmark designed to assess language models' ability to discern more plausible from less plausible scenarios through their parametric knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or t</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a benchmark designed to assess language models' ability to discern more plausible from less plausible scenarios through their parametric knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or truthfulness, and others such as COPA explore plausible scenarios without explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by evaluating models' capabilities to prioritise plausible scenarios that leverage world knowledge over less plausible alternatives. This design allows us to assess the potential of language models for downstream use cases such as literature-based discovery where the focus is on identifying information that is likely but not yet known. Our benchmark is constructed from a dataset curated from Wikidata edit histories, tailored to align the temporal bounds of the training data for the evaluated models. PRobELM facilitates the evaluation of language models across multiple prompting types, including statement, text completion, and question-answering. Experiments with 10 models of various sizes and architectures on the relationship between model scales, training recency, and plausibility performance, reveal that factual accuracy does not directly correlate with plausibility performance and that up-to-date training data enhances plausibility assessment across different model architectures.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 13.7 -->
                    
                <!-- Medicine: 6.5 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9008
                </span>
                <a href="https://arxiv.org/abs/2504.18497" target="_blank" rel="noopener noreferrer">DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yifeng Mao, Bozhidar Stevanoski, Yves-Alexandre de Montjoye
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Empirical inference attacks are a popular approach for evaluating the privacy risk of data release mechanisms in practice. While an active attack literature exists to evaluate machine learning models or synthetic data release, we currently lack comparable methods for fixed aggregate statistics, in p</span>
                
                <span class="abstract-full" style="display: none;">Empirical inference attacks are a popular approach for evaluating the privacy risk of data release mechanisms in practice. While an active attack literature exists to evaluate machine learning models or synthetic data release, we currently lack comparable methods for fixed aggregate statistics, in particular when only a limited number of statistics are released. We here propose an inference attack framework against fixed aggregate statistics and an attribute inference attack called DeSIA. We instantiate DeSIA against the U.S. Census PPMF dataset and show it to strongly outperform reconstruction-based attacks. In particular, we show DeSIA to be highly effective at identifying vulnerable users, achieving a true positive rate of 0.14 at a false positive rate of $10^{-3}$. We then show DeSIA to perform well against users whose attributes cannot be verified and when varying the number of aggregate statistics and level of noise addition. We also perform an extensive ablation study of DeSIA and show how DeSIA can be successfully adapted to the membership inference task. Overall, our results show that aggregation alone is not sufficient to protect privacy, even when a relatively small number of aggregates are being released, and emphasize the need for formal privacy mechanisms and testing before aggregate statistics are released.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.7 -->
                    
                <!-- Medicine: 6.1 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9017
                </span>
                <a href="https://arxiv.org/abs/2504.17827" target="_blank" rel="noopener noreferrer">Evolution Meets Diffusion: Efficient Neural Architecture Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bingye Zhou, Caiyang Yu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Neural Architecture Search (NAS) has gained widespread attention for its transformative potential in deep learning model design. However, the vast and complex search space of NAS leads to significant computational and time costs. Neural Architecture Generation (NAG) addresses this by reframing NAS a</span>
                
                <span class="abstract-full" style="display: none;">Neural Architecture Search (NAS) has gained widespread attention for its transformative potential in deep learning model design. However, the vast and complex search space of NAS leads to significant computational and time costs. Neural Architecture Generation (NAG) addresses this by reframing NAS as a generation problem, enabling the precise generation of optimal architectures for specific tasks. Despite its promise, mainstream methods like diffusion models face limitations in global search capabilities and are still hindered by high computational and time demands. To overcome these challenges, we propose Evolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel approach that achieves efficient and training-free architecture generation. EDNAG leverages evolutionary algorithms to simulate the denoising process in diffusion models, using fitness to guide the transition from random Gaussian distributions to optimal architecture distributions. This approach combines the strengths of evolutionary strategies and diffusion models, enabling rapid and effective architecture generation. Extensive experiments demonstrate that EDNAG achieves state-of-the-art (SOTA) performance in architecture optimization, with an improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need for time-consuming training and boosts inference speed by an average of 50 times, showcasing its exceptional efficiency and effectiveness.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.0 -->
                    
                <!-- Medicine: 6.6 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- T2I: 2.0 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9364
                </span>
                <a href="https://arxiv.org/abs/2504.18047" target="_blank" rel="noopener noreferrer">Spatiotemporal Analysis of Parallelized Computing at the Extreme Edge</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yasser Nabil, Mahmoud Abdelhadi, Sameh Sorour, Hesham ElSawy, Sara A. Elsayed, Hossam S. Hassanein
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Extreme Edge Computing (EEC) pushes computing even closer to end users than traditional Multi-access Edge Computing (MEC), harnessing the idle resources of Extreme Edge Devices (EEDs) to enable low-latency, distributed processing. However, EEC faces key challenges, including spatial randomness in de</span>
                
                <span class="abstract-full" style="display: none;">Extreme Edge Computing (EEC) pushes computing even closer to end users than traditional Multi-access Edge Computing (MEC), harnessing the idle resources of Extreme Edge Devices (EEDs) to enable low-latency, distributed processing. However, EEC faces key challenges, including spatial randomness in device distribution, limited EED computational power necessitating parallel task execution, vulnerability to failure, and temporal randomness due to variability in wireless communication and execution times. These challenges highlight the need for a rigorous analytical framework to evaluate EEC performance. We present the first spatiotemporal mathematical model for EEC over large-scale millimeter-wave networks. Utilizing stochastic geometry and an Absorbing Continuous-Time Markov Chain (ACTMC), the framework captures the complex interaction between communication and computation performance, including their temporal overlap during parallel execution. We evaluate two key metrics: average task response delay and task completion probability. Together, they provide a holistic view of latency and reliability. The analysis considers fundamental offloading strategies, including randomized and location-aware schemes, while accounting for EED failures. Results show that there exists an optimal task segmentation that minimizes delay. Under limited EED availability, we investigate a bias-based EEC and MEC collaboration that offloads excess demand to MEC resources, effectively reducing congestion and improving system responsiveness.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.2 -->
                    
                <!-- Medicine: 7.5 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9468
                </span>
                <a href="https://arxiv.org/abs/2504.18323" target="_blank" rel="noopener noreferrer">Outlier-aware Tensor Robust Principal Component Analysis with Self-guided Data Augmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yangyang Xu, Kexin Li, Li Yang, You-Wei Wen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique for decomposing multi-dimensional data into a low-rank tensor and an outlier tensor, yet existing methods relying on sparse outlier assumptions often fail under structured corruptions. In this paper, we propose a self-guid</span>
                
                <span class="abstract-full" style="display: none;">Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique for decomposing multi-dimensional data into a low-rank tensor and an outlier tensor, yet existing methods relying on sparse outlier assumptions often fail under structured corruptions. In this paper, we propose a self-guided data augmentation approach that employs adaptive weighting to suppress outlier influence, reformulating the original TRPCA problem into a standard Tensor Principal Component Analysis (TPCA) problem. The proposed model involves an optimization-driven weighting scheme that dynamically identifies and downweights outlier contributions during tensor augmentation. We develop an efficient proximal block coordinate descent algorithm with closed-form updates to solve the resulting optimization problem, ensuring computational efficiency. Theoretical convergence is guaranteed through a framework combining block coordinate descent with majorization-minimization principles. Numerical experiments on synthetic and real-world datasets, including face recovery, background subtraction, and hyperspectral denoising, demonstrate that our method effectively handles various corruption patterns. The results show the improvements in both accuracy and computational efficiency compared to state-of-the-art methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.5 -->
                    
                <!-- LLMs: 7.4 -->
                    
                <!-- 3D: 3.1 -->
                    
                <!-- GNN: 2.7 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9965
                </span>
                <a href="https://arxiv.org/abs/2504.18522" target="_blank" rel="noopener noreferrer">Representation Learning for Distributional Perturbation Extrapolation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Julius von K\"ugelgen, Jakob Ketterer, Xinwei Shen, Nicolai Meinshausen, Jonas Peters
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We consider the problem of modelling the effects of unseen perturbations such as gene knockdowns or drug combinations on low-level measurements such as RNA sequencing data. Specifically, given data collected under some perturbations, we aim to predict the distribution of measurements for new perturb</span>
                
                <span class="abstract-full" style="display: none;">We consider the problem of modelling the effects of unseen perturbations such as gene knockdowns or drug combinations on low-level measurements such as RNA sequencing data. Specifically, given data collected under some perturbations, we aim to predict the distribution of measurements for new perturbations. To address this challenging extrapolation task, we posit that perturbations act additively in a suitable, unknown embedding space. More precisely, we formulate the generative process underlying the observed data as a latent variable model, in which perturbations amount to mean shifts in latent space and can be combined additively. Unlike previous work, we prove that, given sufficiently diverse training perturbations, the representation and perturbation effects are identifiable up to affine transformation, and use this to characterize the class of unseen perturbations for which we obtain extrapolation guarantees. To estimate the model from data, we propose a new method, the perturbation distribution autoencoder (PDAE), which is trained by maximising the distributional similarity between true and predicted perturbation distributions. The trained model can then be used to predict previously unseen perturbation distributions. Empirical evidence suggests that PDAE compares favourably to existing methods and baselines at predicting the effects of unseen perturbations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.0 -->
                    
                <!-- LLMs: 4.7 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.0336
                </span>
                <a href="https://arxiv.org/abs/2504.18003" target="_blank" rel="noopener noreferrer">Self-Balancing, Memory Efficient, Dynamic Metric Space Data Maintenance, for Rapid Multi-Kernel Estimation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aditya S Ellendula, Chandrajit Bajaj
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a dynamic self-balancing octree data structure that enables efficient neighborhood maintenance in evolving metric spaces, a key challenge in modern machine learning systems. Many learning and generative models operate as dynamical systems whose representations evolve during training, requ</span>
                
                <span class="abstract-full" style="display: none;">We present a dynamic self-balancing octree data structure that enables efficient neighborhood maintenance in evolving metric spaces, a key challenge in modern machine learning systems. Many learning and generative models operate as dynamical systems whose representations evolve during training, requiring fast, adaptive spatial organization. Our two-parameter octree supports logarithmic-time updates and queries, eliminating the need for costly full rebuilds as data distributions shift. We demonstrate its effectiveness in four areas: (1) accelerating Stein variational gradient descent by supporting more particles with lower overhead; (2) enabling real-time, incremental KNN classification with logarithmic complexity; (3) facilitating efficient, dynamic indexing and retrieval for retrieval-augmented generation; and (4) improving sample efficiency by jointly optimizing input and latent spaces. Across all applications, our approach yields exponential speedups while preserving accuracy, particularly in high-dimensional spaces where maintaining adaptive spatial structure is critical.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.9 -->
                    
                <!-- LLMs: 7.1 -->
                    
                <!-- Quantum Computing: 4.4 -->
                    
                <!-- 3D: 3.1 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.0585
                </span>
                <a href="https://arxiv.org/abs/2503.08263" target="_blank" rel="noopener noreferrer">Analyzing the Usage of Donation Platforms for PyPI Libraries</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alexandros Tsakpinis, Alexander Pretschner
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Software systems rely heavily on open source software (OSS) libraries, which offer benefits but also pose risks. When vulnerabilities arise, the OSS community may struggle to address them due to inactivity or lack of resources. Research highlights the link between OSS maintenance and financial suppo</span>
                
                <span class="abstract-full" style="display: none;">Software systems rely heavily on open source software (OSS) libraries, which offer benefits but also pose risks. When vulnerabilities arise, the OSS community may struggle to address them due to inactivity or lack of resources. Research highlights the link between OSS maintenance and financial support. To sustain the OSS ecosystem, maintainers should register on donation platforms and link these profiles on their project pages, enabling financial support from users and industry stakeholders. However, a detailed study on donation platform usage in OSS is missing. This study analyzes the adoption of donation platforms in the PyPI ecosystem. For each PyPI library, we retrieve assigned URLs, dependencies, and, when available, owner type and GitHub donation links. Using PageRank, we analyze different subsets of libraries from both a library and dependency chain perspective. Our findings reveal that donation platform links are often omitted from PyPI project pages and instead listed on GitHub repositories. GitHub Sponsors is the dominant platform, though many PyPI-listed links are outdated, emphasizing the need for automated link verification. Adoption rates vary significantly across libraries and dependency chains: while individual PyPI libraries show low adoption, those used as dependencies have much higher usage. This suggests that many dependencies actively seek financial support, benefiting developers relying on PyPI libraries.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.2 -->
                    
                <!-- LLMs: 4.6 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Blockchain: 2.4 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.0601
                </span>
                <a href="https://arxiv.org/abs/2504.18151" target="_blank" rel="noopener noreferrer">Leveraging Decoder Architectures for Learned Sparse Retrieval</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jingfen Qiao, Thong Nguyen, Evangelos Kanoulas, Andrew Yates
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Learned Sparse Retrieval (LSR) has traditionally focused on small-scale encoder-only transformer architectures. With the advent of large-scale pre-trained language models, their capability to generate sparse representations for retrieval tasks across different transformer-based architectures, includ</span>
                
                <span class="abstract-full" style="display: none;">Learned Sparse Retrieval (LSR) has traditionally focused on small-scale encoder-only transformer architectures. With the advent of large-scale pre-trained language models, their capability to generate sparse representations for retrieval tasks across different transformer-based architectures, including encoder-only, decoder-only, and encoder-decoder models, remains largely unexplored. This study investigates the effectiveness of LSR across these architectures, exploring various sparse representation heads and model scales. Our results highlight the limitations of using large language models to create effective sparse representations in zero-shot settings, identifying challenges such as inappropriate term expansions and reduced performance due to the lack of expansion. We find that the encoder-decoder architecture with multi-tokens decoding approach achieves the best performance among the three backbones. While the decoder-only model performs worse than the encoder-only model, it demonstrates the potential to outperform when scaled to a high number of parameters.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.5 -->
                    
                <!-- Medicine: 7.3 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.0987
                </span>
                <a href="https://arxiv.org/abs/2504.17964" target="_blank" rel="noopener noreferrer">Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Celia Chen, Alex Leitch
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper examines how graduate students develop frameworks for evaluating machine-generated expertise in web-based interactions with large language models (LLMs). Through a qualitative study combining surveys, LLM interaction transcripts, and in-depth interviews with 14 graduate students, we ident</span>
                
                <span class="abstract-full" style="display: none;">This paper examines how graduate students develop frameworks for evaluating machine-generated expertise in web-based interactions with large language models (LLMs). Through a qualitative study combining surveys, LLM interaction transcripts, and in-depth interviews with 14 graduate students, we identify patterns in how these emerging professionals assess and engage with AI-generated content. Our findings reveal that students construct evaluation frameworks shaped by three main factors: professional identity, verification capabilities, and system navigation experience. Rather than uniformly accepting or rejecting LLM outputs, students protect domains central to their professional identities while delegating others--with managers preserving conceptual work, designers safeguarding creative processes, and programmers maintaining control over core technical expertise. These evaluation frameworks are further influenced by students' ability to verify different types of content and their experience navigating complex systems. This research contributes to web science by highlighting emerging human-genAI interaction patterns and suggesting how platforms might better support users in developing effective frameworks for evaluating machine-generated expertise signals in AI-mediated web environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 18.1 -->
                    
                <!-- Medicine: 7.1 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- RAG: 2.6 -->
                    
                <!-- 3D: 2.5 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1186
                </span>
                <a href="https://arxiv.org/abs/2504.12285" target="_blank" rel="noopener noreferrer">BitNet b1.58 2B4T Technical Report</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shuming Ma, Hongyu Wang, Shaohan Huang, Xingxing Zhang, Ying Hu, Ting Song, Yan Xia, Furu Wei
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding profici</span>
                
                <span class="abstract-full" style="display: none;">We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 13.6 -->
                    
                <!-- Medicine: 8.7 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1296
                </span>
                <a href="https://arxiv.org/abs/2504.18054" target="_blank" rel="noopener noreferrer">A locking free multiscale method for linear elasticity in stress-displacement formulation with high contrast coefficients</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Eric T. Chung, Changqing Ye, Xiang Zhong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Achieving strongly symmetric stress approximations for linear elasticity problems in high-contrast media poses a significant computational challenge. Conventional methods often struggle with prohibitively high computational costs due to excessive degrees of freedom, limiting their practical applicab</span>
                
                <span class="abstract-full" style="display: none;">Achieving strongly symmetric stress approximations for linear elasticity problems in high-contrast media poses a significant computational challenge. Conventional methods often struggle with prohibitively high computational costs due to excessive degrees of freedom, limiting their practical applicability. To overcome this challenge, we introduce an efficient multiscale model reduction method and a computationally inexpensive coarse-grid simulation technique for linear elasticity equations in highly heterogeneous, high-contrast media. We first utilize a stable stress-displacement mixed finite element method to discretize the linear elasticity problem and then present the construction of multiscale basis functions for the displacement and the stress. The mixed formulation offers several advantages such as direct stress computation without post-processing, local momentum conservation (ensuring physical consistency), and robustness against locking effects, even for nearly incompressible materials. Theoretical analysis confirms that our method is inf-sup stable and locking-free, with first-order convergence relative to the coarse mesh size. Notably, the convergence remains independent of contrast ratios as enlarging oversampling regions. Numerical experiments validate the method's effectiveness, demonstrating its superior performance even under extreme contrast conditions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.3 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1468
                </span>
                <a href="https://arxiv.org/abs/2504.17999" target="_blank" rel="noopener noreferrer">Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chang Xiao, Brenda Yang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generative conversational interfaces powered by large language models (LLMs) typically stream output token-by-token at a rate determined by computational budget, often neglecting actual human reading speeds and the cognitive load associated with the content. This mismatch frequently leads to ineffic</span>
                
                <span class="abstract-full" style="display: none;">Generative conversational interfaces powered by large language models (LLMs) typically stream output token-by-token at a rate determined by computational budget, often neglecting actual human reading speeds and the cognitive load associated with the content. This mismatch frequently leads to inefficient use of computational resources. For example, in cloud-based services, streaming content faster than users can read appears unnecessary, resulting in wasted computational resources and potential delays for other users, particularly during peak usage periods. To address this issue, we propose an adaptive streaming method that dynamically adjusts the pacing of LLM streaming output in real-time based on inferred cognitive load. Our approach estimates the cognitive load associated with streaming content and strategically slows down the stream during complex or information-rich segments, thereby freeing computational resources for other users. Our statistical analysis of computational savings, combined with crowdsourced user studies, provides insights into the trade-offs between service efficiency and user satisfaction, demonstrating that our method can significantly reduce computational consumption up to 16.8\%. This context-aware computational resource management strategy presents a practical framework for enhancing system efficiency in cloud-based conversational AI interfaces without compromising user experience.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 15.8 -->
                    
                <!-- Medicine: 8.1 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1565
                </span>
                <a href="https://arxiv.org/abs/2501.00057" target="_blank" rel="noopener noreferrer">VisTabNet: Adapting Vision Transformers for Tabular Data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Witold Wydma\'nski, Ulvi Movsum-zada, Jacek Tabor, Marek \'Smieja
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is cha</span>
                
                <span class="abstract-full" style="display: none;">Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is challenging to transfer large-scale pre-trained models to downstream tasks defined on small tabular datasets. To address this, we propose VisTabNet -- a cross-modal transfer learning method, which allows for adapting Vision Transformer (ViT) with pre-trained weights to process tabular data. By projecting tabular inputs to patch embeddings acceptable by ViT, we can directly apply a pre-trained Transformer Encoder to tabular inputs. This approach eliminates the conceptual cost of designing a suitable architecture for processing tabular data, while reducing the computational cost of training the model from scratch. Experimental results on multiple small tabular datasets (less than 1k samples) demonstrate VisTabNet's superiority, outperforming both traditional ensemble methods and recent deep learning models. The proposed method goes beyond conventional transfer learning practice and shows that pre-trained image models can be transferred to solve tabular problems, extending the boundaries of transfer learning. We share our example implementation as a GitHub repository available at https://github.com/wwydmanski/VisTabNet.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.0 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1746
                </span>
                <a href="https://arxiv.org/abs/2504.18072" target="_blank" rel="noopener noreferrer">A Model Zoo on Phase Transitions in Neural Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Konstantin Sch\"urholt, L\'eo Meynent, Yefan Zhou, Haiquan Lu, Yaoqing Yang, Damian Borth
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Using the weights of trained Neural Network (NN) models as data modality has recently gained traction as a research field - dubbed Weight Space Learning (WSL). Multiple recent works propose WSL methods to analyze models, evaluate methods, or synthesize weights. Weight space learning methods require </span>
                
                <span class="abstract-full" style="display: none;">Using the weights of trained Neural Network (NN) models as data modality has recently gained traction as a research field - dubbed Weight Space Learning (WSL). Multiple recent works propose WSL methods to analyze models, evaluate methods, or synthesize weights. Weight space learning methods require populations of trained models as datasets for development and evaluation. However, existing collections of models - called `model zoos' - are unstructured or follow a rudimentary definition of diversity. In parallel, work rooted in statistical physics has identified phases and phase transitions in NN models. Models are homogeneous within the same phase but qualitatively differ from one phase to another. We combine the idea of `model zoos' with phase information to create a controlled notion of diversity in populations. We introduce 12 large-scale zoos that systematically cover known phases and vary over model architecture, size, and datasets. These datasets cover different modalities, such as computer vision, natural language processing, and scientific ML. For every model, we compute loss landscape metrics and validate full coverage of the phases. With this dataset, we provide the community with a resource with a wide range of potential applications for WSL and beyond. Evidence suggests the loss landscape phase plays a role in applications such as model training, analysis, or sparsification. We demonstrate this in an exploratory study of the downstream methods like transfer learning or model weights averaging.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.0 -->
                    
                <!-- LLMs: 7.0 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1777
                </span>
                <a href="https://arxiv.org/abs/2504.18332" target="_blank" rel="noopener noreferrer">SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shuting Zhao, Linxin Bai, Liangjing Shao, Ye Zhang, Xinrong Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The growing applications of AR/VR increase the demand for real-time full-body pose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint signals from the head and hands, reconstructing a full-body pose remains challenging due to the unconstrained lower body. Recent advancements o</span>
                
                <span class="abstract-full" style="display: none;">The growing applications of AR/VR increase the demand for real-time full-body pose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint signals from the head and hands, reconstructing a full-body pose remains challenging due to the unconstrained lower body. Recent advancements often rely on conventional neural networks and generative models to improve performance in this task, such as Transformers and diffusion models. However, these approaches struggle to strike a balance between achieving precise pose reconstruction and maintaining fast inference speed. To overcome these challenges, a lightweight and efficient model, SSD-Poser, is designed for robust full-body motion estimation from sparse observations. SSD-Poser incorporates a well-designed hybrid encoder, State Space Attention Encoders, to adapt the state space duality to complex motion poses and enable real-time realistic pose reconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate jitter caused by variable-frequency motion signals, remarkably enhancing the motion smoothness. Comprehensive experiments on the AMASS dataset demonstrate that SSD-Poser achieves exceptional accuracy and computational efficiency, showing outstanding inference efficiency compared to state-of-the-art methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.7 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- 3D: 2.9 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2189
                </span>
                <a href="https://arxiv.org/abs/2504.18356" target="_blank" rel="noopener noreferrer">Numerical method for the inverse scattering by random periodic structures</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yi Wang, Lei Lin, Junliang Lv
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Due to manufacturing defects or wear and tear, industrial components may have uncertainties. In order to evaluate the performance of machined components, it is crucial to quantify the uncertainty of the scattering surface. This brings up an important class of inverse scattering problems for random i</span>
                
                <span class="abstract-full" style="display: none;">Due to manufacturing defects or wear and tear, industrial components may have uncertainties. In order to evaluate the performance of machined components, it is crucial to quantify the uncertainty of the scattering surface. This brings up an important class of inverse scattering problems for random interface reconstruction. In this paper, we present an efficient numerical algorithm for the inverse scattering problem of acoustic-elastic interaction with random periodic interfaces. The proposed algorithm combines the Monte Carlo technique and the continuation method with respect to the wavenumber, which can accurately reconstruct the key statistics of random periodic interfaces from the measured data of the acoustic scattered field. In the implementation of our algorithm, a key two-step strategy is employed: Firstly, the elastic displacement field below the interface is determined by Tikhonov regularization based on the dynamic interface condition; Secondly, the profile function is iteratively updated and optimised using the Landweber method according to the kinematic interface condition. Such a algorithm does not require a priori information about the stochastic structures and performs well for both stationary Gaussian and non-Gaussian stochastic processes. Numerical experiments demonstrate the reliability and effectiveness of our proposed method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.7 -->
                    
                <!-- Reinforcement Learning: 4.8 -->
                    
                <!-- Math: 3.1 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Pathfinding: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2404
                </span>
                <a href="https://arxiv.org/abs/1711.08058" target="_blank" rel="noopener noreferrer">Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ahmad AbdulKader, Kareem Nassar, Mohamed El-Geish, Daniel Galvez, Chetan Patil
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose using cascaded classifiers for a keyword spotting (KWS) task on narrow-band (NB), 8kHz audio acquired in non-IID environments -- a more challenging task than most state-of-the-art KWS systems face. We present a model that incorporates Deep Neural Networks (DNNs), cascading, multiple-featu</span>
                
                <span class="abstract-full" style="display: none;">We propose using cascaded classifiers for a keyword spotting (KWS) task on narrow-band (NB), 8kHz audio acquired in non-IID environments -- a more challenging task than most state-of-the-art KWS systems face. We present a model that incorporates Deep Neural Networks (DNNs), cascading, multiple-feature representations, and multiple-instance learning. The cascaded classifiers handle the task's class imbalance and reduce power consumption on computationally-constrained devices via early termination. The KWS system achieves a false negative rate of 6% at an hourly false positive rate of 0.75</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.0 -->
                    
                <!-- LLMs: 5.6 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- 3D: 2.7 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- RAG: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2972
                </span>
                <a href="https://arxiv.org/abs/2504.17865" target="_blank" rel="noopener noreferrer">Set Phasers to Stun: Beaming Power and Control to Mobile Robots with Laser Light</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Charles J. Carver, Hadleigh Schwartz, Toma Itagaki, Zachary Englhardt, Kechen Liu, Megan Graciela Nauli Manik, Chun-Cheng Chang, Vikram Iyer, Brian Plancher, Xia Zhou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present Phaser, a flexible system that directs narrow-beam laser light to moving robots for concurrent wireless power delivery and communication. We design a semi-automatic calibration procedure to enable fusion of stereo-vision-based 3D robot tracking with high-power beam steering, and a low-pow</span>
                
                <span class="abstract-full" style="display: none;">We present Phaser, a flexible system that directs narrow-beam laser light to moving robots for concurrent wireless power delivery and communication. We design a semi-automatic calibration procedure to enable fusion of stereo-vision-based 3D robot tracking with high-power beam steering, and a low-power optical communication scheme that reuses the laser light as a data channel. We fabricate a Phaser prototype using off-the-shelf hardware and evaluate its performance with battery-free autonomous robots. Phaser delivers optical power densities of over 110 mW/cm$^2$ and error-free data to mobile robots at multi-meter ranges, with on-board decoding drawing 0.3 mA (97\% less current than Bluetooth Low Energy). We demonstrate Phaser fully powering gram-scale battery-free robots to nearly 2x higher speeds than prior work while simultaneously controlling them to navigate around obstacles and along paths. Code, an open-source design guide, and a demonstration video of Phaser is available at https://mobilex.cs.columbia.edu/phaser.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.5 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Networks: 4.4 -->
                    
                <!-- 3D: 3.0 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.303
                </span>
                <a href="https://arxiv.org/abs/2504.18455" target="_blank" rel="noopener noreferrer">Generalization Guarantees for Multi-View Representation Learning and Application to Regularization via Gaussian Product Mixture Prior</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Milad Sefidgaran, Abdellatif Zaidi, Piotr Krasnowski
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the problem of distributed multi-view representation learning. In this problem, $K$ agents observe each one distinct, possibly statistically correlated, view and independently extracts from it a suitable representation in a manner that a decoder that gets all $K$ representations estimates c</span>
                
                <span class="abstract-full" style="display: none;">We study the problem of distributed multi-view representation learning. In this problem, $K$ agents observe each one distinct, possibly statistically correlated, view and independently extracts from it a suitable representation in a manner that a decoder that gets all $K$ representations estimates correctly the hidden label. In the absence of any explicit coordination between the agents, a central question is: what should each agent extract from its view that is necessary and sufficient for a correct estimation at the decoder? In this paper, we investigate this question from a generalization error perspective. First, we establish several generalization bounds in terms of the relative entropy between the distribution of the representations extracted from training and "test" datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for all views and training and test datasets. Then, we use the obtained bounds to devise a regularizer; and investigate in depth the question of the selection of a suitable prior. In particular, we show and conduct experiments that illustrate that our data-dependent Gaussian mixture priors with judiciously chosen weights lead to good performance. For single-view settings (i.e., $K=1$), our experimental results are shown to outperform existing prior art Variational Information Bottleneck (VIB) and Category-Dependent VIB (CDVIB) approaches. Interestingly, we show that a weighted attention mechanism emerges naturally in this setting. Finally, for the multi-view setting, we show that the selection of the joint prior as a Gaussians product mixture induces a Gaussian mixture marginal prior for each marginal view and implicitly encourages the agents to extract and output redundant features, a finding which is somewhat counter-intuitive.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.1 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3178
                </span>
                <a href="https://arxiv.org/abs/2504.17898" target="_blank" rel="noopener noreferrer">Material Identification Via RFID For Smart Shopping</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: David Wang, Derek Goh, Jiale Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Cashierless stores rely on computer vision and RFID tags to associate shoppers with items, but concealed items placed in backpacks, pockets, or bags create challenges for theft prevention. We introduce a system that turns existing RFID tagged items into material sensors by exploiting how different c</span>
                
                <span class="abstract-full" style="display: none;">Cashierless stores rely on computer vision and RFID tags to associate shoppers with items, but concealed items placed in backpacks, pockets, or bags create challenges for theft prevention. We introduce a system that turns existing RFID tagged items into material sensors by exploiting how different containers attenuate and scatter RF signals. Using RSSI and phase angle, we trained a neural network to classify seven common containers. In a simulated retail environment, the model achieves 89% accuracy with one second samples and 74% accuracy from single reads. Incorporating distance measurements, our system achieves 82% accuracy across 0.3-2m tag to reader separations. When deployed at aisle or doorway choke points, the system can flag suspicious events in real time, prompting camera screening or staff intervention. By combining material identification with computer vision tracking, our system provides proactive loss prevention for cashierless retail while utilizing existing infrastructure.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.8 -->
                    
                <!-- LLMs: 6.2 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- 3D: 3.1 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.326
                </span>
                <a href="https://arxiv.org/abs/2504.18325" target="_blank" rel="noopener noreferrer">Depth3DLane: Monocular 3D Lane Detection via Depth Prior Distillation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dongxin Lyu, Han Huang, Cheng Tan, Zimu Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Monocular 3D lane detection is challenging due to the difficulty in capturing depth information from single-camera images. A common strategy involves transforming front-view (FV) images into bird's-eye-view (BEV) space through inverse perspective mapping (IPM), facilitating lane detection using BEV </span>
                
                <span class="abstract-full" style="display: none;">Monocular 3D lane detection is challenging due to the difficulty in capturing depth information from single-camera images. A common strategy involves transforming front-view (FV) images into bird's-eye-view (BEV) space through inverse perspective mapping (IPM), facilitating lane detection using BEV features. However, IPM's flat-ground assumption and loss of contextual information lead to inaccuracies in reconstructing 3D information, especially height. In this paper, we introduce a BEV-based framework to address these limitations and improve 3D lane detection accuracy. Our approach incorporates a Hierarchical Depth-Aware Head that provides multi-scale depth features, mitigating the flat-ground assumption by enhancing spatial awareness across varying depths. Additionally, we leverage Depth Prior Distillation to transfer semantic depth knowledge from a teacher model, capturing richer structural and contextual information for complex lane structures. To further refine lane continuity and ensure smooth lane reconstruction, we introduce a Conditional Random Field module that enforces spatial coherence in lane predictions. Extensive experiments validate that our method achieves state-of-the-art performance in terms of z-axis error and outperforms other methods in the field in overall performance. The code is released at: https://anonymous.4open.science/r/Depth3DLane-DCDD.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.3 -->
                    
                <!-- Medicine: 6.2 -->
                    
                <!-- 3D: 5.2 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- GNN: 2.8 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3542
                </span>
                <a href="https://arxiv.org/abs/2504.18463" target="_blank" rel="noopener noreferrer">A Taylor Series Approach to Correction of Input Errors in Gaussian Process Regression</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Muzaffar Qureshi, Tochukwu Elijah Ogri, Zachary I. Bell, Wanjiku A. Makumi, Rushikesh Kamalapurkar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Gaussian Processes (GPs) are widely recognized as powerful non-parametric models for regression and classification. Traditional GP frameworks predominantly operate under the assumption that the inputs are either accurately known or subject to zero-mean noise. However, several real-world applications</span>
                
                <span class="abstract-full" style="display: none;">Gaussian Processes (GPs) are widely recognized as powerful non-parametric models for regression and classification. Traditional GP frameworks predominantly operate under the assumption that the inputs are either accurately known or subject to zero-mean noise. However, several real-world applications such as mobile sensors have imperfect localization, leading to inputs with biased errors. These biases can typically be estimated through measurements collected over time using, for example, Kalman filters. To avoid recomputation of the entire GP model when better estimates of the inputs used in the training data become available, we introduce a technique for updating a trained GP model to incorporate updated estimates of the inputs. By leveraging the differentiability of the mean and covariance functions derived from the squared exponential kernel, a second-order correction algorithm is developed to update the trained GP models. Precomputed Jacobians and Hessians of kernels enable real-time refinement of the mean and covariance predictions. The efficacy of the developed approach is demonstrated using two simulation studies, with error analyses revealing improvements in both predictive accuracy and uncertainty quantification.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.8 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3576
                </span>
                <a href="https://arxiv.org/abs/2409.09781" target="_blank" rel="noopener noreferrer">RandALO: Out-of-sample risk estimation in no time flat</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Parth Nobel, Daniel LeJeune, Emmanuel J. Cand\`es
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Estimating out-of-sample risk for models trained on large high-dimensional datasets is an expensive but essential part of the machine learning process, enabling practitioners to optimally tune hyperparameters. Cross-validation (CV) serves as the de facto standard for risk estimation but poorly trade</span>
                
                <span class="abstract-full" style="display: none;">Estimating out-of-sample risk for models trained on large high-dimensional datasets is an expensive but essential part of the machine learning process, enabling practitioners to optimally tune hyperparameters. Cross-validation (CV) serves as the de facto standard for risk estimation but poorly trades off high bias ($K$-fold CV) for computational cost (leave-one-out CV). We propose a randomized approximate leave-one-out (RandALO) risk estimator that is not only a consistent estimator of risk in high dimensions but also less computationally expensive than $K$-fold CV. We support our claims with extensive simulations on synthetic and real data and provide a user-friendly Python package implementing RandALO available on PyPI as randalo and at https://github.com/cvxgrp/randalo.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.4 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4772
                </span>
                <a href="https://arxiv.org/abs/2504.18271" target="_blank" rel="noopener noreferrer">LEAM: A Prompt-only Large Language Model-enabled Antenna Modeling Method</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tao Wu, Kexue Fu, Qiang Hua, Xinxin Liu, Muhammad Ali Imran, Bo Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Antenna modeling is a time-consuming and complex process, decreasing the speed of antenna analysis and design. In this paper, a large language model (LLM)- enabled antenna modeling method, called LEAM, is presented to address this challenge. LEAM enables automatic antenna model generation based on l</span>
                
                <span class="abstract-full" style="display: none;">Antenna modeling is a time-consuming and complex process, decreasing the speed of antenna analysis and design. In this paper, a large language model (LLM)- enabled antenna modeling method, called LEAM, is presented to address this challenge. LEAM enables automatic antenna model generation based on language descriptions via prompt input, images, descriptions from academic papers, patents, and technical reports (either one or multiple). The effectiveness of LEAM is demonstrated by three examples: a Vivaldi antenna generated from a complete user description, a slotted patch antenna generated from an incomplete user description and the operating frequency, and a monopole slotted antenna generated from images and descriptions scanned from the literature. For all the examples, correct antenna models are generated in a few minutes. The code can be accessed via https://github.com/TaoWu974/LEAM.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.8 -->
                    
                <!-- LLMs: 7.7 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4882
                </span>
                <a href="https://arxiv.org/abs/2504.18064" target="_blank" rel="noopener noreferrer">AllTact Fin Ray: A Compliant Robot Gripper with Omni-Directional Tactile Sensing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Siwei Liang, Yixuan Guan, Jing Xu, Hongyu Qian, Xiangjun Zhang, Dan Wu, Wenbo Ding, Rui Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Tactile sensing plays a crucial role in robot grasping and manipulation by providing essential contact information between the robot and the environment. In this paper, we present AllTact Fin Ray, a novel compliant gripper design with omni-directional and local tactile sensing capabilities. The fing</span>
                
                <span class="abstract-full" style="display: none;">Tactile sensing plays a crucial role in robot grasping and manipulation by providing essential contact information between the robot and the environment. In this paper, we present AllTact Fin Ray, a novel compliant gripper design with omni-directional and local tactile sensing capabilities. The finger body is unibody-casted using transparent elastic silicone, and a camera positioned at the base of the finger captures the deformation of the whole body and the contact face. Due to the global deformation of the adaptive structure, existing vision-based tactile sensing approaches that assume constant illumination are no longer applicable. To address this, we propose a novel sensing method where the global deformation is first reconstructed from the image using edge features and spatial constraints. Then, detailed contact geometry is computed from the brightness difference against a dynamically retrieved reference image. Extensive experiments validate the effectiveness of our proposed gripper design and sensing method in contact detection, force estimation, object grasping, and precise manipulation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.1 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.5104
                </span>
                <a href="https://arxiv.org/abs/2504.18519" target="_blank" rel="noopener noreferrer">Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Han Zhang, Hao Zhou, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Federated learning (FL) is a promising technique for learning-based functions in wireless networks, thanks to its distributed implementation capability. On the other hand, distributed learning may increase the risk of exposure to malicious attacks where attacks on a local model may spread to other m</span>
                
                <span class="abstract-full" style="display: none;">Federated learning (FL) is a promising technique for learning-based functions in wireless networks, thanks to its distributed implementation capability. On the other hand, distributed learning may increase the risk of exposure to malicious attacks where attacks on a local model may spread to other models by parameter exchange. Meanwhile, such attacks can be hard to detect due to the dynamic wireless environment, especially considering local models can be heterogeneous with non-independent and identically distributed (non-IID) data. Therefore, it is critical to evaluate the effect of malicious attacks and develop advanced defense techniques for FL-enabled wireless networks. In this work, we introduce a federated deep reinforcement learning-based cell sleep control scenario that enhances the energy efficiency of the network. We propose multiple intelligent attacks targeting the learning-based approach and we propose defense methods to mitigate such attacks. In particular, we have designed two attack models, generative adversarial network (GAN)-enhanced model poisoning attack and regularization-based model poisoning attack. As a counteraction, we have proposed two defense schemes, autoencoder-based defense, and knowledge distillation (KD)-enabled defense. The autoencoder-based defense method leverages an autoencoder to identify the malicious participants and only aggregate the parameters of benign local models during the global aggregation, while KD-based defense protects the model from attacks by controlling the knowledge transferred between the global model and local models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.5 -->
                    
                <!-- LLMs: 5.4 -->
                    
                <!-- Federated Learning: 3.6 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.562
                </span>
                <a href="https://arxiv.org/abs/2504.18078" target="_blank" rel="noopener noreferrer">Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiaolu Chen, Chenghao Huang, Yanru Zhang, Hao Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rapid expansion of distributed photovoltaic (PV) installations worldwide, many being behind-the-meter systems, has significantly challenged energy management and grid operations, as unobservable PV generation further complicates the supply-demand balance. Therefore, estimating this generation fr</span>
                
                <span class="abstract-full" style="display: none;">The rapid expansion of distributed photovoltaic (PV) installations worldwide, many being behind-the-meter systems, has significantly challenged energy management and grid operations, as unobservable PV generation further complicates the supply-demand balance. Therefore, estimating this generation from net load, known as PV disaggregation, is critical. Given privacy concerns and the need for large training datasets, federated learning becomes a promising approach, but statistical heterogeneity, arising from geographical and behavioral variations among prosumers, poses new challenges to PV disaggregation. To overcome these challenges, a privacy-preserving distributed PV disaggregation framework is proposed using Personalized Federated Learning (PFL). The proposed method employs a two-level framework that combines local and global modeling. At the local level, a transformer-based PV disaggregation model is designed to generate solar irradiance embeddings for representing local PV conditions. A novel adaptive local aggregation mechanism is adopted to mitigate the impact of statistical heterogeneity on the local model, extracting a portion of global information that benefits the local model. At the global level, a central server aggregates information uploaded from multiple data centers, preserving privacy while enabling cross-center knowledge sharing. Experiments on real-world data demonstrate the effectiveness of this proposed framework, showing improved accuracy and robustness compared to benchmark methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.5 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.6335
                </span>
                <a href="https://arxiv.org/abs/2504.03096" target="_blank" rel="noopener noreferrer">Scaling Open-Vocabulary Action Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhen Hao Sia, Yogesh Singh Rawat
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this work, we focus on scaling open-vocabulary action detection. Existing approaches for action detection are predominantly limited to closed-set scenarios and rely on complex, parameter-heavy architectures. Extending these models to the open-vocabulary setting poses two key challenges: (1) the l</span>
                
                <span class="abstract-full" style="display: none;">In this work, we focus on scaling open-vocabulary action detection. Existing approaches for action detection are predominantly limited to closed-set scenarios and rely on complex, parameter-heavy architectures. Extending these models to the open-vocabulary setting poses two key challenges: (1) the lack of large-scale datasets with many action classes for robust training, and (2) parameter-heavy adaptations to a pretrained vision-language contrastive model to convert it for detection, risking overfitting the additional non-pretrained parameters to base action classes. Firstly, we introduce an encoder-only multimodal model for video action detection, reducing the reliance on parameter-heavy additions for video action detection. Secondly, we introduce a simple weakly supervised training strategy to exploit an existing closed-set action detection dataset for pretraining. Finally, we depart from the ill-posed base-to-novel benchmark used by prior works in open-vocabulary action detection and devise a new benchmark to evaluate on existing closed-set action detection datasets without ever using them for training, showing novel results to serve as baselines for future work. Our code is available at: https://siatheindochinese.github.io/sia_act_page/</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.7 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.6468
                </span>
                <a href="https://arxiv.org/abs/2504.18215" target="_blank" rel="noopener noreferrer">Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nanjie Yao, Gangjian Zhang, Wenhao Shen, Jian Shu, Hao Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Monocular 3D clothed human reconstruction aims to create a complete 3D avatar from a single image. To tackle the human geometry lacking in one RGB image, current methods typically resort to a preceding model for an explicit geometric representation. For the reconstruction itself, focus is on modelin</span>
                
                <span class="abstract-full" style="display: none;">Monocular 3D clothed human reconstruction aims to create a complete 3D avatar from a single image. To tackle the human geometry lacking in one RGB image, current methods typically resort to a preceding model for an explicit geometric representation. For the reconstruction itself, focus is on modeling both it and the input image. This routine is constrained by the preceding model, and overlooks the integrity of the reconstruction task. To address this, this paper introduces a novel paradigm that treats human reconstruction as a holistic process, utilizing an end-to-end network for direct prediction from 2D image to 3D avatar, eliminating any explicit intermediate geometry display. Based on this, we further propose a novel reconstruction framework consisting of two core components: the Anatomy Shaping Extraction module, which captures implicit shape features taking into account the specialty of human anatomy, and the Twins Negotiating Reconstruction U-Net, which enhances reconstruction through feature interaction between two U-Nets of different modalities. Moreover, we propose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to bolster model performance in more complex case input. Extensive experiments on two test sets and many in-the-wild cases show the superiority of our method over SOTA methods. Our demos can be found in : https://e2e3dgsrecon.github.io/e2e3dgsrecon/.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.8 -->
                    
                <!-- 3D: 4.2 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.6947
                </span>
                <a href="https://arxiv.org/abs/2504.18070" target="_blank" rel="noopener noreferrer">PropRAG: Guiding Retrieval with Beam Search over Proposition Paths</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jingjin Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Retrieval Augmented Generation (RAG) has become the standard non-parametric approach for equipping Large Language Models (LLMs) with up-to-date knowledge and mitigating catastrophic forgetting common in continual learning. However, standard RAG, relying on independent passage retrieval, fails to cap</span>
                
                <span class="abstract-full" style="display: none;">Retrieval Augmented Generation (RAG) has become the standard non-parametric approach for equipping Large Language Models (LLMs) with up-to-date knowledge and mitigating catastrophic forgetting common in continual learning. However, standard RAG, relying on independent passage retrieval, fails to capture the interconnected nature of human memory crucial for complex reasoning (associativity) and contextual understanding (sense-making). While structured RAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples, the inherent context loss limits fidelity. We introduce PropRAG, a framework leveraging contextually rich propositions and a novel beam search algorithm over proposition paths to explicitly discover multi-step reasoning chains. Crucially, PropRAG's online retrieval process operates entirely without invoking generative LLMs, relying instead on efficient graph traversal and pre-computed embeddings. This avoids online LLM inference costs and potential inconsistencies during evidence gathering. LLMs are used effectively offline for high-quality proposition extraction and post-retrieval for answer generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside top F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through richer representation and explicit, LLM-free online path finding, PropRAG advances non-parametric continual learning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 13.2 -->
                    
                <!-- Medicine: 10.3 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- RAG: 2.4 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7188
                </span>
                <a href="https://arxiv.org/abs/2504.18044" target="_blank" rel="noopener noreferrer">AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What to How</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Omid Veisi, Sasan Bahrami, Roman Englert, Claudia M\"uller
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social Computing requires the examination of ethical and social norms to ensure safe incorporation into human life. We conducted a mixed-method study, including an online survey with 111 participants and an interview study with 38 ex</span>
                
                <span class="abstract-full" style="display: none;">Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social Computing requires the examination of ethical and social norms to ensure safe incorporation into human life. We conducted a mixed-method study, including an online survey with 111 participants and an interview study with 38 experts, to investigate the AI ethics and social norms in ChatGPT as everyday life tools. This study aims to evaluate whether ChatGPT in an empirical context operates following ethics and social norms, which is critical for understanding actions in industrial and academic research and achieving machine ethics. The findings of this study provide initial insights into six important aspects of AI ethics, including bias, trustworthiness, security, toxicology, social norms, and ethical data. Significant obstacles related to transparency and bias in unsupervised data collection methods are identified as ChatGPT's ethical concerns.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 12.4 -->
                    
                <!-- Medicine: 9.6 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7707
                </span>
                <a href="https://arxiv.org/abs/2504.18344" target="_blank" rel="noopener noreferrer">NUDF: Neural Unsigned Distance Fields for high resolution 3D medical image segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kristine S{\o}rensen, Oscar Camara, Ole de Backer, Klaus Kofoed, Rasmus Paulsen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Medical image segmentation is often considered as the task of labelling each pixel or voxel as being inside or outside a given anatomy. Processing the images at their original size and resolution often result in insuperable memory requirements, but downsampling the images leads to a loss of importan</span>
                
                <span class="abstract-full" style="display: none;">Medical image segmentation is often considered as the task of labelling each pixel or voxel as being inside or outside a given anatomy. Processing the images at their original size and resolution often result in insuperable memory requirements, but downsampling the images leads to a loss of important details. Instead of aiming to represent a smooth and continuous surface in a binary voxel-grid, we propose to learn a Neural Unsigned Distance Field (NUDF) directly from the image. The small memory requirements of NUDF allow for high resolution processing, while the continuous nature of the distance field allows us to create high resolution 3D mesh models of shapes of any topology (i.e. open surfaces). We evaluate our method on the task of left atrial appendage (LAA) segmentation from Computed Tomography (CT) images. The LAA is a complex and highly variable shape, being thus difficult to represent with traditional segmentation methods using discrete labelmaps. With our proposed method, we are able to predict 3D mesh models that capture the details of the LAA and achieve accuracy in the order of the voxel spacing in the CT images.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.2 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7868
                </span>
                <a href="https://arxiv.org/abs/2404.19689" target="_blank" rel="noopener noreferrer">Continuum limit of $p$-biharmonic equations on graphs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kehan Shi, Martin Burger
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper studies the $p$-biharmonic equation on graphs, which arises in point cloud processing and can be interpreted as a natural extension of the graph $p$-Laplacian from the perspective of hypergraph. The asymptotic behavior of the solution is investigated when the random geometric graph is con</span>
                
                <span class="abstract-full" style="display: none;">This paper studies the $p$-biharmonic equation on graphs, which arises in point cloud processing and can be interpreted as a natural extension of the graph $p$-Laplacian from the perspective of hypergraph. The asymptotic behavior of the solution is investigated when the random geometric graph is considered and the number of data points goes to infinity. We show that the continuum limit is an appropriately weighted $p$-biharmonic equation with homogeneous Neumann boundary conditions. The result relies on the uniform $L^p$ estimates for solutions and gradients of nonlocal and graph Poisson equations. The $L^\infty$ estimates of solutions are also obtained as a byproduct.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.3 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Math: 2.8 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7922
                </span>
                <a href="https://arxiv.org/abs/2504.18415" target="_blank" rel="noopener noreferrer">BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hongyu Wang, Shuming Ma, Furu Wei
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward net</span>
                
                <span class="abstract-full" style="display: none;">Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 13.3 -->
                    
                <!-- Medicine: 9.8 -->
                    
                <!-- 3D: 3.2 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8014
                </span>
                <a href="https://arxiv.org/abs/2504.17883" target="_blank" rel="noopener noreferrer">PowerSensor3: A Fast and Accurate Open Source Power Measurement Tool</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Steven van der Vlugt, Leon Oostrum, Gijs Schoonderbeek, Ben van Werkhoven, Bram Veenboer, Krijn Doekemeijer, John W. Romein
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Power consumption is a major concern in data centers and HPC applications, with GPUs typically accounting for more than half of system power usage. While accurate power measurement tools are crucial for optimizing the energy efficiency of (GPU) applications, both built-in power sensors as well as st</span>
                
                <span class="abstract-full" style="display: none;">Power consumption is a major concern in data centers and HPC applications, with GPUs typically accounting for more than half of system power usage. While accurate power measurement tools are crucial for optimizing the energy efficiency of (GPU) applications, both built-in power sensors as well as state-of-the-art power meters often lack the accuracy and temporal granularity needed, or are impractical to use. Released as open hardware, firmware, and software, PowerSensor3 provides a cost-effective solution for evaluating energy efficiency, enabling advancements in sustainable computing. The toolkit consists of a baseboard with a variety of sensor modules accompanied by host libraries with C++ and Python bindings. PowerSensor3 enables real-time power measurements of SoC boards and PCIe cards, including GPUs, FPGAs, NICs, SSDs, and domain-specific AI and ML accelerators. Additionally, it provides significant improvements over previous tools, such as a robust and modular design, current sensors resistant to external interference, simplified calibration, and a sampling rate up to 20 kHz, which is essential to identify GPU behavior at high temporal granularity. This work describes the toolkit design, evaluates its performance characteristics, and shows several use cases (GPUs, NVIDIA Jetson AGX Orin, and SSD), demonstrating PowerSensor3's potential to significantly enhance energy efficiency in modern computing environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.1 -->
                    
                <!-- LLMs: 5.5 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8386
                </span>
                <a href="https://arxiv.org/abs/2504.16269" target="_blank" rel="noopener noreferrer">COBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ye Qiao, Zhiheng Chen, Yian Wang, Yifan Zhang, Yunzhe Deng, Sitao Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Transformer-based models have demonstrated superior performance in various fields, including natural language processing and computer vision. However, their enormous model size and high demands in computation, memory, and communication limit their deployment to edge platforms for local, secure infer</span>
                
                <span class="abstract-full" style="display: none;">Transformer-based models have demonstrated superior performance in various fields, including natural language processing and computer vision. However, their enormous model size and high demands in computation, memory, and communication limit their deployment to edge platforms for local, secure inference. Binary transformers offer a compact, low-complexity solution for edge deployment with reduced bandwidth needs and acceptable accuracy. However, existing binary transformers perform inefficiently on current hardware due to the lack of binary specific optimizations. To address this, we introduce COBRA, an algorithm-architecture co-optimized binary Transformer accelerator for edge computing. COBRA features a real 1-bit binary multiplication unit, enabling matrix operations with -1, 0, and +1 values, surpassing ternary methods. With further hardware-friendly optimizations in the attention block, COBRA achieves up to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x throughput improvement over the state-of-the-art binary accelerator, with only negligible inference accuracy degradation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.1 -->
                    
                <!-- LLMs: 7.1 -->
                    
                <!-- 3D: 2.6 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- T2I: 1.8 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Attention: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8417
                </span>
                <a href="https://arxiv.org/abs/2504.18155" target="_blank" rel="noopener noreferrer">Hierarchical Cell-Free Massive MIMO: A Simplified Design for Uniform Service Quality</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wei Jiang, Hans Dieter Schotten
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In traditional cellular networks, users at the cell edge often suffer from poor quality of service (QoS) due to large distance-dependent path loss and severe inter-cell interference. While cell-free (CF) massive multi-input multi-out (MIMO) mitigates this issue by distributing access points (APs) to</span>
                
                <span class="abstract-full" style="display: none;">In traditional cellular networks, users at the cell edge often suffer from poor quality of service (QoS) due to large distance-dependent path loss and severe inter-cell interference. While cell-free (CF) massive multi-input multi-out (MIMO) mitigates this issue by distributing access points (APs) to ensure uniform QoS, the deployment of numerous distributed APs and a fronthaul network incurs high infrastructure costs. To balance performance and cost efficiency, this article proposes a simplified design called hierarchical cell-free (HCF) massive MIMO. The key idea is to reduce the number of APs, thus minimizing the scale of the fronthaul network. The antennas from the decommissioned APs are aggregated at a central base station (cBS), which also serves as the coordinator for distributed APs. We derive closed-form expressions for uplink and downlink spectral efficiency (SE) for HCF, CF, and cellular massive MIMO under pilot contamination and correlated fading channels, considering the use of multi-antenna APs. Numerical results confirm that the hierarchical architecture achieves $95\%$-likely per-user SE comparable to CF, enhancing cell-edge user rates in cellular systems by over 100 times, while significantly reducing the complexity and cost of the fronthaul network in CF. We develop max-min fairness algorithms for joint power control of the cBS and APs in the downlink, and the users in the uplink. These algorithms not only boost fairness and system capacity but also dramatically lower transmission power, e.g., achieving over $70\%$ savings in uplink, particularly beneficial for battery-powered mobile devices.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.5 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Pathfinding: 1.4 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8856
                </span>
                <a href="https://arxiv.org/abs/2504.18386" target="_blank" rel="noopener noreferrer">A UD Treebank for Bohairic Coptic</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Amir Zeldes, Nina Speransky, Nicholas Wagner, Caroline T. Schroeder
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Despite recent advances in digital resources for other Coptic dialects, especially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk, late Byzantine Egypt, and the contemporary language of the Coptic Church, remains critically under-resourced. This paper presents and evaluates the fir</span>
                
                <span class="abstract-full" style="display: none;">Despite recent advances in digital resources for other Coptic dialects, especially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk, late Byzantine Egypt, and the contemporary language of the Coptic Church, remains critically under-resourced. This paper presents and evaluates the first syntactically annotated corpus of Bohairic Coptic, sampling data from a range of works, including Biblical text, saints' lives and Christian ascetic writing. We also explore some of the main differences we observe compared to the existing UD treebank of Sahidic Coptic, the classical dialect of the language, and conduct joint and cross-dialect parsing experiments, revealing the unique nature of Bohairic as a related, but distinct variety from the more often studied Sahidic.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.6 -->
                    
                <!-- LLMs: 4.3 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Hardware: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0412
                </span>
                <a href="https://arxiv.org/abs/2504.18453" target="_blank" rel="noopener noreferrer">Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Peiyuan Jing, Kinhei Lee, Zhenxuan Zhang, Huichi Zhou, Zhengqing Yuan, Zhifan Gao, Lei Zhu, Giorgos Papanastasiou, Yingying Fang, Guang Yang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Radiology report generation is critical for efficiency but current models lack the structured reasoning of experts, hindering clinical trust and explainability by failing to link visual findings to precise anatomical locations. This paper introduces BoxMed-RL, a groundbreaking unified training frame</span>
                
                <span class="abstract-full" style="display: none;">Radiology report generation is critical for efficiency but current models lack the structured reasoning of experts, hindering clinical trust and explainability by failing to link visual findings to precise anatomical locations. This paper introduces BoxMed-RL, a groundbreaking unified training framework for generating spatially verifiable and explainable radiology reports. Built on a large vision-language model, BoxMed-RL revolutionizes report generation through two integrated phases: (1) In the Pretraining Phase, we refine the model via medical concept learning, using Chain-of-Thought supervision to internalize the radiologist-like workflow, followed by spatially verifiable reinforcement, which applies reinforcement learning to align medical findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze the pretrained weights and train a downstream adapter to ensure fluent and clinically credible reports. This framework precisely mimics radiologists' workflow, compelling the model to connect high-level medical concepts with definitive anatomical evidence. Extensive experiments on public datasets demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR and ROUGE-L metrics compared to state-of-the-art methods. An average 5% improvement in large language model-based metrics further underscores BoxMed-RL's robustness in generating high-quality radiology reports.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.9 -->
                    
                <!-- LLMs: 6.1 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0748
                </span>
                <a href="https://arxiv.org/abs/2408.04032" target="_blank" rel="noopener noreferrer">The Evolution of Information Seeking in Software Development: Understanding the Role and Impact of AI Assistants</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ebtesam Al Haque, Chris Brown, Thomas D. LaToza, Brittany Johnson
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">About 32% of a software practitioners' day involves seeking and using information to support task completion. Although the information needs of software practitioners have been studied extensively, the impact of AI-assisted tools on their needs and information-seeking behaviors remains largely unexp</span>
                
                <span class="abstract-full" style="display: none;">About 32% of a software practitioners' day involves seeking and using information to support task completion. Although the information needs of software practitioners have been studied extensively, the impact of AI-assisted tools on their needs and information-seeking behaviors remains largely unexplored. To addresses this gap, we conducted a mixed-method study to understand AI-assisted information seeking behavior of practitioners and its impact on their perceived productivity and skill development. We found that developers are increasingly using AI tools to support their information seeking, citing increased efficiency as a key benefit. Our findings also amplify caveats that come with effectively using AI tools for information seeking, especially for learning and skill development, such as the importance of foundational developer knowledge that can guide and inform the information provided by AI tools. Our efforts have implications for the effective integration of AI tools into developer workflows as information retrieval systems and learning aids.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.0 -->
                    
                <!-- LLMs: 7.3 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0813
                </span>
                <a href="https://arxiv.org/abs/2504.18315" target="_blank" rel="noopener noreferrer">Advanced Channel Decomposition Techniques in OTFS: A GSVD Approach for Multi-User Downlink</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Omid Abbassi Aghd, Oussama Ben Haj Belkacem, Dou Hu, Jo\~ao Guerreiro, Nuno Souto, Michal Szczachor, Rui Dinis
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we propose a multi-user downlink system for two users based on the orthogonal time frequency space (OTFS) modulation scheme. The design leverages the generalized singular value decomposition (GSVD) of the channels between the base station and the two users, applying precoding and dete</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we propose a multi-user downlink system for two users based on the orthogonal time frequency space (OTFS) modulation scheme. The design leverages the generalized singular value decomposition (GSVD) of the channels between the base station and the two users, applying precoding and detection matrices based on the right and left singular vectors, respectively. We derive the analytical expressions for three scenarios and present the corresponding simulation results. These results demonstrate that, in terms of bit error rate (BER), the proposed system outperforms the conventional multi-user OTFS system in two scenarios when using minimum mean square error (MMSE) equalizers or precoder, both for perfect channel state information and for a scenario with channel estimation errors. In the third scenario, the design is equivalent to zero-forcing (ZF) precoding at the transmitter.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.2 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- LLMs: 2.3 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Pathfinding: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0952
                </span>
                <a href="https://arxiv.org/abs/2406.10370" target="_blank" rel="noopener noreferrer">Papers-to-Posts: Supporting Detailed Long-Document Summarization with an Interactive LLM-Powered Source Outline</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Marissa Radensky, Daniel S. Weld, Joseph Chee Chang, Pao Siangliulue, Jonathan Bragg
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Compressing long and technical documents (e.g., >10 pages) into shorter-form articles (e.g., <2 pages) is critical for communicating information to different audiences, for example, blog posts of scientific research paper or legal briefs of dense court proceedings. While large language models (LLMs)</span>
                
                <span class="abstract-full" style="display: none;">Compressing long and technical documents (e.g., >10 pages) into shorter-form articles (e.g., <2 pages) is critical for communicating information to different audiences, for example, blog posts of scientific research paper or legal briefs of dense court proceedings. While large language models (LLMs) are powerful tools for condensing large amounts of text, current interfaces to these models lack support for understanding and controlling what content is included in a detailed summarizing article. Such capability is especially important for detail- and technical-oriented domains, in which tactical selection and coherent synthesis of key details is critical for effective communication to the target audience. For this, we present interactive reverse source outlines, a novel mechanism for controllable long-form summarization featuring outline bullet points with automatic point selections that the user can iteratively adjust to obtain an article with the desired content coverage. We implement this mechanism in Papers-to-Posts, a new LLM-powered system for authoring research-paper blog posts. Through a within-subjects lab study (n=20) and a between-subjects deployment study (n=37 blog posts, 26 participants), we compare Papers-to-Posts to a strong baseline tool that provides an LLM-generated draft and access to free-form prompting. Under time constraints, Papers-to-Posts significantly increases writer satisfaction with blog post quality, particularly with respect to content coverage. Furthermore, quantitative results showed an increase in editing power (change in text for an amount of time or writing actions) while using Papers-to-Posts, and qualitative results showed that participants found incorporating key research-paper insights in their blog posts easier while using Papers-to-Posts.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.0 -->
                    
                <!-- LLMs: 8.1 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0977
                </span>
                <a href="https://arxiv.org/abs/2411.06018" target="_blank" rel="noopener noreferrer">A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time Series via Visualization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Haoxin Liu, Chenghao Liu, B. Aditya Prakash
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language models (LLMs), with demonstrated reasoning abilities across multiple domains, are largely underexplored for time-series reasoning (TsR), which is ubiquitous in the real world. In this work, we propose TimerBed, the first comprehensive testbed for evaluating LLMs' TsR performance. Spec</span>
                
                <span class="abstract-full" style="display: none;">Large language models (LLMs), with demonstrated reasoning abilities across multiple domains, are largely underexplored for time-series reasoning (TsR), which is ubiquitous in the real world. In this work, we propose TimerBed, the first comprehensive testbed for evaluating LLMs' TsR performance. Specifically, TimerBed includes stratified reasoning patterns with real-world tasks, comprehensive combinations of LLMs and reasoning strategies, and various supervised models as comparison anchors. We perform extensive experiments with TimerBed, test multiple current beliefs, and verify the initial failures of LLMs in TsR, evidenced by the ineffectiveness of zero shot (ZST) and performance degradation of few shot in-context learning (ICL). Further, we identify one possible root cause: the numerical modeling of data. To address this, we propose a prompt-based solution VL-Time, using visualization-modeled data and language-guided reasoning. Experimental results demonstrate that Vl-Time enables multimodal LLMs to be non-trivial ZST and powerful ICL reasoners for time series, achieving about 140% average performance improvement and 99% average token costs reduction.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 22.4 -->
                    
                <!-- Medicine: 10.6 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1055
                </span>
                <a href="https://arxiv.org/abs/2504.18404" target="_blank" rel="noopener noreferrer">Paradigm shift on Coding Productivity Using GenAI</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Liang Yu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generative AI (GenAI) applications are transforming software engineering by enabling automated code co-creation. However, empirical evidence on GenAI's productivity effects in industrial settings remains limited. This paper investigates the adoption of GenAI coding assistants (e.g., Codeium, Amazon </span>
                
                <span class="abstract-full" style="display: none;">Generative AI (GenAI) applications are transforming software engineering by enabling automated code co-creation. However, empirical evidence on GenAI's productivity effects in industrial settings remains limited. This paper investigates the adoption of GenAI coding assistants (e.g., Codeium, Amazon Q) within telecommunications and FinTech domains. Through surveys and interviews with industrial domain-experts, we identify primary productivity-influencing factors, including task complexity, coding skills, domain knowledge, and GenAI integration. Our findings indicate that GenAI tools enhance productivity in routine coding tasks (e.g., refactoring and Javadoc generation) but face challenges in complex, domain-specific activities due to limited context-awareness of codebases and insufficient support for customized design rules. We highlight new paradigms for coding transfer, emphasizing iterative prompt refinement, immersive development environment, and automated code evaluation as essential for effective GenAI usage.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.0 -->
                    
                <!-- LLMs: 10.4 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- T2I: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1286
                </span>
                <a href="https://arxiv.org/abs/2504.17813" target="_blank" rel="noopener noreferrer">CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dileepa Pitawela, Gustavo Carneiro, Hsiang-Ting Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In ordinal classification, misclassifying neighboring ranks is common, yet the consequences of these errors are not the same. For example, misclassifying benign tumor categories is less consequential, compared to an error at the pre-cancerous to cancerous threshold, which could profoundly influence </span>
                
                <span class="abstract-full" style="display: none;">In ordinal classification, misclassifying neighboring ranks is common, yet the consequences of these errors are not the same. For example, misclassifying benign tumor categories is less consequential, compared to an error at the pre-cancerous to cancerous threshold, which could profoundly influence treatment choices. Despite this, existing ordinal classification methods do not account for the varying importance of these margins, treating all neighboring classes as equally significant. To address this limitation, we propose CLOC, a new margin-based contrastive learning method for ordinal classification that learns an ordered representation based on the optimization of multiple margins with a novel multi-margin n-pair loss (MMNP). CLOC enables flexible decision boundaries across key adjacent categories, facilitating smooth transitions between classes and reducing the risk of overfitting to biases present in the training data. We provide empirical discussion regarding the properties of MMNP and show experimental results on five real-world image datasets (Adience, Historical Colour Image Dating, Knee Osteoarthritis, Indian Diabetic Retinopathy Image, and Breast Carcinoma Subtyping) and one synthetic dataset simulating clinical decision bias. Our results demonstrate that CLOC outperforms existing ordinal classification methods and show the interpretability and controllability of CLOC in learning meaningful, ordered representations that align with clinical and practical needs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.5 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Robotics: 1.0 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.199
                </span>
                <a href="https://arxiv.org/abs/2504.17833" target="_blank" rel="noopener noreferrer">The Role of Open-Source LLMs in Shaping the Future of GeoAI</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiao Huang, Zhengzhong Tu, Xinyue Ye, Michael Goodchild
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large Language Models (LLMs) are transforming geospatial artificial intelligence (GeoAI), offering new capabilities in data processing, spatial analysis, and decision support. This paper examines the open-source paradigm's pivotal role in this transformation. While proprietary LLMs offer accessibili</span>
                
                <span class="abstract-full" style="display: none;">Large Language Models (LLMs) are transforming geospatial artificial intelligence (GeoAI), offering new capabilities in data processing, spatial analysis, and decision support. This paper examines the open-source paradigm's pivotal role in this transformation. While proprietary LLMs offer accessibility, they often limit the customization, interoperability, and transparency vital for specialized geospatial tasks. Conversely, open-source alternatives significantly advance Geographic Information Science (GIScience) by fostering greater adaptability, reproducibility, and community-driven innovation. Open frameworks empower researchers to tailor solutions, integrate cutting-edge methodologies (e.g., reinforcement learning, advanced spatial indexing), and align with FAIR principles. However, the growing reliance on any LLM necessitates careful consideration of security vulnerabilities, ethical risks, and robust governance for AI-generated geospatial outputs. Ongoing debates on accessibility, regulation, and misuse underscore the critical need for responsible AI development strategies. This paper argues that GIScience advances best not through a single model type, but by cultivating a diverse, interoperable ecosystem combining open-source foundations for innovation, bespoke geospatial models, and interdisciplinary collaboration. By critically evaluating the opportunities and challenges of open-source LLMs within the broader GeoAI landscape, this work contributes to a nuanced discourse on leveraging AI to effectively advance spatial research, policy, and decision-making in an equitable, sustainable, and scientifically rigorous manner.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 16.7 -->
                    
                <!-- Medicine: 10.1 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.2499
                </span>
                <a href="https://arxiv.org/abs/2504.18113" target="_blank" rel="noopener noreferrer">Learning from Less: SINDy Surrogates in RL</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aniket Dixit, Muhammad Ibrahim Khan, Faizan Ahmed, James Brusey
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces an approach for developing surrogate environments in reinforcement learning (RL) using the Sparse Identification of Nonlinear Dynamics (SINDy) algorithm. We demonstrate the effectiveness of our approach through extensive experiments in OpenAI Gym environments, particularly Moun</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces an approach for developing surrogate environments in reinforcement learning (RL) using the Sparse Identification of Nonlinear Dynamics (SINDy) algorithm. We demonstrate the effectiveness of our approach through extensive experiments in OpenAI Gym environments, particularly Mountain Car and Lunar Lander. Our results show that SINDy-based surrogate models can accurately capture the underlying dynamics of these environments while reducing computational costs by 20-35%. With only 75 interactions for Mountain Car and 1000 for Lunar Lander, we achieve state-wise correlations exceeding 0.997, with mean squared errors as low as 3.11e-06 for Mountain Car velocity and 1.42e-06 for LunarLander position. RL agents trained in these surrogate environments require fewer total steps (65,075 vs. 100,000 for Mountain Car and 801,000 vs. 1,000,000 for Lunar Lander) while achieving comparable performance to those trained in the original environments, exhibiting similar convergence patterns and final performance metrics. This work contributes to the field of model-based RL by providing an efficient method for generating accurate, interpretable surrogate environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.6 -->
                    
                <!-- LLMs: 7.9 -->
                    
                <!-- Quantum Computing: 4.6 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3434
                </span>
                <a href="https://arxiv.org/abs/2504.18368" target="_blank" rel="noopener noreferrer">Renewable-Colocated Green Hydrogen Production: Optimal Scheduling and Profitability</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Siying Li, Lang Tong, Timothy Mount, Kanchan Upadhyay, Harris Eisenhardt, Pradip Kumar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the optimal green hydrogen production and energy market participation of a renewable-colocated hydrogen producer (RCHP) that utilizes onsite renewable generation for both hydrogen production and grid services. Under deterministic and stochastic profit-maximization frameworks, we analyze RCH</span>
                
                <span class="abstract-full" style="display: none;">We study the optimal green hydrogen production and energy market participation of a renewable-colocated hydrogen producer (RCHP) that utilizes onsite renewable generation for both hydrogen production and grid services. Under deterministic and stochastic profit-maximization frameworks, we analyze RCHP's multiple market participation models and derive closed-form optimal scheduling policies that dynamically allocate renewable energy to hydrogen production and electricity export to the wholesale market. Analytical characterizations of the RCHP's operating profit and the optimal sizing of renewable and electrolyzer capacities are obtained. We use real-time renewable production and electricity price data from three independent system operators to assess the impacts of hydrogen market prices, renewable generation, and electricity prices on RCHP's profitability.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.9 -->
                    
                <!-- LLMs: 7.8 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3646
                </span>
                <a href="https://arxiv.org/abs/2504.18471" target="_blank" rel="noopener noreferrer">Action Flow Matching for Continual Robot Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alejandro Murillo-Gonzalez, Lantao Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Continual learning in robotics seeks systems that can constantly adapt to changing environments and tasks, mirroring human adaptability. A key challenge is refining dynamics models, essential for planning and control, while addressing issues such as safe adaptation, catastrophic forgetting, outlier </span>
                
                <span class="abstract-full" style="display: none;">Continual learning in robotics seeks systems that can constantly adapt to changing environments and tasks, mirroring human adaptability. A key challenge is refining dynamics models, essential for planning and control, while addressing issues such as safe adaptation, catastrophic forgetting, outlier management, data efficiency, and balancing exploration with exploitation -- all within task and onboard resource constraints. Towards this goal, we introduce a generative framework leveraging flow matching for online robot dynamics model alignment. Rather than executing actions based on a misaligned model, our approach refines planned actions to better match with those the robot would take if its model was well aligned. We find that by transforming the actions themselves rather than exploring with a misaligned model -- as is traditionally done -- the robot collects informative data more efficiently, thereby accelerating learning. Moreover, we validate that the method can handle an evolving and possibly imperfect model while reducing, if desired, the dependency on replay buffers or legacy model snapshots. We validate our approach using two platforms: an unmanned ground vehicle and a quadrotor. The results highlight the method's adaptability and efficiency, with a record 34.2\% higher task success rate, demonstrating its potential towards enabling continual robot learning. Code: https://github.com/AlejandroMllo/action_flow_matching.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.2 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3738
                </span>
                <a href="https://arxiv.org/abs/2504.18068" target="_blank" rel="noopener noreferrer">S3MOT: Monocular 3D Object Tracking with Selective State Space Model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhuohao Yan, Shaoquan Feng, Xingxing Li, Yuxuan Zhou, Chunxi Xia, Shengyu Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate and reliable multi-object tracking (MOT) in 3D space is essential for advancing robotics and computer vision applications. However, it remains a significant challenge in monocular setups due to the difficulty of mining 3D spatiotemporal associations from 2D video streams. In this work, we p</span>
                
                <span class="abstract-full" style="display: none;">Accurate and reliable multi-object tracking (MOT) in 3D space is essential for advancing robotics and computer vision applications. However, it remains a significant challenge in monocular setups due to the difficulty of mining 3D spatiotemporal associations from 2D video streams. In this work, we present three innovative techniques to enhance the fusion and exploitation of heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State Space Model (HSSM), a novel data association mechanism that compresses contextual tracking cues across multiple paths, enabling efficient and comprehensive assignment decisions with linear complexity. HSSM features a global receptive field and dynamic weights, in contrast to traditional linear assignment algorithms that rely on hand-crafted association costs. (2) We propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI pooling by directly using dense feature maps for contrastive learning, thus improving object re-identification accuracy under challenging conditions such as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation through VeloSSM, an encoder-decoder architecture that models temporal dependencies in velocity to capture motion dynamics, overcoming the limitations of frame-based 3D inference. Experiments on the KITTI public test benchmark demonstrate the effectiveness of our method, achieving a new state-of-the-art performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness and efficiency for monocular 3D MOT tasks. The code and models are available at https://github.com/bytepioneerX/s3mot.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.0 -->
                    
                <!-- LLMs: 5.9 -->
                    
                <!-- 3D: 3.1 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.4125
                </span>
                <a href="https://arxiv.org/abs/2504.18326" target="_blank" rel="noopener noreferrer">Exhaled Breath Analysis Through the Lens of Molecular Communication: A Survey</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sunasheer Bhattacharjee, Dadi Bi, Pit Hofmann, Alexander Wietfeld, Sophie Becke, Michael Lommel, Pengjie Zhou, Ruifeng Zheng, Ulrich Kertzscher, Yansha Deng, Wolfgang Kellerer, Frank H. P. Fitzek, Falko Dressler
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Molecular Communication (MC) has long been envisioned to enable an Internet of Bio-Nano Things (IoBNT) with medical applications, where nanomachines within the human body conduct monitoring, diagnosis, and therapy at micro- and nanoscale levels. MC involves information transfer via molecules and is </span>
                
                <span class="abstract-full" style="display: none;">Molecular Communication (MC) has long been envisioned to enable an Internet of Bio-Nano Things (IoBNT) with medical applications, where nanomachines within the human body conduct monitoring, diagnosis, and therapy at micro- and nanoscale levels. MC involves information transfer via molecules and is supported by well-established theoretical models. However, practically achieving reliable, energy-efficient, and bio-compatible communication at these scales still remains a challenge. Air-Based Molecular Communication (ABMC) is a type of MC that operates over larger, meter-scale distances and extends even outside the human body. Therefore, devices and techniques to realize ABMC are readily accessible, and associated use cases can be very promising in the near future. Exhaled breath analysis has previously been proposed. It provides a non-invasive approach for health monitoring, leveraging existing commercial sensor technologies and reducing deployment barriers. The breath contains a diverse range of molecules and particles that serve as biomarkers linked to various physiological and pathological conditions. The plethora of proven methods, models, and optimization approaches in MC enable macroscale breath analysis, treating human as the transmitter, the breath as the information carrier, and macroscale sensors as the receiver. Using ABMC to interface with the inherent dynamic networks of cells, tissues, and organs could create a novel Internet of Bio Things (IoBT), a preliminary macroscale stage of the IoBNT. This survey extensively reviews exhaled breath modeling and analysis through the lens of MC, offering insights into theoretical frameworks and practical implementations from ABMC, bringing the IoBT a step closer to real-world use.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.9 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.4664
                </span>
                <a href="https://arxiv.org/abs/2504.18474" target="_blank" rel="noopener noreferrer">Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: James D. Finch, Yasasvi Josyula, Jinho D. Choi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is essential for automatically identifying key information slots from dialogue data without manual intervention. This paper presents a novel state-of-the-art (SoTA) approach that formulates SSI as a text generation task, where a la</span>
                
                <span class="abstract-full" style="display: none;">In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is essential for automatically identifying key information slots from dialogue data without manual intervention. This paper presents a novel state-of-the-art (SoTA) approach that formulates SSI as a text generation task, where a language model incrementally constructs and refines a slot schema over a stream of dialogue data. To develop this approach, we present a fully automatic LLM-based TOD simulation method that creates data with high-quality state labels for novel task domains. Furthermore, we identify issues in SSI evaluation due to data leakage and poor metric alignment with human judgment. We resolve these by creating new evaluation data using our simulation method with human guidance and correction, as well as designing improved evaluation metrics. These contributions establish a foundation for future SSI research and advance the SoTA in dialogue understanding and system development.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.3 -->
                    
                <!-- LLMs: 6.8 -->
                    
                <!-- 3D: 3.1 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- T2I: 2.0 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.4763
                </span>
                <a href="https://arxiv.org/abs/2504.17934" target="_blank" rel="noopener noreferrer">Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chaoran Chen, Zhiping Zhang, Ibrahim Khalilov, Bingcan Guo, Simret A Gebreegziabher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, Toby Jia-Jun Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks</span>
                
                <span class="abstract-full" style="display: none;">The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks of GUI agents and examines how they differ from traditional GUI automation and general autonomous agents. Despite these risks, existing evaluations focus primarily on performance, leaving privacy and security assessments largely unexplored. We review current evaluation metrics for both GUI and general LLM agents and outline five key challenges in integrating human evaluators for GUI agent assessments. To address these gaps, we advocate for a human-centered evaluation framework that incorporates risk assessments, enhances user awareness through in-context consent, and embeds privacy and security considerations into GUI agent design and evaluation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 17.5 -->
                    
                <!-- Medicine: 13.3 -->
                    
                <!-- 3D: 3.3 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- RAG: 2.3 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.7058
                </span>
                <a href="https://arxiv.org/abs/2504.18329" target="_blank" rel="noopener noreferrer">PHEATPRUNER: Interpretable Data-centric Feature Selection for Multivariate Time Series Classification through Persistent Homology</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Anh-Duy Pham, Olivier Basole Kashongwe, Martin Atzmueller, Tim R\"omer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Balancing performance and interpretability in multivariate time series classification is a significant challenge due to data complexity and high dimensionality. This paper introduces PHeatPruner, a method integrating persistent homology and sheaf theory to address these challenges. Persistent homolo</span>
                
                <span class="abstract-full" style="display: none;">Balancing performance and interpretability in multivariate time series classification is a significant challenge due to data complexity and high dimensionality. This paper introduces PHeatPruner, a method integrating persistent homology and sheaf theory to address these challenges. Persistent homology facilitates the pruning of up to 45% of the applied variables while maintaining or enhancing the accuracy of models such as Random Forest, CatBoost, XGBoost, and LightGBM, all without depending on posterior probabilities or supervised optimization algorithms. Concurrently, sheaf theory contributes explanatory vectors that provide deeper insights into the data's structural nuances. The approach was validated using the UEA Archive and a mastitis detection dataset for dairy cows. The results demonstrate that PHeatPruner effectively preserves model accuracy. Furthermore, our results highlight PHeatPruner's key features, i.e. simplifying complex data and offering actionable insights without increasing processing time or complexity. This method bridges the gap between complexity reduction and interpretability, suggesting promising applications in various fields.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.1 -->
                    
                <!-- LLMs: 6.5 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.7882
                </span>
                <a href="https://arxiv.org/abs/2412.11704" target="_blank" rel="noopener noreferrer">ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Atsuki Yamaguchi, Terufumi Morishita, Aline Villavicencio, Nikolaos Aletras
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Vocabulary expansion (VE) is the de-facto approach to language adaptation of large language models (LLMs) by adding new tokens and continuing pre-training on target data. While this is effective for base models trained on unlabeled data, it poses challenges for chat models trained to follow instruct</span>
                
                <span class="abstract-full" style="display: none;">Vocabulary expansion (VE) is the de-facto approach to language adaptation of large language models (LLMs) by adding new tokens and continuing pre-training on target data. While this is effective for base models trained on unlabeled data, it poses challenges for chat models trained to follow instructions through labeled conversation data. Directly adapting the latter with VE on target unlabeled data may result in forgetting chat abilities. While ideal, target chat data is often unavailable or costly to create for low-resource languages, and machine-translated alternatives are not always effective. To address this issue, previous work proposed using a base and chat model from the same family. This method first adapts the base LLM with VE on target unlabeled data and then converts it to a chat model by adding a chat vector (CV) derived from the weight difference between the source base and chat models. We propose ElChat, a new language adaptation method for chat LLMs that adapts a chat model directly on target unlabeled data, without a base model. It elicits chat abilities by injecting information from the source chat model. ElChat offers more robust and competitive target language and safety performance while achieving superior English, chat, and instruction-following abilities compared to CV.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.3 -->
                    
                <!-- LLMs: 9.7 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.7899
                </span>
                <a href="https://arxiv.org/abs/2504.18126" target="_blank" rel="noopener noreferrer">Lecture Notes on Normalizing Flows for Lattice Quantum Field Theories</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Miranda C. N. Cheng, Niki Stratikopoulou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Numerical simulations of quantum field theories on lattices serve as a fundamental tool for studying the non-perturbative regime of the theories, where analytic tools often fall short. Challenges arise when one takes the continuum limit or as the system approaches a critical point, especially in the</span>
                
                <span class="abstract-full" style="display: none;">Numerical simulations of quantum field theories on lattices serve as a fundamental tool for studying the non-perturbative regime of the theories, where analytic tools often fall short. Challenges arise when one takes the continuum limit or as the system approaches a critical point, especially in the presence of non-trivial topological structures in the theory. Rapid recent advances in machine learning provide a promising avenue for progress in this area. These lecture notes aim to give a brief account of lattice field theories, normalizing flows, and how the latter can be applied to study the former. The notes are based on the lectures given by the first author in various recent research schools.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 5.2 -->
                    
                <!-- Medicine: 3.8 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Pathfinding: 1.6 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8463
                </span>
                <a href="https://arxiv.org/abs/2503.13939" target="_blank" rel="noopener noreferrer">Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Vision-language models (VLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored. Medical vision-language tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity o</span>
                
                <span class="abstract-full" style="display: none;">Vision-language models (VLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored. Medical vision-language tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional supervised fine-tuning (SFT) and Chain-of-Thought (CoT) strategies that work well in general domains. To address these challenges, we propose Med-R1, a reinforcement learning (RL)-enhanced vision-language model designed to improve generalization and reliability in medical reasoning. Built on the DeepSeek strategy, Med-R1 adopts Group Relative Policy Optimization (GRPO) to encourage reward-guided learning beyond static annotations. We comprehensively evaluate Med-R1 across eight distinct medical imaging modalities. Med-R1 achieves a 29.94% improvement in average accuracy over its base model Qwen2-VL-2B, and even outperforms Qwen2-VL-72B-a model with 36x more parameters. To assess cross-task generalization, we further evaluate Med-R1 on five question types. Med-R1 outperforms Qwen2-VL-2B by 32.06% in question-type generalization, also surpassing Qwen2-VL-72B. We further explore the thinking process in Med-R1, a crucial component for the success of Deepseek-R1. Our results show that omitting intermediate rationales (No-Thinking-Med-R1) not only improves in-domain and cross-domain generalization with less training, but also challenges the assumption that more reasoning always helps. These findings suggest that in medical VQA, it is not reasoning itself, but its quality and domain alignment, that determine effectiveness. Together, these results highlight that RL improves medical reasoning and generalization, enabling efficient and reliable VLMs for real-world deployment.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.8 -->
                    
                <!-- LLMs: 9.9 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.9112
                </span>
                <a href="https://arxiv.org/abs/2504.18490" target="_blank" rel="noopener noreferrer">An Improved ResNet50 Model for Predicting Pavement Condition Index (PCI) Directly from Pavement Images</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andrews Danyo, Anthony Dontoh, Armstrong Aboah
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurately predicting the Pavement Condition Index (PCI), a measure of roadway conditions, from pavement images is crucial for infrastructure maintenance. This study proposes an enhanced version of the Residual Network (ResNet50) architecture, integrated with a Convolutional Block Attention Module (</span>
                
                <span class="abstract-full" style="display: none;">Accurately predicting the Pavement Condition Index (PCI), a measure of roadway conditions, from pavement images is crucial for infrastructure maintenance. This study proposes an enhanced version of the Residual Network (ResNet50) architecture, integrated with a Convolutional Block Attention Module (CBAM), to predict PCI directly from pavement images without additional annotations. By incorporating CBAM, the model autonomously prioritizes critical features within the images, improving prediction accuracy. Compared to the original baseline ResNet50 and DenseNet161 architectures, the enhanced ResNet50-CBAM model achieved a significantly lower mean absolute percentage error (MAPE) of 58.16%, compared to the baseline models that achieved 70.76% and 65.48% respectively. These results highlight the potential of using attention mechanisms to refine feature extraction, ultimately enabling more accurate and efficient assessments of pavement conditions. This study emphasizes the importance of targeted feature refinement in advancing automated pavement analysis through attention mechanisms.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.9 -->
                    
                <!-- LLMs: 5.5 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.9533
                </span>
                <a href="https://arxiv.org/abs/2504.17807" target="_blank" rel="noopener noreferrer">Research on Cloud Platform Network Traffic Monitoring and Anomaly Detection System based on Large Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ze Yang, Yihong Jin, Juntian Liu, Xinhe Xu, Yihan Zhang, Shuyang Ji
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rapidly evolving cloud platforms and the escalating complexity of network traffic demand proper network traffic monitoring and anomaly detection to ensure network security and performance. This paper introduces a large language model (LLM)-based network traffic monitoring and anomaly detection s</span>
                
                <span class="abstract-full" style="display: none;">The rapidly evolving cloud platforms and the escalating complexity of network traffic demand proper network traffic monitoring and anomaly detection to ensure network security and performance. This paper introduces a large language model (LLM)-based network traffic monitoring and anomaly detection system. In addition to existing models such as autoencoders and decision trees, we harness the power of large language models for processing sequence data from network traffic, which allows us a better capture of underlying complex patterns, as well as slight fluctuations in the dataset. We show for a given detection task, the need for a hybrid model that incorporates the attention mechanism of the transformer architecture into a supervised learning framework in order to achieve better accuracy. A pre-trained large language model analyzes and predicts the probable network traffic, and an anomaly detection layer that considers temporality and context is added. Moreover, we present a novel transfer learning-based methodology to enhance the model's effectiveness to quickly adapt to unknown network structures and adversarial conditions without requiring extensive labeled datasets. Actual results show that the designed model outperforms traditional methods in detection accuracy and computational efficiency, effectively identify various network anomalies such as zero-day attacks and traffic congestion pattern, and significantly reduce the false positive rate.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.3 -->
                    
                <!-- LLMs: 7.7 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.0173
                </span>
                <a href="https://arxiv.org/abs/2309.14016" target="_blank" rel="noopener noreferrer">Virtuoso: High Resource Utilization and {\mu}s-scale Performance Isolation in a Shared Virtual Machine TCP Network Stack</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Matheus Stolet, Liam Arzola, Simon Peter, Antoine Kaufmann
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Virtualization improves resource efficiency and ensures security and performance isolation for cloud applications. Today, operators use a layered architecture with separate network stack instances in each VM and container connected to a virtual switch. Decoupling through layering reduces complexity,</span>
                
                <span class="abstract-full" style="display: none;">Virtualization improves resource efficiency and ensures security and performance isolation for cloud applications. Today, operators use a layered architecture with separate network stack instances in each VM and container connected to a virtual switch. Decoupling through layering reduces complexity, but induces performance and resource overheads at odds with increasing demands for network bandwidth, connection scalability, and low latency.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.5 -->
                    
                <!-- LLMs: 6.3 -->
                    
                <!-- 3D: 3.9 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- T2I: 2.0 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.1026
                </span>
                <a href="https://arxiv.org/abs/2410.01535" target="_blank" rel="noopener noreferrer">GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene by Primitives and Gaussians</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shuyi Jiang, Qihao Zhao, Hossein Rahmani, De Wen Soh, Jun Liu, Na Zhao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recently, with the development of Neural Radiance Fields and Gaussian Splatting, 3D reconstruction techniques have achieved remarkably high fidelity. However, the latent representations learnt by these methods are highly entangled and lack interpretability. In this paper, we propose a novel part-awa</span>
                
                <span class="abstract-full" style="display: none;">Recently, with the development of Neural Radiance Fields and Gaussian Splatting, 3D reconstruction techniques have achieved remarkably high fidelity. However, the latent representations learnt by these methods are highly entangled and lack interpretability. In this paper, we propose a novel part-aware compositional reconstruction method, called GaussianBlock, that enables semantically coherent and disentangled representations, allowing for precise and physical editing akin to building blocks, while simultaneously maintaining high fidelity. Our GaussianBlock introduces a hybrid representation that leverages the advantages of both primitives, known for their flexible actionability and editability, and 3D Gaussians, which excel in reconstruction quality. Specifically, we achieve semantically coherent primitives through a novel attention-guided centering loss derived from 2D semantic priors, complemented by a dynamic splitting and fusion strategy. Furthermore, we utilize 3D Gaussians that hybridize with primitives to refine structural details and enhance fidelity. Additionally, a binding inheritance strategy is employed to strengthen and maintain the connection between the two. Our reconstructed scenes are evidenced to be disentangled, compositional, and compact across diverse benchmarks, enabling seamless, direct and precise editing while maintaining high quality.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.8 -->
                    
                <!-- LLMs: 6.4 -->
                    
                <!-- 3D: 5.1 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.1144
                </span>
                <a href="https://arxiv.org/abs/2406.10602" target="_blank" rel="noopener noreferrer">Multilingual Large Language Models and Curse of Multilinguality</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Daniil Gurgurov, Tanja B\"aumel, Tatiana Anikina
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models, trained on huge datasets, show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navig</span>
                
                <span class="abstract-full" style="display: none;">Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models, trained on huge datasets, show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navigates the landscape of multilingual LLMs, providing an introductory overview of their technical aspects. It explains underlying architectures, objective functions, pre-training data sources, and tokenization methods. This work explores the unique features of different model types: encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5, mBART). Additionally, it addresses one of the significant limitations of multilingual LLMs - the curse of multilinguality - and discusses current attempts to overcome it.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 43.8 -->
                    
                <!-- Medicine: 9.5 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.2579
                </span>
                <a href="https://arxiv.org/abs/2504.18256" target="_blank" rel="noopener noreferrer">SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Elena Plekhanova, Damien Robert, Johannes Dollinger, Emilia Arens, Philipp Brun, Jan Dirk Wegner, Niklaus Zimmermann
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">With the exacerbation of the biodiversity and climate crises, macroecological pursuits such as global biodiversity mapping become more urgent. Remote sensing offers a wealth of Earth observation data for ecological studies, but the scarcity of labeled datasets remains a major challenge. Recently, se</span>
                
                <span class="abstract-full" style="display: none;">With the exacerbation of the biodiversity and climate crises, macroecological pursuits such as global biodiversity mapping become more urgent. Remote sensing offers a wealth of Earth observation data for ecological studies, but the scarcity of labeled datasets remains a major challenge. Recently, self-supervised learning has enabled learning representations from unlabeled data, triggering the development of pretrained geospatial models with generalizable features. However, these models are often trained on datasets biased toward areas of high human activity, leaving entire ecological regions underrepresented. Additionally, while some datasets attempt to address seasonality through multi-date imagery, they typically follow calendar seasons rather than local phenological cycles. To better capture vegetation seasonality at a global scale, we propose a simple phenology-informed sampling strategy and introduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we train an existing model with a season-contrastive objective. We compare representations learned from SSL4Eco against other datasets on diverse ecological downstream tasks and demonstrate that our straightforward sampling method consistently improves representation quality, highlighting the importance of dataset construction. The model pretrained on SSL4Eco reaches state of the art performance on 7 out of 8 downstream tasks spanning (multi-label) classification and regression. We release our code, data, and model weights to support macroecological and computer vision research at https://github.com/PlekhanovaElena/ssl4eco.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.0 -->
                    
                <!-- LLMs: 5.5 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.3262
                </span>
                <a href="https://arxiv.org/abs/2504.18444" target="_blank" rel="noopener noreferrer">Boosting-Enabled Robust System Identification of Partially Observed LTI Systems Under Heavy-Tailed Noise</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vinay Kanakeri, Aritra Mitra
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We consider the problem of system identification of partially observed linear time-invariant (LTI) systems. Given input-output data, we provide non-asymptotic guarantees for identifying the system parameters under general heavy-tailed noise processes. Unlike previous works that assume Gaussian or su</span>
                
                <span class="abstract-full" style="display: none;">We consider the problem of system identification of partially observed linear time-invariant (LTI) systems. Given input-output data, we provide non-asymptotic guarantees for identifying the system parameters under general heavy-tailed noise processes. Unlike previous works that assume Gaussian or sub-Gaussian noise, we consider significantly broader noise distributions that are required to admit only up to the second moment. For this setting, we leverage tools from robust statistics to propose a novel system identification algorithm that exploits the idea of boosting. Despite the much weaker noise assumptions, we show that our proposed algorithm achieves sample complexity bounds that nearly match those derived under sub-Gaussian noise. In particular, we establish that our bounds retain a logarithmic dependence on the prescribed failure probability. Interestingly, we show that such bounds can be achieved by requiring just a finite fourth moment on the excitatory input process.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.5 -->
                    
                <!-- Quantum Computing: 5.3 -->
                    
                <!-- Reinforcement Learning: 4.5 -->
                    
                <!-- GNN: 3.6 -->
                    
                <!-- Math: 3.4 -->
                    
                <!-- Medicine: 2.6 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.4798
                </span>
                <a href="https://arxiv.org/abs/2504.17967" target="_blank" rel="noopener noreferrer">LLM Agent Swarm for Hypothesis-Driven Drug Discovery</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kevin Song, Andrew Trotter, Jake Y. Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, h</span>
                
                <span class="abstract-full" style="display: none;">Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress. Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows. We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM "agents" to propose, validate, and refine hypotheses for novel drug targets and lead compounds. Each agent accesses dedicated functionality--automated genomic and expression analysis; a curated biomedical knowledge graph; pathway enrichment and network simulation; interpretable binding affinity prediction--while a central Evaluator LLM continuously ranks proposals by biological plausibility, novelty, in silico efficacy, and safety. A shared memory layer captures validated insights and fine-tunes underlying submodels over time, yielding a self-improving system. Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm supports literature-driven discovery, omics-guided target identification, and market-informed repurposing. We also describe a rigorous four-tier validation pipeline spanning retrospective benchmarking, independent computational assays, experimental testing, and expert user studies to ensure transparency, reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm can accelerate translational research and deliver high-confidence hypotheses more efficiently than traditional pipelines.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.0 -->
                    
                <!-- LLMs: 12.2 -->
                    
                <!-- 3D: 3.4 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.5482
                </span>
                <a href="https://arxiv.org/abs/2504.18225" target="_blank" rel="noopener noreferrer">Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pierre-Carl Langlais, Pavel Chizhov, Mattia Nee, Carlos Rosas Hinostroza, Matthieu Delsart, Ir\`ene Girard, Othman Hicheur, Anastasia Stasenko, Ivan P. Yamshchikov
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support</span>
                
                <span class="abstract-full" style="display: none;">We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support for citation and grounding with literal quotes and reintegrate multiple features associated with RAG workflows, such as query routing, query reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B outperform SLMs below 4 billion parameters on standardized RAG benchmarks (HotPotQA, 2wiki) and are competitive with popular larger models, including Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date maintaining consistent RAG performance across leading European languages and ensuring systematic reference grounding for statements. Due to their size and ease of deployment on constrained infrastructure and higher factuality by design, the models unlock a range of new use cases for generative AI.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.0 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.5751
                </span>
                <a href="https://arxiv.org/abs/2504.18536" target="_blank" rel="noopener noreferrer">Adapting Probabilistic Risk Assessment for AI</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Anna Katariina Wisakanto, Joe Rogero, Avyay M. Casheekar, Richard Mallah
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modern general-purpose artificial intelligence (AI) systems present an urgent risk management challenge, as their rapidly evolving capabilities and potential for catastrophic harm outpace our ability to reliably assess their risks. Current methods often rely on selective testing and undocumented ass</span>
                
                <span class="abstract-full" style="display: none;">Modern general-purpose artificial intelligence (AI) systems present an urgent risk management challenge, as their rapidly evolving capabilities and potential for catastrophic harm outpace our ability to reliably assess their risks. Current methods often rely on selective testing and undocumented assumptions about risk priorities, frequently failing to make a serious attempt at assessing the set of pathways through which Al systems pose direct or indirect risks to society and the biosphere. This paper introduces the probabilistic risk assessment (PRA) for AI framework, adapting established PRA techniques from high-reliability industries (e.g., nuclear power, aerospace) for the new challenges of advanced AI. The framework guides assessors in identifying potential risks, estimating likelihood and severity, and explicitly documenting evidence, underlying assumptions, and analyses at appropriate granularities. The framework's implementation tool synthesizes the results into a risk report card with aggregated risk estimates from all assessed risks. This systematic approach integrates three advances: (1) Aspect-oriented hazard analysis provides systematic hazard coverage guided by a first-principles taxonomy of AI system aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk pathway modeling analyzes causal chains from system aspects to societal impacts using bidirectional analysis and incorporating prospective techniques; and (3) Uncertainty management employs scenario decomposition, reference scales, and explicit tracing protocols to structure credible projections with novelty or limited data. Additionally, the framework harmonizes diverse assessment methods by integrating evidence into comparable, quantified absolute risk estimates for critical decisions. We have implemented this as a workbook tool for AI developers, evaluators, and regulators, available on the project website.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.8 -->
                    
                <!-- LLMs: 4.8 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.6481
                </span>
                <a href="https://arxiv.org/abs/2504.18241" target="_blank" rel="noopener noreferrer">Switch-Based Multi-Part Neural Network</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Surajit Majumder, Paritosh Ranjan, Prodip Roy, Bhuban Padhan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces decentralized and modular neural network framework designed to enhance the scalability, interpretability, and performance of artificial intelligence (AI) systems. At the heart of this framework is a dynamic switch mechanism that governs the selective activation and training of </span>
                
                <span class="abstract-full" style="display: none;">This paper introduces decentralized and modular neural network framework designed to enhance the scalability, interpretability, and performance of artificial intelligence (AI) systems. At the heart of this framework is a dynamic switch mechanism that governs the selective activation and training of individual neurons based on input characteristics, allowing neurons to specialize in distinct segments of the data domain. This approach enables neurons to learn from disjoint subsets of data, mimicking biological brain function by promoting task specialization and improving the interpretability of neural network behavior. Furthermore, the paper explores the application of federated learning and decentralized training for real-world AI deployments, particularly in edge computing and distributed environments. By simulating localized training on non-overlapping data subsets, we demonstrate how modular networks can be efficiently trained and evaluated. The proposed framework also addresses scalability, enabling AI systems to handle large datasets and distributed processing while preserving model transparency and interpretability. Finally, we discuss the potential of this approach in advancing the design of scalable, privacy-preserving, and efficient AI systems for diverse applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.1 -->
                    
                <!-- LLMs: 7.6 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.8283
                </span>
                <a href="https://arxiv.org/abs/2408.12133" target="_blank" rel="noopener noreferrer">Self-Supervised Representation Learning for Geospatial Objects: A Survey</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yile Chen, Weiming Huang, Kaiqi Zhao, Yue Jiang, Gao Cong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The proliferation of various data sources in urban and territorial environments has significantly facilitated the development of geospatial artificial intelligence (GeoAI) across a wide range of geospatial applications. However, geospatial data, which is inherently linked to geospatial objects, ofte</span>
                
                <span class="abstract-full" style="display: none;">The proliferation of various data sources in urban and territorial environments has significantly facilitated the development of geospatial artificial intelligence (GeoAI) across a wide range of geospatial applications. However, geospatial data, which is inherently linked to geospatial objects, often exhibits data heterogeneity that necessitates specialized fusion and representation strategies while simultaneously being inherently sparse in labels for downstream tasks. Consequently, there is a growing demand for techniques that can effectively leverage geospatial data without heavy reliance on task-specific labels and model designs. This need aligns with the principles of self-supervised learning (SSL), which has garnered increasing attention for its ability to learn effective and generalizable representations directly from data without extensive labeled supervision. This paper presents a comprehensive and up-to-date survey of SSL techniques specifically applied to or developed for geospatial objects in three primary vector geometric types: Point, Polyline, and Polygon. We systematically categorize various SSL techniques into predictive and contrastive methods, and analyze their adaptation to different data types for representation learning across various downstream tasks. Furthermore, we examine the emerging trends in SSL for geospatial objects, particularly the gradual advancements towards geospatial foundation models. Finally, we discuss key challenges in current research and outline promising directions for future investigation. By offering a structured analysis of existing studies, this paper aims to inspire continued progress in integrating SSL with geospatial objects, and the development of geospatial foundation models in a longer term.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.8 -->
                    
                <!-- LLMs: 7.5 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.9527
                </span>
                <a href="https://arxiv.org/abs/2504.18242" target="_blank" rel="noopener noreferrer">Demand Private Coded Caching: Small Cache Size</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Qinyi Lu, Nan Liu, Wei Kang, Chunguo Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We investigate the demand private coded caching problem, which is an $(N,K)$ coded caching problem with $N$ files, $K$ users, each equipped with a cache of size $M$, and an additional privacy constraint on user demands, i.e., each user can not gain any information about the demands of other users. W</span>
                
                <span class="abstract-full" style="display: none;">We investigate the demand private coded caching problem, which is an $(N,K)$ coded caching problem with $N$ files, $K$ users, each equipped with a cache of size $M$, and an additional privacy constraint on user demands, i.e., each user can not gain any information about the demands of other users. We focus on scenarios where the size of users' caches is small, aiming to further characterize the fundamental limits of this problem. We first present a new virtual-user-based achievable scheme for arbitrary number of users and files, and two MDS-code-based achievable schemes for the case $N \le K$. With a newly derived converse bound for the case $N \le K$, these proposed schemes lead to the optimal memory-rate tradeoff of the demand private coded caching problem for $M \in \big[0, \frac{N}{(K+1)(N-1)} \big] $ where $N \le K \le 2N-2$, and the optimal memory-rate tradeoff for $M \in \big[0, \frac{1}{K+1} \big] $ where $ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users, by deriving another new converse bound, the optimal memory-rate tradeoff is characterized for $M\in \big[0,\frac{2}{K}\big] \cup \big[\frac{2(K-1)}{K+1},2\big]$. Finally, we provide the optimal memory-rate tradeoff of the demand private coded caching problem for 2 files and 3 users.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.6 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- LLMs: 2.8 -->
                    
                <!-- Math: 2.7 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.0117
                </span>
                <a href="https://arxiv.org/abs/2504.17977" target="_blank" rel="noopener noreferrer">From Bugs to Benchmarks: A Comprehensive Survey of Software Defect Datasets</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hao-Nan Zhu, Robert M. Furth, Michael Pradel, Cindy Rubio-Gonz\'alez
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Software defect datasets, which are collections of software bugs and their associated information, are essential resources for researchers and practitioners in software engineering and beyond. Such datasets facilitate empirical research and enable standardized benchmarking for a wide range of techni</span>
                
                <span class="abstract-full" style="display: none;">Software defect datasets, which are collections of software bugs and their associated information, are essential resources for researchers and practitioners in software engineering and beyond. Such datasets facilitate empirical research and enable standardized benchmarking for a wide range of techniques, including fault detection, fault localization, test generation, test prioritization, automated program repair, and emerging areas like agentic AI-based software development. Over the years, numerous software defect datasets with diverse characteristics have been developed, providing rich resources for the community, yet making it increasingly difficult to navigate the landscape. To address this challenge, this article provides a comprehensive survey of 132 software defect datasets. The survey discusses the scope of existing datasets, e.g., regarding the application domain of the buggy software, the types of defects, and the programming languages used. We also examine the construction of these datasets, including the data sources and construction methods employed. Furthermore, we assess the availability and usability of the datasets, validating their availability and examining how defects are presented. To better understand the practical uses of these datasets, we analyze the publications that cite them, revealing that the primary use cases are evaluations of new techniques and empirical research. Based on our comprehensive review of the existing datasets, this paper suggests potential opportunities for future research, including addressing underrepresented kinds of defects, enhancing availability and usability through better dataset organization, and developing more efficient strategies for dataset construction and maintenance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.2 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.1815
                </span>
                <a href="https://arxiv.org/abs/2504.17927" target="_blank" rel="noopener noreferrer">Recursive Identification of Structured Systems: An Instrumental-Variable Approach Applied to Mechanical Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Koen Classens, Rodrigo A. Gonz\'alez, Tom Oomen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Online system identification algorithms are widely used for monitoring, diagnostics and control by continuously adapting to time-varying dynamics. Typically, these algorithms consider a model structure that lacks parsimony and offers limited physical interpretability. The objective of this paper is </span>
                
                <span class="abstract-full" style="display: none;">Online system identification algorithms are widely used for monitoring, diagnostics and control by continuously adapting to time-varying dynamics. Typically, these algorithms consider a model structure that lacks parsimony and offers limited physical interpretability. The objective of this paper is to develop a real-time parameter estimation algorithm aimed at identifying time-varying dynamics within an interpretable model structure. An additive model structure is adopted for this purpose, which offers enhanced parsimony and is shown to be particularly suitable for mechanical systems. The proposed approach integrates the recursive simplified refined instrumental variable method with block-coordinate descent to minimize an exponentially-weighted output error cost function. This novel recursive identification method delivers parametric continuous-time additive models and is applicable in both open-loop and closed-loop controlled systems. Its efficacy is shown using numerical simulations and is further validated using experimental data to detect the time-varying resonance dynamics of a flexible beam system. These results demonstrate the effectiveness of the proposed approach for online and interpretable estimation for advanced monitoring and control applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.6 -->
                    
                <!-- LLMs: 5.3 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.1855
                </span>
                <a href="https://arxiv.org/abs/2504.18489" target="_blank" rel="noopener noreferrer">Tight Lower Bound for Multicolor Discrepancy</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pasin Manurangsi, Raghu Meka
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We prove the following asymptotically tight lower bound for $k$-color discrepancy: For any $k \geq 2$, there exists a hypergraph with $n$ vertices such that its $k$-color discrepancy is at least $\Omega(\sqrt{n})$. This improves on the previously known lower bound of $\Omega(\sqrt{n/\log k})$ due to</span>
                
                <span class="abstract-full" style="display: none;">We prove the following asymptotically tight lower bound for $k$-color discrepancy: For any $k \geq 2$, there exists a hypergraph with $n$ vertices such that its $k$-color discrepancy is at least $\Omega(\sqrt{n})$. This improves on the previously known lower bound of $\Omega(\sqrt{n/\log k})$ due to Caragiannis et al. (arXiv:2502.10516). As an application, we show that our result implies improved lower bounds for group fair division.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.6 -->
                    
                <!-- Quantum Computing: 5.3 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- GNN: 2.8 -->
                    
                <!-- 3D: 2.4 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.3626
                </span>
                <a href="https://arxiv.org/abs/2504.17960" target="_blank" rel="noopener noreferrer">VIGMA: An Open-Access Framework for Visual Gait and Motion Analytics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kazi Shahrukh Omar, Shuaijie Wang, Ridhuparan Kungumaraju, Tanvi Bhatt, Fabio Miranda
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Gait disorders are commonly observed in older adults, who frequently experience various issues related to walking. Additionally, researchers and clinicians extensively investigate mobility related to gait in typically and atypically developing children, athletes, and individuals with orthopedic and </span>
                
                <span class="abstract-full" style="display: none;">Gait disorders are commonly observed in older adults, who frequently experience various issues related to walking. Additionally, researchers and clinicians extensively investigate mobility related to gait in typically and atypically developing children, athletes, and individuals with orthopedic and neurological disorders. Effective gait analysis enables the understanding of the causal mechanisms of mobility and balance control of patients, the development of tailored treatment plans to improve mobility, the reduction of fall risk, and the tracking of rehabilitation progress. However, analyzing gait data is a complex task due to the multivariate nature of the data, the large volume of information to be interpreted, and the technical skills required. Existing tools for gait analysis are often limited to specific patient groups (e.g., cerebral palsy), only handle a specific subset of tasks in the entire workflow, and are not openly accessible. To address these shortcomings, we conducted a requirements assessment with gait practitioners (e.g., researchers, clinicians) via surveys and identified key components of the workflow, including (1) data processing and (2) data analysis and visualization. Based on the findings, we designed VIGMA, an open-access visual analytics framework integrated with computational notebooks and a Python library, to meet the identified requirements. Notably, the framework supports analytical capabilities for assessing disease progression and for comparing multiple patient groups. We validated the framework through usage scenarios with experts specializing in gait and mobility rehabilitation. VIGMA is available at https://github.com/komar41/VIGMA.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.3 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.5219
                </span>
                <a href="https://arxiv.org/abs/2504.18230" target="_blank" rel="noopener noreferrer">Learning to fuse: dynamic integration of multi-source data for accurate battery lifespan prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: He Shanxuan, Lin Zuhong, Yu Bolun, Gao Xu, Long Biao, Yao Jingjing
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate prediction of lithium-ion battery lifespan is vital for ensuring operational reliability and reducing maintenance costs in applications like electric vehicles and smart grids. This study presents a hybrid learning framework for precise battery lifespan prediction, integrating dynamic multi-</span>
                
                <span class="abstract-full" style="display: none;">Accurate prediction of lithium-ion battery lifespan is vital for ensuring operational reliability and reducing maintenance costs in applications like electric vehicles and smart grids. This study presents a hybrid learning framework for precise battery lifespan prediction, integrating dynamic multi-source data fusion with a stacked ensemble (SE) modeling approach. By leveraging heterogeneous datasets from the National Aeronautics and Space Administration (NASA), Center for Advanced Life Cycle Engineering (CALCE), MIT-Stanford-Toyota Research Institute (TRC), and nickel cobalt aluminum (NCA) chemistries, an entropy-based dynamic weighting mechanism mitigates variability across heterogeneous datasets. The SE model combines Ridge regression, long short-term memory (LSTM) networks, and eXtreme Gradient Boosting (XGBoost), effectively capturing temporal dependencies and nonlinear degradation patterns. It achieves a mean absolute error (MAE) of 0.0058, root mean square error (RMSE) of 0.0092, and coefficient of determination (R2) of 0.9839, outperforming established baseline models with a 46.2% improvement in R2 and an 83.2% reduction in RMSE. Shapley additive explanations (SHAP) analysis identifies differential discharge capacity (Qdlin) and temperature of measurement (Temp_m) as critical aging indicators. This scalable, interpretable framework enhances battery health management, supporting optimized maintenance and safety across diverse energy storage systems, thereby contributing to improved battery health management in energy storage systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 20.0 -->
                    
                <!-- LLMs: 8.2 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.5618
                </span>
                <a href="https://arxiv.org/abs/2402.14801" target="_blank" rel="noopener noreferrer">Mochi: Collision Detection for Spherical Particles using GPU Ray Tracing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Durga Keerthi Mandarapu, Isaac Fuksman, Artem Pelenitsyn, Gilbert Bernstein, Milind Kulkarni
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Efficient Discrete Collision Detection (DCD) uses indexing structures for acceleration, and developing these structures demands meticulous programmer efforts to achieve performance.</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.1 -->
                    
                <!-- LLMs: 7.2 -->
                    
                <!-- Quantum Computing: 5.2 -->
                    
                <!-- 3D: 3.6 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- RAG: 2.3 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- T2I: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Datasets: 1.0 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.6841
                </span>
                <a href="https://arxiv.org/abs/2504.17796" target="_blank" rel="noopener noreferrer">Structural Resilience Analysis of an Internet Fragment Against Targeted and Random Attacks -- A Case Study Based on iThena Project Data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lukasz Swierczewski
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This article presents an analysis of the structural resilience of a fragment of Internet topology against both targeted and random attacks, based on empirical data obtained from the iThena project. Using a processed visualization of the network, a graph representing node connections was generated an</span>
                
                <span class="abstract-full" style="display: none;">This article presents an analysis of the structural resilience of a fragment of Internet topology against both targeted and random attacks, based on empirical data obtained from the iThena project. Using a processed visualization of the network, a graph representing node connections was generated and subsequently subjected to detailed analysis using centrality metrics and community detection algorithms. Two attack scenarios were carried out: removal of nodes with the highest betweenness centrality and random removal of an equivalent number of nodes. The results indicate that targeted attacks have a significantly more destructive impact on the cohesion and functionality of the network than random disruptions. The article highlights the importance of identifying critical nodes and developing monitoring and protection mechanisms for Internet infrastructure in the context of cybersecurity.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.9 -->
                    
                <!-- LLMs: 5.4 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.6996
                </span>
                <a href="https://arxiv.org/abs/2504.18020" target="_blank" rel="noopener noreferrer">Federated Client-tailored Adapter for Medical Image Segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Guyue Hu, Siyuan Song, Yukun Kang, Zhu Yin, Gangming Zhao, Chenglong Li, Jin Tang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Medical image segmentation in X-ray images is beneficial for computer-aided diagnosis and lesion localization. Existing methods mainly fall into a centralized learning paradigm, which is inapplicable in the practical medical scenario that only has access to distributed data islands. Federated Learni</span>
                
                <span class="abstract-full" style="display: none;">Medical image segmentation in X-ray images is beneficial for computer-aided diagnosis and lesion localization. Existing methods mainly fall into a centralized learning paradigm, which is inapplicable in the practical medical scenario that only has access to distributed data islands. Federated Learning has the potential to offer a distributed solution but struggles with heavy training instability due to client-wise domain heterogeneity (including distribution diversity and class imbalance). In this paper, we propose a novel Federated Client-tailored Adapter (FCA) framework for medical image segmentation, which achieves stable and client-tailored adaptive segmentation without sharing sensitive local data. Specifically, the federated adapter stirs universal knowledge in off-the-shelf medical foundation models to stabilize the federated training process. In addition, we develop two client-tailored federated updating strategies that adaptively decompose the adapter into common and individual components, then globally and independently update the parameter groups associated with common client-invariant and individual client-specific units, respectively. They further stabilize the heterogeneous federated learning process and realize optimal client-tailored instead of sub-optimal global-compromised segmentation models. Extensive experiments on three large-scale datasets demonstrate the effectiveness and superiority of the proposed FCA framework for federated medical segmentation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 19.0 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.8673
                </span>
                <a href="https://arxiv.org/abs/2504.18346" target="_blank" rel="noopener noreferrer">Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Toghrul Abbasli, Kentaroh Toyoda, Yuan Wang, Leon Witt, Muhammad Asif Ali, Yukai Miao, Dan Li, Qingsong Wei
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive li</span>
                
                <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 19.2 -->
                    
                <!-- Medicine: 11.1 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.0801
                </span>
                <a href="https://arxiv.org/abs/2504.18081" target="_blank" rel="noopener noreferrer">Hype and Adoption of Generative Artificial Intelligence Applications</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vinh Truong (RMIT University)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">New technologies create opportunities while displacing others. They enhance life by supporting entertainment, education, and social connectivity but also replace humans in productivity and analytical tasks. Adapting to these shifts requires technical adjustments and social readiness. For digital tra</span>
                
                <span class="abstract-full" style="display: none;">New technologies create opportunities while displacing others. They enhance life by supporting entertainment, education, and social connectivity but also replace humans in productivity and analytical tasks. Adapting to these shifts requires technical adjustments and social readiness. For digital transformation to succeed, organizations and their workforce must be psychologically prepared. We are entering the era of Generative AI with tools like ChatGPT, Bing AI, and Microsoft Office Copilot. Understanding public sentiment toward these innovations is crucial for refining technology acceptance models and informing market strategies. Using the Gartner Hype Cycle and Kubler-Ross Change Curve, this study suggests that generative AI adoption is a dual-stage process. It follows the phases of technology trigger, peak of expectations, trough of disillusionment, slope of enlightenment, and plateau of productivity, while also reflecting emotional stages like shock, denial, and integration. The study used sentiment and emotion analysis on a large dataset of tweets about generative AI, translating them into scores to track user responses over time. Unlike prior research, which offered a snapshot of sentiment, this study captures the dynamic evolution of attitudes, linking empirical evidence with theoretical frameworks. It shifts the focus from information seekers to content creators. With the release of generative AI tools, there is a significant gap in understanding societal reception and adaptation. Policymakers face uncertainty about guiding markets for these changes. This research validates the applicability of the Gartner Hype Cycle and Kubler-Ross Change Curve to generative AI. It provides insights for businesses in integrating these tools and crafting policies to enhance readiness and resilience.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.9 -->
                    
                <!-- LLMs: 9.5 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Networks: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.1677
                </span>
                <a href="https://arxiv.org/abs/2504.18095" target="_blank" rel="noopener noreferrer">Subject-independent Classification of Meditative State from the Resting State using EEG</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jerrin Thomas Panachakel, Pradeep Kumar G., Suryaa Seran, Kanishka Sharma, Ramakrishnan Angarai Ganesan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While it is beneficial to objectively determine whether a subject is meditating, most research in the literature reports good results only in a subject-dependent manner. This study aims to distinguish the modified state of consciousness experienced during Rajyoga meditation from the resting state of</span>
                
                <span class="abstract-full" style="display: none;">While it is beneficial to objectively determine whether a subject is meditating, most research in the literature reports good results only in a subject-dependent manner. This study aims to distinguish the modified state of consciousness experienced during Rajyoga meditation from the resting state of the brain in a subject-independent manner using EEG data. Three architectures have been proposed and evaluated: The CSP-LDA Architecture utilizes common spatial pattern (CSP) for feature extraction and linear discriminant analysis (LDA) for classification. The CSP-LDA-LSTM Architecture employs CSP for feature extraction, LDA for dimensionality reduction, and long short-term memory (LSTM) networks for classification, modeling the binary classification problem as a sequence learning problem. The SVD-NN Architecture uses singular value decomposition (SVD) to select the most relevant components of the EEG signals and a shallow neural network (NN) for classification. The CSP-LDA-LSTM architecture gives the best performance with 98.2% accuracy for intra-subject classification. The SVD-NN architecture provides significant performance with 96.4\% accuracy for inter-subject classification. This is comparable to the best-reported accuracies in the literature for intra-subject classification. Both architectures are capable of capturing subject-invariant EEG features for effectively classifying the meditative state from the resting state. The high intra-subject and inter-subject classification accuracies indicate these systems' robustness and their ability to generalize across different subjects.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 18.5 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- GNN: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.1681
                </span>
                <a href="https://arxiv.org/abs/2504.17878" target="_blank" rel="noopener noreferrer">Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xu Wang, Yiquan Wang, Tin-yeh Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In the looming post-quantum era, traditional cryptographic systems are increasingly vulnerable to quantum computing attacks that can compromise their mathematical foundations. To address this critical challenge, we propose crypto-ncRNA-a bio-convergent cryptographic framework that leverages the dyna</span>
                
                <span class="abstract-full" style="display: none;">In the looming post-quantum era, traditional cryptographic systems are increasingly vulnerable to quantum computing attacks that can compromise their mathematical foundations. To address this critical challenge, we propose crypto-ncRNA-a bio-convergent cryptographic framework that leverages the dynamic folding properties of non-coding RNA (ncRNA) to generate high-entropy, quantum-resistant keys and produce unpredictable ciphertexts. The framework employs a novel, multi-stage process: encoding plaintext into RNA sequences, predicting and manipulating RNA secondary structures using advanced algorithms, and deriving cryptographic keys through the intrinsic physical unclonability of RNA molecules. Experimental evaluations indicate that, although crypto-ncRNA's encryption speed is marginally lower than that of AES, it significantly outperforms RSA in terms of efficiency and scalability while achieving a 100% pass rate on the NIST SP 800-22 randomness tests. These results demonstrate that crypto-ncRNA offers a promising and robust approach for securing digital infrastructures against the evolving threats posed by quantum computing.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 7.2 -->
                    
                <!-- LLMs: 6.7 -->
                    
                <!-- Medicine: 5.3 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.2209
                </span>
                <a href="https://arxiv.org/abs/2504.17935" target="_blank" rel="noopener noreferrer">Masked strategies for images with small objects</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: H. Martin Gillis, Ming Hill, Paul Hollensen, Alan Fine, Thomas Trappenberg
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The hematology analytics used for detection and classification of small blood components is a significant challenge. In particular, when objects exists as small pixel-sized entities in a large context of similar objects. Deep learning approaches using supervised models with pre-trained weights, such</span>
                
                <span class="abstract-full" style="display: none;">The hematology analytics used for detection and classification of small blood components is a significant challenge. In particular, when objects exists as small pixel-sized entities in a large context of similar objects. Deep learning approaches using supervised models with pre-trained weights, such as residual networks and vision transformers have demonstrated success for many applications. Unfortunately, when applied to images outside the domain of learned representations, these methods often result with less than acceptable performance. A strategy to overcome this can be achieved by using self-supervised models, where representations are learned and weights are then applied for downstream applications. Recently, masked autoencoders have proven to be effective to obtain representations that captures global context information. By masking regions of an image and having the model learn to reconstruct both the masked and non-masked regions, weights can be used for various applications. However, if the sizes of the objects in images are less than the size of the mask, the global context information is lost, making it almost impossible to reconstruct the image. In this study, we investigated the effect of mask ratios and patch sizes for blood components using a MAE to obtain learned ViT encoder representations. We then applied the encoder weights to train a U-Net Transformer for semantic segmentation to obtain both local and global contextual information. Our experimental results demonstrates that both smaller mask ratios and patch sizes improve the reconstruction of images using a MAE. We also show the results of semantic segmentation with and without pre-trained weights, where smaller-sized blood components benefited with pre-training. Overall, our proposed method offers an efficient and effective strategy for the segmentation and classification of small objects.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.6 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.6807
                </span>
                <a href="https://arxiv.org/abs/2504.18468" target="_blank" rel="noopener noreferrer">RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Georgios Kouros, Minye Wu, Tinne Tuytelaars
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce RGS-DR, a novel inverse rendering method for reconstructing and rendering glossy and reflective objects with support for flexible relighting and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2</span>
                
                <span class="abstract-full" style="display: none;">We introduce RGS-DR, a novel inverse rendering method for reconstructing and rendering glossy and reflective objects with support for flexible relighting and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D Gaussian surfel representation to accurately estimate geometry and surface normals, an essential property for high-quality inverse rendering. Our approach explicitly models geometric and material properties through learnable primitives rasterized into a deferred shading pipeline, effectively reducing rendering artifacts and preserving sharp reflections. By employing a multi-level cube mipmap, RGS-DR accurately approximates environment lighting integrals, facilitating high-quality reconstruction and relighting. A residual pass with spherical-mipmap-based directional encoding further refines the appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality reconstruction and rendering quality for shiny objects, often outperforming reconstruction-exclusive state-of-the-art methods incapable of relighting.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 21.6 -->
                    
                <!-- 3D: 7.4 -->
                    
                <!-- LLMs: 4.7 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.8315
                </span>
                <a href="https://arxiv.org/abs/2504.18231" target="_blank" rel="noopener noreferrer">Time and Frequency Domain-based Anomaly Detection in Smart Meter Data for Distribution Network Studies</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Petar Labura, Tomislav Antic, Tomislav Capuder
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The widespread integration of new technologies in low-voltage distribution networks on the consumer side creates the need for distribution system operators to perform advanced real-time calculations to estimate network conditions. In recent years, data-driven models based on machine learning and big</span>
                
                <span class="abstract-full" style="display: none;">The widespread integration of new technologies in low-voltage distribution networks on the consumer side creates the need for distribution system operators to perform advanced real-time calculations to estimate network conditions. In recent years, data-driven models based on machine learning and big data analysis have emerged for calculation purposes, leveraging the information available in large datasets obtained from smart meters and other advanced measurement infrastructure. However, existing data-driven algorithms do not take into account the quality of data collected from smart meters. They lack built-in anomaly detection mechanisms and fail to differentiate anomalies based on whether the value or context of anomalous data instances deviates from the norm. This paper focuses on methods for detecting and mitigating the impact of anomalies on the consumption of active and reactive power datasets. It proposes an anomaly detection framework based on the Isolation Forest machine learning algorithm and Fast Fourier Transform filtering that works in both the time and frequency domain and is unaffected by point anomalies or contextual anomalies of the power consumption data. The importance of integrating anomaly detection methods is demonstrated in the analysis important for distribution networks with a high share of smart meters.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 22.8 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.9582
                </span>
                <a href="https://arxiv.org/abs/2504.18103" target="_blank" rel="noopener noreferrer">Bayesian Quantum Orthogonal Neural Networks for Anomaly Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Natansh Mathur, Brian Coyle, Nishant Jain, Snehal Raj, Akshat Tandon, Jasper Simon Krauser, Rainer Stoessel
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Identification of defects or anomalies in 3D objects is a crucial task to ensure correct functionality. In this work, we combine Bayesian learning with recent developments in quantum and quantum-inspired machine learning, specifically orthogonal neural networks, to tackle this anomaly detection prob</span>
                
                <span class="abstract-full" style="display: none;">Identification of defects or anomalies in 3D objects is a crucial task to ensure correct functionality. In this work, we combine Bayesian learning with recent developments in quantum and quantum-inspired machine learning, specifically orthogonal neural networks, to tackle this anomaly detection problem for an industrially relevant use case. Bayesian learning enables uncertainty quantification of predictions, while orthogonality in weight matrices enables smooth training. We develop orthogonal (quantum) versions of 3D convolutional neural networks and show that these models can successfully detect anomalies in 3D objects. To test the feasibility of incorporating quantum computers into a quantum-enhanced anomaly detection pipeline, we perform hardware experiments with our models on IBM's 127-qubit Brisbane device, testing the effect of noise and limited measurement shots.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.6 -->
                    
                <!-- Medicine: 8.0 -->
                    
                <!-- Quantum Computing: 7.3 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.1258
                </span>
                <a href="https://arxiv.org/abs/2504.18400" target="_blank" rel="noopener noreferrer">A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yui Lo, Yuqian Chen, Dongnan Liu, Leo Zekelman, Jarrett Rushmore, Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Fan Zhang, Weidong Cai, Lauren J. O'Donnell
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Shape measures have emerged as promising descriptors of white matter tractography, offering complementary insights into anatomical variability and associations with cognitive and clinical phenotypes. However, conventional methods for computing shape measures are computationally expensive and time-co</span>
                
                <span class="abstract-full" style="display: none;">Shape measures have emerged as promising descriptors of white matter tractography, offering complementary insights into anatomical variability and associations with cognitive and clinical phenotypes. However, conventional methods for computing shape measures are computationally expensive and time-consuming for large-scale datasets due to reliance on voxel-based representations. We propose Tract2Shape, a novel multimodal deep learning framework that leverages geometric (point cloud) and scalar (tabular) features to predict ten white matter tractography shape measures. To enhance model efficiency, we utilize a dimensionality reduction algorithm for the model to predict five primary shape components. The model is trained and evaluated on two independently acquired datasets, the HCP-YA dataset, and the PPMI dataset. We evaluate the performance of Tract2Shape by training and testing it on the HCP-YA dataset and comparing the results with state-of-the-art models. To further assess its robustness and generalization ability, we also test Tract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep learning models across all ten shape measures, achieving the highest average Pearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows that both multimodal input and PCA contribute to performance gains. On the unseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low nMSE, demonstrating strong generalizability in cross-dataset evaluation. Tract2Shape enables fast, accurate, and generalizable prediction of white matter shape measures from tractography data, supporting scalable analysis across datasets. This framework lays a promising foundation for future large-scale white matter shape analysis.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 23.8 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.4559
                </span>
                <a href="https://arxiv.org/abs/2504.18360" target="_blank" rel="noopener noreferrer">On the Generalization of Kitaev Codes as Generalized Bicycle Codes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fran\c{c}ois Arnault, Philippe Gaborit, Nicolas Saussay
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Surface codes have historically been the dominant choice for quantum error correction due to their superior error threshold performance. However, recently, a new class of Generalized Bicycle (GB) codes, constructed from binary circulant matrices with three non-zero elements per row, achieved compara</span>
                
                <span class="abstract-full" style="display: none;">Surface codes have historically been the dominant choice for quantum error correction due to their superior error threshold performance. However, recently, a new class of Generalized Bicycle (GB) codes, constructed from binary circulant matrices with three non-zero elements per row, achieved comparable performance with fewer physical qubits and higher encoding efficiency.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 9.2 -->
                    
                <!-- Medicine: 7.7 -->
                    
                <!-- LLMs: 5.8 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.5393
                </span>
                <a href="https://arxiv.org/abs/2408.13632" target="_blank" rel="noopener noreferrer">FungiTastic: A multi-modal dataset and benchmark for image categorization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lukas Picek, Klara Janouskova, Vojtech Cermak, Jiri Matas
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce a new, challenging benchmark and a dataset, FungiTastic, based on fungal records continuously collected over a twenty-year span. The dataset is labelled and curated by experts and consists of about 350k multimodal observations of 6k fine-grained categories (species). The fungi observati</span>
                
                <span class="abstract-full" style="display: none;">We introduce a new, challenging benchmark and a dataset, FungiTastic, based on fungal records continuously collected over a twenty-year span. The dataset is labelled and curated by experts and consists of about 350k multimodal observations of 6k fine-grained categories (species). The fungi observations include photographs and additional data, e.g., meteorological and climatic data, satellite images, and body part segmentation masks. FungiTastic is one of the few benchmarks that include a test set with DNA-sequenced ground truth of unprecedented label reliability. The benchmark is designed to support (i) standard closed-set classification, (ii) open-set classification, (iii) multi-modal classification, (iv) few-shot learning, (v) domain shift, and many more. We provide tailored baselines for many use cases, a multitude of ready-to-use pre-trained models on https://huggingface.co/collections/BVRA/fungitastic-66a227ce0520be533dc6403b, and a framework for model training. The documentation and the baselines are available at https://github.com/BohemianVRA/FungiTastic/ and https://www.kaggle.com/datasets/picekl/fungitastic.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 25.1 -->
                    
                <!-- LLMs: 4.2 -->
                    
                <!-- 3D: 2.5 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.5857
                </span>
                <a href="https://arxiv.org/abs/2504.18049" target="_blank" rel="noopener noreferrer">A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xin Li, Wenhui Zhu, Peijie Qiu, Oana M. Dumitrascu, Amal Youssef, Yalin Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In the field of medical imaging, the advent of deep learning, especially the application of convolutional neural networks (CNNs) has revolutionized the analysis and interpretation of medical images. Nevertheless, deep learning methods usually rely on large amounts of labeled data. In medical imaging</span>
                
                <span class="abstract-full" style="display: none;">In the field of medical imaging, the advent of deep learning, especially the application of convolutional neural networks (CNNs) has revolutionized the analysis and interpretation of medical images. Nevertheless, deep learning methods usually rely on large amounts of labeled data. In medical imaging research, the acquisition of high-quality labels is both expensive and difficult. The introduction of Vision Transformers (ViT) and self-supervised learning provides a pre-training strategy that utilizes abundant unlabeled data, effectively alleviating the label acquisition challenge while broadening the breadth of data utilization. However, ViT's high computational density and substantial demand for computing power, coupled with the lack of localization characteristics of its operations on image patches, limit its efficiency and applicability in many application scenarios. In this study, we employ nn-MobileNet, a lightweight CNN framework, to implement a BERT-style self-supervised learning approach. We pre-train the network on the unlabeled retinal fundus images from the UK Biobank to improve downstream application performance. We validate the results of the pre-trained model on Alzheimer's disease (AD), Parkinson's disease (PD), and various retinal diseases identification. The results show that our approach can significantly improve performance in the downstream tasks. In summary, this study combines the benefits of CNNs with the capabilities of advanced self-supervised learning in handling large-scale unlabeled data, demonstrating the potential of CNNs in the presence of label scarcity.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 20.2 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Reinforcement Learning: 4.6 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Math: 2.5 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -8.2368
                </span>
                <a href="https://arxiv.org/abs/2112.14988" target="_blank" rel="noopener noreferrer">Deniable Encryption in a Quantum World</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andrea Coladangelo, Shafi Goldwasser, Umesh Vazirani
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">(Sender-)Deniable encryption provides a very strong privacy guarantee: a sender who is coerced by an attacker into "opening" their ciphertext after-the-fact is able to generate "fake" local random choices that are consistent with any plaintext of their choice. In this work, we study (sender-)deniabl</span>
                
                <span class="abstract-full" style="display: none;">(Sender-)Deniable encryption provides a very strong privacy guarantee: a sender who is coerced by an attacker into "opening" their ciphertext after-the-fact is able to generate "fake" local random choices that are consistent with any plaintext of their choice. In this work, we study (sender-)deniable encryption in a setting where the encryption procedure is a quantum algorithm, but the ciphertext is classical. We show that quantum computation unlocks a fundamentally stronger form of deniable encryption, which we call perfect unexplainability. The primitive at the heart of unexplainability is a quantum computation for which there is provably no efficient way, such as exhibiting the "history of the computation", to establish that the output was indeed the result of the computation. We give a construction that is secure in the random oracle model, assuming the quantum hardness of LWE. Crucially, this notion implies a form of protection against coercion "before-the-fact", a property that is impossible to achieve classically.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 9.3 -->
                    
                <!-- Networks: 3.9 -->
                    
                <!-- Math: 2.6 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Medicine: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.015
                </span>
                <a href="https://arxiv.org/abs/2504.18236" target="_blank" rel="noopener noreferrer">"Two Means to an End Goal": Connecting Explainability and Contestability in the Regulation of Public Sector AI</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Timoth\'ee Schmude, Mireia Yurrita, Kars Alfrink, Thomas Le Goff, Tiphaine Viard
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Explainability and its emerging counterpart contestability have become important normative and design principles for the trustworthy use of AI as they enable users and subjects to understand and challenge AI decisions. However, the regulation of AI systems spans technical, legal, and organizational </span>
                
                <span class="abstract-full" style="display: none;">Explainability and its emerging counterpart contestability have become important normative and design principles for the trustworthy use of AI as they enable users and subjects to understand and challenge AI decisions. However, the regulation of AI systems spans technical, legal, and organizational dimensions, producing a multiplicity in meaning that complicates the implementation of explainability and contestability. Resolving this conceptual ambiguity requires specifying and comparing the meaning of both principles across regulation dimensions, disciplines, and actors. This process, here defined as translation, is essential to provide guidance on the principles' realization. We present the findings of a semi-structured interview study with 14 interdisciplinary AI regulation experts. We report on the experts' understanding of the intersection between explainability and contestability in public AI regulation, their advice for a decision subject and a public agency in a welfare allocation AI use case, and their perspectives on the connections and gaps within the research landscape. We provide differentiations between descriptive and normative explainability, judicial and non-judicial channels of contestation, and individual and collective contestation action. We further outline three translation processes in the alignment of top-down and bottom-up regulation, the assignment of responsibility for interpreting regulations, and the establishment of interdisciplinary collaboration. Our contributions include an empirically grounded conceptualization of the intersection between explainability and contestability and recommendations on implementing these principles in public institutions. We believe our contributions can inform policy-making and regulation of these core principles and enable more effective and equitable design, development, and deployment of trustworthy public AI systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.8 -->
                    
                <!-- LLMs: 6.7 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.1834
                </span>
                <a href="https://arxiv.org/abs/2504.17943" target="_blank" rel="noopener noreferrer">Predicting Dairy Calf Body Weight from Depth Images Using Deep Learning (YOLOv8) and Threshold Segmentation with Cross-Validation and Longitudinal Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mingsi Liao, Gota Morota, Ye Bi, Rebecca R. Cockrum
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Monitoring calf body weight (BW) before weaning is essential for assessing growth, feed efficiency, health, and weaning readiness. However, labor, time, and facility constraints limit BW collection. Additionally, Holstein calf coat patterns complicate image-based BW estimation, and few studies have </span>
                
                <span class="abstract-full" style="display: none;">Monitoring calf body weight (BW) before weaning is essential for assessing growth, feed efficiency, health, and weaning readiness. However, labor, time, and facility constraints limit BW collection. Additionally, Holstein calf coat patterns complicate image-based BW estimation, and few studies have explored non-contact measurements taken at early time points for predicting later BW. The objectives of this study were to (1) develop deep learning-based segmentation models for extracting calf body metrics, (2) compare deep learning segmentation with threshold-based methods, and (3) evaluate BW prediction using single-time-point cross-validation with linear regression (LR) and extreme gradient boosting (XGBoost) and multiple-time-point cross-validation with LR, XGBoost, and a linear mixed model (LMM). Depth images from Holstein (n = 63) and Jersey (n = 5) pre-weaning calves were collected, with 20 Holstein calves being weighed manually. Results showed that You Only Look Once version 8 (YOLOv8) deep learning segmentation (intersection over union = 0.98) outperformed threshold-based methods (0.89). In single-time-point cross-validation, XGBoost achieved the best BW prediction (R^2 = 0.91, mean absolute percentage error (MAPE) = 4.37%), while LMM provided the most accurate longitudinal BW prediction (R^2 = 0.99, MAPE = 2.39%). These findings highlight the potential of deep learning for automated BW prediction, enhancing farm management.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 36.6 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -10.0593
                </span>
                <a href="https://arxiv.org/abs/2504.18188" target="_blank" rel="noopener noreferrer">Quantum Lifting for Invertible Permutations and Ideal Ciphers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alexandru Cojocaru, Minki Hhan, Qipeng Liu, Takashi Yamakawa, Aaram Yun
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this work, we derive the first lifting theorems for establishing security in the quantum random permutation and ideal cipher models. These theorems relate the success probability of an arbitrary quantum adversary to that of a classical algorithm making only a small number of classical queries.</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 13.0 -->
                    
                <!-- LLMs: 7.1 -->
                    
                <!-- Medicine: 5.2 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -11.904
                </span>
                <a href="https://arxiv.org/abs/2504.17923" target="_blank" rel="noopener noreferrer">EAQGA: A Quantum-Enhanced Genetic Algorithm with Novel Entanglement-Aware Crossovers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mohammad Kashfi Haghighi, Matthieu Fortin-Desch\^enes, Christophe Pere, Micka\"el Camus
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Genetic algorithms are highly effective optimization techniques for many computationally challenging problems, including combinatorial optimization tasks like portfolio optimization. Quantum computing has also shown potential in addressing these complex challenges. Combining these approaches, quantu</span>
                
                <span class="abstract-full" style="display: none;">Genetic algorithms are highly effective optimization techniques for many computationally challenging problems, including combinatorial optimization tasks like portfolio optimization. Quantum computing has also shown potential in addressing these complex challenges. Combining these approaches, quantum genetic algorithms leverage the principles of superposition and entanglement to enhance the performance of classical genetic algorithms. In this work, we propose a novel quantum genetic algorithm introducing an innovative crossover strategy to generate quantum circuits from a binary solution. We incorporate a heuristic method to encode entanglement patterns from parent solutions into circuits for the next generation. Our algorithm advances quantum genetic algorithms by utilizing a limited number of entanglements, enabling efficient exploration of optimal solutions without significantly increasing circuit depth, making it suitable for near-term applications. We test this approach on a portfolio optimization problem using an IBM 127 qubits Eagle processor (ibm_quebec) and simulators. Compared to state-of-the-art algorithms, our results show that the proposed method improves fitness values by 33.6% over classical genetic algorithm and 37.2% over quantum-inspired genetic algorithm, using the same iteration counts and population sizes with real quantum hardware employing 100 qubits. These findings highlight the potential of current quantum computers to address real-world utility-scale combinatorial optimization problems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 16.3 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -12.5953
                </span>
                <a href="https://arxiv.org/abs/2503.11450" target="_blank" rel="noopener noreferrer">The Road to Hybrid Quantum Programs: Characterizing the Evolution from Classical to Hybrid Quantum Software</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vincenzo De Maio, Ivona Brandic, Ewa Deelman, J\"urgen Cito
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Quantum computing exhibits the unique capability to natively and efficiently encode various natural phenomena, promising theoretical speedups of several orders of magnitude. However, not all computational tasks can be efficiently executed on quantum machines, giving rise to hybrid systems, where som</span>
                
                <span class="abstract-full" style="display: none;">Quantum computing exhibits the unique capability to natively and efficiently encode various natural phenomena, promising theoretical speedups of several orders of magnitude. However, not all computational tasks can be efficiently executed on quantum machines, giving rise to hybrid systems, where some portions of an application run on classical machines, while others utilize quantum resources. Efforts to identify quantum candidate code fragments that can meaningfully execute on quantum machines primarily rely on static code analysis. Yet, the state-of-the-art in static code analysis for quantum candidates remains in its infancy, with limited applicability to specific frameworks and languages, and a lack of generalizability. Existing methods often involve a trial-and-error approach, relying on the intuition and expertise of computer scientists, resulting in varying identification durations ranging from minutes to days for a single application.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 15.7 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Medicine: 3.8 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -13.6052
                </span>
                <a href="https://arxiv.org/abs/2504.18441" target="_blank" rel="noopener noreferrer">Expectation-based Analysis of Higher-Order Quantum Programs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Martin Avanzini, Alejandro D\'iaz-Caro, Emmanuel Hainry, Romain P\'echoux
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The paper extends the expectation transformer based analysis of higher-order probabilistic programs to the quantum higher-order setting. The quantum language we are considering can be seen as an extension of PCF, featuring unbounded recursion. The language admits classical and quantum data, as well </span>
                
                <span class="abstract-full" style="display: none;">The paper extends the expectation transformer based analysis of higher-order probabilistic programs to the quantum higher-order setting. The quantum language we are considering can be seen as an extension of PCF, featuring unbounded recursion. The language admits classical and quantum data, as well as a tick operator to account for costs. Our quantum expectation transformer translates such programs into a functional, non-quantum language, enriched with a type and operations over so called cost-structures. By specializing the cost-structure, this methodology makes it possible to study several expectation based properties of quantum programs, such as average case cost, probabilities of events or expected values, in terms of the translated non-quantum programs, this way enabling classical reasoning techniques. As a show-case, we adapt a refinement type system, capable of reasoning on upper-bounds.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 16.9 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -15.3858
                </span>
                <a href="https://arxiv.org/abs/2504.18359" target="_blank" rel="noopener noreferrer">Predicting sampling advantage of stochastic Ising Machines for Quantum Simulations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rutger J. L. F. Berns, Davi R. Rodrigues, Giovanni Finocchio, Johan H. Mentink
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Stochastic Ising machines, sIMs, are highly promising accelerators for optimization and sampling of computational problems that can be formulated as an Ising model. Here we investigate the computational advantage of sIM for simulations of quantum magnets with neural-network quantum states (NQS), in </span>
                
                <span class="abstract-full" style="display: none;">Stochastic Ising machines, sIMs, are highly promising accelerators for optimization and sampling of computational problems that can be formulated as an Ising model. Here we investigate the computational advantage of sIM for simulations of quantum magnets with neural-network quantum states (NQS), in which the quantum many-body wave function is mapped onto an Ising model. We study the sampling performance of sIM for NQS by comparing sampling on a software-emulated sIM with standard Metropolis-Hastings sampling for NQS. We quantify the sampling efficiency by the number of steps required to reach iso-accurate stochastic estimation of the variational energy and show that this is entirely determined by the autocorrelation time of the sampling. This enables predications of sampling advantage without direct deployment on hardware. For the quantum Heisenberg models studied and experimental results on the runtime of sIMs, we project a possible speed-up of 100 to 10000, suggesting great opportunities for studying complex quantum systems at larger scales.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 12.9 -->
                    
                <!-- Medicine: 6.7 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Math: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -32.3975
                </span>
                <a href="https://arxiv.org/abs/2504.18098" target="_blank" rel="noopener noreferrer">Efficient witnessing and testing of magic in mixed quantum states</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tobias Haug, Poetri Sonya Tarabunga
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Nonstabilizerness or `magic' is a crucial resource for quantum computers which can be distilled from noisy quantum states. However, determining the magic of mixed quantum has been a notoriously difficult task. Here, we provide efficient witnesses of magic based on the stabilizer R\'enyi entropy whic</span>
                
                <span class="abstract-full" style="display: none;">Nonstabilizerness or `magic' is a crucial resource for quantum computers which can be distilled from noisy quantum states. However, determining the magic of mixed quantum has been a notoriously difficult task. Here, we provide efficient witnesses of magic based on the stabilizer R\'enyi entropy which robustly indicate the presence of magic and quantitatively estimate magic monotones. We also design efficient property testing algorithms to reliably distinguish states with high and low magic, assuming the entropy is bounded. We apply our methods to certify the number of noisy T-gates under a wide class of noise models. Additionally, using the IonQ quantum computer, we experimentally verify the magic of noisy random quantum circuits. Surprisingly, we find that magic is highly robust, persisting even under exponentially strong noise. Our witnesses can also be efficiently computed for matrix product states, revealing that subsystems of many-body quantum states can contain extensive magic despite entanglement. Finally, our work also has direct implications for cryptography and pseudomagic: To mimic high magic states with as little magic as possible, one requires an extensive amount of entropy. This implies that entropy is a necessary resource to hide magic from eavesdroppers. Our work uncovers powerful tools to verify and study the complexity of noisy quantum systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 30.5 -->
                    
                <!-- Medicine: 5.0 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
    </div>
    
    
    <div id="jsonPopup" class="json-popup">
        <pre id="jsonContent"></pre>
        <button onclick="copyJson()">Copy to Clipboard</button>
        <button onclick="closePopup()">Close</button>
    </div>

    <script>
        function extractPaperData(paperElement) {
            const titleElement = paperElement.querySelector('.paper-title a');
            const metaElement = paperElement.querySelector('.paper-meta');
            const abstractElement = paperElement.querySelector('.paper-abstract');
            const tagsElement = paperElement.querySelector('.paper-tags');
            
            // Get the date from the parent date-section header
            const dateSection = paperElement.closest('.date-section');
            const dateText = dateSection.querySelector('.date-header').textContent.trim();
            
            const authorsText = metaElement.textContent.replace('Authors:', '').trim();
            
            const paperData = {
                title: titleElement.textContent,
                url: titleElement.href,
                authors: authorsText.split(',').map(author => author.trim()),
                created: dateText,
                abstract: abstractElement.querySelector('.abstract-full').textContent
            };
            
            return paperData;
        }

        function showJson(paperElement) {
            const popup = document.getElementById('jsonPopup');
            const content = document.getElementById('jsonContent');
            const paperData = extractPaperData(paperElement);
            content.textContent = JSON.stringify(paperData, null, null);
            popup.style.display = 'block';
            document.addEventListener('click', function closePopupOnClick(event) {
                if (!popup.contains(event.target)) {
                    popup.style.display = 'none';
                    document.removeEventListener('click', closePopupOnClick);
                }
            });
        }
        function toggleAbstract(element) {
            const abstract = element.parentElement;
            const short = abstract.querySelector('.abstract-short');
            const full = abstract.querySelector('.abstract-full');
            const lowConfidenceTags = abstract.parentElement.querySelectorAll('.tag-badge.low-confidence');
            
            if (element.textContent === '... more') {
                short.style.display = 'none';
                full.style.display = 'inline';
                element.textContent = ' less';
                lowConfidenceTags.forEach(tag => tag.style.display = 'inline-block');
            } else {
                short.style.display = 'inline';
                full.style.display = 'none';
                element.textContent = '... more';
                lowConfidenceTags.forEach(tag => tag.style.display = 'none');
            }
        }

        function closePopup() {
            document.getElementById('jsonPopup').style.display = 'none';
        }

        function copyJson() {
            const content = document.getElementById('jsonContent').textContent;
            navigator.clipboard.writeText(content).catch(() => {
                // If clipboard API is not available, just show the popup
                alert('Could not copy to clipboard. JSON is displayed in the popup.');
            });
        }

        // Close popup when clicking outside
        window.onclick = function(event) {
            const popup = document.getElementById('jsonPopup');
            if (event.target === popup) {
                popup.style.display = 'none';
            }
        }
    </script>
</body>
</html> 