<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Frontpage</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .paper {
            margin-bottom: 30px;
            margin-top: 30px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .paper-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .paper-meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        .paper-abstract {
            margin-bottom: 10px;
        }
        .abstract-short {
            display: inline;
        }
        .abstract-full {
            display: none;
        }
        .more-link {
            color: blue;
            cursor: pointer;
            text-decoration: underline;
        }
        .tag-badge {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 5px;
            margin-bottom: 5px;
            border-radius: 3px;
            font-size: 0.8em;
            color: white;
        }
        .tag-badge.high-confidence {
            opacity: 1;
        }
        .tag-badge.low-confidence {
            opacity: 0.6;
            display: none;
        }
        .interestingness-score {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 10px;
            color: white;
            border-radius: 3px;
            font-weight: bold;
        }
        .interestingness-positive {
            background-color: #4CAF50;
        }
        .interestingness-negative {
            background-color: #f44336;
        }
        .last-updated {
            text-align: right;
            color: #666;
            font-size: 0.9em;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .intro {
            text-align: center;
            max-width: 60em;
            margin: 0 auto;
            color: #888;
        }
        .copy-icon {
            display: inline-block;
            width: 16px;
            height: 16px;
            cursor: pointer;
            margin-left: 5px;
            opacity: 0.5;
        }
        .copy-icon:hover {
            opacity: 1;
        }
        .json-popup {
            display: none;
            position: fixed;
            top: 20px;
            right: 20px;
            background: white;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            max-width: 500px;
            max-height: 300px;
            overflow: auto;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        a {
            color: inherit;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h1 {
            text-align: center;
        }
        h1 a {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>
        <a href="https://github.com/DataWraith/arxiv-frontpage">DataWraith's</a> ArXiv Frontpage
    </h1>

    <div class="last-updated">
        Last updated: 2025-04-15
    </div>

    <p class="intro">
        This frontpage is made by scraping ArXiv's computer science RSS feed and tagging papers with a classifier.
    </p>

    <p class="intro">
        Each tag is weighted according to my preferences to compute a paper's <i>interestingness</i> score.
    </p>
    
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                49.9425
            </span>
            <a href="https://arxiv.org/abs/2504.08359" target="_blank" rel="noopener noreferrer">Kernel-Level Energy-Efficient Neural Architecture Search for Tabular Dataset</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hoang-Loc La, Phuong Hoai Ha | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neur</span>
            
            <span class="abstract-full" style="display: none;">Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neural Architecture Search (NAS) method that directly focuses on identifying architectures that minimize energy consumption while maintaining acceptable accuracy. Unlike previous methods that primarily target vision and language tasks, the approach proposed here specifically addresses tabular datasets. Remarkably, the optimal architecture suggested by this method can reduce energy consumption by up to 92% compared to architectures recommended by conventional NAS.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #19d3a1" title="Confidence: 58.2%">
                        HPO and AutoML
                    </span>
            <!-- LLMs: 17.8 -->
                
            <!-- RAG: 2.6 -->
                
            <!-- 3D: 2.3 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Finance: 1.2 -->
                
            <!-- Bayesian Optimization: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                48.323
            </span>
            <a href="https://arxiv.org/abs/2504.08057" target="_blank" rel="noopener noreferrer">Vector Quantized-Elites: Unsupervised and Problem-Agnostic Quality-Diversity Optimization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Constantinos Tsakonas, Konstantinos Chatzilygeroudis | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Quality-Diversity algorithms have transformed optimization by prioritizing the discovery of diverse, high-performing solutions over a single optimal result. However, traditional Quality-Diversity methods, such as MAP-Elites, rely heavily on predefined behavioral descriptors and complete prior knowle</span>
            
            <span class="abstract-full" style="display: none;">Quality-Diversity algorithms have transformed optimization by prioritizing the discovery of diverse, high-performing solutions over a single optimal result. However, traditional Quality-Diversity methods, such as MAP-Elites, rely heavily on predefined behavioral descriptors and complete prior knowledge of the task to define the behavioral space grid, limiting their flexibility and applicability. In this work, we introduce Vector Quantized-Elites (VQ-Elites), a novel Quality-Diversity algorithm that autonomously constructs a structured behavioral space grid using unsupervised learning, eliminating the need for prior task-specific knowledge. At the core of VQ-Elites is the integration of Vector Quantized Variational Autoencoders, which enables the dynamic learning of behavioral descriptors and the generation of a structured, rather than unstructured, behavioral space grid - a significant advancement over existing unsupervised Quality-Diversity approaches. This design establishes VQ-Elites as a flexible, robust, and task-agnostic optimization framework. To further enhance the performance of unsupervised Quality-Diversity algorithms, we introduce two key components: behavioral space bounding and cooperation mechanisms, which significantly improve convergence and performance. We validate VQ-Elites on robotic arm pose-reaching and mobile robot space-covering tasks. The results demonstrate its ability to efficiently generate diverse, high-quality solutions, emphasizing its adaptability, scalability, robustness to hyperparameters, and potential to extend Quality-Diversity optimization to complex, previously inaccessible domains.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Quality Diversity: 48.9 -->
                
            <!-- LLMs: 11.3 -->
                
            <!-- Medicine: 4.2 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Blockchain: 1.7 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                13.2827
            </span>
            <a href="https://arxiv.org/abs/2504.09415" target="_blank" rel="noopener noreferrer">Nash Equilibrium Between Consumer Electronic Devices and DoS Attacker for Distributed IoT-enabled RSE Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Gengcan Chen, Donghong Cai, Zahid Khan, Jawad Ahmad, Wadii Boulila | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between cons</span>
            
            <span class="abstract-full" style="display: none;">In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between consumer electronic devices and DoS attackers is critical. This paper focuses on the adversarial strategy between consumer electronic devices and DoS attackers in IoT-enabled RSE Systems. We first propose a remote joint estimation model for distributed measurements to effectively reduce consumer electronic device workload and minimize data leakage risks. The Kalman filter is deployed on the remote estimator, and the DoS attacks with open-loop as well as closed-loop are considered. We further introduce advanced reinforcement learning techniques, including centralized and distributed Minimax-DQN, to address high-dimensional decision-making challenges in both open-loop and closed-loop scenarios. Especially, the Q-network instead of the Q-table is used in the proposed approaches, which effectively solves the challenge of Q-learning. Moreover, the proposed distributed Minimax-DQN reduces the action space to expedite the search for Nash Equilibrium (NE). The experimental results validate that the proposed model can expeditiously restore the RSE error covariance to a stable state in the presence of DoS attacks, exhibiting notable attack robustness. The proposed centralized and distributed Minimax-DQN effectively resolves the NE in both open and closed-loop case, showcasing remarkable performance in terms of convergence. It reveals that substantial advantages in both efficiency and stability are achieved compared with the state-of-the-art methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 77.4%">
                        Reinforcement Learning
                    </span>
            <span class="tag-badge high-confidence" style="background-color: #546bc5" title="Confidence: 75.9%">
                        Game Theory
                    </span>
            <!-- Medicine: 6.0 -->
                
            <!-- LLMs: 3.0 -->
                
            <!-- Robotics: 2.9 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                11.6369
            </span>
            <a href="https://arxiv.org/abs/2504.01332" target="_blank" rel="noopener noreferrer">When to Truncate the Archive? On the Effect of the Truncation Frequency in Multi-Objective Optimisation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhiji Cui, Zimin Liang, Lie Meng Pang, Hisao Ishibuchi, Miqing Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Using an archive to store nondominated solutions found during the search of a multi-objective evolutionary algorithm (MOEA) is a useful practice. However, as nondominated solutions of a multi-objective optimisation problem can be enormous or infinitely many, it is desirable to provide the decision-m</span>
            
            <span class="abstract-full" style="display: none;">Using an archive to store nondominated solutions found during the search of a multi-objective evolutionary algorithm (MOEA) is a useful practice. However, as nondominated solutions of a multi-objective optimisation problem can be enormous or infinitely many, it is desirable to provide the decision-maker with only a small, representative portion of all the nondominated solutions in the archive, thus entailing a truncation operation. Then, an important issue is when to truncate the archive. This can be done once a new solution generated, a batch of new solutions generated, or even using an unbounded archive to keep all nondominated solutions generated and truncate it later. Intuitively, the last approach may lead to a better result since we have all the information in hand before performing the truncation. In this paper, we study this issue and investigate the effect of the timing of truncating the archive. We apply well-established truncation criteria that are commonly used in the population maintenance procedure of MOEAs (e.g., crowding distance, hypervolume indicator, and decomposition). We show that, interestingly, truncating the archive once a new solution generated tends to be the best, whereas considering an unbounded archive is often the worst. We analyse and discuss this phenomenon. Our results highlight the importance of developing effective subset selection techniques (rather than employing the population maintenance methods in MOEAs) when using a large archive.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #ae668e" title="Confidence: 76.2%">
                        Evolutionary Algorithms
                    </span>
            <!-- Reinforcement Learning: 3.9 -->
                
            <!-- Math: 3.2 -->
                
            <!-- LLMs: 2.8 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Medicine: 2.1 -->
                
            <!-- Quantum Computing: 1.8 -->
                
            <!-- Pathfinding: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                7.1057
            </span>
            <a href="https://arxiv.org/abs/2409.11267" target="_blank" rel="noopener noreferrer">Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Caio Fabio Oliveira da Silva, Azita Dabiri, Bart De Schutter | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work proposes an approach that integrates reinforcement learning and model predictive control (MPC) to solve finite-horizon optimal control problems in mixed-logical dynamical systems efficiently. Optimization-based control of such systems with discrete and continuous decision variables entails</span>
            
            <span class="abstract-full" style="display: none;">This work proposes an approach that integrates reinforcement learning and model predictive control (MPC) to solve finite-horizon optimal control problems in mixed-logical dynamical systems efficiently. Optimization-based control of such systems with discrete and continuous decision variables entails the online solution of mixed-integer linear programs, which suffer from the curse of dimensionality. Our approach aims to mitigate this issue by decoupling the decision on the discrete variables from the decision on the continuous variables. In the proposed approach, reinforcement learning determines the discrete decision variables and simplifies the online optimization problem of the MPC controller from a mixed-integer linear program to a linear program, significantly reducing the computational time. A fundamental contribution of this work is the definition of the decoupled Q-function, which plays a crucial role in making the learning problem tractable in a combinatorial action space. We motivate the use of recurrent neural networks to approximate the decoupled Q-function and show how they can be employed in a reinforcement learning setting. Simulation experiments on a microgrid system using real-world data demonstrate that the proposed method substantially reduces the online computation time of MPC while maintaining high feasibility and low suboptimality.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 80.6%">
                        Reinforcement Learning
                    </span>
            <!-- Medicine: 3.3 -->
                
            <!-- Math: 2.6 -->
                
            <!-- LLMs: 2.0 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                7.1007
            </span>
            <a href="https://arxiv.org/abs/2504.09035" target="_blank" rel="noopener noreferrer">InterQ: A DQN Framework for Optimal Intermittent Control</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shubham Aggarwal, Dipankar Maity, Tamer Ba\c{s}ar | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this letter, we explore the communication-control co-design of discrete-time stochastic linear systems through reinforcement learning. Specifically, we examine a closed-loop system involving two sequential decision-makers: a scheduler and a controller. The scheduler continuously monitors the syst</span>
            
            <span class="abstract-full" style="display: none;">In this letter, we explore the communication-control co-design of discrete-time stochastic linear systems through reinforcement learning. Specifically, we examine a closed-loop system involving two sequential decision-makers: a scheduler and a controller. The scheduler continuously monitors the system's state but transmits it to the controller intermittently to balance the communication cost and control performance. The controller, in turn, determines the control input based on the intermittently received information. Given the partially nested information structure, we show that the optimal control policy follows a certainty-equivalence form. Subsequently, we analyze the qualitative behavior of the scheduling policy. To develop the optimal scheduling policy, we propose InterQ, a deep reinforcement learning algorithm which uses a deep neural network to approximate the Q-function. Through extensive numerical evaluations, we analyze the scheduling landscape and further compare our approach against two baseline strategies: (a) a multi-period periodic scheduling policy, and (b) an event-triggered policy. The results demonstrate that our proposed method outperforms both baselines. The open source implementation can be found at https://github.com/AC-sh/InterQ.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 76.2%">
                        Reinforcement Learning
                    </span>
            <!-- Medicine: 4.2 -->
                
            <!-- LLMs: 2.6 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Federated Learning: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Multi-armed Bandit: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                6.9174
            </span>
            <a href="https://arxiv.org/abs/2504.08534" target="_blank" rel="noopener noreferrer">Genetic Algorithm Design Exploration for On-Device Training on FPGAs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alaa Mazouz, Van-Tam Nguyen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose an automated Design Space Exploration (DSE) workflow for generating adaptive and reconfigurable deep learning models on FPGA hardware. The workflow consists of two main components: Offline Design Exploration (ODE) and Online Design Reconfiguration (ODR). ODE applies a multi-objective gene</span>
            
            <span class="abstract-full" style="display: none;">We propose an automated Design Space Exploration (DSE) workflow for generating adaptive and reconfigurable deep learning models on FPGA hardware. The workflow consists of two main components: Offline Design Exploration (ODE) and Online Design Reconfiguration (ODR). ODE applies a multi-objective genetic algorithm to explore CNN-based hardware configurations, optimizing for latency and resource utilization by leveraging intra-layer parallelism. Given a CNN architecture and user-defined constraints, the hardware model is generated automatically. ODR enables runtime hardware adaptability by dynamically selecting between partial or full reconfigurable designs based on application requirements. This flexibility is essential for time-critical, autonomous onboard systems. We demonstrate the proposed workflow on the Xilinx Zynq-7100 FPGA operating at 200 MHz, using CNN models trained on MNIST, SVHN, and CIFAR-10. ODE-generated designs show latency improvements of up to 95 times for MNIST, 71 times for CIFAR-10, and 18 times for SVHN. Resource utilization in DSP slices was improved by up to 44 times for MNIST, 52 times for SVHN, and 24 times for CIFAR-10. The ODR approach achieved trade-offs between accuracy and performance, such as a 0.7 percent accuracy drop for a 13 times speedup and 25 percent power reduction on MNIST, a 2 percent drop for 14 times speedup and 28 percent power savings on SVHN, and a 4 percent drop for 50 times speedup with 32.5 percent power reduction on CIFAR-10.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #ae668e" title="Confidence: 72.0%">
                        Evolutionary Algorithms
                    </span>
            <!-- Medicine: 15.5 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- LLMs: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                6.4345
            </span>
            <a href="https://arxiv.org/abs/2504.08943" target="_blank" rel="noopener noreferrer">Investigating the Treacherous Turn in Deep Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chace Ashcraft, Kiran Karra, Josh Carney, Nathan Drenkow | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by </span>
            
            <span class="abstract-full" style="display: none;">The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by the human supervisor, but when deployed to perform its task, it performs an alternate behavior without the supervisor there to prevent it. Initial experiments applying DRL to an implementation of the A Link to the Past example do not produce the treacherous turn effect naturally, despite various modifications to the environment intended to produce it. However, in this work, we find the treacherous behavior to be reproducible in a DRL agent when using other trojan injection strategies. This approach deviates from the prototypical treacherous turn behavior since the behavior is explicitly trained into the agent, rather than occurring as an emergent consequence of environmental complexity or poor objective specification. Nonetheless, these experiments provide new insights into the challenges of producing agents capable of true treacherous turn behavior.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 77.3%">
                        Reinforcement Learning
                    </span>
            <!-- LLMs: 4.7 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Medicine: 2.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Attention: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                5.8703
            </span>
            <a href="https://arxiv.org/abs/2504.08667" target="_blank" rel="noopener noreferrer">Faster shortest-path algorithms using the acyclic-connected tree</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Elis Stefansson, Oliver Biggar, Karl H. Johansson | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper gives a fixed-parameter linear algorithm for the single-source shortest path problem (SSSP) on directed graphs. The parameter in question is the nesting width, a measure of the extent to which a graph can be represented as a nested collection of graphs. We present a novel directed graph d</span>
            
            <span class="abstract-full" style="display: none;">This paper gives a fixed-parameter linear algorithm for the single-source shortest path problem (SSSP) on directed graphs. The parameter in question is the nesting width, a measure of the extent to which a graph can be represented as a nested collection of graphs. We present a novel directed graph decomposition called the acyclic-connected tree (A-C tree), which breaks the graph into a recursively nested sequence of strongly connected components in topological order. We prove that the A-C tree is optimal in the sense that its width, the size of the largest nested graph, is equal to the nesting width of the graph. We then provide a linear-time algorithm for constructing the A-C tree of any graph. Finally, we show how the A-C tree allows us to construct a simple variant of Dijkstra's algorithm which achieves a time complexity of $O(e+n\log w)$, where $n$ ($e$) is the number of nodes (arcs) in the graph and $w$ is the nesting width. The idea is to apply the shortest path algorithm separately to each component in the order dictated by the A-C tree. We obtain an asymptotic improvement over Dijkstra's algorithm: when $w=n$, our algorithm reduces to Dijkstra's algorithm, but it is faster when $w \in o(n)$, and linear-time for classes of graphs with bounded width, such as directed acyclic graphs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #5caaa5" title="Confidence: 80.7%">
                        Pathfinding
                    </span>
            <!-- Reinforcement Learning: 4.1 -->
                
            <!-- Math: 4.1 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Quantum Computing: 1.8 -->
                
            <!-- Medicine: 1.3 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                4.9372
            </span>
            <a href="https://arxiv.org/abs/2504.05108" target="_blank" rel="noopener noreferrer">Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, Caglar Gulcehre | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating t</span>
            
            <span class="abstract-full" style="display: none;">Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #ae668e" title="Confidence: 69.8%">
                        Evolutionary Algorithms
                    </span>
            <!-- LLMs: 38.1 -->
                
            <!-- Medicine: 4.7 -->
                
            <!-- Reinforcement Learning: 3.8 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Federated Learning: 2.5 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                3.8057
            </span>
            <a href="https://arxiv.org/abs/2412.14865" target="_blank" rel="noopener noreferrer">Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anthony Kobanda, R\'emy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Su</span>
            
            <span class="abstract-full" style="display: none;">We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Such issues prevail in autonomous robotics and video game simulations, notably for navigation tasks prone to topological or kinematic changes. To address these issues, we introduce HiSPO, a novel hierarchical framework designed specifically for continual learning in navigation settings from offline data. Our method leverages distinct policy subspaces of neural networks to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like navigation simulations, showcasing competitive performances and satisfying adaptability with respect to classical continual learning metrics, in particular regarding the memory usage and efficiency.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #44f899" title="Confidence: 70.6%">
                        Reinforcement Learning
                    </span>
            <!-- Medicine: 7.7 -->
                
            <!-- LLMs: 6.4 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Math: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                2.7683
            </span>
            <a href="https://arxiv.org/abs/2504.09716" target="_blank" rel="noopener noreferrer">Dominated Actions in Imperfect-Information Games</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sam Ganzfried | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Na</span>
            
            <span class="abstract-full" style="display: none;">Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Nash equilibrium. For imperfect-information games in extensive form, we could convert the game to strategic form and then iteratively remove dominated strategies in the same way; however, this conversion may cause an exponential blowup in game size. In this paper we define and study the concept of dominated actions in imperfect-information games. Our main result is a polynomial-time algorithm for determining whether an action is dominated (strictly or weakly) by any mixed strategy in n-player games, which can be extended to an algorithm for iteratively removing dominated actions. This allows us to efficiently reduce the size of the game tree as a preprocessing step for Nash equilibrium computation. We explore the role of dominated actions empirically in the "All In or Fold" No-Limit Texas Hold'em poker variant.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #546bc5" title="Confidence: 74.7%">
                        Game Theory
                    </span>
            <!-- Medicine: 3.4 -->
                
            <!-- LLMs: 3.4 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Pathfinding: 2.2 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Evolutionary Algorithms: 1.9 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -1.6481
            </span>
            <a href="https://arxiv.org/abs/2409.04663" target="_blank" rel="noopener noreferrer">On pattern formation in the thermodynamically-consistent variational Gray-Scott model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wenrui Hao, Chun Liu, Yiwei Wang, Yahong Yang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we explore pattern formation in a four-species variational Gary-Scott model, which includes all reverse reactions and introduces a virtual species to describe the birth-death process in the classical Gray-Scott model. This modification transforms the classical Gray-Scott model into a </span>
            
            <span class="abstract-full" style="display: none;">In this paper, we explore pattern formation in a four-species variational Gary-Scott model, which includes all reverse reactions and introduces a virtual species to describe the birth-death process in the classical Gray-Scott model. This modification transforms the classical Gray-Scott model into a thermodynamically consistent closed system. The classical two-species Gray-Scott model can be viewed as a subsystem of the variational model in the limiting case when the small parameter $\epsilon$, related to the reaction rate of the reverse reactions, approaches zero. We numerically explore pattern formation in this physically more complete Gray-Scott model in one spatial dimension, using non-uniform steady states of the classical model as initial conditions. By decreasing $\epsilon$, we observed that the stationary pattern in the classical Gray-Scott model can be stabilized as the transient state in the variational model for a significantly small $\epsilon$. Additionally, the variational model admits oscillating and traveling-wave-like patterns for small $\epsilon$. The persistent time of these patterns is on the order of $O(\epsilon^{-1})$. We also analyze the energy stability of two uniform steady states in the variational Gary-Scott model for fixed $\epsilon$. Although both states are stable in a certain sense, the gradient flow type dynamics of the variational model exhibit a selection effect based on the initial conditions, with pattern formation occurring only if the initial condition does not converge to the boundary steady state, which corresponds to the trivial uniform steady state in the classical Gray-Scott model.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.6 -->
                
            <!-- Math: 3.7 -->
                
            <!-- Federated Learning: 3.4 -->
                
            <!-- Medicine: 3.0 -->
                
            <!-- Robotics: 2.8 -->
                
            <!-- Pathfinding: 2.8 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Quantum Computing: 1.5 -->
                
            <!-- LLMs: 1.2 -->
                
            <!-- Game Theory: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -1.6727
            </span>
            <a href="https://arxiv.org/abs/2411.09525" target="_blank" rel="noopener noreferrer">Data-driven parameterization refinement for the structural optimization of cruise ship hulls</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lorenzo Fabris, Marco Tezzele, Ciro Busiello, Mauro Sicchiero, Gianluigi Rozza | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this work, we focus on the early design phase of cruise ship hulls, where the designers are tasked with ensuring the structural resilience of the ship against extreme waves while reducing steel usage and respecting safety and manufacturing constraints. At this stage the geometry of the ship is al</span>
            
            <span class="abstract-full" style="display: none;">In this work, we focus on the early design phase of cruise ship hulls, where the designers are tasked with ensuring the structural resilience of the ship against extreme waves while reducing steel usage and respecting safety and manufacturing constraints. At this stage the geometry of the ship is already finalized and the designer choose the thickness of the primary structural elements, such as decks, bulkheads, and the shell. Reduced order modeling and black-box optimization techniques reduce the use of expensive finite element analysis to only validate the most promising configurations, thanks to the efficient exploration of the domain of decision variables. However, the quality of the final results heavily relies on the problem formulation, and on how the structural elements are assigned to the decision variables. With the increased request for alternative fuels and engine technologies, the designers are often faced with novel configurations and risk producing ill-suited parameterizations. To address this issue, we enhanced a structural optimization pipeline for cruise ships developed in collaboration with Fincantieri S.p.A. with a novel data-driven hierarchical reparameterization procedure, based on the optimization of a series of sub-problems. Moreover, we implemented a multi-objective optimization module to provide the designers with insights into the efficient trade-offs between competing quantities of interest and enhanced the single-objective Bayesian optimization module. The new pipeline is tested on a simplified midship section and a full ship hull, comparing the automated reparameterization to a baseline model provided by the designers. The tests show that the iterative refinement outperforms the baseline, thus streamlining the initial design phase and helping tackle more innovative projects.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.1 -->
                
            <!-- Reinforcement Learning: 5.7 -->
                
            <!-- Math: 4.3 -->
                
            <!-- Federated Learning: 2.3 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- LLMs: 1.6 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Networks: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -1.9428
            </span>
            <a href="https://arxiv.org/abs/2501.01383" target="_blank" rel="noopener noreferrer">Electrical networks and data analysis in phylogenetics</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: V. Gorbounov, A. Kazakov | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A classic problem in data analysis is studying the systems of subsets defined by either a similarity or a dissimilarity function on $X$ which is either observed directly or derived from a data set. For an electrical network there are two functions on the set of the nodes defined by the resistance ma</span>
            
            <span class="abstract-full" style="display: none;">A classic problem in data analysis is studying the systems of subsets defined by either a similarity or a dissimilarity function on $X$ which is either observed directly or derived from a data set. For an electrical network there are two functions on the set of the nodes defined by the resistance matrix and the response matrix either of which defines the network completely. We argue that these functions should be viewed as a similarity and a dissimilarity function on the set of the nodes moreover they are related via the covariance mapping also known as the Farris transform or the Gromov product. We will explore the properties of electrical networks from this point of view. It has been known for a while that the resistance matrix defines a metric on the nodes of the electrical networks. Moreover for a circular electrical network this metric obeys the Kalmanson property as it was shown recently. We will call such a metric an electrical Kalmanson metric. The main results of this paper is a complete description of the electrical Kalmanson metrics in the set of all Kalmanson metrics in terms of the geometry of the positive Isotropic Grassmannian whose connection to the theory of electrical networks was discovered earlier. One important area of applications where Kalmanson metrics are actively used is the theory of phylogenetic networks which are a generalization of phylogenetic trees. Our results allow us to use in phylogenetics the powerful methods of reconstruction of the minimal graphs of electrical networks and possibly open the door into data analysis for the methods of the theory of cluster algebras.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 7.9 -->
                
            <!-- Reinforcement Learning: 4.7 -->
                
            <!-- Medicine: 2.4 -->
                
            <!-- Pathfinding: 2.4 -->
                
            <!-- Robotics: 2.3 -->
                
            <!-- Quantum Computing: 1.4 -->
                
            <!-- Networks: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- LLMs: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.0313
            </span>
            <a href="https://arxiv.org/abs/2404.10550" target="_blank" rel="noopener noreferrer">Analytical Approximation of the ELBO Gradient in the Context of the Clutter Problem</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Roumen Nikolaev Popov | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose an analytical solution for approximating the gradient of the Evidence Lower Bound (ELBO) in variational inference problems where the statistical model is a Bayesian network consisting of observations drawn from a mixture of a Gaussian distribution embedded in unrelated clutter, known as t</span>
            
            <span class="abstract-full" style="display: none;">We propose an analytical solution for approximating the gradient of the Evidence Lower Bound (ELBO) in variational inference problems where the statistical model is a Bayesian network consisting of observations drawn from a mixture of a Gaussian distribution embedded in unrelated clutter, known as the clutter problem. The method employs the reparameterization trick to move the gradient operator inside the expectation and relies on the assumption that, because the likelihood factorizes over the observed data, the variational distribution is generally more compactly supported than the Gaussian distribution in the likelihood factors. This allows efficient local approximation of the individual likelihood factors, which leads to an analytical solution for the integral defining the gradient expectation. We integrate the proposed gradient approximation as the expectation step in an EM (Expectation Maximization) algorithm for maximizing ELBO and test against classical deterministic approaches in Bayesian inference, such as the Laplace approximation, Expectation Propagation and Mean-Field Variational Inference. The proposed method demonstrates good accuracy and rate of convergence together with linear computational complexity.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 4.5 -->
                
            <!-- Medicine: 4.3 -->
                
            <!-- Reinforcement Learning: 4.3 -->
                
            <!-- Robotics: 2.6 -->
                
            <!-- Pathfinding: 2.6 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- LLMs: 1.8 -->
                
            <!-- Quantum Computing: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.1339
            </span>
            <a href="https://arxiv.org/abs/2504.08801" target="_blank" rel="noopener noreferrer">Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Andrew Kiruluta, Priscilla Burity, Samantha Williams | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Transformer architectures, underpinned by the self-attention mechanism, have achieved state-of-the-art results across numerous natural language processing (NLP) tasks by effectively modeling long-range dependencies. However, the computational complexity of self-attention, scaling quadratically with </span>
            
            <span class="abstract-full" style="display: none;">Transformer architectures, underpinned by the self-attention mechanism, have achieved state-of-the-art results across numerous natural language processing (NLP) tasks by effectively modeling long-range dependencies. However, the computational complexity of self-attention, scaling quadratically with input sequence length, presents significant challenges for processing very long sequences or operating under resource constraints. This paper introduces the Learnable Multi-Scale Wavelet Transformer (LMWT), a novel architecture that replaces the standard dot-product self-attention with a learnable multi-scale Haar wavelet transform module. Leveraging the intrinsic multi-resolution properties of wavelets, the LMWT efficiently captures both local details and global context. Crucially, the parameters of the wavelet transform, including scale-specific coefficients, are learned end-to-end during training, allowing the model to adapt its decomposition strategy to the data and task. We present the detailed mathematical formulation of the learnable Haar wavelet module and its integration into the transformer framework, supplemented by an architectural diagram. We conduct a comprehensive experimental evaluation on a standard machine translation benchmark (WMT16 En-De), comparing the LMWT against a baseline self-attention transformer using metrics like BLEU score, perplexity, and token accuracy. Furthermore, we analyze the computational complexity, highlighting the linear scaling of our approach, discuss its novelty in the context of related work, and explore the interpretability offered by visualizing the learned Haar coefficients. Our results indicate that the LMWT achieves competitive performance while offering substantial computational advantages, positioning it as a promising and novel alternative for efficient sequence modeling.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #2aa97e" title="Confidence: 73.3%">
                        Attention
                    </span>
            <!-- Medicine: 5.2 -->
                
            <!-- Reinforcement Learning: 3.6 -->
                
            <!-- LLMs: 2.9 -->
                
            <!-- Quantum Computing: 2.9 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Federated Learning: 2.3 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.1577
            </span>
            <a href="https://arxiv.org/abs/2503.23600" target="_blank" rel="noopener noreferrer">Online Convex Optimization and Integral Quadratic Constraints: A new approach to regret analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Fabian Jakob, Andrea Iannelli | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose a novel approach for analyzing dynamic regret of first-order constrained online convex optimization algorithms for strongly convex and Lipschitz-smooth objectives. Crucially, we provide a general analysis that is applicable to a wide range of first-order algorithms that can be expressed a</span>
            
            <span class="abstract-full" style="display: none;">We propose a novel approach for analyzing dynamic regret of first-order constrained online convex optimization algorithms for strongly convex and Lipschitz-smooth objectives. Crucially, we provide a general analysis that is applicable to a wide range of first-order algorithms that can be expressed as an interconnection of a linear dynamical system in feedback with a first-order oracle. By leveraging Integral Quadratic Constraints (IQCs), we derive a semi-definite program which, when feasible, provides a regret guarantee for the online algorithm. For this, the concept of variational IQCs is introduced as the generalization of IQCs to time-varying monotone operators. Our bounds capture the temporal rate of change of the problem in the form of the path length of the time-varying minimizer and the objective function variation. In contrast to standard results in OCO, our results do not require nerither the assumption of gradient boundedness, nor that of a bounded feasible set. Numerical analyses showcase the ability of the approach to capture the dependence of the regret on the function class condition number.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 5.5 -->
                
            <!-- LLMs: 4.2 -->
                
            <!-- Math: 3.3 -->
                
            <!-- Medicine: 3.1 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Pathfinding: 1.8 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.2046
            </span>
            <a href="https://arxiv.org/abs/2504.08698" target="_blank" rel="noopener noreferrer">Performance Evaluation of Trajectory Tracking Controllers for a Quadruped Robot Leg</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hossein Shojaei, Hamid Rahmanei, Seyed Hossein Sadati | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The complexities in the dynamic model of the legged robots make it necessary to utilize model-free controllers in the task of trajectory tracking. In This paper, an adaptive transpose Jacobian approach is proposed to deal with the dynamic model complexity, which utilizes an adaptive PI-algorithm to </span>
            
            <span class="abstract-full" style="display: none;">The complexities in the dynamic model of the legged robots make it necessary to utilize model-free controllers in the task of trajectory tracking. In This paper, an adaptive transpose Jacobian approach is proposed to deal with the dynamic model complexity, which utilizes an adaptive PI-algorithm to adjust the control gains. The performance of the proposed control algorithm is compared with the conventional transpose Jacobian and sliding mode control algorithms and evaluated by the root mean square of the errors and control input energy criteria. In order to appraise the effectiveness of the proposed control system, simulations are carried out in MATLAB/Simulink software for a quadruped robot leg for semi-elliptical path tracking. The obtained results show that the proposed adaptive transpose Jacobian reduces the overshoot and root mean square of the errors and at the same time, decreases the control input energy. Moreover, transpose Jacobin and adaptive transpose Jacobian are more robust to changes in initial conditions compared to the conventional sliding mode control. Furthermore, sliding mode control performs well up to 20% uncertainties in the parameters due to its model-based nature, whereas the transpose Jacobin and the proposed adaptive transpose Jacobian algorithms show promising results even in higher mass uncertainties.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.7 -->
                
            <!-- Medicine: 4.2 -->
                
            <!-- LLMs: 3.3 -->
                
            <!-- Robotics: 2.8 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Quantum Computing: 1.7 -->
                
            <!-- Pathfinding: 1.6 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.2305
            </span>
            <a href="https://arxiv.org/abs/2504.09273" target="_blank" rel="noopener noreferrer">Arnold diffusion in the full three-body problem</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Maciej J. Capinski, Marian Gidea | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We show the existence of Arnold diffusion in the planar full three-body problem, which is expressed as a perturbation of a Kepler problem and a planar circular restricted three-body problem, with the perturbation parameter being the mass of the smallest body. In this context, we obtain Arnold diffus</span>
            
            <span class="abstract-full" style="display: none;">We show the existence of Arnold diffusion in the planar full three-body problem, which is expressed as a perturbation of a Kepler problem and a planar circular restricted three-body problem, with the perturbation parameter being the mass of the smallest body. In this context, we obtain Arnold diffusion in terms of a transfer of energy, in an amount independent of the perturbation parameter, between the Kepler problem and the restricted three-body problem. Our argument is based on a topological method based on correctly aligned windows which is implemented into a computer assisted proof. This approach can be applied to physically relevant masses of the bodies, such as those in a Neptune-Triton-asteroid system. In this case, we obtain explicit estimates for the range of the perturbation parameter and for the diffusion time.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 4.2 -->
                
            <!-- Medicine: 3.9 -->
                
            <!-- Reinforcement Learning: 3.5 -->
                
            <!-- LLMs: 2.5 -->
                
            <!-- Robotics: 2.4 -->
                
            <!-- Pathfinding: 2.2 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Quantum Computing: 1.8 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.4465
            </span>
            <a href="https://arxiv.org/abs/2504.08626" target="_blank" rel="noopener noreferrer">Task-conditioned Ensemble of Expert Models for Continuous Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Renu Sharma, Debasmita Pal, Arun Ross | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed mode</span>
            
            <span class="abstract-full" style="display: none;">One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed model with new data could be one remedy. However, the question arises as to how we should update the model with new training data so that it retains its accuracy on the old data while adapting to the new data. In this work, we propose a task-conditioned ensemble of models to maintain the performance of the existing model. The method involves an ensemble of expert models based on task membership information. The in-domain models-based on the local outlier concept (different from the expert models) provide task membership information dynamically at run-time to each probe sample. To evaluate the proposed method, we experiment with three setups: the first represents distribution shift between tasks (LivDet-Iris-2017), the second represents distribution shift both between and within tasks (LivDet-Iris-2020), and the third represents disjoint distribution between tasks (Split MNIST). The experiments highlight the benefits of the proposed method. The source code is available at https://github.com/iPRoBe-lab/Continuous_Learning_FE_DM.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.1 -->
                
            <!-- Medicine: 5.5 -->
                
            <!-- LLMs: 4.7 -->
                
            <!-- Federated Learning: 4.2 -->
                
            <!-- Math: 3.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Quantum Computing: 1.5 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.5526
            </span>
            <a href="https://arxiv.org/abs/2504.08421" target="_blank" rel="noopener noreferrer">Poisson multi-Bernoulli mixture filter for trajectory measurements</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Marco Fontana, \'Angel F. Garc\'ia-Fern\'andez, Simon Maskell | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper presents a Poisson multi-Bernoulli mixture (PMBM) filter for multi-target filtering based on sensor measurements that are sets of trajectories in the last two-time step window. The proposed filter, the trajectory measurement PMBM (TM-PMBM) filter, propagates a PMBM density on the set of t</span>
            
            <span class="abstract-full" style="display: none;">This paper presents a Poisson multi-Bernoulli mixture (PMBM) filter for multi-target filtering based on sensor measurements that are sets of trajectories in the last two-time step window. The proposed filter, the trajectory measurement PMBM (TM-PMBM) filter, propagates a PMBM density on the set of target states. In prediction, the filter obtains the PMBM density on the set of trajectories over the last two time steps. This density is then updated with the set of trajectory measurements. After the update step, the PMBM posterior on the set of two-step trajectories is marginalised to obtain a PMBM density on the set of target states. The filter provides a closed-form solution for multi-target filtering based on sets of trajectory measurements, estimating the set of target states at the end of each time window. Additionally, the paper proposes computationally lighter alternatives to the TM-PMBM filter by deriving a Poisson multi-Bernoulli (PMB) density through Kullback-Leibler divergence minimisation in an augmented space with auxiliary variables. The performance of the proposed filters are evaluated in a simulation study.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 5.1 -->
                
            <!-- Reinforcement Learning: 4.7 -->
                
            <!-- Medicine: 4.4 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- LLMs: 2.1 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Pathfinding: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.5666
            </span>
            <a href="https://arxiv.org/abs/2504.09804" target="_blank" rel="noopener noreferrer">BO-SA-PINNs: Self-adaptive physics-informed neural networks based on Bayesian optimization for automatically designing PDE solvers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rui Zhang, Liang Li, St\'ephane Lanteri, Hao Kang, Jiaqi Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Physics-informed neural networks (PINNs) is becoming a popular alternative method for solving partial differential equations (PDEs). However, they require dedicated manual modifications to the hyperparameters of the network, the sampling methods and loss function weights for different PDEs, which re</span>
            
            <span class="abstract-full" style="display: none;">Physics-informed neural networks (PINNs) is becoming a popular alternative method for solving partial differential equations (PDEs). However, they require dedicated manual modifications to the hyperparameters of the network, the sampling methods and loss function weights for different PDEs, which reduces the efficiency of the solvers. In this paper, we pro- pose a general multi-stage framework, i.e. BO-SA-PINNs to alleviate this issue. In the first stage, Bayesian optimization (BO) is used to select hyperparameters for the training process, and based on the results of the pre-training, the network architecture, learning rate, sampling points distribution and loss function weights suitable for the PDEs are automatically determined. The proposed hyperparameters search space based on experimental results can enhance the efficiency of BO in identifying optimal hyperparameters. After selecting the appropriate hyperparameters, we incorporate a global self-adaptive (SA) mechanism the second stage. Using the pre-trained model and loss information in the second-stage training, the exponential moving average (EMA) method is employed to optimize the loss function weights, and residual-based adaptive refinement with distribution (RAR-D) is used to optimize the sampling points distribution. In the third stage, L-BFGS is used for stable training. In addition, we introduce a new activation function that enables BO-SA-PINNs to achieve higher accuracy. In numerical experiments, we conduct comparative and ablation experiments to verify the performance of the model on Helmholtz, Maxwell, Burgers and high-dimensional Poisson equations. The comparative experiment results show that our model can achieve higher accuracy and fewer iterations in test cases, and the ablation experiments demonstrate the positive impact of every improvement.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.7 -->
                
            <!-- Reinforcement Learning: 4.3 -->
                
            <!-- LLMs: 3.9 -->
                
            <!-- Math: 3.3 -->
                
            <!-- Federated Learning: 2.9 -->
                
            <!-- Quantum Computing: 1.7 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.5708
            </span>
            <a href="https://arxiv.org/abs/2011.11692" target="_blank" rel="noopener noreferrer">NOMA-Based Cooperative Relaying with Receive Diversity in Nakagami-m Fading Channels</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vaibhav Kumar, Barry Cardiff, Mark F Flanagan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Non-orthogonal multiple access (NOMA) is being widely considered as a potential candidate to enhance the spectrum utilization in beyond fifth-generation (B5G) communications. In this paper, we derive closed-form expressions for the ergodic rate and outage probability of a multiple-antenna-assisted N</span>
            
            <span class="abstract-full" style="display: none;">Non-orthogonal multiple access (NOMA) is being widely considered as a potential candidate to enhance the spectrum utilization in beyond fifth-generation (B5G) communications. In this paper, we derive closed-form expressions for the ergodic rate and outage probability of a multiple-antenna-assisted NOMA-based cooperative relaying system (CRS-NOMA). We present the performance analysis of the system for two different receive diversity schemes - selection combining (SC) and maximal-ratio combining (MRC), in Nakagami-m fading. We also evaluate the asymptotic behavior of the CRS-NOMA to determine the slope of the ergodic rate and diversity order. Our results show that in contrast to the existing CRS-NOMA systems, the CRS-NOMA with receive diversity outperforms its orthogonal multiple access (OMA) based counterpart even in the low-SNR regime, by achieving higher ergodic rate. Diversity analysis confirms that the CRS-NOMA achieves full diversity order using both SC and MRC schemes, and this diversity order depends on both the shape parameter m and the number of receive antennas. We also discuss the problem of optimal power allocation for the minimization of the outage probability of the system, and subsequently use this optimal value to obtain the ergodic rate. An excellent match is observed between the numerical and the analytical results, confirming the correctness of the derived analytical expressions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.9 -->
                
            <!-- Reinforcement Learning: 4.0 -->
                
            <!-- LLMs: 3.5 -->
                
            <!-- Math: 3.4 -->
                
            <!-- Robotics: 2.5 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Pathfinding: 1.7 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.5881
            </span>
            <a href="https://arxiv.org/abs/2504.08341" target="_blank" rel="noopener noreferrer">Deep learning-based moment closure for multi-phase computation of semiclassical limit of the Schr\"odinger equation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jin Woo Jang, Jae Yong Lee, Liu Liu, Zhenyi Zhu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a deep learning approach for computing multi-phase solutions to the semiclassical limit of the Schr\"odinger equation. Traditional methods require deriving a multi-phase ansatz to close the moment system of the Liouville equation, a process that is often computationally intensive and impr</span>
            
            <span class="abstract-full" style="display: none;">We present a deep learning approach for computing multi-phase solutions to the semiclassical limit of the Schr\"odinger equation. Traditional methods require deriving a multi-phase ansatz to close the moment system of the Liouville equation, a process that is often computationally intensive and impractical. Our method offers an efficient alternative by introducing a novel two-stage neural network framework to close the $2N\times 2N$ moment system, where $N$ represents the number of phases in the solution ansatz. In the first stage, we train neural networks to learn the mapping between higher-order moments and lower-order moments (along with their derivatives). The second stage incorporates physics-informed neural networks (PINNs), where we substitute the learned higher-order moments to systematically close the system. We provide theoretical guarantees for the convergence of both the loss functions and the neural network approximations. Numerical experiments demonstrate the effectiveness of our method for one- and two-dimensional problems with various phase numbers $N$ in the multi-phase solutions. The results confirm the accuracy and computational efficiency of the proposed approach compared to conventional techniques.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.9 -->
                
            <!-- Medicine: 5.8 -->
                
            <!-- Math: 2.6 -->
                
            <!-- LLMs: 2.6 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.6041
            </span>
            <a href="https://arxiv.org/abs/2504.09458" target="_blank" rel="noopener noreferrer">The Whitney method of fundamental solutions with Lusin wavelets</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jakob Jonsson, Andreas Ros\'en, Emil Timlin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We establish the theoretical foundation for a variant of the method of fundamental solutions (MFS), where the source points $\{q_j\}_{j=1}^\infty$ accumulate towards the domain in a Whitney fashion, meaning that their separation is proportional to the distance to the domain. We prove that the normal</span>
            
            <span class="abstract-full" style="display: none;">We establish the theoretical foundation for a variant of the method of fundamental solutions (MFS), where the source points $\{q_j\}_{j=1}^\infty$ accumulate towards the domain in a Whitney fashion, meaning that their separation is proportional to the distance to the domain. We prove that the normalized Lusin wavelets $\psi_j(w) = b_j(w-q_j)^{-2}$ constitute a generalized basis, known as a frame, for the Hardy subspace of $L_2$-traces of holomorphic functions on the domain. Consequently, our method, where $\psi_j$ are used as basis functions in the MFS, enables a numerically stable approximation of solutions to Laplace boundary value problems, even when the solutions lack analytic continuation across the boundary. Despite the source points accumulating towards the domain, our computations show no loss of accuracy near the boundary, in contrast to the boundary integral equation method.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 5.8 -->
                
            <!-- Reinforcement Learning: 5.4 -->
                
            <!-- LLMs: 2.7 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Medicine: 2.2 -->
                
            <!-- Pathfinding: 2.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.623
            </span>
            <a href="https://arxiv.org/abs/2504.10435" target="_blank" rel="noopener noreferrer">What metric to optimize for suppressing instability in a Vlasov-Poisson system?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Martin Guerra, Qin Li, Yukun Yue | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Stabilizing plasma dynamics is an important task in green energy generation via nuclear fusion. One common strategy is to introduce an external field to prevent the plasma distribution from developing turbulence. However, finding such external fields efficiently remains an open question, even for si</span>
            
            <span class="abstract-full" style="display: none;">Stabilizing plasma dynamics is an important task in green energy generation via nuclear fusion. One common strategy is to introduce an external field to prevent the plasma distribution from developing turbulence. However, finding such external fields efficiently remains an open question, even for simplified models such as the Vlasov-Poisson (VP) system. In this work, we leverage two different approaches to build such fields: for the first approach, we use an analytical derivation of the dispersion relation of the VP system to find a range of reasonable fields that can potentially suppress instability, providing a qualitative suggestion. For the second approach, we leverage PDE-constrained optimization to obtain a locally optimal field using different loss functions. As the stability of the system can be characterized in several different ways, the objective functions need to be tailored accordingly. We show, through extensive numerical tests, that objective functions such as the relative entropy (KL divergence) and the $L^{2}$ norm result in a highly non-convex problem, rendering the global minimum difficult to find. However, we show that using the electric energy of the system as a loss function is advantageous, as it has a large convex basin close to the global minimum. Unfortunately, outside the basin, the electric energy landscape consists of unphysical flat local minima, thus rendering a good initial guess key for the overall convergence of the optimization problem, particularly for solvers with adaptive steps.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.1 -->
                
            <!-- Math: 2.9 -->
                
            <!-- Robotics: 2.7 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- LLMs: 2.3 -->
                
            <!-- Medicine: 2.1 -->
                
            <!-- Federated Learning: 2.1 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.6777
            </span>
            <a href="https://arxiv.org/abs/2504.09768" target="_blank" rel="noopener noreferrer">Robust Output-Feedback MPC for Nonlinear Systems with Applications to Robotic Exploration</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Scott Brown, Mohammad Khajenejad, Aamodh Suresh, Sonia Martinez | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper introduces a novel method for robust output-feedback model predictive control (MPC) for a class of nonlinear discrete-time systems. We propose a novel interval-valued predictor which, given an initial estimate of the state, produces intervals which are guaranteed to contain the future tra</span>
            
            <span class="abstract-full" style="display: none;">This paper introduces a novel method for robust output-feedback model predictive control (MPC) for a class of nonlinear discrete-time systems. We propose a novel interval-valued predictor which, given an initial estimate of the state, produces intervals which are guaranteed to contain the future trajectory of the system. By parameterizing the control input with an initial stabilizing feedback term, we are able to reduce the width of the predicted state intervals compared to existing methods. We demonstrate this through a numerical comparison where we show that our controller performs better in the presence of large amounts of noise. Finally, we present a simulation study of a robot navigation scenario, where we incorporate a time-varying entropy term into the cost function in order to autonomously explore an uncertain area.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 5.9 -->
                
            <!-- LLMs: 5.7 -->
                
            <!-- Medicine: 3.6 -->
                
            <!-- Networks: 3.3 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Attention: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.6969
            </span>
            <a href="https://arxiv.org/abs/2410.19631" target="_blank" rel="noopener noreferrer">Efficient Biological Data Acquisition through Inference Set Design</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ihor Neporozhnii, Julien Roy, Emmanuel Bengio, Jason Hartford | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so one might hope to reduce their cost by only experimenting on a subset of the compounds, and predicting the outcomes of the</span>
            
            <span class="abstract-full" style="display: none;">In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so one might hope to reduce their cost by only experimenting on a subset of the compounds, and predicting the outcomes of the remaining experiments. In this work, we model this scenario as a sequential subset selection problem: we aim to select the smallest set of candidates in order to achieve some desired level of accuracy for the system as a whole. Our key observation is that, if there is heterogeneity in the difficulty of the prediction problem across the input space, selectively obtaining the labels for the hardest examples in the acquisition pool will leave only the relatively easy examples to remain in the inference set, leading to better overall system performance. We call this mechanism inference set design, and propose the use of a confidence-based active learning solution to prune out these challenging examples. Our algorithm includes an explicit stopping criterion that interrupts the acquisition loop when it is sufficiently confident that the system has reached the target performance. Our empirical studies on image and molecular datasets, as well as a real-world large-scale biological assay, show that active learning for inference set design leads to significant reduction in experimental cost while retaining high system performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 3.7 -->
                
            <!-- LLMs: 3.6 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Medicine: 2.6 -->
                
            <!-- Robotics: 2.6 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.7905
            </span>
            <a href="https://arxiv.org/abs/2503.19609" target="_blank" rel="noopener noreferrer">Nanopass Back-Translation of Call-Return Trees for Mechanized Secure Compilation Proofs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: J\'er\'emy Thibault, Joseph Lenormand, Catalin Hritcu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Researchers aim to build secure compilation chains enforcing that if there is no attack a source context can mount against a source program then there is also no attack an adversarial target context can mount against the compiled program. Proving that these compilation chains are secure is, however,</span>
            
            <span class="abstract-full" style="display: none;">Researchers aim to build secure compilation chains enforcing that if there is no attack a source context can mount against a source program then there is also no attack an adversarial target context can mount against the compiled program. Proving that these compilation chains are secure is, however, challenging, and involves a non-trivial back-translation step: for any attack a target context mounts against the compiled program one has to exhibit a source context mounting the same attack against the source program. We describe a novel back-translation technique, which results in simpler proofs that can be more easily mechanized in a proof assistant. Given a finite set of finite trace prefixes, capturing the interaction recorded during an attack between a target context and the compiled program, we build a call-return tree that we back-translate into a source context producing the same trace prefixes. We use state in the generated source context to record the current location in the call-return tree. The back-translation is done in several small steps, each adding to the tree new information describing how the location should change depending on how the context regains control. To prove this back-translation correct we give semantics to every intermediate call-return tree language, using ghost state to store information and explicitly enforce execution invariants. We prove several small forward simulations, basically seeing the back-translation as a verified nanopass compiler. Thanks to this modular structure, we are able to mechanize this complex back-translation and its correctness proof in the Rocq prover without too much effort.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.2 -->
                
            <!-- LLMs: 3.3 -->
                
            <!-- Networks: 3.2 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Medicine: 2.2 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Attention: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8058
            </span>
            <a href="https://arxiv.org/abs/2504.09831" target="_blank" rel="noopener noreferrer">Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent Demand</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Korel Gundem, Zhengling Qi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we study the offline sequential feature-based pricing and inventory control problem where the current demand depends on the past demand levels and any demand exceeding the available inventory is lost. Our goal is to leverage the offline dataset, consisting of past prices, ordering qua</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we study the offline sequential feature-based pricing and inventory control problem where the current demand depends on the past demand levels and any demand exceeding the available inventory is lost. Our goal is to leverage the offline dataset, consisting of past prices, ordering quantities, inventory levels, covariates, and censored sales levels, to estimate the optimal pricing and inventory control policy that maximizes long-term profit. While the underlying dynamic without censoring can be modeled by Markov decision process (MDP), the primary obstacle arises from the observed process where demand censoring is present, resulting in missing profit information, the failure of the Markov property, and a non-stationary optimal policy. To overcome these challenges, we first approximate the optimal policy by solving a high-order MDP characterized by the number of consecutive censoring instances, which ultimately boils down to solving a specialized Bellman equation tailored for this problem. Inspired by offline reinforcement learning and survival analysis, we propose two novel data-driven algorithms to solving these Bellman equations and, thus, estimate the optimal policy. Furthermore, we establish finite sample regret bounds to validate the effectiveness of these algorithms. Finally, we conduct numerical experiments to demonstrate the efficacy of our algorithms in estimating the optimal policy. To the best of our knowledge, this is the first data-driven approach to learning optimal pricing and inventory control policies in a sequential decision-making environment characterized by censored and dependent demand. The implementations of the proposed algorithms are available at https://github.com/gundemkorel/Inventory_Pricing_Control</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 7.0 -->
                
            <!-- LLMs: 4.9 -->
                
            <!-- Medicine: 3.9 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Federated Learning: 2.1 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.828
            </span>
            <a href="https://arxiv.org/abs/2504.10451" target="_blank" rel="noopener noreferrer">Minimizing Functions of Age of Incorrect Information for Remote Estimation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ismail Cosandal, Sennur Ulukus, Nail Akar | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The age of incorrect information (AoII) process which keeps track of the time since the source and monitor processes are in sync, has been extensively used in remote estimation problems. In this paper, we consider a push-based remote estimation system with a discrete-time Markov chain (DTMC) informa</span>
            
            <span class="abstract-full" style="display: none;">The age of incorrect information (AoII) process which keeps track of the time since the source and monitor processes are in sync, has been extensively used in remote estimation problems. In this paper, we consider a push-based remote estimation system with a discrete-time Markov chain (DTMC) information source transmitting status update packets towards the monitor once the AoII process exceeds a certain estimation-based threshold. In this paper, the time average of an arbitrary function of AoII is taken as the AoII cost, as opposed to using the average AoII as the mismatch metric, whereas this function is also allowed to depend on the estimation value. In this very general setting, our goal is to minimize a weighted sum of AoII and transmission costs. For this purpose, we formulate a discrete-time semi-Markov decision process (SMDP) regarding the multi-threshold status update policy. We propose a novel tool in discrete-time called 'dual-regime absorbing Markov chain' (DR-AMC) and its corresponding absorption time distribution named as 'dual-regime phase-type' (DR-PH) distribution, to obtain the characterizing parameters of the SMDP, which allows us to obtain the distribution of the AoII process for a given policy, and hence the average of any function of AoII. The proposed method is validated with numerical results by which we compare our proposed method against other policies obtained by exhaustive-search, and also various benchmark policies.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.0 -->
                
            <!-- LLMs: 3.8 -->
                
            <!-- Reinforcement Learning: 3.8 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Robotics: 2.4 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Quantum Computing: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8464
            </span>
            <a href="https://arxiv.org/abs/2504.08690" target="_blank" rel="noopener noreferrer">Fast-Slow-Thinking: Complex Task Solving with Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yiliu Sun, Yanfang Zhang, Zicheng Zhao, Sheng Wan, Dacheng Tao, Chen Gong | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Nowadays, Large Language Models (LLMs) have been gradually employed to solve complex tasks. To face the challenge, task decomposition has become an effective way, which proposes to divide a complex task into multiple simpler subtasks and then solve them separately so that the difficulty of the origi</span>
            
            <span class="abstract-full" style="display: none;">Nowadays, Large Language Models (LLMs) have been gradually employed to solve complex tasks. To face the challenge, task decomposition has become an effective way, which proposes to divide a complex task into multiple simpler subtasks and then solve them separately so that the difficulty of the original task can be reduced. However, the performance of existing task decomposition methods can be suboptimal when the task contains overly complex logic and constraints. In this situation, the solution generated by LLMs may deviate from the original purpose of the task, or contain redundant or even erroneous content. Therefore, inspired by the fact that humans possess two thinking systems including fast thinking and slow thinking, this paper introduces a new task decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates LLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow Thinking (ST) steps. Here FT focuses more on the general and concise aspect of the task, and ST focuses more on the details of the task. In FT, LLMs are prompted to remove the constraints of the original task, therefore simplifying it to a general and concise one. In ST, we recall the constraints removed in FT, so that LLMs can improve the answer generated in FT to meet the requirements of the original task. Therefore, our FST method enables LLMs to consider a complex problem via a human-like cognition process from coarse to fine, the effectiveness of which has been well demonstrated by the experiments on three types of tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 42.8 -->
                
            <!-- Reinforcement Learning: 2.7 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Quantum Computing: 1.6 -->
                
            <!-- Medicine: 1.6 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8581
            </span>
            <a href="https://arxiv.org/abs/2409.16163" target="_blank" rel="noopener noreferrer">The anonymization problem in social networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rachel G. de Jong, Mark P. J. van der Loo, Frank W. Takes | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper we introduce a general version of the anonymization problem in social networks, in which the goal is to maximize the number of anonymous nodes by altering a given graph. We define three variants of this optimization problem being full, partial and budgeted anonymization. In each, the o</span>
            
            <span class="abstract-full" style="display: none;">In this paper we introduce a general version of the anonymization problem in social networks, in which the goal is to maximize the number of anonymous nodes by altering a given graph. We define three variants of this optimization problem being full, partial and budgeted anonymization. In each, the objective is to maximize the number of k-anonymous nodes, i.e., nodes for which there are at least k-1 equivalent nodes, according to a particular anonymity measure of structural node equivalence. We propose four new heuristic algorithms for solving the anonymization problem which we implement into a reusable computational framework. As a baseline, we use an edge sampling method introduced in previous work. Experiments on both graph models and 23 real-world network datasets result in three empirical findings. First, we demonstrate that edge deletion is the most effective graph alteration operation. Second, we compare four commonly used anonymity measures from the literature and highlight how the choice of anonymity measure has a tremendous effect on both the initial anonymity as well as the difficulty of solving the anonymization problem. Third, we find that the proposed algorithm that preferentially deletes edges with a larger effect on nodes at a structurally unique position consistently outperforms heuristics solely based on network structure. Our best performing algorithm retains on average 14 times more edges in full anonymization, and overall ensures a better trade-off between anonymity and data utility. In the budgeted variant, it achieves 4.8 times more anonymous nodes than the baseline. This work lays foundations for future development of algorithms for anonymizing social networks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.5 -->
                
            <!-- LLMs: 3.9 -->
                
            <!-- Medicine: 3.3 -->
                
            <!-- Math: 2.8 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Pathfinding: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8586
            </span>
            <a href="https://arxiv.org/abs/2503.18236" target="_blank" rel="noopener noreferrer">Research impact evaluation based on effective authorship contribution sensitivity: h-leadership index</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hardik A. Jain, Rohitash Chandra | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The evaluation of a researcher's performance has traditionally relied on various bibliometric measures, with the h-index being one of the most prominent. However, the h-index only accounts for the number of citations received in a publication and does not account for other factors such as the number</span>
            
            <span class="abstract-full" style="display: none;">The evaluation of a researcher's performance has traditionally relied on various bibliometric measures, with the h-index being one of the most prominent. However, the h-index only accounts for the number of citations received in a publication and does not account for other factors such as the number of authors or their specific contributions in collaborative works. Therefore, the h-index has been placed on scrutiny as it has motivated academic integrity issues where non-contributing authors get authorship merely for raising their h-index. In this study, we comprehensively evaluate existing metrics in their ability to account for authorship contribution by their position and introduce a novel variant of the h-index, known as the h-leadership index. The h-leadership index aims to advance the fair evaluation of academic contributions in multi-authored publications by giving importance to authorship position beyond the first and last authors, focused by Stanford's ranking of the top 2 \% of world scientists. We assign weighted citations based on a modified complementary unit Gaussian curve, ensuring that the contributions of middle authors are appropriately recognised. We apply the h-leadership index to analyse the top 50 researchers across the Group of 8 (Go8) universities in Australia, demonstrating its potential to provide a more balanced assessment of research performance. We provide open-source software for extending the work further.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 5.4 -->
                
            <!-- Reinforcement Learning: 3.7 -->
                
            <!-- Medicine: 3.5 -->
                
            <!-- Math: 3.0 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8596
            </span>
            <a href="https://arxiv.org/abs/2406.18739" target="_blank" rel="noopener noreferrer">RetroGFN: Diverse and Feasible Retrosynthesis using GFlowNets</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Piotr Gai\'nski, Micha{\l} Koziarski, Krzysztof Maziarz, Marwin Segler, Jacek Tabor, Marek \'Smieja | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Single-step retrosynthesis aims to predict a set of reactions that lead to the creation of a target molecule, which is a crucial task in molecular discovery. Although a target molecule can often be synthesized with multiple different reactions, it is not clear how to verify the feasibility of a reac</span>
            
            <span class="abstract-full" style="display: none;">Single-step retrosynthesis aims to predict a set of reactions that lead to the creation of a target molecule, which is a crucial task in molecular discovery. Although a target molecule can often be synthesized with multiple different reactions, it is not clear how to verify the feasibility of a reaction, because the available datasets cover only a tiny fraction of the possible solutions. Consequently, the existing models are not encouraged to explore the space of possible reactions sufficiently. In this paper, we propose a novel single-step retrosynthesis model, RetroGFN, that can explore outside the limited dataset and return a diverse set of feasible reactions by leveraging a feasibility proxy model during the training. We show that RetroGFN achieves competitive results on standard top-k accuracy while outperforming existing methods on round-trip accuracy. Moreover, we provide empirical arguments in favor of using round-trip accuracy, which expands the notion of feasibility with respect to the standard top-k accuracy metric.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 6.1 -->
                
            <!-- Reinforcement Learning: 4.1 -->
                
            <!-- Medicine: 3.6 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Math: 2.1 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Attention: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.9276
            </span>
            <a href="https://arxiv.org/abs/2504.09492" target="_blank" rel="noopener noreferrer">Hybrid Radial Kernels for Solving Weakly Singular Fredholm Integral Equations: Balancing Accuracy and Stability in Meshless Methods</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Davoud Moazami, Mohsen Esmaeilbeigi, Tahereh Akbari | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Over the past few decades, kernel-based approximation methods had achieved astonishing success in solving different problems in the field of science and engineering. However, when employing the direct or standard method of performing computations using infinitely smooth kernels, a conflict arises be</span>
            
            <span class="abstract-full" style="display: none;">Over the past few decades, kernel-based approximation methods had achieved astonishing success in solving different problems in the field of science and engineering. However, when employing the direct or standard method of performing computations using infinitely smooth kernels, a conflict arises between the accuracy that can be theoretically attained and the numerical stability. In other words, when the shape parameter tends to zero, the operational matrix for the standard bases with infinitely smooth kernels become severely ill-conditioned. This conflict can be managed applying hybrid kernels. The hybrid kernels extend the approximation space and provide high flexibility to strike the best possible balance between accuracy and stability. In the current study, an innovative approach using hybrid radial kernels (HRKs) is provided to solve weakly singular Fredholm integral equations (WSFIEs) of the second kind in a meshless scheme. The approach employs hybrid kernels built on dispersed nodes as a basis within the discrete collocation technique. This method transforms the problem being studied into a linear system of algebraic equations. Also, the particle swarm optimization (PSO) algorithm is utilized to calculate the optimal parameters for the hybrid kernels, which is based on minimizing the maximum absolute error (MAE). We also study the error estimate of the suggested scheme. Lastly, we assess the accuracy and validity of the hybrid technique by carrying out various numerical experiments. The numerical findings show that the estimates obtained from hybrid kernels are significantly more accurate in solving WSFIEs compared to pure kernels. Additionally, it was revealed that the hybrid bases remain stable across various values of the shape parameters.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.4 -->
                
            <!-- Math: 4.4 -->
                
            <!-- Medicine: 4.1 -->
                
            <!-- LLMs: 2.6 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Pathfinding: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.9724
            </span>
            <a href="https://arxiv.org/abs/2401.00592" target="_blank" rel="noopener noreferrer">Majority voting is not good for heaven or hell, with mirrored performance</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pavel Chebotarev, Vadim Afonkin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Within the ViSE (Voting in Stochastic Environment) model, we study the effectiveness of majority voting in various environments. By the pit of losses paradox identified in previous work, majority decisions in apparently hostile environments tend to reduce the capital of society. In such cases, the s</span>
            
            <span class="abstract-full" style="display: none;">Within the ViSE (Voting in Stochastic Environment) model, we study the effectiveness of majority voting in various environments. By the pit of losses paradox identified in previous work, majority decisions in apparently hostile environments tend to reduce the capital of society. In such cases, the simple social decision rule of "rejecting all proposals without voting" outperforms majority voting. In this paper, we identify another pit of losses appearing in favorable environments. Here, the simple social decision rule of "accepting all proposals without voting" is superior to majority voting. We prove that under a version of simple majority called symmetrized majority and the antisymmetry of the voting body, the second pit of losses is a mirror image of the pit of losses in hostile environments and explain this phenomenon. Technically, we consider a voting society consisting of individualists whose strategy is supporting all proposals that increase their capital and a group (groups) whose members vote to increase the wealth of their group. According to the main result, the expected capital gain of each agent in the environment whose generator $X$ has mean $\mu>0$ exceeds by $\mu$ their expected capital gain under generator $-X$. This result extends to location families of generators with distributions symmetric about their mean. The mentioned result determines the symmetry of the difference between the expected capital gain under the symmetrized majority and that under the "basic" social decision rule that rejects (resp. accepts) all proposals in unfavorable (resp. favorable) environments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.0 -->
                
            <!-- Math: 3.8 -->
                
            <!-- Medicine: 3.2 -->
                
            <!-- Reinforcement Learning: 2.8 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Pathfinding: 2.2 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Networks: 1.5 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.9824
            </span>
            <a href="https://arxiv.org/abs/2504.09666" target="_blank" rel="noopener noreferrer">Uncertainty Guided Refinement for Fine-Grained Salient Object Detection</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yao Yuan, Pan Gao, Qun Dai, Jie Qin, Wei Xiang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recently, salient object detection (SOD) methods have achieved impressive performance. However, salient regions predicted by existing methods usually contain unsaturated regions and shadows, which limits the model for reliable fine-grained predictions. To address this, we introduce the uncertainty g</span>
            
            <span class="abstract-full" style="display: none;">Recently, salient object detection (SOD) methods have achieved impressive performance. However, salient regions predicted by existing methods usually contain unsaturated regions and shadows, which limits the model for reliable fine-grained predictions. To address this, we introduce the uncertainty guidance learning approach to SOD, intended to enhance the model's perception of uncertain regions. Specifically, we design a novel Uncertainty Guided Refinement Attention Network (UGRAN), which incorporates three important components, i.e., the Multilevel Interaction Attention (MIA) module, the Scale Spatial-Consistent Attention (SSCA) module, and the Uncertainty Refinement Attention (URA) module. Unlike conventional methods dedicated to enhancing features, the proposed MIA facilitates the interaction and perception of multilevel features, leveraging the complementary characteristics among multilevel features. Then, through the proposed SSCA, the salient information across diverse scales within the aggregated features can be integrated more comprehensively and integrally. In the subsequent steps, we utilize the uncertainty map generated from the saliency prediction map to enhance the model's perception capability of uncertain regions, generating a highly-saturated fine-grained saliency prediction map. Additionally, we devise an adaptive dynamic partition (ADP) mechanism to minimize the computational overhead of the URA module and improve the utilization of uncertainty guidance. Experiments on seven benchmark datasets demonstrate the superiority of the proposed UGRAN over the state-of-the-art methodologies. Codes will be released at https://github.com/I2-Multimedia-Lab/UGRAN.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.4 -->
                
            <!-- Medicine: 5.9 -->
                
            <!-- LLMs: 3.3 -->
                
            <!-- Federated Learning: 3.2 -->
                
            <!-- Math: 3.1 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.9845
            </span>
            <a href="https://arxiv.org/abs/2503.16316" target="_blank" rel="noopener noreferrer">On the Cone Effect in the Learning Dynamics</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhanpeng Zhou, Yongyi Yang, Jie Ren, Mahito Sugiyama, Junchi Yan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Understanding the learning dynamics of neural networks is a central topic in the deep learning community. In this paper, we take an empirical perspective to study the learning dynamics of neural networks in real-world settings. Specifically, we investigate the evolution process of the empirical Neur</span>
            
            <span class="abstract-full" style="display: none;">Understanding the learning dynamics of neural networks is a central topic in the deep learning community. In this paper, we take an empirical perspective to study the learning dynamics of neural networks in real-world settings. Specifically, we investigate the evolution process of the empirical Neural Tangent Kernel (eNTK) during training. Our key findings reveal a two-phase learning process: i) in Phase I, the eNTK evolves significantly, signaling the rich regime, and ii) in Phase II, the eNTK keeps evolving but is constrained in a narrow space, a phenomenon we term the cone effect. This two-phase framework builds on the hypothesis proposed by Fort et al. (2020), but we uniquely identify the cone effect in Phase II, demonstrating its significant performance advantages over fully linearized training.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 5.1 -->
                
            <!-- Medicine: 3.2 -->
                
            <!-- Federated Learning: 2.9 -->
                
            <!-- LLMs: 2.6 -->
                
            <!-- Robotics: 2.6 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Pathfinding: 2.0 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.9857
            </span>
            <a href="https://arxiv.org/abs/2504.03162" target="_blank" rel="noopener noreferrer">Beyond Progress Measures: Theoretical Insights into the Mechanism of Grokking</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zihan Gu, Ruoyu Chen, Hua Zhang, Yue Hu, Xiaochun Cao | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Grokking, referring to the abrupt improvement in test accuracy after extended overfitting, offers valuable insights into the mechanisms of model generalization. Existing researches based on progress measures imply that grokking relies on understanding the optimization dynamics when the loss function</span>
            
            <span class="abstract-full" style="display: none;">Grokking, referring to the abrupt improvement in test accuracy after extended overfitting, offers valuable insights into the mechanisms of model generalization. Existing researches based on progress measures imply that grokking relies on understanding the optimization dynamics when the loss function is dominated solely by the weight decay term. However, we find that this optimization merely leads to token uniformity, which is not a sufficient condition for grokking. In this work, we investigate the grokking mechanism underlying the Transformer in the task of prime number operations. Based on theoretical analysis and experimental validation, we present the following insights: (i) The weight decay term encourages uniformity across all tokens in the embedding space when it is minimized. (ii) The occurrence of grokking is jointly determined by the uniformity of the embedding space and the distribution of the training dataset. Building on these insights, we provide a unified perspective for understanding various previously proposed progress measures and introduce a novel, concise, and effective progress measure that could trace the changes in test loss more accurately. Finally, to demonstrate the versatility of our theoretical framework, we design a dedicated dataset to validate our theory on ResNet-18, successfully showcasing the occurrence of grokking. The code is released at https://github.com/Qihuai27/Grokking-Insight.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.3 -->
                
            <!-- Medicine: 4.4 -->
                
            <!-- Reinforcement Learning: 3.9 -->
                
            <!-- Math: 3.2 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.0437
            </span>
            <a href="https://arxiv.org/abs/2402.17097" target="_blank" rel="noopener noreferrer">Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Juyeon Kim, Jeongeun Lee, Yoonho Chang, Chanyeol Choi, Junseong Kim, Jy-yong Sohn | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Mitigating hallucination issues is a key challenge that must be overcome to reliably deploy large language models (LLMs) in real-world scenarios. Recently, various methods have been proposed to detect and revise factual errors in LLM-generated texts, in order to reduce hallucination. In this paper, </span>
            
            <span class="abstract-full" style="display: none;">Mitigating hallucination issues is a key challenge that must be overcome to reliably deploy large language models (LLMs) in real-world scenarios. Recently, various methods have been proposed to detect and revise factual errors in LLM-generated texts, in order to reduce hallucination. In this paper, we propose Re-Ex, a method for post-editing LLM-generated responses. Re-Ex introduces a novel reasoning step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps : first, external tools are used to retrieve the evidences of the factual errors in the initial LLM response; next, LLM is instructed to explain the problematic parts of the response based on the gathered evidence; finally, LLM revises the initial response using the explanations provided in the previous step. In addition to the explanation step, Re-Ex also incorporates new prompting techniques to reduce the token count and inference time required for the response revision process. Compared with existing methods including FacTool, CoVE, and RARR, Re-Ex provides better detection and revision performance with less inference time and fewer tokens in multiple benchmarks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.5 -->
                
            <!-- Medicine: 4.6 -->
                
            <!-- Reinforcement Learning: 3.1 -->
                
            <!-- Robotics: 2.7 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.0565
            </span>
            <a href="https://arxiv.org/abs/2501.07824" target="_blank" rel="noopener noreferrer">Real-time Verification and Refinement of Language Model Text Generation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Joonho Ko, Jinheon Baek, Sung Ju Hwang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their genera</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their generation and further refining them, they are slow in deployment since they are designed to verify the response from LLMs only after their entire generation (from the first to last tokens) is done. Further, we observe that once LLMs generate incorrect tokens early on, there is a higher likelihood that subsequent tokens will also be factually incorrect. To this end, in this work, we propose Streaming-VR (Streaming Verification and Refinement), a novel approach designed to enhance the efficiency of verification and refinement of LLM outputs. Specifically, the proposed Streaming-VR enables on-the-fly verification and correction of tokens as they are being generated, similar to a streaming process, ensuring that each subset of tokens is checked and refined in real-time by another LLM as the LLM constructs its response. Through comprehensive evaluations on multiple datasets, we demonstrate that our approach not only enhances the factual accuracy of LLMs, but also offers a more efficient solution compared to prior refinement methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 57.9%">
                        LLMs
                    </span>
            <!-- Blockchain: 1.9 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- RAG: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Medicine: 1.4 -->
                
            <!-- Quantum Computing: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Reinforcement Learning: 1.1 -->
                
            <!-- Quality Diversity: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.065
            </span>
            <a href="https://arxiv.org/abs/2504.10178" target="_blank" rel="noopener noreferrer">MSCoT: Structured Chain-of-Thought Generation for Multiple Programming Languages</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Naizhu Jin, Zhong Li, Tian Zhang, Qingkai Zeng | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">With the rapid development of code intelligence, the application of multiple programming languages is becoming increasingly widespread. However, most existing code generation models mainly focus on a single or a few programming languages, resulting in unsatisfactory performance in a multilingual env</span>
            
            <span class="abstract-full" style="display: none;">With the rapid development of code intelligence, the application of multiple programming languages is becoming increasingly widespread. However, most existing code generation models mainly focus on a single or a few programming languages, resulting in unsatisfactory performance in a multilingual environment. Chain-of-Thought (CoT) reasoning can significantly improve the performance of the model without the need for retraining or fine-tuning the code generation model by reasonably decomposing complex code generation tasks into multiple subtasks and gradually deriving solutions for each subtask. Nevertheless, the existing CoT generation methods mainly concentrate on Python code, and the performance on other programming languages remains unclear. To fill this gap, we first constructed a CoT generation dataset for 12 programming languages through multi-agent technology. On this basis, we proposed a CoT generation method MSCoT applicable to multiple programming languages. By introducing CoT into the code generation large model, the performance of the code generation large model in a multilingual environment can be improved. Through large-scale empirical research, we compared the generalization abilities of MSCoT and the existing CoT generation methods on multiple programming languages and proved the effectiveness of MSCoT for multiple programming languages. In addition, we also designed a human study to prove the quality of the CoT generated by MSCoT. Finally, we opensourced the model and dataset of MSCoT to promote the research on CoT generation for multiple programming languages.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.7 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Reinforcement Learning: 3.1 -->
                
            <!-- Federated Learning: 2.4 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Quantum Computing: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.0655
            </span>
            <a href="https://arxiv.org/abs/2504.07997" target="_blank" rel="noopener noreferrer">BiasCause: Evaluate Socially Biased Causal Reasoning of Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tian Xie, Tongxin Yin, Vaishakh Keshava, Xueru Zhang, Siddhartha Reddy Jonnalagadda | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">While large language models (LLMs) already play significant roles in society, research has shown that LLMs still generate content including social bias against certain sensitive groups. While existing benchmarks have effectively identified social biases in LLMs, a critical gap remains in our underst</span>
            
            <span class="abstract-full" style="display: none;">While large language models (LLMs) already play significant roles in society, research has shown that LLMs still generate content including social bias against certain sensitive groups. While existing benchmarks have effectively identified social biases in LLMs, a critical gap remains in our understanding of the underlying reasoning that leads to these biased outputs. This paper goes one step further to evaluate the causal reasoning process of LLMs when they answer questions eliciting social biases. We first propose a novel conceptual framework to classify the causal reasoning produced by LLMs. Next, we use LLMs to synthesize $1788$ questions covering $8$ sensitive attributes and manually validate them. The questions can test different kinds of causal reasoning by letting LLMs disclose their reasoning process with causal graphs. We then test 4 state-of-the-art LLMs. All models answer the majority of questions with biased causal reasoning, resulting in a total of $4135$ biased causal graphs. Meanwhile, we discover $3$ strategies for LLMs to avoid biased causal reasoning by analyzing the "bias-free" cases. Finally, we reveal that LLMs are also prone to "mistaken-biased" causal reasoning, where they first confuse correlation with causality to infer specific sensitive group names and then incorporate biased causal reasoning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 67.1%">
                        LLMs
                    </span>
            <!-- RAG: 2.1 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Blockchain: 1.7 -->
                
            <!-- T2I: 1.6 -->
                
            <!-- Quantum Computing: 1.4 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Medicine: 1.2 -->
                
            <!-- Datasets: 1.1 -->
                
            <!-- Quality Diversity: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.073
            </span>
            <a href="https://arxiv.org/abs/2504.08356" target="_blank" rel="noopener noreferrer">An Adaptive Clustering Scheme for Client Selections in Communication-Efficient Federated Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yan-Ann Chen, Guan-Lin Chen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Federated learning is a novel decentralized learning architecture. During the training process, the client and server must continuously upload and receive model parameters, which consumes a lot of network transmission resources. Some methods use clustering to find more representative customers, sele</span>
            
            <span class="abstract-full" style="display: none;">Federated learning is a novel decentralized learning architecture. During the training process, the client and server must continuously upload and receive model parameters, which consumes a lot of network transmission resources. Some methods use clustering to find more representative customers, select only a part of them for training, and at the same time ensure the accuracy of training. However, in federated learning, it is not trivial to know what the number of clusters can bring the best training result. Therefore, we propose to dynamically adjust the number of clusters to find the most ideal grouping results. It may reduce the number of users participating in the training to achieve the effect of reducing communication costs without affecting the model performance. We verify its experimental results on the non-IID handwritten digit recognition dataset and reduce the cost of communication and transmission by almost 50% compared with traditional federated learning without affecting the accuracy of the model.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.1 -->
                
            <!-- Reinforcement Learning: 3.9 -->
                
            <!-- Federated Learning: 3.8 -->
                
            <!-- LLMs: 3.1 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.1202
            </span>
            <a href="https://arxiv.org/abs/2410.10212" target="_blank" rel="noopener noreferrer">Large Language Model-Enhanced Reinforcement Learning for Generic Bus Holding Control Strategies</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiajie Yu, Yuhong Wang, Wei Ma | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Bus holding control is a widely-adopted strategy for maintaining stability and improving the operational efficiency of bus systems. Traditional model-based methods often face challenges with the low accuracy of bus state prediction and passenger demand estimation. In contrast, Reinforcement Learning</span>
            
            <span class="abstract-full" style="display: none;">Bus holding control is a widely-adopted strategy for maintaining stability and improving the operational efficiency of bus systems. Traditional model-based methods often face challenges with the low accuracy of bus state prediction and passenger demand estimation. In contrast, Reinforcement Learning (RL), as a data-driven approach, has demonstrated great potential in formulating bus holding strategies. RL determines the optimal control strategies in order to maximize the cumulative reward, which reflects the overall control goals. However, translating sparse and delayed control goals in real-world tasks into dense and real-time rewards for RL is challenging, normally requiring extensive manual trial-and-error. In view of this, this study introduces an automatic reward generation paradigm by leveraging the in-context learning and reasoning capabilities of Large Language Models (LLMs). This new paradigm, termed the LLM-enhanced RL, comprises several LLM-based modules: reward initializer, reward modifier, performance analyzer, and reward refiner. These modules cooperate to initialize and iteratively improve the reward function according to the feedback from training and test results for the specified RL-based task. Ineffective reward functions generated by the LLM are filtered out to ensure the stable evolution of the RL agents' performance over iterations. To evaluate the feasibility of the proposed LLM-enhanced RL paradigm, it is applied to extensive bus holding control scenarios that vary in the number of bus lines, stops, and passenger demand. The results demonstrate the superiority, generalization capability, and robustness of the proposed paradigm compared to vanilla RL strategies, the LLM-based controller, physics-based feedback controllers, and optimization-based controllers. This study sheds light on the great potential of utilizing LLMs in various smart mobility applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.5 -->
                
            <!-- Medicine: 5.8 -->
                
            <!-- Reinforcement Learning: 5.7 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.1487
            </span>
            <a href="https://arxiv.org/abs/2501.00016" target="_blank" rel="noopener noreferrer">Predicting Crack Nucleation and Propagation in Brittle Materials Using Deep Operator Networks with Diverse Trunk Architectures</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Elham Kiyani (Division of Applied Mathematics, Brown University, Providence, RI, USA), Manav Manav (Department of Mechanical and Process Engineering, ETH Zurich, Zurich, Switzerland), Nikhil Kadivar (School of Engineering, Providence, RI, USA), Laura De Lorenzis (Department of Mechanical and Process Engineering, ETH Zurich, Zurich, Switzerland), George Em Karniadakis (Division of Applied Mathematics, Brown University, Providence, RI, USA) | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Phase-field modeling reformulates fracture problems as energy minimization problems and enables a comprehensive characterization of the fracture process, including crack nucleation, propagation, merging, and branching, without relying on ad-hoc assumptions. However, the numerical solution of phase-f</span>
            
            <span class="abstract-full" style="display: none;">Phase-field modeling reformulates fracture problems as energy minimization problems and enables a comprehensive characterization of the fracture process, including crack nucleation, propagation, merging, and branching, without relying on ad-hoc assumptions. However, the numerical solution of phase-field fracture problems is characterized by a high computational cost. To address this challenge, in this paper, we employ a deep neural operator (DeepONet) consisting of a branch network and a trunk network to solve brittle fracture problems. We explore three distinct approaches that vary in their trunk network configurations. In the first approach, we demonstrate the effectiveness of a two-step DeepONet, which results in a simplification of the learning task. In the second approach, we employ a physics-informed DeepONet, whereby the mathematical expression of the energy is integrated into the trunk network's loss to enforce physical consistency. The integration of physics also results in a substantially smaller data size needed for training. In the third approach, we replace the neural network in the trunk with a Kolmogorov-Arnold Network and train it without the physics loss. Using these methods, we model crack nucleation in a one-dimensional homogeneous bar under prescribed end displacements, as well as crack propagation and branching in single edge-notched specimens with varying notch lengths subjected to tensile and shear loading. We show that the networks predict the solution fields accurately, and the error in the predicted fields is localized near the crack.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.7 -->
                
            <!-- Reinforcement Learning: 3.3 -->
                
            <!-- LLMs: 3.2 -->
                
            <!-- Robotics: 2.8 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Pathfinding: 1.6 -->
                
            <!-- Quantum Computing: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.1651
            </span>
            <a href="https://arxiv.org/abs/2410.10370" target="_blank" rel="noopener noreferrer">Innovative Thinking, Infinite Humor: Humor Research of Large Language Models through Structured Thought Leaps</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Han Wang, Yilin Zhao, Dian Li, Xiaohan Wang, Gang Liu, Xuguang Lan, Hui Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Humor is previously regarded as a gift exclusive to humans for the following reasons. Humor is a culturally nuanced aspect of human language, presenting challenges for its understanding and generation. Humor generation necessitates a multi-hop reasoning process, with each hop founded on proper ratio</span>
            
            <span class="abstract-full" style="display: none;">Humor is previously regarded as a gift exclusive to humans for the following reasons. Humor is a culturally nuanced aspect of human language, presenting challenges for its understanding and generation. Humor generation necessitates a multi-hop reasoning process, with each hop founded on proper rationales. Although many studies, such as those related to GPT-o1, focus on logical reasoning with reflection and correction, they still fall short in humor generation. Due to the sparsity of the knowledge graph in creative thinking, it is arduous to achieve multi-hop reasoning. Consequently, in this paper, we propose a more robust framework for addressing the humor reasoning task, named LoL. LoL aims to inject external information to mitigate the sparsity of the knowledge graph, thereby enabling multi-hop reasoning. In the first stage of LoL, we put forward an automatic instruction-evolution method to incorporate the deeper and broader thinking processes underlying humor. Judgment-oriented instructions are devised to enhance the model's judgment capability, dynamically supplementing and updating the sparse knowledge graph. Subsequently, through reinforcement learning, the reasoning logic for each online-generated response is extracted using GPT-4o. In this process, external knowledge is re-introduced to aid the model in logical reasoning and the learning of human preferences. Finally, experimental results indicate that the combination of these two processes can enhance both the model's judgment ability and its generative capacity. These findings deepen our comprehension of the creative capabilities of large language models (LLMs) and offer approaches to boost LLMs' creative abilities for cross-domain innovative applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 40.5 -->
                
            <!-- Medicine: 2.6 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Quantum Computing: 1.4 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.188
            </span>
            <a href="https://arxiv.org/abs/2504.10071" target="_blank" rel="noopener noreferrer">Pay Attention to What and Where? Interpretable Feature Extractor in Vision-based Deep Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tien Pham, Angelo Cangelosi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Current approaches in Explainable Deep Reinforcement Learning have limitations in which the attention mask has a displacement with the objects in visual input. This work addresses a spatial problem within traditional Convolutional Neural Networks (CNNs). We propose the Interpretable Feature Extracto</span>
            
            <span class="abstract-full" style="display: none;">Current approaches in Explainable Deep Reinforcement Learning have limitations in which the attention mask has a displacement with the objects in visual input. This work addresses a spatial problem within traditional Convolutional Neural Networks (CNNs). We propose the Interpretable Feature Extractor (IFE) architecture, aimed at generating an accurate attention mask to illustrate both "what" and "where" the agent concentrates on in the spatial domain. Our design incorporates a Human-Understandable Encoding module to generate a fully interpretable attention mask, followed by an Agent-Friendly Encoding module to enhance the agent's learning efficiency. These two components together form the Interpretable Feature Extractor for vision-based deep reinforcement learning to enable the model's interpretability. The resulting attention mask is consistent, highly understandable by humans, accurate in spatial dimension, and effectively highlights important objects or locations in visual input. The Interpretable Feature Extractor is integrated into the Fast and Data-efficient Rainbow framework, and evaluated on 57 ATARI games to show the effectiveness of the proposed approach on Spatial Preservation, Interpretability, and Data-efficiency. Finally, we showcase the versatility of our approach by incorporating the IFE into the Asynchronous Advantage Actor-Critic Model.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.4 -->
                
            <!-- Reinforcement Learning: 4.9 -->
                
            <!-- LLMs: 4.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Robotics: 2.3 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- Pathfinding: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2237
            </span>
            <a href="https://arxiv.org/abs/2501.18563" target="_blank" rel="noopener noreferrer">No Equations Needed: Learning System Dynamics Without Relying on Closed-Form ODEs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Krzysztof Kacprzyk, Mihaela van der Schaar | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Data-driven modeling of dynamical systems is a crucial area of machine learning. In many scenarios, a thorough understanding of the model's behavior becomes essential for practical applications. For instance, understanding the behavior of a pharmacokinetic model, constructed as part of drug developm</span>
            
            <span class="abstract-full" style="display: none;">Data-driven modeling of dynamical systems is a crucial area of machine learning. In many scenarios, a thorough understanding of the model's behavior becomes essential for practical applications. For instance, understanding the behavior of a pharmacokinetic model, constructed as part of drug development, may allow us to both verify its biological plausibility (e.g., the drug concentration curve is non-negative and decays to zero) and to design dosing guidelines. Discovery of closed-form ordinary differential equations (ODEs) can be employed to obtain such insights by finding a compact mathematical equation and then analyzing it (a two-step approach). However, its widespread use is currently hindered because the analysis process may be time-consuming, requiring substantial mathematical expertise, or even impossible if the equation is too complex. Moreover, if the found equation's behavior does not satisfy the requirements, editing it or influencing the discovery algorithms to rectify it is challenging as the link between the symbolic form of an ODE and its behavior can be elusive. This paper proposes a conceptual shift to modeling low-dimensional dynamical systems by departing from the traditional two-step modeling process. Instead of first discovering a closed-form equation and then analyzing it, our approach, direct semantic modeling, predicts the semantic representation of the dynamical system (i.e., description of its behavior) directly from data, bypassing the need for complex post-hoc analysis. This direct approach also allows the incorporation of intuitive inductive biases into the optimization algorithm and editing the model's behavior directly, ensuring that the model meets the desired specifications. Our approach not only simplifies the modeling pipeline but also enhances the transparency and flexibility of the resulting models compared to traditional closed-form ODEs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 4.8 -->
                
            <!-- Reinforcement Learning: 4.7 -->
                
            <!-- Medicine: 4.0 -->
                
            <!-- Math: 3.2 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2242
            </span>
            <a href="https://arxiv.org/abs/2504.09157" target="_blank" rel="noopener noreferrer">Dose-finding design based on level set estimation in phase I cancer clinical trials</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Keiichiro Seno, Kota Matsui, Shogo Iwazaki, Yu Inatsu, Shion Takeno, Shigeyuki Matsui | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The primary objective of phase I cancer clinical trials is to evaluate the safety of a new experimental treatment and to find the maximum tolerated dose (MTD). We show that the MTD estimation problem can be regarded as a level set estimation (LSE) problem whose objective is to determine the regions </span>
            
            <span class="abstract-full" style="display: none;">The primary objective of phase I cancer clinical trials is to evaluate the safety of a new experimental treatment and to find the maximum tolerated dose (MTD). We show that the MTD estimation problem can be regarded as a level set estimation (LSE) problem whose objective is to determine the regions where an unknown function value is above or below a given threshold. Then, we propose a novel dose-finding design in the framework of LSE. The proposed design determines the next dose on the basis of an acquisition function incorporating uncertainty in the posterior distribution of the dose-toxicity curve as well as overdose control. Simulation experiments show that the proposed LSE design achieves a higher accuracy in estimating the MTD and involves a lower risk of overdosing allocation compared to existing designs, thereby indicating that it provides an effective methodology for phase I cancer clinical trial design.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.5 -->
                
            <!-- LLMs: 3.9 -->
                
            <!-- Medicine: 3.2 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- Math: 3.0 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2334
            </span>
            <a href="https://arxiv.org/abs/2504.08666" target="_blank" rel="noopener noreferrer">Variability-Driven User-Story Generation using LLM and Triadic Concept Analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alexandre Bazin (Huaxi), Alain Gutierrez (Huaxi), Marianne Huchard (Huaxi), Pierre Martin (Huaxi), Yulin (Huaxi), Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A widely used Agile practice for requirements is to produce a set of user stories (also called ``agile product backlog''), which roughly includes a list of pairs (role, feature), where the role handles the feature for a certain purpose. In the context of Software Product Lines, the requirements for </span>
            
            <span class="abstract-full" style="display: none;">A widely used Agile practice for requirements is to produce a set of user stories (also called ``agile product backlog''), which roughly includes a list of pairs (role, feature), where the role handles the feature for a certain purpose. In the context of Software Product Lines, the requirements for a family of similar systems is thus a family of user-story sets, one per system, leading to a 3-dimensional dataset composed of sets of triples (system, role, feature). In this paper, we combine Triadic Concept Analysis (TCA) and Large Language Model (LLM) prompting to suggest the user-story set required to develop a new system relying on the variability logic of an existing system family. This process consists in 1) computing 3-dimensional variability expressed as a set of TCA implications, 2) providing the designer with intelligible design options, 3) capturing the designer's selection of options, 4) proposing a first user-story set corresponding to this selection, 5) consolidating its validity according to the implications identified in step 1, while completing it if necessary, and 6) leveraging LLM to have a more comprehensive website. This process is evaluated with a dataset comprising the user-story sets of 67 similar-purpose websites.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.9 -->
                
            <!-- LLMs: 4.6 -->
                
            <!-- Networks: 4.0 -->
                
            <!-- Reinforcement Learning: 3.5 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2478
            </span>
            <a href="https://arxiv.org/abs/2504.09970" target="_blank" rel="noopener noreferrer">IsoSEL: Isometric Structural Entropy Learning for Deep Graph Clustering in Hyperbolic Space</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Li Sun, Zhenhao Huang, Yujie Wang, Hongbo Lv, Chunyang Liu, Hao Peng, Philip S. Yu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Graph clustering is a longstanding topic in machine learning. In recent years, deep learning methods have achieved encouraging results, but they still require predefined cluster numbers K, and typically struggle with imbalanced graphs, especially in identifying minority clusters. The limitations mot</span>
            
            <span class="abstract-full" style="display: none;">Graph clustering is a longstanding topic in machine learning. In recent years, deep learning methods have achieved encouraging results, but they still require predefined cluster numbers K, and typically struggle with imbalanced graphs, especially in identifying minority clusters. The limitations motivate us to study a challenging yet practical problem: deep graph clustering without K considering the imbalance in reality. We approach this problem from a fresh perspective of information theory (i.e., structural information). In the literature, structural information has rarely been touched in deep clustering, and the classic definition falls short in its discrete formulation, neglecting node attributes and exhibiting prohibitive complexity. In this paper, we first establish a new Differentiable Structural Information, generalizing the discrete formalism to continuous realm, so that the optimal partitioning tree, revealing the cluster structure, can be created by the gradient backpropagation. Theoretically, we demonstrate its capability in clustering without requiring K and identifying the minority clusters in imbalanced graphs, while reducing the time complexity to O(N) w.r.t. the number of nodes. Subsequently, we present a novel IsoSEL framework for deep graph clustering, where we design a hyperbolic neural network to learn the partitioning tree in the Lorentz model of hyperbolic space, and further conduct Lorentz Tree Contrastive Learning with isometric augmentation. As a result, the partitioning tree incorporates node attributes via mutual information maximization, while the cluster assignment is refined by the proposed tree contrastive learning. Extensive experiments on five benchmark datasets show the IsoSEL outperforms 14 recent baselines by an average of +1.3% in NMI.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.3 -->
                
            <!-- Reinforcement Learning: 3.8 -->
                
            <!-- LLMs: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Federated Learning: 2.2 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Pathfinding: 2.2 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2488
            </span>
            <a href="https://arxiv.org/abs/2405.15172" target="_blank" rel="noopener noreferrer">Learning the Distribution Map in Reverse Causal Performative Prediction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Daniele Bracale, Subha Maity, Moulinath Banerjee, Yuekai Sun | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In numerous predictive scenarios, the predictive model affects the sampling distribution; for example, job applicants often meticulously craft their resumes to navigate through a screening systems. Such shifts in distribution are particularly prevalent in the realm of social computing, yet, the stra</span>
            
            <span class="abstract-full" style="display: none;">In numerous predictive scenarios, the predictive model affects the sampling distribution; for example, job applicants often meticulously craft their resumes to navigate through a screening systems. Such shifts in distribution are particularly prevalent in the realm of social computing, yet, the strategies to learn these shifts from data remain remarkably limited. Inspired by a microeconomic model that adeptly characterizes agents' behavior within labor markets, we introduce a novel approach to learn the distribution shift. Our method is predicated on a reverse causal model, wherein the predictive model instigates a distribution shift exclusively through a finite set of agents' actions. Within this framework, we employ a microfoundation model for the agents' actions and develop a statistically justified methodology to learn the distribution shift map, which we demonstrate to be effective in minimizing the performative prediction risk.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 5.5 -->
                
            <!-- Reinforcement Learning: 4.9 -->
                
            <!-- Medicine: 3.8 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- GNN: 2.4 -->
                
            <!-- Federated Learning: 2.2 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2595
            </span>
            <a href="https://arxiv.org/abs/2504.08198" target="_blank" rel="noopener noreferrer">The More is not the Merrier: Investigating the Effect of Client Size on Federated Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Eleanor Wallach, Sage Siler, Jing Deng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Federated Learning (FL) has been introduced as a way to keep data local to clients while training a shared machine learning model, as clients train on their local data and send trained models to a central aggregator. It is expected that FL will have a huge implication on Mobile Edge Computing, the I</span>
            
            <span class="abstract-full" style="display: none;">Federated Learning (FL) has been introduced as a way to keep data local to clients while training a shared machine learning model, as clients train on their local data and send trained models to a central aggregator. It is expected that FL will have a huge implication on Mobile Edge Computing, the Internet of Things, and Cross-Silo FL. In this paper, we focus on the widely used FedAvg algorithm to explore the effect of the number of clients in FL. We find a significant deterioration of learning accuracy for FedAvg as the number of clients increases. To address this issue for a general application, we propose a method called Knowledgeable Client Insertion (KCI) that introduces a very small number of knowledgeable clients to the MEC setting. These knowledgeable clients are expected to have accumulated a large set of data samples to help with training. With the help of KCI, the learning accuracy of FL increases much faster even with a normal FedAvg aggregation technique. We expect this approach to be able to provide great privacy protection for clients against security attacks such as model inversion attacks. Our code is available at https://github.com/Eleanor-W/KCI_for_FL.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.8 -->
                
            <!-- Medicine: 3.7 -->
                
            <!-- Federated Learning: 3.1 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- LLMs: 2.7 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- SpikingNN: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2697
            </span>
            <a href="https://arxiv.org/abs/2501.03933" target="_blank" rel="noopener noreferrer">Data-driven Optimization for the Evolve-Filter-Relax regularization of convection-dominated flows</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anna Ivagnes, Maria Strazzullo, Michele Girfoglio, Traian Iliescu, Gianluigi Rozza | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Numerical stabilization techniques are often employed in under-resolved simulations of convection-dominated flows to improve accuracy and mitigate spurious oscillations. Specifically, the evolve--filter--relax (EFR) algorithm is a framework which consists in evolving the solution, applying a filteri</span>
            
            <span class="abstract-full" style="display: none;">Numerical stabilization techniques are often employed in under-resolved simulations of convection-dominated flows to improve accuracy and mitigate spurious oscillations. Specifically, the evolve--filter--relax (EFR) algorithm is a framework which consists in evolving the solution, applying a filtering step to remove high-frequency noise, and relaxing through a convex combination of filtered and original solutions. The stability and accuracy of the EFR solution strongly depend on two parameters, the filter radius $\delta$ and the relaxation parameter $\chi$. Standard choices for these parameters are usually fixed in time, and related to the full order model setting, i.e., the grid size for $\delta$ and the time step for $\chi$. The key novelties with respect to the standard EFR approach are: (i) time-dependent parameters $\delta(t)$ and $\chi(t)$, and (ii) data-driven adaptive optimization of the parameters in time, considering a fully-resolved simulation as reference. In particular, we propose three different classes of optimized-EFR (Opt-EFR) strategies, aiming to optimize one or both parameters. The new Opt-EFR strategies are tested in the under-resolved simulation of a turbulent flow past a cylinder at $Re=1000$. The Opt-EFR proved to be more accurate than standard approaches by up to 99$\%$, while maintaining a similar computational time. In particular, the key new finding of our analysis is that such accuracy can be obtained only if the optimized objective function includes: (i) a global metric (as the kinetic energy), and (ii) spatial gradients' information.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.5 -->
                
            <!-- LLMs: 3.8 -->
                
            <!-- Reinforcement Learning: 3.2 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Robotics: 2.3 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2732
            </span>
            <a href="https://arxiv.org/abs/2504.09414" target="_blank" rel="noopener noreferrer">Appointed-Time Fault-Tolerant Control for Flexible Hypersonic Vehicles with Unmeasurable States Independent of Initial Errors</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tianlong Zhao, Fei Hao | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This article aims to derive a practical tracking control algorithm for flexible air-breathing hypersonic vehicles (FAHVs) with lumped disturbances, unmeasurable states and actuator failures. Based on the framework of the backstepping technique, an appointed-time fault-tolerant protocol independent o</span>
            
            <span class="abstract-full" style="display: none;">This article aims to derive a practical tracking control algorithm for flexible air-breathing hypersonic vehicles (FAHVs) with lumped disturbances, unmeasurable states and actuator failures. Based on the framework of the backstepping technique, an appointed-time fault-tolerant protocol independent of initial errors is proposed. Firstly, a new type of a state observer is constructed to reconstruct the unmeasurable states. Then, an error transformation function is designed to achieve prescribed performance control that does not depend on the initial tracking error. To deal with the actuator failures, practical fixed-time neural network observers are established to provide the estimation of the lumped disturbances. Finally, the proposed control strategy can ensure the practical fixed-time convergence of the closed-loop system, thereby greatly enhancing the transient performance. The proposed method addresses the challenges of ensuring real-time measurement accuracy for angle of attack and flight path angle in hypersonic vehicles, coupled with potential sudden actuator failures, effectively overcoming the drawback of prescribed performance control that requires knowledge of initial tracking errors. Some simulation results are provided to demonstrate the feasibility and the effectiveness of the proposed strategy</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 5.8 -->
                
            <!-- Medicine: 5.4 -->
                
            <!-- Math: 4.3 -->
                
            <!-- LLMs: 4.2 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2766
            </span>
            <a href="https://arxiv.org/abs/2409.19777" target="_blank" rel="noopener noreferrer">Automatic debiasing of neural networks via moment-constrained learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Christian L. Hines, Oliver J. Hines | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Causal and nonparametric estimands in economics and biostatistics can often be viewed as the mean of a linear functional applied to an unknown outcome regression function. Naively learning the regression function and taking a sample mean of the target functional results in biased estimators, and a r</span>
            
            <span class="abstract-full" style="display: none;">Causal and nonparametric estimands in economics and biostatistics can often be viewed as the mean of a linear functional applied to an unknown outcome regression function. Naively learning the regression function and taking a sample mean of the target functional results in biased estimators, and a rich debiasing literature has developed where one additionally learns the so-called Riesz representer (RR) of the target estimand (targeted learning, double ML, automatic debiasing etc.). Learning the RR via its derived functional form can be challenging, e.g. due to extreme inverse probability weights or the need to learn conditional density functions. Such challenges have motivated recent advances in automatic debiasing (AD), where the RR is learned directly via minimization of a bespoke loss. We propose moment-constrained learning as a new RR learning approach that addresses some shortcomings in AD, constraining the predicted moments and improving the robustness of RR estimates to optimization hyperparamters. Though our approach is not tied to a particular class of learner, we illustrate it using neural networks, and evaluate on the problems of average treatment/derivative effect estimation using semi-synthetic data. Our numerical experiments show improved performance versus state of the art benchmarks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 4.9 -->
                
            <!-- Reinforcement Learning: 4.8 -->
                
            <!-- Medicine: 4.6 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2826
            </span>
            <a href="https://arxiv.org/abs/2504.10437" target="_blank" rel="noopener noreferrer">Model Order Reduction of Linear Systems via $(\gamma,\delta)$-Similarity</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shivam Bajaj, Carolyn L. Beck, Vijay Gupta | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Model order reduction aims to determine a low-order approximation of high-order models with least possible approximation errors. For application to physical systems, it is crucial that the reduced order model (ROM) is robust to any disturbance that acts on the full order model (FOM) -- in the sense </span>
            
            <span class="abstract-full" style="display: none;">Model order reduction aims to determine a low-order approximation of high-order models with least possible approximation errors. For application to physical systems, it is crucial that the reduced order model (ROM) is robust to any disturbance that acts on the full order model (FOM) -- in the sense that the output of the ROM remains a good approximation of that of the FOM, even in the presence of such disturbances. In this work, we present a framework for model order reduction for a class of continuous-time linear systems that ensures this property for any $L_2$ disturbance. Apart from robustness to disturbances in this sense, the proposed framework also displays other desirable properties for model order reduction: (1) a provable bound on the error defined as the $L_2$ norm of the difference between the output of the ROM and FOM, (2) preservation of stability, (3) compositionality properties and a provable error bound for arbitrary interconnected systems, (4) a provable bound on the output of the FOM when the controller designed for the ROM is used with the FOM, and finally, (5) compatibility with existing approaches such as balanced truncation and moment matching. Property (4) does not require computation of any gap metric and property (5) is beneficial as existing approaches can also be equipped with some of the preceding properties. The theoretical results are corroborated on numerical case studies, including on a building model.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.6 -->
                
            <!-- Reinforcement Learning: 4.7 -->
                
            <!-- Math: 3.6 -->
                
            <!-- LLMs: 2.6 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2848
            </span>
            <a href="https://arxiv.org/abs/2504.06280" target="_blank" rel="noopener noreferrer">Different Paths, Same Destination: Designing New Physics-Inspired Dynamical Systems with Engineered Stability to Minimize the Ising Hamiltonian</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: E. M. H. E. B. Ekanayake, N. Shukla | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Oscillator Ising machines (OIMs) represent an exemplar case of using physics-inspired non-linear dynamical systems to solve computationally challenging combinatorial optimization problems (COPs). The computational performance of such systems is highly sensitive to the underlying dynamical properties</span>
            
            <span class="abstract-full" style="display: none;">Oscillator Ising machines (OIMs) represent an exemplar case of using physics-inspired non-linear dynamical systems to solve computationally challenging combinatorial optimization problems (COPs). The computational performance of such systems is highly sensitive to the underlying dynamical properties, the topology of the input graph, and their relative compatibility. In this work, we explore the concept of designing different dynamical systems that minimize the same objective function but exhibit drastically different dynamical properties. Our goal is to leverage this diversification in dynamics to reduce the sensitivity of the computational performance to the underlying graph, and subsequently, enhance the overall effectiveness of such physics-based computational methods. To this end, we introduce a novel dynamical system, the Dynamical Ising Machine (DIM), which, like the OIM, minimizes the Ising Hamiltonian but offers significantly different dynamical properties. We analyze the characteristic properties of the DIM and compare them with those of the OIM. We also show that the relative performance of each model is dependent on the input graph. Our work illustrates that using multiple dynamical systems with varying properties to solve the same COP enables an effective method that is less sensitive to the input graph, while producing robust solutions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 5.7 -->
                
            <!-- Reinforcement Learning: 5.7 -->
                
            <!-- Math: 5.0 -->
                
            <!-- Medicine: 3.2 -->
                
            <!-- Federated Learning: 2.8 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2872
            </span>
            <a href="https://arxiv.org/abs/2503.09017" target="_blank" rel="noopener noreferrer">Accurate Control under Voltage Drop for Rotor Drones</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yuhang Liu, Jindou Jia, Zihan Yang, Kexin Guo | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This letter proposes an anti-disturbance control scheme for rotor drones to counteract voltage drop (VD) disturbance caused by voltage drop of the battery, which is a common case for long-time flight or aggressive maneuvers. Firstly, the refined dynamics of rotor drones considering VD disturbance ar</span>
            
            <span class="abstract-full" style="display: none;">This letter proposes an anti-disturbance control scheme for rotor drones to counteract voltage drop (VD) disturbance caused by voltage drop of the battery, which is a common case for long-time flight or aggressive maneuvers. Firstly, the refined dynamics of rotor drones considering VD disturbance are presented. Based on the dynamics, a voltage drop observer (VDO) is developed to accurately estimate the VD disturbance by decoupling the disturbance and state information of the drone, reducing the conservativeness of conventional disturbance observers. Subsequently, the control scheme integrates the VDO within the translational loop and a fixed-time sliding mode observer (SMO) within the rotational loop, enabling it to address force and torque disturbances caused by voltage drop of the battery. Sufficient real flight experiments are conducted to demonstrate the effectiveness of the proposed control scheme under VD disturbance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.7 -->
                
            <!-- Reinforcement Learning: 4.4 -->
                
            <!-- LLMs: 3.6 -->
                
            <!-- Math: 3.3 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2959
            </span>
            <a href="https://arxiv.org/abs/2504.05963" target="_blank" rel="noopener noreferrer">Learning Verified Monitors for Hidden Markov Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Luko van der Maas, Sebastian Junges | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Runtime monitors assess whether a system is in an unsafe state based on a stream of observations. We study the problem where the system is subject to probabilistic uncertainty and described by a hidden Markov model. A stream of observations is then unsafe if the probability of being in an unsafe sta</span>
            
            <span class="abstract-full" style="display: none;">Runtime monitors assess whether a system is in an unsafe state based on a stream of observations. We study the problem where the system is subject to probabilistic uncertainty and described by a hidden Markov model. A stream of observations is then unsafe if the probability of being in an unsafe state is above a threshold. A correct monitor recognizes the set of unsafe observations. The key contribution of this paper is the first correct-by-construction synthesis method for such monitors, represented as finite automata. The contribution combines four ingredients: First, we establish the coNP-hardness of checking whether an automaton is a correct monitor, i.e., a monitor without misclassifications. Second, we provide a reduction that reformulates the search for misclassifications into a standard probabilistic system synthesis problem. Third, we integrate the verification routine into an active automata learning routine to synthesize correct monitors. Fourth, we provide a prototypical implementation that shows the feasibility and limitations of the approach on a series of benchmarks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.0 -->
                
            <!-- Reinforcement Learning: 4.0 -->
                
            <!-- LLMs: 3.2 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2969
            </span>
            <a href="https://arxiv.org/abs/2504.09246" target="_blank" rel="noopener noreferrer">Type-Constrained Code Generation with Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Niels M\"undler, Jingxuan He, Hao Wang, Koushik Sen, Dawn Song, Martin Vechev | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) have achieved notable success in code generation. However, they still frequently produce uncompilable output because their next-token inference procedure does not model formal aspects of code. Although constrained decoding is a promising approach to alleviate this issue,</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) have achieved notable success in code generation. However, they still frequently produce uncompilable output because their next-token inference procedure does not model formal aspects of code. Although constrained decoding is a promising approach to alleviate this issue, it has only been applied to handle either domain-specific languages or syntactic language features. This leaves typing errors, which are beyond the domain of syntax and generally hard to adequately constrain. To address this challenge, we introduce a type-constrained decoding approach that leverages type systems to guide code generation. We develop novel prefix automata for this purpose and introduce a sound approach to enforce well-typedness based on type inference and a search over inhabitable types. We formalize our approach on a simply-typed language and extend it to TypeScript to demonstrate practicality. Our evaluation on HumanEval shows that our approach reduces compilation errors by more than half and increases functional correctness in code synthesis, translation, and repair tasks across LLMs of various sizes and model families, including SOTA open-weight models with more than 30B parameters.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 50.3%">
                        LLMs
                    </span>
            <!-- RAG: 3.1 -->
                
            <!-- 3D: 2.4 -->
                
            <!-- Medicine: 2.1 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- T2I: 1.8 -->
                
            <!-- HPO and AutoML: 1.3 -->
                
            <!-- Quantum Computing: 1.1 -->
                
            <!-- Quality Diversity: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2985
            </span>
            <a href="https://arxiv.org/abs/2504.09472" target="_blank" rel="noopener noreferrer">CamMimic: Zero-Shot Image To Camera Motion Personalized Video Generation Using Diffusion Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pooja Guhan, Divya Kothandaraman, Tsung-Wei Huang, Guan-Ming Su, Dinesh Manocha | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We introduce CamMimic, an innovative algorithm tailored for dynamic video editing needs. It is designed to seamlessly transfer the camera motion observed in a given reference video onto any scene of the user's choice in a zero-shot manner without requiring any additional data. Our algorithm achieves</span>
            
            <span class="abstract-full" style="display: none;">We introduce CamMimic, an innovative algorithm tailored for dynamic video editing needs. It is designed to seamlessly transfer the camera motion observed in a given reference video onto any scene of the user's choice in a zero-shot manner without requiring any additional data. Our algorithm achieves this using a two-phase strategy by leveraging a text-to-video diffusion model. In the first phase, we develop a multi-concept learning method using a combination of LoRA layers and an orthogonality loss to capture and understand the underlying spatial-temporal characteristics of the reference video as well as the spatial features of the user's desired scene. The second phase proposes a unique homography-based refinement strategy to enhance the temporal and spatial alignment of the generated video. We demonstrate the efficacy of our method through experiments conducted on a dataset containing combinations of diverse scenes and reference videos containing a variety of camera motions. In the absence of an established metric for assessing camera motion transfer between unrelated scenes, we propose CameraScore, a novel metric that utilizes homography representations to measure camera motion similarity between the reference and generated videos. Extensive quantitative and qualitative evaluations demonstrate that our approach generates high-quality, motion-enhanced videos. Additionally, a user study reveals that 70.31% of participants preferred our method for scene preservation, while 90.45% favored it for motion transfer. We hope this work lays the foundation for future advancements in camera motion transfer across different scenes.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.7 -->
                
            <!-- LLMs: 5.8 -->
                
            <!-- Reinforcement Learning: 4.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Math: 1.5 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.3036
            </span>
            <a href="https://arxiv.org/abs/2504.09248" target="_blank" rel="noopener noreferrer">Asymptotic stabilization under homomorphic encryption: A re-encryption free method</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shuai Feng, Qian Ma, Junsoo Kim, Shengyuan Xu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we propose methods to encrypted a pre-given dynamic controller with homomorphic encryption, without re-encrypting the control inputs. We first present a preliminary result showing that the coefficients in a pre-given dynamic controller can be scaled up into integers by the zooming-in </span>
            
            <span class="abstract-full" style="display: none;">In this paper, we propose methods to encrypted a pre-given dynamic controller with homomorphic encryption, without re-encrypting the control inputs. We first present a preliminary result showing that the coefficients in a pre-given dynamic controller can be scaled up into integers by the zooming-in factor in dynamic quantization, without utilizing re-encryption. However, a sufficiently small zooming-in factor may not always exist because it requires that the convergence speed of the pre-given closed-loop system should be sufficiently fast. Then, as the main result, we design a new controller approximating the pre-given dynamic controller, in which the zooming-in factor is decoupled from the convergence rate of the pre-given closed-loop system. Therefore, there always exist a (sufficiently small) zooming-in factor of dynamic quantization scaling up all the controller's coefficients to integers, and a finite modulus preventing overflow in cryptosystems. The process is asymptotically stable and the quantizer is not saturated.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.7 -->
                
            <!-- LLMs: 4.4 -->
                
            <!-- Medicine: 3.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Robotics: 2.4 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.3159
            </span>
            <a href="https://arxiv.org/abs/2212.09519" target="_blank" rel="noopener noreferrer">Fuzzing: On Benchmarking Outcome as a Function of Benchmark Properties</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dylan Wolff, Marcel B\"ohme, Abhik Roychoudhury | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Characteristics of a benchmarking setup clearly can have some impact on the benchmark outcome. In this paper, we explore two methodologies to quantify the impact of the specific properties on the benchmarking outcome. Our first methodology is the controlled experiment to identify a causal relationsh</span>
            
            <span class="abstract-full" style="display: none;">Characteristics of a benchmarking setup clearly can have some impact on the benchmark outcome. In this paper, we explore two methodologies to quantify the impact of the specific properties on the benchmarking outcome. Our first methodology is the controlled experiment to identify a causal relationship between a single property in isolation and the benchmarking outcome. However, manipulating one property exactly may not always be practical or possible. Hence, our second methodology is randomization and non-parametric regression to identify the strength of the relationship between arbitrary benchmark properties (i.e., covariates) and outcome. Together, these two fundamental aspects of experimental design, control and randomization, can provide a comprehensive picture of the impact of various properties of the current benchmark on the fuzzer ranking. These analyses can be used to guide fuzzer developers towards areas of improvement in their tools and allow researchers to make more nuanced claims about fuzzer effectiveness. We instantiate each approach on a subset of properties suspected of impacting the relative effectiveness of fuzzers and quantify the effects of these properties on the evaluation outcome. In doing so, we identify multiple novel properties which can have statistically significant effect on the relative effectiveness of fuzzers.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 5.6 -->
                
            <!-- Reinforcement Learning: 4.2 -->
                
            <!-- Medicine: 4.1 -->
                
            <!-- Math: 3.0 -->
                
            <!-- Robotics: 2.6 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.3227
            </span>
            <a href="https://arxiv.org/abs/2006.16505" target="_blank" rel="noopener noreferrer">Delay Violation Probability and Effective Rate of Downlink NOMA over $\alpha$-$\mu$ Fading Channels</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vaibhav Kumar, Barry Cardiff, Shankar Prakriya, Mark F. Flanagan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Non-orthogonal multiple access (NOMA) is a potential candidate to further enhance the spectrum utilization efficiency in beyond fifth-generation (B5G) standards. However, there has been little attention on the quantification of the delay-limited performance of downlink NOMA systems. In this paper, w</span>
            
            <span class="abstract-full" style="display: none;">Non-orthogonal multiple access (NOMA) is a potential candidate to further enhance the spectrum utilization efficiency in beyond fifth-generation (B5G) standards. However, there has been little attention on the quantification of the delay-limited performance of downlink NOMA systems. In this paper, we analyze the performance of a two-user downlink NOMA system over generalized {\alpha}-{\mu} fading in terms of delay violation probability (DVP) and effective rate (ER). In particular, we derive an analytical expression for an upper bound on the DVP and we derive the exact sum ER of the downlink NOMA system. We also derive analytical expressions for high and low signal-to-noise ratio (SNR) approximations to the sum ER, as well as a fundamental upper bound on the sum ER which represents the ergodic sum-rate for the downlink NOMA system. We also analyze the sum ER of a corresponding time-division-multiplexed orthogonal multiple access (OMA) system. Our results show that while NOMA consistently outperforms OMA over the practical SNR range, the relative gain becomes smaller in more severe fading conditions, and is also smaller in the presence a more strict delay quality-of-service (QoS) constraint.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.5 -->
                
            <!-- LLMs: 3.8 -->
                
            <!-- Reinforcement Learning: 3.7 -->
                
            <!-- Math: 3.2 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.3291
            </span>
            <a href="https://arxiv.org/abs/2503.20299" target="_blank" rel="noopener noreferrer">Finding Near-Optimal Maximum Set of Disjoint $k$-Cliques in Real-World Social Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wenqing Lin, Xin Chen, Haoxuan Xie, Sibo Wang, Siqiang Luo | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A $k$-clique is a dense graph, consisting of $k$ fully-connected nodes, that finds numerous applications, such as community detection and network analysis. In this paper, we study a new problem, that finds a maximum set of disjoint $k$-cliques in a given large real-world graph with a user-defined fi</span>
            
            <span class="abstract-full" style="display: none;">A $k$-clique is a dense graph, consisting of $k$ fully-connected nodes, that finds numerous applications, such as community detection and network analysis. In this paper, we study a new problem, that finds a maximum set of disjoint $k$-cliques in a given large real-world graph with a user-defined fixed number $k$, which can contribute to a good performance of teaming collaborative events in online games. However, this problem is NP-hard when $k \geq 3$, making it difficult to solve. To address that, we propose an efficient lightweight method that avoids significant overheads and achieves a $k$-approximation to the optimal, which is equipped with several optimization techniques, including the ordering method, degree estimation in the clique graph, and a lightweight implementation. Besides, to handle dynamic graphs that are widely seen in real-world social networks, we devise an efficient indexing method with careful swapping operations, leading to the efficient maintenance of a near-optimal result with frequent updates in the graph. In various experiments on several large graphs, our proposed approaches significantly outperform the competitors by up to 2 orders of magnitude in running time and 13.3\% in the number of computed disjoint $k$-cliques, which demonstrates the superiority of the proposed approaches in terms of efficiency and effectiveness.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.1 -->
                
            <!-- Reinforcement Learning: 3.2 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- Medicine: 2.9 -->
                
            <!-- GNN: 2.6 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Pathfinding: 1.7 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.3388
            </span>
            <a href="https://arxiv.org/abs/2503.15854" target="_blank" rel="noopener noreferrer">Persistent Stiefel-Whitney Classes of Tangent Bundles</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dongwoo Gang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Stiefel-Whitney classes are invariants of the tangent bundle of a smooth manifold, represented as cohomology classes of the base manifold. These classes are essential in obstruction theory, embedding problems, and cobordism theory. In this work, we first reestablish an appropriate notion of vector b</span>
            
            <span class="abstract-full" style="display: none;">Stiefel-Whitney classes are invariants of the tangent bundle of a smooth manifold, represented as cohomology classes of the base manifold. These classes are essential in obstruction theory, embedding problems, and cobordism theory. In this work, we first reestablish an appropriate notion of vector bundles in a persistent setting, allowing characteristic classes to be interpreted through topological data analysis. Next, we propose a concrete algorithm to compute persistent cohomology classes that represent the Stiefel-Whitney classes of the tangent bundle of a smooth manifold. Given a point cloud, we construct a \v{C}ech or alpha filtration. By applying the Wu formula in this setting, we derive a sequence of persistent cohomology classes from the filtration. We show that if the filtration is homotopy equivalent to a smooth manifold, then one of these persistent cohomology classes corresponds to the $k$-th Stiefel-Whitney class of the tangent bundle of that manifold. To demonstrate the effectiveness of our approach, we present experiments on real-world datasets, including applications to complex manifolds, image patches, and molecular conformation space.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 6.8 -->
                
            <!-- Reinforcement Learning: 4.6 -->
                
            <!-- Medicine: 2.9 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Robotics: 2.3 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.3542
            </span>
            <a href="https://arxiv.org/abs/2011.08159" target="_blank" rel="noopener noreferrer">On the performance of downlink NOMA in underlay spectrum sharing</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vaibhav Kumar, Zhiguo Ding, Mark F. Flanagan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Non-orthogonal multiple access (NOMA) and spectrum sharing are two potential technologies for providing massive connectivity in beyond fifth-generation (B5G) networks. In this paper, we present the performance analysis of a multi-antenna-assisted two-user downlink NOMA system in an underlay spectrum</span>
            
            <span class="abstract-full" style="display: none;">Non-orthogonal multiple access (NOMA) and spectrum sharing are two potential technologies for providing massive connectivity in beyond fifth-generation (B5G) networks. In this paper, we present the performance analysis of a multi-antenna-assisted two-user downlink NOMA system in an underlay spectrum sharing system. We derive closed-form expressions for the average achievable sum-rate and outage probability of the secondary network under a peak interference constraint and/or peak power constraint, depending on the availability of channel state information (CSI) of the interference link between secondary transmitter (ST) and primary receiver (PR). For the case where the ST has a fixed power budget, we show that performance can be divided into two specific regimes, where either the interference constraint or the power constraint primarily dictates the performance. Our results confirm that the NOMA-based underlay spectrum sharing system significantly outperforms its orthogonal multiple access (OMA) based counterpart, by achieving higher average sum-rate and lower outage probability. We also show the effect of information loss at the ST in terms of CSI of the link between the ST and PR on the system performance. Moreover, we also present closed-form expressions for the optimal power allocation coefficient that minimizes the outage probability of the NOMA system for the special case where the secondary users are each equipped with a single antenna. A close agreement between the simulation and analytical results confirms the correctness of the presented analysis.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 5.1 -->
                
            <!-- Medicine: 4.9 -->
                
            <!-- Reinforcement Learning: 4.3 -->
                
            <!-- LLMs: 2.3 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Pathfinding: 1.7 -->
                
            <!-- Networks: 1.7 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.361
            </span>
            <a href="https://arxiv.org/abs/2203.06663" target="_blank" rel="noopener noreferrer">Global2Local: A Joint-Hierarchical Attention for Video Captioning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chengpeng Dai, Fuhai Chen, Xiaoshuai Sun, Rongrong Ji, Qixiang Ye, Yongjian Wu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recently, automatic video captioning has attracted increasing attention, where the core challenge lies in capturing the key semantic items, like objects and actions as well as their spatial-temporal correlations from the redundant frames and semantic content. To this end, existing works select eithe</span>
            
            <span class="abstract-full" style="display: none;">Recently, automatic video captioning has attracted increasing attention, where the core challenge lies in capturing the key semantic items, like objects and actions as well as their spatial-temporal correlations from the redundant frames and semantic content. To this end, existing works select either the key video clips in a global level~(across multi frames), or key regions within each frame, which, however, neglect the hierarchical order, i.e., key frames first and key regions latter. In this paper, we propose a novel joint-hierarchical attention model for video captioning, which embeds the key clips, the key frames and the key regions jointly into the captioning model in a hierarchical manner. Such a joint-hierarchical attention model first conducts a global selection to identify key frames, followed by a Gumbel sampling operation to identify further key regions based on the key frames, achieving an accurate global-to-local feature representation to guide the captioning. Extensive quantitative evaluations on two public benchmark datasets MSVD and MSR-VTT demonstrates the superiority of the proposed method over the state-of-the-art methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.2 -->
                
            <!-- Reinforcement Learning: 4.5 -->
                
            <!-- LLMs: 3.7 -->
                
            <!-- Federated Learning: 2.6 -->
                
            <!-- Robotics: 2.4 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Quantum Computing: 2.0 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.3666
            </span>
            <a href="https://arxiv.org/abs/2410.05837" target="_blank" rel="noopener noreferrer">A noise-corrected Langevin algorithm and sampling by half-denoising</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Aapo Hyv\"arinen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The Langevin algorithm is a classic method for sampling from a given pdf in a real space. In its basic version, it only requires knowledge of the gradient of the log-density, also called the score function. However, in deep learning, it is often easier to learn the so-called "noisy-data score functi</span>
            
            <span class="abstract-full" style="display: none;">The Langevin algorithm is a classic method for sampling from a given pdf in a real space. In its basic version, it only requires knowledge of the gradient of the log-density, also called the score function. However, in deep learning, it is often easier to learn the so-called "noisy-data score function", i.e. the gradient of the log-density of noisy data, more precisely when Gaussian noise is added to the data. Such an estimate is biased and complicates the use of the Langevin method. Here, we propose a noise-corrected version of the Langevin algorithm, where the bias due to noisy data is removed, at least regarding first-order terms. Unlike diffusion models, our algorithm needs to know the noisy score function for one single noise level only. We further propose a simple special case which has an interesting intuitive interpretation of iteratively adding noise the data and then attempting to remove half of that noise.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.9 -->
                
            <!-- LLMs: 3.9 -->
                
            <!-- Reinforcement Learning: 3.7 -->
                
            <!-- Math: 2.8 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.3753
            </span>
            <a href="https://arxiv.org/abs/2409.19075" target="_blank" rel="noopener noreferrer">Meta-RTL: Reinforcement-Based Meta-Transfer Learning for Low-Resource Commonsense Reasoning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yu Fu, Jie He, Yifan Yang, Qun Liu, Deyi Xiong | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Meta learning has been widely used to exploit rich-resource source tasks to improve the performance of low-resource target tasks. Unfortunately, most existing meta learning approaches treat different source tasks equally, ignoring the relatedness of source tasks to the target task in knowledge trans</span>
            
            <span class="abstract-full" style="display: none;">Meta learning has been widely used to exploit rich-resource source tasks to improve the performance of low-resource target tasks. Unfortunately, most existing meta learning approaches treat different source tasks equally, ignoring the relatedness of source tasks to the target task in knowledge transfer. To mitigate this issue, we propose a reinforcement-based multi-source meta-transfer learning framework (Meta-RTL) for low-resource commonsense reasoning. In this framework, we present a reinforcement-based approach to dynamically estimating source task weights that measure the contribution of the corresponding tasks to the target task in the meta-transfer learning. The differences between the general loss of the meta model and task-specific losses of source-specific temporal meta models on sampled target data are fed into the policy network of the reinforcement learning module as rewards. The policy network is built upon LSTMs that capture long-term dependencies on source task weight estimation across meta learning iterations. We evaluate the proposed Meta-RTL using both BERT and ALBERT as the backbone of the meta model on three commonsense reasoning benchmark datasets. Experimental results demonstrate that Meta-RTL substantially outperforms strong baselines and previous task selection strategies and achieves larger improvements on extremely low-resource settings.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 7.5 -->
                
            <!-- Medicine: 6.0 -->
                
            <!-- LLMs: 6.0 -->
                
            <!-- Federated Learning: 2.9 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.382
            </span>
            <a href="https://arxiv.org/abs/2504.09465" target="_blank" rel="noopener noreferrer">Evolutionary Defense: Advancing Moving Target Strategies with Bio-Inspired Reinforcement Learning to Secure Misconfigured Software Applications</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Niloofar Heidarikohol, Shuvalaxmi Dass, Akbar Siami Namin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Improper configurations in software systems often create vulnerabilities, leaving them open to exploitation. Static architectures exacerbate this issue by allowing misconfigurations to persist, providing adversaries with opportunities to exploit them during attacks. To address this challenge, a dyna</span>
            
            <span class="abstract-full" style="display: none;">Improper configurations in software systems often create vulnerabilities, leaving them open to exploitation. Static architectures exacerbate this issue by allowing misconfigurations to persist, providing adversaries with opportunities to exploit them during attacks. To address this challenge, a dynamic proactive defense strategy known as Moving Target Defense (MTD) can be applied. MTD continually changes the attack surface of the system, thwarting potential threats. In the previous research, we developed a proof of concept for a single-player MTD game model called RL-MTD, which utilizes Reinforcement Learning (RL) to generate dynamic secure configurations. While the model exhibited satisfactory performance in generating secure configurations, it grappled with an unoptimized and sparse search space, leading to performance issues. To tackle this obstacle, this paper addresses the search space optimization problem by leveraging two bio-inspired search algorithms: Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). Additionally, we extend our base RL-MTD model by integrating these algorithms, resulting in the creation of PSO-RL andGA-RL. We compare the performance of three models: base RL-MTD, GA-RL, and PSO-RL, across four misconfigured SUTs in terms of generating the most secure configuration. Results show that the optimal search space derived from both GA-RL and PSO-RL significantly enhances the performance of the base RL-MTD model compared to the version without optimized search space. While both GA-RL and PSO-RL demonstrate effective search capabilities, PSO-RL slightly outperforms GA-RL for most SUTs. Overall, both algorithms excel in seeking an optimal search space which in turn improves the performance of the model in generating optimal secure configuration.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 4.4 -->
                
            <!-- Federated Learning: 3.8 -->
                
            <!-- Reinforcement Learning: 3.2 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.5856
            </span>
            <a href="https://arxiv.org/abs/2504.08831" target="_blank" rel="noopener noreferrer">Anti-Slip AI-Driven Model-Free Control with Global Exponential Stability in Skid-Steering Robots</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mehdi Heydari Shahna, Pauli Mustalahti, Jouni Mattila | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Undesired lateral and longitudinal wheel slippage can disrupt a mobile robot's heading angle, traction, and, eventually, desired motion. This issue makes the robotization and accurate modeling of heavy-duty machinery very challenging because the application primarily involves off-road terrains, whic</span>
            
            <span class="abstract-full" style="display: none;">Undesired lateral and longitudinal wheel slippage can disrupt a mobile robot's heading angle, traction, and, eventually, desired motion. This issue makes the robotization and accurate modeling of heavy-duty machinery very challenging because the application primarily involves off-road terrains, which are susceptible to uneven motion and severe slippage. As a step toward robotization in skid-steering heavy-duty robot (SSHDR), this paper aims to design an innovative robust model-free control system developed by neural networks to strongly stabilize the robot dynamics in the presence of a broad range of potential wheel slippages. Before the control design, the dynamics of the SSHDR are first investigated by mathematically incorporating slippage effects, assuming that all functional modeling terms of the system are unknown to the control system. Then, a novel tracking control framework to guarantee global exponential stability of the SSHDR is designed as follows: 1) the unknown modeling of wheel dynamics is approximated using radial basis function neural networks (RBFNNs); and 2) a new adaptive law is proposed to compensate for slippage effects and tune the weights of the RBFNNs online during execution. Simulation and experimental results verify the proposed tracking control performance of a 4,836 kg SSHDR operating on slippery terrain.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.7 -->
                
            <!-- LLMs: 4.1 -->
                
            <!-- Reinforcement Learning: 4.0 -->
                
            <!-- Robotics: 2.6 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.0163
            </span>
            <a href="https://arxiv.org/abs/2504.09481" target="_blank" rel="noopener noreferrer">Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chenbin Zhang, Zhiqiang Hu, Chuchu Jiang, Wen Chen, Jie Xu, Shaoting Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Drug-target binding affinity prediction is a fundamental task for drug discovery. It has been extensively explored in literature and promising results are reported. However, in this paper, we demonstrate that the results may be misleading and cannot be well generalized to real practice. The core obs</span>
            
            <span class="abstract-full" style="display: none;">Drug-target binding affinity prediction is a fundamental task for drug discovery. It has been extensively explored in literature and promising results are reported. However, in this paper, we demonstrate that the results may be misleading and cannot be well generalized to real practice. The core observation is that the canonical randomized split of a test set in conventional evaluation leaves the test set dominated by samples with high similarity to the training set. The performance of models is severely degraded on samples with lower similarity to the training set but the drawback is highly overlooked in current evaluation. As a result, the performance can hardly be trusted when the model meets low-similarity samples in real practice. To address this problem, we propose a framework of similarity aware evaluation in which a novel split methodology is proposed to adapt to any desired distribution. This is achieved by a formulation of optimization problems which are approximately and efficiently solved by gradient descent. We perform extensive experiments across five representative methods in four datasets for two typical target evaluations and compare them with various counterpart methods. Results demonstrate that the proposed split methodology can significantly better fit desired distributions and guide the development of models. Code is released at https://github.com/Amshoreline/SAE/tree/main.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 5.7 -->
                
            <!-- Medicine: 4.6 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 2.8 -->
                
            <!-- GNN: 2.5 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.1027
            </span>
            <a href="https://arxiv.org/abs/2504.09117" target="_blank" rel="noopener noreferrer">HARQ-based Quantized Average Consensus over Unreliable Directed Network Topologies</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Neofytos Charalampous, Evagoras Makridis, Apostolos I. Rikos, Themistoklis Charalambous | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we propose a distributed algorithm (herein called HARQ-QAC) that enables nodes to calculate the average of their initial states by exchanging quantized messages over a directed communication network. In our setting, we assume that our communication network consists of unreliable commu</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we propose a distributed algorithm (herein called HARQ-QAC) that enables nodes to calculate the average of their initial states by exchanging quantized messages over a directed communication network. In our setting, we assume that our communication network consists of unreliable communication links (i.e., links suffering from packet drops). For countering link unreliability our algorithm leverages narrowband error-free feedback channels for acknowledging whether a packet transmission between nodes was successful. Additionally, we show that the feedback channels play a crucial role in enabling our algorithm to exhibit finite-time convergence. We analyze our algorithm and demonstrate its operation via an example, where we illustrate its operational advantages. Finally, simulations corroborate that our proposed algorithm converges to the average of the initial quantized values in a finite number of steps, despite the packet losses. This is the first quantized consensus algorithm in the literature that can handle packet losses and converge to the average. Additionally, the use of the retransmission mechanism allows for accelerating the convergence.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Reinforcement Learning: 4.3 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Medicine: 2.6 -->
                
            <!-- Math: 2.6 -->
                
            <!-- GNN: 2.6 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.2323
            </span>
            <a href="https://arxiv.org/abs/2501.06583" target="_blank" rel="noopener noreferrer">Optimizing wheel loader performance -- an end-to-end approach</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Koji Aoshima, Eddie Wadbro, Martin Servin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Wheel loaders in mines and construction sites repeatedly load soil from a pile to load receivers. This task presents a challenging optimization problem since each loading's performance depends on the pile state, which depends on previous loadings. We investigate an end-to-end optimization approach c</span>
            
            <span class="abstract-full" style="display: none;">Wheel loaders in mines and construction sites repeatedly load soil from a pile to load receivers. This task presents a challenging optimization problem since each loading's performance depends on the pile state, which depends on previous loadings. We investigate an end-to-end optimization approach considering future loading outcomes and transportation costs between the pile and load receivers. To predict the evolution of the pile state and the loading performance, we use world models that leverage deep neural networks trained on numerous simulated loading cycles. A look-ahead tree search optimizes the sequence of loading actions by evaluating the performance of thousands of action candidates, which expand into subsequent action candidates under the predicted pile states recursively. Test results demonstrate that, over a horizon of 15 sequential loadings, the look-ahead tree search is 6% more efficient than a greedy strategy, which always selects the action that maximizes the current single loading performance, and 14% more efficient than using a fixed loading controller optimized for the nominal case.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.3 -->
                
            <!-- LLMs: 5.3 -->
                
            <!-- Reinforcement Learning: 3.6 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.273
            </span>
            <a href="https://arxiv.org/abs/2502.05833" target="_blank" rel="noopener noreferrer">Machine learning-based hybrid dynamic modeling and economic predictive control of carbon capture process for ship decarbonization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xuewen Zhang, Kuniadi Wandy Huang, Dat-Nguyen Vo, Minghao Han, Benjamin Decardi-Nelson, Xunyuan Yin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Implementing carbon capture technology on-board ships holds promise as a solution to facilitate the reduction of carbon intensity in international shipping, as mandated by the International Maritime Organization. In this work, we address the energy-efficient operation of shipboard carbon capture pro</span>
            
            <span class="abstract-full" style="display: none;">Implementing carbon capture technology on-board ships holds promise as a solution to facilitate the reduction of carbon intensity in international shipping, as mandated by the International Maritime Organization. In this work, we address the energy-efficient operation of shipboard carbon capture processes by proposing a hybrid modeling-based economic predictive control scheme. Specifically, we consider a comprehensive shipboard carbon capture process that encompasses the ship engine system and the shipboard post-combustion carbon capture plant. To accurately and robustly characterize the dynamic behaviors of this shipboard plant, we develop a hybrid dynamic process model that integrates available imperfect physical knowledge with neural networks trained using process operation data. An economic model predictive control approach is proposed based on the hybrid model to ensure carbon capture efficiency while minimizing energy consumption required for the carbon capture process operation. The cross-entropy method is employed to efficiently solve the complex non-convex optimization problem associated with the proposed hybrid model-based economic model predictive control method. Extensive simulations, analyses, and comparisons are conducted to verify the effectiveness and illustrate the superiority of the proposed framework.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.5 -->
                
            <!-- LLMs: 7.1 -->
                
            <!-- Reinforcement Learning: 4.4 -->
                
            <!-- Federated Learning: 2.2 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.3752
            </span>
            <a href="https://arxiv.org/abs/2502.19460" target="_blank" rel="noopener noreferrer">Practical Evaluation of Copula-based Survival Metrics: Beyond the Independent Censoring Assumption</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Christian Marius Lillelund, Shi-ang Qi, Russell Greiner | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Conventional survival metrics, such as Harrell's concordance index and the Brier Score, rely on the independent censoring assumption for valid inference in the presence of right-censored data. However, when instances are censored for reasons related to the event of interest, this assumption no longe</span>
            
            <span class="abstract-full" style="display: none;">Conventional survival metrics, such as Harrell's concordance index and the Brier Score, rely on the independent censoring assumption for valid inference in the presence of right-censored data. However, when instances are censored for reasons related to the event of interest, this assumption no longer holds, as this kind of dependent censoring biases the marginal survival estimates of popular nonparametric estimators. In this paper, we propose three copula-based metrics to evaluate survival models in the presence of dependent censoring, and design a framework to create realistic, semi-synthetic datasets with dependent censoring to facilitate the evaluation of the metrics. Our empirical analyses in synthetic and semi-synthetic datasets show that our metrics can give error estimates that are closer to the true error, mainly in terms of prediction accuracy.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.7 -->
                
            <!-- Medicine: 4.0 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Math: 2.7 -->
                
            <!-- Reinforcement Learning: 2.5 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.3907
            </span>
            <a href="https://arxiv.org/abs/2504.09428" target="_blank" rel="noopener noreferrer">FROG: Effective Friend Recommendation in Online Games via Modality-aware User Preferences</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Qiwei Wang, Dandan Lin, Wenqing Lin, Ziming Wu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Due to the convenience of mobile devices, the online games have become an important part for user entertainments in reality, creating a demand for friend recommendation in online games. However, none of existing approaches can effectively incorporate the multi-modal user features (\emph{e.g.}, image</span>
            
            <span class="abstract-full" style="display: none;">Due to the convenience of mobile devices, the online games have become an important part for user entertainments in reality, creating a demand for friend recommendation in online games. However, none of existing approaches can effectively incorporate the multi-modal user features (\emph{e.g.}, images and texts) with the structural information in the friendship graph, due to the following limitations: (1) some of them ignore the high-order structural proximity between users, (2) some fail to learn the pairwise relevance between users at modality-specific level, and (3) some cannot capture both the local and global user preferences on different modalities. By addressing these issues, in this paper, we propose an end-to-end model \textsc{FROG} that better models the user preferences on potential friends. Comprehensive experiments on both offline evaluation and online deployment at \kw{Tencent} have demonstrated the superiority of \textsc{FROG} over existing approaches.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.8 -->
                
            <!-- Medicine: 6.6 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- Federated Learning: 2.9 -->
                
            <!-- Reinforcement Learning: 2.7 -->
                
            <!-- Math: 2.2 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.4999
            </span>
            <a href="https://arxiv.org/abs/2504.10089" target="_blank" rel="noopener noreferrer">Convergence Analysis of a Stochastic Interacting Particle-Field Algorithm for 3D Parabolic-Parabolic Keller-Segel Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Boyi Hu, Zhongjian Wang, Jack Xin, Zhiwen Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Chemotaxis models describe the movement of organisms in response to chemical gradients. In this paper, we present a stochastic interacting particle-field algorithm with random batch approximation (SIPF-$r$) for the three-dimensional (3D) parabolic-parabolic Keller-Segel (KS) system, also known as th</span>
            
            <span class="abstract-full" style="display: none;">Chemotaxis models describe the movement of organisms in response to chemical gradients. In this paper, we present a stochastic interacting particle-field algorithm with random batch approximation (SIPF-$r$) for the three-dimensional (3D) parabolic-parabolic Keller-Segel (KS) system, also known as the fully parabolic KS system. The SIPF-$r$ method approximates the KS system by coupling particle-based representations of density with a smooth field variable computed using spectral methods. By incorporating the random batch method (RBM), we bypass the mean-field limit and significantly reduce computational complexity. Under mild assumptions on the regularity of the original KS system and the boundedness of numerical approximations, we prove that, with high probability, the empirical measure of the SIPF-$r$ particle system converges to the exact measure of the limiting McKean-Vlasov process in the $1$-Wasserstein distance. Numerical experiments validate the theoretical convergence rates and demonstrate the robustness and accuracy of the SIPF-$r$ method.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.1 -->
                
            <!-- LLMs: 4.6 -->
                
            <!-- Reinforcement Learning: 4.1 -->
                
            <!-- Math: 3.8 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Federated Learning: 2.2 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Networks: 1.5 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.569
            </span>
            <a href="https://arxiv.org/abs/2504.02623" target="_blank" rel="noopener noreferrer">Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Peijie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, Feng Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly acce</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly access agents in single-mission scenarios, failing to capture real-world complexity. To bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark, each test case comprises multiple interrelated missions. This design requires agents to dynamically adapt to evolving demands. Moreover, the proposed benchmark explores all possible mission-switching patterns within a fixed mission number. Specifically, we propose a multi-agent data generation framework to construct the benchmark. We also propose a novel method to evaluate the accuracy and efficiency of agent decisions with dynamic decision trees. Experiments on diverse open-source and closed-source LLMs reveal critical factors influencing agent robustness and provide actionable insights to the tool invocation society.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 22.2 -->
                
            <!-- Medicine: 4.4 -->
                
            <!-- Reinforcement Learning: 2.9 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.5787
            </span>
            <a href="https://arxiv.org/abs/2504.08901" target="_blank" rel="noopener noreferrer">HAL-NeRF: High Accuracy Localization Leveraging Neural Radiance Fields</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Asterios Reppas, Grigorios-Aris Cheimariotis, Panos K. Papadopoulos, Panagiotis Frasiolas, Dimitrios Zarpalas | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Precise camera localization is a critical task in XR applications and robotics. Using only the camera captures as input to a system is an inexpensive option that enables localization in large indoor and outdoor environments, but it presents challenges in achieving high accuracy. Specifically, camera</span>
            
            <span class="abstract-full" style="display: none;">Precise camera localization is a critical task in XR applications and robotics. Using only the camera captures as input to a system is an inexpensive option that enables localization in large indoor and outdoor environments, but it presents challenges in achieving high accuracy. Specifically, camera relocalization methods, such as Absolute Pose Regression (APR), can localize cameras with a median translation error of more than $0.5m$ in outdoor scenes. This paper presents HAL-NeRF, a high-accuracy localization method that combines a CNN pose regressor with a refinement module based on a Monte Carlo particle filter. The Nerfacto model, an implementation of Neural Radiance Fields (NeRFs), is used to augment the data for training the pose regressor and to measure photometric loss in the particle filter refinement module. HAL-NeRF leverages Nerfacto's ability to synthesize high-quality novel views, significantly improving the performance of the localization pipeline. HAL-NeRF achieves state-of-the-art results that are conventionally measured as the average of the median per scene errors. The translation error was $0.025m$ and the rotation error was $0.59$ degrees and 0.04m and 0.58 degrees on the 7-Scenes dataset and Cambridge Landmarks datasets respectively, with the trade-off of increased computational time. This work highlights the potential of combining APR with NeRF-based refinement techniques to advance monocular camera relocalization accuracy.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.8 -->
                
            <!-- LLMs: 5.5 -->
                
            <!-- Networks: 3.2 -->
                
            <!-- Reinforcement Learning: 2.7 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Robotics: 2.4 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.6465
            </span>
            <a href="https://arxiv.org/abs/2504.08588" target="_blank" rel="noopener noreferrer">Hardware, Algorithms, and Applications of the Neuromorphic Vision Sensor: a Review</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Claudio Cimarelli, Jose Andres Millan-Romera, Holger Voos, Jose Luis Sanchez-Lopez | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Neuromorphic, or event, cameras represent a transformation in the classical approach to visual sensing encodes detected instantaneous per-pixel illumination changes into an asynchronous stream of event packets. Their novelty compared to standard cameras lies in the transition from capturing full pic</span>
            
            <span class="abstract-full" style="display: none;">Neuromorphic, or event, cameras represent a transformation in the classical approach to visual sensing encodes detected instantaneous per-pixel illumination changes into an asynchronous stream of event packets. Their novelty compared to standard cameras lies in the transition from capturing full picture frames at fixed time intervals to a sparse data format which, with its distinctive qualities, offers potential improvements in various applications. However, these advantages come at the cost of reinventing algorithmic procedures or adapting them to effectively process the new data format.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.1 -->
                
            <!-- Medicine: 5.3 -->
                
            <!-- Quantum Computing: 4.7 -->
                
            <!-- Reinforcement Learning: 3.3 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Attention: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.6468
            </span>
            <a href="https://arxiv.org/abs/2504.08902" target="_blank" rel="noopener noreferrer">LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pascal Chang, Sergio Sancho, Jingwei Tang, Markus Gross, Vinicius C. Azevedo | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of the</span>
            
            <span class="abstract-full" style="display: none;">Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce Laplacian Pyramid Warping, a frequency-aware image warping technique key to generating high-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to latent space models and to a wider range of spatial transforms, enabling the creation of novel generative perceptual illusions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Reinforcement Learning: 3.5 -->
                
            <!-- Medicine: 3.2 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 2.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.7805
            </span>
            <a href="https://arxiv.org/abs/2502.05424" target="_blank" rel="noopener noreferrer">SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xingtong Yu, Zechuan Gong, Chang Zhou, Yuan Fang, Hui Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Graphs are able to model interconnected entities in many online services, supporting a wide range of applications on the Web. This raises an important question: How can we train a graph foundational model on multiple source domains and adapt to an unseen target domain? A major obstacle is that graph</span>
            
            <span class="abstract-full" style="display: none;">Graphs are able to model interconnected entities in many online services, supporting a wide range of applications on the Web. This raises an important question: How can we train a graph foundational model on multiple source domains and adapt to an unseen target domain? A major obstacle is that graphs from different domains often exhibit divergent characteristics. Some studies leverage large language models to align multiple domains based on textual descriptions associated with the graphs, limiting their applicability to text-attributed graphs. For text-free graphs, a few recent works attempt to align different feature distributions across domains, while generally neglecting structural differences. In this work, we propose a novel Structure Alignment framework for text-free Multi-domain Graph Pre-Training and cross-domain adaptation (SAMGPT). It is designed to learn multi-domain knowledge from graphs originating in multiple source domains, which can then be adapted to address applications in an unseen target domain. Specifically, we introduce a set of structure tokens to harmonize structure-based aggregation across source domains during the pre-training phase. Next, for cross-domain adaptation, we design dual prompts, namely, holistic prompts and specific prompts, which adapt unified multi-domain structural knowledge and fine-grained, domain-specific information, respectively, to a target domain. Finally, we conduct comprehensive experiments on seven public datasets to evaluate and analyze the effectiveness of SAMGPT.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.7 -->
                
            <!-- Medicine: 6.1 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- GNN: 2.6 -->
                
            <!-- Reinforcement Learning: 2.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.8616
            </span>
            <a href="https://arxiv.org/abs/2503.13423" target="_blank" rel="noopener noreferrer">SuperBPE: Space Travel for Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alisa Liu, Jonathan Hayase, Valentin Hofmann, Sewoong Oh, Noah A. Smith, Yejin Choi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The assumption across nearly all language model (LM) tokenization schemes is that tokens should be subwords, i.e., contained within word boundaries. While providing a seemingly reasonable inductive bias, is this common practice limiting the potential of modern LMs? Whitespace is not a reliable delim</span>
            
            <span class="abstract-full" style="display: none;">The assumption across nearly all language model (LM) tokenization schemes is that tokens should be subwords, i.e., contained within word boundaries. While providing a seemingly reasonable inductive bias, is this common practice limiting the potential of modern LMs? Whitespace is not a reliable delimiter of meaning, as evidenced by multi-word expressions (e.g., "by the way"), crosslingual variation in the number of words needed to express a concept (e.g., "spacesuit helmet" in German is "raumanzughelm"), and languages that do not use whitespace at all (e.g., Chinese). To explore the potential of tokenization beyond subwords, we introduce a "superword" tokenizer, SuperBPE, which incorporates a simple pretokenization curriculum into the byte-pair encoding (BPE) algorithm to first learn subwords, then superwords that bridge whitespace. This brings dramatic improvements in encoding efficiency: when fixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with up to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B transformer LMs from scratch while fixing the model size, vocabulary size, and train compute, varying *only* the algorithm for learning the vocabulary. Our model trained with SuperBPE achieves an average +4.0% absolute improvement over the BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while simultaneously requiring 27% less compute at inference time. In analysis, we find that SuperBPE results in segmentations of text that are more uniform in per-token difficulty. Qualitatively, this may be because SuperBPE tokens often capture common multi-word expressions that function semantically as a single unit. SuperBPE is a straightforward, local modification to tokenization that improves both encoding efficiency and downstream performance, yielding better language models overall.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.4 -->
                
            <!-- Medicine: 4.0 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Reinforcement Learning: 2.8 -->
                
            <!-- GNN: 2.4 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.9481
            </span>
            <a href="https://arxiv.org/abs/2504.09484" target="_blank" rel="noopener noreferrer">An overview of condensation phenomenon in deep learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhi-Qin John Xu, Yaoyu Zhang, Zhangchen Zhou | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we provide an overview of a common phenomenon, condensation, observed during the nonlinear training of neural networks: During the nonlinear training of neural networks, neurons in the same layer tend to condense into groups with similar outputs. Empirical observations suggest that th</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we provide an overview of a common phenomenon, condensation, observed during the nonlinear training of neural networks: During the nonlinear training of neural networks, neurons in the same layer tend to condense into groups with similar outputs. Empirical observations suggest that the number of condensed clusters of neurons in the same layer typically increases monotonically as training progresses. Neural networks with small weight initializations or Dropout optimization can facilitate this condensation process. We also examine the underlying mechanisms of condensation from the perspectives of training dynamics and the structure of the loss landscape. The condensation phenomenon offers valuable insights into the generalization abilities of neural networks and correlates to stronger reasoning abilities in transformer-based language models.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.4 -->
                
            <!-- Medicine: 5.4 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Reinforcement Learning: 3.0 -->
                
            <!-- Math: 2.9 -->
                
            <!-- Robotics: 2.3 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- SpikingNN: 1.8 -->
                
            <!-- Pathfinding: 1.6 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0035
            </span>
            <a href="https://arxiv.org/abs/2307.11170" target="_blank" rel="noopener noreferrer">UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Aidan Mannion, Thierry Chevalier, Didier Schwab, Lorraine Geouriot | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the bio</span>
            
            <span class="abstract-full" style="display: none;">Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS. This allows for graph-based learning objectives to be combined with masked-language pre-training. Preliminary results from experiments in the extension of pre-trained LMs as well as training from scratch show that this framework improves downstream performance on multiple biomedical and clinical Named Entity Recognition (NER) tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.6 -->
                
            <!-- Medicine: 5.3 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0326
            </span>
            <a href="https://arxiv.org/abs/2305.14985" target="_blank" rel="noopener noreferrer">IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Haoxuan You, Zhecan Wang, Rui Sun, Long Chen, Gengyu Wang, Hammad A. Ayyubi, Kai-Wei Chang, Shih-Fu Chang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer</span>
            
            <span class="abstract-full" style="display: none;">The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE. Code is available at https://github.com/Hxyou/IdealGPT</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 24.0 -->
                
            <!-- Medicine: 3.8 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- Reinforcement Learning: 2.8 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0421
            </span>
            <a href="https://arxiv.org/abs/2307.12909" target="_blank" rel="noopener noreferrer">Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shangzan Zhang, Sida Peng, Yinji ShenTu, Qing Shuai, Tianrun Chen, Kaicheng Yu, Hujun Bao, Xiaowei Zhou | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recently, the editing of neural radiance fields (NeRFs) has gained considerable attention, but most prior works focus on static scenes while research on the appearance editing of dynamic scenes is relatively lacking. In this paper, we propose a novel framework to edit the local appearance of dynamic</span>
            
            <span class="abstract-full" style="display: none;">Recently, the editing of neural radiance fields (NeRFs) has gained considerable attention, but most prior works focus on static scenes while research on the appearance editing of dynamic scenes is relatively lacking. In this paper, we propose a novel framework to edit the local appearance of dynamic NeRFs by manipulating pixels in a single frame of training video. Specifically, to locally edit the appearance of dynamic NeRFs while preserving unedited regions, we introduce a local surface representation of the edited region, which can be inserted into and rendered along with the original NeRF and warped to arbitrary other frames through a learned invertible motion representation network. By employing our method, users without professional expertise can easily add desired content to the appearance of a dynamic scene. We extensively evaluate our approach on various scenes and show that our approach achieves spatially and temporally consistent editing results. Notably, our approach is versatile and applicable to different variants of dynamic NeRF representations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.0 -->
                
            <!-- Medicine: 5.7 -->
                
            <!-- Reinforcement Learning: 3.1 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1804
            </span>
            <a href="https://arxiv.org/abs/2309.15531" target="_blank" rel="noopener noreferrer">Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have recently demonstrated remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to the large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promisin</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have recently demonstrated remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to the large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output-channel (per-OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as a new outlier-friendly scheme, we propose Adaptive Dimensions (AdaDim), a versatile quantization framework that can adapt to various weight sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting prior methods such as Round-To-Nearest and GPTQ, showing significant improvements across various language modeling benchmarks for both base (up to +4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs. Code is available at https://github.com/johnheo/adadim-llm</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 22.6 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Medicine: 2.6 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.192
            </span>
            <a href="https://arxiv.org/abs/2402.01122" target="_blank" rel="noopener noreferrer">Generalized Multi-Speed Dubins Motion Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: James P. Wilson, Shalabh Gupta, Thomas A. Wettergren | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The paper develops a novel motion model, called Generalized Multi-Speed Dubins Motion Model (GMDM), which extends the Dubins model by considering multiple speeds. While the Dubins model produces time-optimal paths under a constant speed constraint, these paths could be suboptimal if this constraint </span>
            
            <span class="abstract-full" style="display: none;">The paper develops a novel motion model, called Generalized Multi-Speed Dubins Motion Model (GMDM), which extends the Dubins model by considering multiple speeds. While the Dubins model produces time-optimal paths under a constant speed constraint, these paths could be suboptimal if this constraint is relaxed to include multiple speeds. This is because a constant speed results in a large minimum turning radius, thus producing paths with longer maneuvers and larger travel times. In contrast, multi-speed relaxation allows for slower speed sharp turns, thus producing more direct paths with shorter maneuvers and smaller travel times. Furthermore, the inability of the Dubins model to reduce speed could result in fast maneuvers near obstacles, thus producing paths with high collision risks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.0 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 3.4 -->
                
            <!-- GNN: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.263
            </span>
            <a href="https://arxiv.org/abs/2504.10433" target="_blank" rel="noopener noreferrer">MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via Diffusion Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jian Liu, Wei Sun, Hui Yang, Jin Zheng, Zichen Geng, Hossein Rahmani, Ajmal Mian | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Object pose estimation is a core means for robots to understand and interact with their environment. For this task, monocular category-level methods are attractive as they require only a single RGB camera. However, current methods rely on shape priors or CAD models of the intra-class known objects. </span>
            
            <span class="abstract-full" style="display: none;">Object pose estimation is a core means for robots to understand and interact with their environment. For this task, monocular category-level methods are attractive as they require only a single RGB camera. However, current methods rely on shape priors or CAD models of the intra-class known objects. We propose a diffusion-based monocular category-level 9D object pose generation method, MonoDiff9D. Our motivation is to leverage the probabilistic nature of diffusion models to alleviate the need for shape priors, CAD models, or depth sensors for intra-class unknown object pose estimation. We first estimate coarse depth via DINOv2 from the monocular image in a zero-shot manner and convert it into a point cloud. We then fuse the global features of the point cloud with the input image and use the fused features along with the encoded time step to condition MonoDiff9D. Finally, we design a transformer-based denoiser to recover the object pose from Gaussian noise. Extensive experiments on two popular benchmark datasets show that MonoDiff9D achieves state-of-the-art monocular category-level 9D object pose estimation accuracy without the need for shape priors or CAD models at any stage. Our code will be made public at https://github.com/CNJianLiu/MonoDiff9D.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.1 -->
                
            <!-- LLMs: 3.6 -->
                
            <!-- Reinforcement Learning: 3.3 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Robotics: 2.3 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- SpikingNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.3539
            </span>
            <a href="https://arxiv.org/abs/2504.10191" target="_blank" rel="noopener noreferrer">Localized Cultural Knowledge is Conserved and Controllable in Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Veniamin Veselovsky, Berke Argin, Benedikt Stroebl, Chris Wendler, Robert West, James Evans, Thomas L. Griffiths, Arvind Narayanan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Just as humans display language patterns influenced by their native tongue when speaking new languages, LLMs often default to English-centric responses even when generating in other languages. Nevertheless, we observe that local cultural information persists within the models and can be readily acti</span>
            
            <span class="abstract-full" style="display: none;">Just as humans display language patterns influenced by their native tongue when speaking new languages, LLMs often default to English-centric responses even when generating in other languages. Nevertheless, we observe that local cultural information persists within the models and can be readily activated for cultural customization. We first demonstrate that explicitly providing cultural context in prompts significantly improves the models' ability to generate culturally localized responses. We term the disparity in model performance with versus without explicit cultural context the explicit-implicit localization gap, indicating that while cultural knowledge exists within LLMs, it may not naturally surface in multilingual interactions if cultural context is not explicitly provided. Despite the explicit prompting benefit, however, the answers reduce in diversity and tend toward stereotypes. Second, we identify an explicit cultural customization vector, conserved across all non-English languages we explore, which enables LLMs to be steered from the synthetic English cultural world-model toward each non-English cultural world. Steered responses retain the diversity of implicit prompting and reduce stereotypes to dramatically improve the potential for customization. We discuss the implications of explicit cultural customization for understanding the conservation of alternative cultural world models within LLMs, and their controllable utility for translation, cultural customization, and the possibility of making the explicit implicit through soft control for expanded LLM function and appeal.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 19.3 -->
                
            <!-- Medicine: 6.7 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4042
            </span>
            <a href="https://arxiv.org/abs/2504.07951" target="_blank" rel="noopener noreferrer">Scaling Laws for Native Multimodal Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mustafa Shukor, Enrico Fini, Victor Guilherme Turrisi da Costa, Matthieu Cord, Joshua Susskind, Alaaeldin El-Nouby | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approa</span>
            
            <span class="abstract-full" style="display: none;">Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.1 -->
                
            <!-- Medicine: 5.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4105
            </span>
            <a href="https://arxiv.org/abs/2402.04621" target="_blank" rel="noopener noreferrer">Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Soo Yong Lee, Sunwoo Kim, Fanchen Bu, Jaemin Yoo, Jiliang Tang, Kijung Shin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">How would randomly shuffling feature vectors among nodes from the same class affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs the dependence between graph topology and features (A-X dependence) for GNNs to learn from. Surprisingly, we observe a consistent and significa</span>
            
            <span class="abstract-full" style="display: none;">How would randomly shuffling feature vectors among nodes from the same class affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs the dependence between graph topology and features (A-X dependence) for GNNs to learn from. Surprisingly, we observe a consistent and significant improvement in GNN performance following the feature shuffle. Having overlooked the impact of A-X dependence on GNNs, the prior literature does not provide a satisfactory understanding of the phenomenon. Thus, we raise two research questions. First, how should A-X dependence be measured, while controlling for potential confounds? Second, how does A-X dependence affect GNNs? In response, we (i) propose a principled measure for A-X dependence, (ii) design a random graph model that controls A-X dependence, (iii) establish a theory on how A-X dependence relates to graph convolution, and (iv) present empirical analysis on real-world graphs that align with the theory. We conclude that A-X dependence mediates the effect of graph convolution, such that smaller dependence improves GNN-based node classification.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.8 -->
                
            <!-- LLMs: 6.2 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Reinforcement Learning: 3.0 -->
                
            <!-- GNN: 2.5 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4163
            </span>
            <a href="https://arxiv.org/abs/2305.15932" target="_blank" rel="noopener noreferrer">BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jie He, Simon Chi Lok U, V\'ictor Guti\'errez-Basulto, Jeff Z. Pan | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), bu</span>
            
            <span class="abstract-full" style="display: none;">Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry. Our code is available at https://github.com/probe2/BUCA.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.7 -->
                
            <!-- Medicine: 5.5 -->
                
            <!-- Quantum Computing: 2.9 -->
                
            <!-- GNN: 2.4 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Blockchain: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4786
            </span>
            <a href="https://arxiv.org/abs/2504.05522" target="_blank" rel="noopener noreferrer">User Feedback Alignment for LLM-powered Exploration in Large-scale Recommendation Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jianling Wang, Yifan Liu, Yinghao Sun, Xuejian Ma, Yueqi Wang, He Ma, Zhengyang Su, Minmin Chen, Mingyan Gao, Onkar Dalal, Ed H. Chi, Lichan Hong, Ningren Han, Haokai Lu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Exploration, the act of broadening user experiences beyond their established preferences, is challenging in large-scale recommendation systems due to feedback loops and limited signals on user exploration patterns. Large Language Models (LLMs) offer potential by leveraging their world knowledge to r</span>
            
            <span class="abstract-full" style="display: none;">Exploration, the act of broadening user experiences beyond their established preferences, is challenging in large-scale recommendation systems due to feedback loops and limited signals on user exploration patterns. Large Language Models (LLMs) offer potential by leveraging their world knowledge to recommend novel content outside these loops. A key challenge is aligning LLMs with user preferences while preserving their knowledge and reasoning. While using LLMs to plan for the next novel user interest, this paper introduces a novel approach combining hierarchical planning with LLM inference-time scaling to improve recommendation relevancy without compromising novelty. We decouple novelty and user-alignment, training separate LLMs for each objective. We then scale up the novelty-focused LLM's inference and select the best-of-n predictions using the user-aligned LLM. Live experiments demonstrate efficacy, showing significant gains in both user satisfaction (measured by watch activity and active user counts) and exploration diversity.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 21.4 -->
                
            <!-- Medicine: 6.4 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- RAG: 2.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4921
            </span>
            <a href="https://arxiv.org/abs/2504.08480" target="_blank" rel="noopener noreferrer">Toward Realistic Adversarial Attacks in IDS: A Novel Feasibility Metric for Transferability</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sabrine Ennaji, Elhadj Benkhelifa, Luigi Vincenzo Mancini | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Transferability-based adversarial attacks exploit the ability of adversarial examples, crafted to deceive a specific source Intrusion Detection System (IDS) model, to also mislead a target IDS model without requiring access to the training data or any internal model parameters. These attacks exploit</span>
            
            <span class="abstract-full" style="display: none;">Transferability-based adversarial attacks exploit the ability of adversarial examples, crafted to deceive a specific source Intrusion Detection System (IDS) model, to also mislead a target IDS model without requiring access to the training data or any internal model parameters. These attacks exploit common vulnerabilities in machine learning models to bypass security measures and compromise systems. Although the transferability concept has been widely studied, its practical feasibility remains limited due to assumptions of high similarity between source and target models. This paper analyzes the core factors that contribute to transferability, including feature alignment, model architectural similarity, and overlap in the data distributions that each IDS examines. We propose a novel metric, the Transferability Feasibility Score (TFS), to assess the feasibility and reliability of such attacks based on these factors. Through experimental evidence, we demonstrate that TFS and actual attack success rates are highly correlated, addressing the gap between theoretical understanding and real-world impact. Our findings provide needed guidance for designing more realistic transferable adversarial attacks, developing robust defenses, and ultimately improving the security of machine learning-based IDS in critical systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.5 -->
                
            <!-- Medicine: 6.4 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Reinforcement Learning: 2.9 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Math: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4952
            </span>
            <a href="https://arxiv.org/abs/2412.15429" target="_blank" rel="noopener noreferrer">Offline Safe Reinforcement Learning Using Trajectory Classification</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ze Gong, Akshat Kumar, Pradeep Varakantham | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Offline safe reinforcement learning (RL) has emerged as a promising approach for learning safe behaviors without engaging in risky online interactions with the environment. Most existing methods in offline safe RL rely on cost constraints at each time step (derived from global cost constraints) and </span>
            
            <span class="abstract-full" style="display: none;">Offline safe reinforcement learning (RL) has emerged as a promising approach for learning safe behaviors without engaging in risky online interactions with the environment. Most existing methods in offline safe RL rely on cost constraints at each time step (derived from global cost constraints) and this can result in either overly conservative policies or violation of safety constraints. In this paper, we propose to learn a policy that generates desirable trajectories and avoids undesirable trajectories. To be specific, we first partition the pre-collected dataset of state-action trajectories into desirable and undesirable subsets. Intuitively, the desirable set contains high reward and safe trajectories, and undesirable set contains unsafe trajectories and low-reward safe trajectories. Second, we learn a policy that generates desirable trajectories and avoids undesirable trajectories, where (un)desirability scores are provided by a classifier learnt from the dataset of desirable and undesirable trajectories. This approach bypasses the computational complexity and stability issues of a min-max objective that is employed in existing methods. Theoretically, we also show our approach's strong connections to existing learning paradigms involving human feedback. Finally, we extensively evaluate our method using the DSRL benchmark for offline safe RL. Empirically, our method outperforms competitive baselines, achieving higher rewards and better constraint satisfaction across a wide variety of benchmark tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Reinforcement Learning: 3.9 -->
                
            <!-- Quantum Computing: 2.9 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- 3D: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.5727
            </span>
            <a href="https://arxiv.org/abs/2504.08215" target="_blank" rel="noopener noreferrer">Deep Distributional Learning with Non-crossing Quantile Network</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Guohao Shen, Runpeng Dai, Guojun Wu, Shikai Luo, Chengchun Shi, Hongtu Zhu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we introduce a non-crossing quantile (NQ) network for conditional distribution learning. By leveraging non-negative activation functions, the NQ network ensures that the learned distributions remain monotonic, effectively addressing the issue of quantile crossing. Furthermore, the NQ </span>
            
            <span class="abstract-full" style="display: none;">In this paper, we introduce a non-crossing quantile (NQ) network for conditional distribution learning. By leveraging non-negative activation functions, the NQ network ensures that the learned distributions remain monotonic, effectively addressing the issue of quantile crossing. Furthermore, the NQ network-based deep distributional learning framework is highly adaptable, applicable to a wide range of applications, from classical non-parametric quantile regression to more advanced tasks such as causal effect estimation and distributional reinforcement learning (RL). We also develop a comprehensive theoretical foundation for the deep NQ estimator and its application to distributional RL, providing an in-depth analysis that demonstrates its effectiveness across these domains. Our experimental results further highlight the robustness and versatility of the NQ network.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.9 -->
                
            <!-- Medicine: 5.7 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Reinforcement Learning: 3.9 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.7975
            </span>
            <a href="https://arxiv.org/abs/2504.09923" target="_blank" rel="noopener noreferrer">Guiding Reasoning in Small Language Models with LLM Assistance</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yujin Kim, Euiin Yi, Minu Kim, Se-Young Yun, Taehyeon Kim | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The limited reasoning capabilities of small language models (SLMs) cast doubt on their suitability for tasks demanding deep, multi-step logical deduction. This paper introduces a framework called Small Reasons, Large Hints (SMART), which selectively augments SLM reasoning with targeted guidance from</span>
            
            <span class="abstract-full" style="display: none;">The limited reasoning capabilities of small language models (SLMs) cast doubt on their suitability for tasks demanding deep, multi-step logical deduction. This paper introduces a framework called Small Reasons, Large Hints (SMART), which selectively augments SLM reasoning with targeted guidance from large language models (LLMs). Inspired by the concept of cognitive scaffolding, SMART employs a score-based evaluation to identify uncertain reasoning steps and injects corrective LLM-generated reasoning only when necessary. By framing structured reasoning as an optimal policy search, our approach steers the reasoning trajectory toward correct solutions without exhaustive sampling. Our experiments on mathematical reasoning datasets demonstrate that targeted external scaffolding significantly improves performance, paving the way for collaborative use of both SLM and LLM to tackle complex reasoning tasks that are currently unsolvable by SLMs alone.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 36.6 -->
                
            <!-- Medicine: 4.5 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- RAG: 2.4 -->
                
            <!-- 3D: 2.0 -->
                
            <!-- Blockchain: 1.9 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Reinforcement Learning: 1.4 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Datasets: 1.2 -->
                
            <!-- Finance: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.8354
            </span>
            <a href="https://arxiv.org/abs/2412.21063" target="_blank" rel="noopener noreferrer">Navigating Image Restoration with VAR's Distribution Alignment Prior</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Siyang Wang, Feng Zhao | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative models trained on extensive high-quality datasets effectively capture the structural and statistical properties of clean images, rendering them powerful priors for transforming degraded features into clean ones in image restoration. VAR, a novel image generative paradigm, surpasses diffus</span>
            
            <span class="abstract-full" style="display: none;">Generative models trained on extensive high-quality datasets effectively capture the structural and statistical properties of clean images, rendering them powerful priors for transforming degraded features into clean ones in image restoration. VAR, a novel image generative paradigm, surpasses diffusion models in generation quality by applying a next-scale prediction approach. It progressively captures both global structures and fine-grained details through the autoregressive process, consistent with the multi-scale restoration principle widely acknowledged in the restoration community. Furthermore, we observe that during the image reconstruction process utilizing VAR, scale predictions automatically modulate the input, facilitating the alignment of representations at subsequent scales with the distribution of clean images. To harness VAR's adaptive distribution alignment capability in image restoration tasks, we formulate the multi-scale latent representations within VAR as the restoration prior, thus advancing our delicately designed VarFormer framework. The strategic application of these priors enables our VarFormer to achieve remarkable generalization on unseen tasks while also reducing training computational costs. Extensive experiments underscores that our VarFormer outperforms existing multi-task image restoration methods across various restoration tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.4 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Reinforcement Learning: 2.5 -->
                
            <!-- Federated Learning: 2.3 -->
                
            <!-- Math: 2.1 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.8824
            </span>
            <a href="https://arxiv.org/abs/2504.08344" target="_blank" rel="noopener noreferrer">EasyGenNet: An Efficient Framework for Audio-Driven Gesture Video Generation Based on Diffusion Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Renda Li, Xiaohua Qi, Qiang Ling, Jun Yu, Ziyi Chen, Peng Chang, Mei HanJing Xiao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Audio-driven cospeech video generation typically involves two stages: speech-to-gesture and gesture-to-video. While significant advances have been made in speech-to-gesture generation, synthesizing natural expressions and gestures remains challenging in gesture-to-video systems. In order to improve </span>
            
            <span class="abstract-full" style="display: none;">Audio-driven cospeech video generation typically involves two stages: speech-to-gesture and gesture-to-video. While significant advances have been made in speech-to-gesture generation, synthesizing natural expressions and gestures remains challenging in gesture-to-video systems. In order to improve the generation effect, previous works adopted complex input and training strategies and required a large amount of data sets for pre-training, which brought inconvenience to practical applications. We propose a simple one-stage training method and a temporal inference method based on a diffusion model to synthesize realistic and continuous gesture videos without the need for additional training of temporal modules.The entire model makes use of existing pre-trained weights, and only a few thousand frames of data are needed for each character at a time to complete fine-tuning. Built upon the video generator, we introduce a new audio-to-video pipeline to synthesize co-speech videos, using 2D human skeleton as the intermediate motion representation. Our experiments show that our method outperforms existing GAN-based and diffusion-based methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.2 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9083
            </span>
            <a href="https://arxiv.org/abs/2502.06674" target="_blank" rel="noopener noreferrer">RAILS: Risk-Aware Iterated Local Search for Joint SLA Decomposition and Service Provider Management in Multi-Domain Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Cyril Shih-Huan Hsu, Chrysa Papagianni, Paola Grosso | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The emergence of the fifth generation (5G) technology has transformed mobile networks into multi-service environments, necessitating efficient network slicing to meet diverse Service Level Agreements (SLAs). SLA decomposition across multiple network domains, each potentially managed by different ser</span>
            
            <span class="abstract-full" style="display: none;">The emergence of the fifth generation (5G) technology has transformed mobile networks into multi-service environments, necessitating efficient network slicing to meet diverse Service Level Agreements (SLAs). SLA decomposition across multiple network domains, each potentially managed by different service providers, poses a significant challenge due to limited visibility into real-time underlying domain conditions. This paper introduces Risk-Aware Iterated Local Search (RAILS), a novel risk model-driven meta-heuristic framework designed to jointly address SLA decomposition and service provider selection in multi-domain networks. By integrating online risk modeling with iterated local search principles, RAILS effectively navigates the complex optimization landscape, utilizing historical feedback from domain controllers. We formulate the joint problem as a Mixed-Integer Nonlinear Programming (MINLP) problem and prove its NP-hardness. Extensive simulations demonstrate that RAILS achieves near-optimal performance, offering an efficient, real-time solution for adaptive SLA management in modern multi-domain networks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 3.4 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- RAG: 1.7 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Math: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Hardware: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9138
            </span>
            <a href="https://arxiv.org/abs/2504.07992" target="_blank" rel="noopener noreferrer">'Neural howlround' in large language models: a self-reinforcing bias phenomenon, and a dynamic attenuation solution</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Seth Drake | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language model (LLM)-driven AI systems may exhibit an inference failure mode we term `neural howlround,' a self-reinforcing cognitive loop where certain highly weighted inputs become dominant, leading to entrenched response patterns resistant to correction. This paper explores the mechanisms u</span>
            
            <span class="abstract-full" style="display: none;">Large language model (LLM)-driven AI systems may exhibit an inference failure mode we term `neural howlround,' a self-reinforcing cognitive loop where certain highly weighted inputs become dominant, leading to entrenched response patterns resistant to correction. This paper explores the mechanisms underlying this phenomenon, which is distinct from model collapse and biased salience weighting. We propose an attenuation-based correction mechanism that dynamically introduces counterbalancing adjustments and can restore adaptive reasoning, even in `locked-in' AI systems. Additionally, we discuss some other related effects arising from improperly managed reinforcement. Finally, we outline potential applications of this mitigation strategy for improving AI robustness in real-world decision-making tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 19.2 -->
                
            <!-- Medicine: 5.6 -->
                
            <!-- RAG: 3.1 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- 3D: 2.1 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- T2I: 1.7 -->
                
            <!-- Blockchain: 1.6 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9531
            </span>
            <a href="https://arxiv.org/abs/2504.08851" target="_blank" rel="noopener noreferrer">Mimic In-Context Learning for Multimodal Tasks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yuchu Jiang, Jiale Fu, Chenduo Hao, Xinting Hu, Yingzhe Peng, Xin Geng, Xu Yang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recently, In-context Learning (ICL) has become a significant inference paradigm in Large Multimodal Models (LMMs), utilizing a few in-context demonstrations (ICDs) to prompt LMMs for new tasks. However, the synergistic effects in multimodal data increase the sensitivity of ICL performance to the con</span>
            
            <span class="abstract-full" style="display: none;">Recently, In-context Learning (ICL) has become a significant inference paradigm in Large Multimodal Models (LMMs), utilizing a few in-context demonstrations (ICDs) to prompt LMMs for new tasks. However, the synergistic effects in multimodal data increase the sensitivity of ICL performance to the configurations of ICDs, stimulating the need for a more stable and general mapping function. Mathematically, in Transformer-based models, ICDs act as ``shift vectors'' added to the hidden states of query tokens. Inspired by this, we introduce Mimic In-Context Learning (MimIC) to learn stable and generalizable shift effects from ICDs. Specifically, compared with some previous shift vector-based methods, MimIC more strictly approximates the shift effects by integrating lightweight learnable modules into LMMs with four key enhancements: 1) inserting shift vectors after attention layers, 2) assigning a shift vector to each attention head, 3) making shift magnitude query-dependent, and 4) employing a layer-wise alignment loss. Extensive experiments on two LMMs (Idefics-9b and Idefics2-8b-base) across three multimodal tasks (VQAv2, OK-VQA, Captioning) demonstrate that MimIC outperforms existing shift vector-based methods. The code is available at https://github.com/Kamichanw/MimIC.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9531
            </span>
            <a href="https://arxiv.org/abs/2504.09345" target="_blank" rel="noopener noreferrer">MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under Resource Constraints</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yichao Yuan, Lin Ma, Nishil Talati | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Mixture of Experts (MoE) LLMs, characterized by their sparse activation patterns, offer a promising approach to scaling language models while avoiding proportionally increasing the inference cost. However, their large parameter sizes present deployment challenges in resource-constrained environments</span>
            
            <span class="abstract-full" style="display: none;">Mixture of Experts (MoE) LLMs, characterized by their sparse activation patterns, offer a promising approach to scaling language models while avoiding proportionally increasing the inference cost. However, their large parameter sizes present deployment challenges in resource-constrained environments with limited GPU memory capacity, as GPU memory is often insufficient to accommodate the full set of model weights. Consequently, typical deployments rely on CPU-GPU hybrid execution: the GPU handles compute-intensive GEMM operations, while the CPU processes the relatively lightweight attention mechanism. This setup introduces a key challenge: how to effectively optimize resource utilization across CPU and GPU? Prior work has designed system optimizations based on performance models with limited scope. Specifically, such models do not capture the complex interactions between hardware properties and system execution mechanisms. Therefore, previous approaches neither identify nor achieve the hardware limit.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.2 -->
                
            <!-- Medicine: 5.5 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Reinforcement Learning: 2.6 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Hardware: 1.2 -->
                
            <!-- SpikingNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9828
            </span>
            <a href="https://arxiv.org/abs/2407.19546" target="_blank" rel="noopener noreferrer">MMCLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Biao Wu, Yutong Xie, Zeyu Zhang, Minh Hieu Phan, Qi Chen, Ling Chen, Qi Wu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Vision-and-language pretraining (VLP) in the medical field utilizes contrastive learning on image-text pairs to achieve effective transfer across tasks. Yet, current VLP approaches with the masked modeling strategy face two challenges when applied to the medical domain. First, current models struggl</span>
            
            <span class="abstract-full" style="display: none;">Vision-and-language pretraining (VLP) in the medical field utilizes contrastive learning on image-text pairs to achieve effective transfer across tasks. Yet, current VLP approaches with the masked modeling strategy face two challenges when applied to the medical domain. First, current models struggle to accurately reconstruct key pathological features due to the scarcity of medical data. Second, most methods only adopt either paired image-text or image-only data, failing to exploit the combination of both paired and unpaired data. To this end, this paper proposes the MMCLIP (Masked Medical Contrastive Language-Image Pre-Training) framework to enhance pathological learning and feature learning via unpaired data. First, we introduce the attention-masked image modeling (AttMIM) and entity-driven masked language modeling module (EntMLM), which learns to reconstruct pathological visual and textual tokens via multi-modal feature interaction, thus improving medical-enhanced features. The AttMIM module masks a portion of the image features that are highly responsive to textual features. This allows MMCLIP to improve the reconstruction of highly similar image data in medicine efficiency. Second, our MMCLIP capitalizes unpaired data to enhance multimodal learning by introducing disease-kind prompts. The experimental results show that MMCLIP achieves SOTA for zero-shot and fine-tuning classification performance on five datasets. Our code will be available at https://github.com/White65534/MMCLIP.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.9 -->
                
            <!-- LLMs: 8.5 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Reinforcement Learning: 3.4 -->
                
            <!-- Federated Learning: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9899
            </span>
            <a href="https://arxiv.org/abs/2410.00332" target="_blank" rel="noopener noreferrer">Vision Language Models Know Law of Conservation without Understanding More-or-Less</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dezhi Luo, Haiyun Lyu, Qingying Gao, Haoran Sun, Yijiang Li, Hokin Deng | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Understanding law of conservation is a critical milestone in human cognitive development considered to be supported by the apprehension of quantitative concepts and the reversibility of operations. To assess whether this critical component of human intelligence has emerged in Vision Language Models,</span>
            
            <span class="abstract-full" style="display: none;">Understanding law of conservation is a critical milestone in human cognitive development considered to be supported by the apprehension of quantitative concepts and the reversibility of operations. To assess whether this critical component of human intelligence has emerged in Vision Language Models, we have curated the ConserveBench, a battery of 365 cognitive experiments across four dimensions of physical quantities: volume, solid quantity, length, and number. The former two involve transformational tasks which require reversibility understanding. The latter two involve non-transformational tasks which assess quantity understanding. Surprisingly, we find that while Vision Language Models are generally good at transformational tasks, they tend to fail at non-transformational tasks. There is a dissociation between understanding the reversibility of operations and understanding the concept of quantity, which both are believed to be the cornerstones of understanding law of conservation in humans. $\href{https://growing-ai-like-a-child.github.io/}{Website}$</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 19.5 -->
                
            <!-- Medicine: 5.4 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0375
            </span>
            <a href="https://arxiv.org/abs/2504.10415" target="_blank" rel="noopener noreferrer">LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Khoa D Doan, Chandan K Reddy | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypot</span>
            
            <span class="abstract-full" style="display: none;">Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 29.5 -->
                
            <!-- Medicine: 4.1 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- RAG: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Math: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0391
            </span>
            <a href="https://arxiv.org/abs/2504.10465" target="_blank" rel="noopener noreferrer">Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tao Zhang, Xiangtai Li, Zilong Huang, Yanwei Li, Weixian Lei, Xueqing Deng, Shihao Chen, Shunping Ji, Jiashi Feng | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In thi</span>
            
            <span class="abstract-full" style="display: none;">Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformer's fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at https://github.com/magic-research/Sa2VA.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- 3D: 2.1 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0555
            </span>
            <a href="https://arxiv.org/abs/2504.09924" target="_blank" rel="noopener noreferrer">Passive Channel Charting: Locating Passive Targets using Wi-Fi Channel State Information</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Florian Euchner, David Kellner, Phillip Stephan, Stephan ten Brink | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose passive channel charting, an extension of channel charting to passive target localization. As in conventional channel charting, we follow a dimensionality reduction approach to reconstruct a physically interpretable map of target positions from similarities in high-dimensional channel sta</span>
            
            <span class="abstract-full" style="display: none;">We propose passive channel charting, an extension of channel charting to passive target localization. As in conventional channel charting, we follow a dimensionality reduction approach to reconstruct a physically interpretable map of target positions from similarities in high-dimensional channel state information. We show that algorithms and neural network architectures developed in the context of channel charting with active mobile transmitters can be straightforwardly applied to the passive case, where we assume a scenario with static transmitters and receivers and a mobile target. We evaluate our method on a channel state information dataset collected indoors with a distributed setup of ESPARGOS Wi-Fi sensing antenna arrays. This scenario can be interpreted as either a multi-static or passive radar system. We demonstrate that passive channel charting outperforms a baseline based on classical triangulation in terms of localization accuracy. We discuss our results and highlight some unsolved issues related to the proposed concept.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 6.7 -->
                
            <!-- Quantum Computing: 5.3 -->
                
            <!-- Medicine: 4.9 -->
                
            <!-- Networks: 4.7 -->
                
            <!-- Reinforcement Learning: 3.4 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Attention: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1633
            </span>
            <a href="https://arxiv.org/abs/2411.07107" target="_blank" rel="noopener noreferrer">Training Neural Networks as Recognizers of Formal Languages</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alexandra Butoi, Ghazal Khalighinejad, Anej Svete, Josef Valvoda, Ryan Cotterell, Brian DuSell | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Characterizing the computational power of neural network architectures in terms of formal language theory remains a crucial line of research, as it describes lower and upper bounds on the reasoning capabilities of modern AI. However, when empirically testing these bounds, existing work often leaves </span>
            
            <span class="abstract-full" style="display: none;">Characterizing the computational power of neural network architectures in terms of formal language theory remains a crucial line of research, as it describes lower and upper bounds on the reasoning capabilities of modern AI. However, when empirically testing these bounds, existing work often leaves a discrepancy between experiments and the formal claims they are meant to support. The problem is that formal language theory pertains specifically to recognizers: machines that receive a string as input and classify whether it belongs to a language. On the other hand, it is common instead to evaluate language models on proxy tasks, e.g., language modeling or sequence-to-sequence transduction, that are similar in only an informal sense to the underlying theory. We correct this mismatch by training and evaluating neural networks directly as binary classifiers of strings, using a general method that can be applied to a wide variety of languages. As part of this, we extend an algorithm recently proposed by Sn{\ae}bjarnarson et al. (2025) for efficient length-controlled sampling of strings from regular languages. We provide results on a variety of languages across the Chomsky hierarchy for three neural architectures: a simple RNN, an LSTM, and a causally-masked transformer. We find that the RNN and LSTM often outperform the transformer, and that auxiliary training objectives such as language modeling can help, although no single objective uniformly improves performance across languages and architectures. Our contributions will facilitate theoretically sound empirical testing of language recognition claims in future work. We have released our datasets as a benchmark called FLaRe (Formal Language Recognition), along with our code.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.6 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Medicine: 3.3 -->
                
            <!-- Reinforcement Learning: 2.6 -->
                
            <!-- Robotics: 2.4 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1703
            </span>
            <a href="https://arxiv.org/abs/2504.10031" target="_blank" rel="noopener noreferrer">Using Reinforcement Learning to Integrate Subjective Wellbeing into Climate Adaptation Decision Making</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Arthur Vandervoort, Miguel Costa, Morten W. Petersen, Martin Drews, Sonja Haustein, Karyn Morrissey, Francisco C. Pereira | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Subjective wellbeing is a fundamental aspect of human life, influencing life expectancy and economic productivity, among others. Mobility plays a critical role in maintaining wellbeing, yet the increasing frequency and intensity of both nuisance and high-impact floods due to climate change are expec</span>
            
            <span class="abstract-full" style="display: none;">Subjective wellbeing is a fundamental aspect of human life, influencing life expectancy and economic productivity, among others. Mobility plays a critical role in maintaining wellbeing, yet the increasing frequency and intensity of both nuisance and high-impact floods due to climate change are expected to significantly disrupt access to activities and destinations, thereby affecting overall wellbeing. Addressing climate adaptation presents a complex challenge for policymakers, who must select and implement policies from a broad set of options with varying effects while managing resource constraints and uncertain climate projections. In this work, we propose a multi-modular framework that uses reinforcement learning as a decision-support tool for climate adaptation in Copenhagen, Denmark. Our framework integrates four interconnected components: long-term rainfall projections, flood modeling, transport accessibility, and wellbeing modeling. This approach enables decision-makers to identify spatial and temporal policy interventions that help sustain or enhance subjective wellbeing over time. By modeling climate adaptation as an open-ended system, our framework provides a structured framework for exploring and evaluating adaptation policy pathways. In doing so, it supports policymakers to make informed decisions that maximize wellbeing in the long run.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.0 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- RAG: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1955
            </span>
            <a href="https://arxiv.org/abs/2503.05439" target="_blank" rel="noopener noreferrer">An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for Robust Reasoning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Navdeep Kaur, Lachlan McPheat, Alessandra Russo, Anthony G Cohn, Pranava Madhyastha | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we examine the use of Conformal Language Modelling (CLM) alongside Answer Set Programming (ASP) to enhance the performance of standard open-weight LLMs on complex multi-step reasoning tasks. Using the StepGame dataset, which requires spatial reasoning, we apply CLM to generate sets of</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we examine the use of Conformal Language Modelling (CLM) alongside Answer Set Programming (ASP) to enhance the performance of standard open-weight LLMs on complex multi-step reasoning tasks. Using the StepGame dataset, which requires spatial reasoning, we apply CLM to generate sets of ASP programs from an LLM, providing statistical guarantees on the correctness of the outputs. Experimental results show that CLM significantly outperforms baseline models that use standard sampling methods, achieving substantial accuracy improvements across different levels of reasoning complexity. Additionally, the LLM-as-Judge metric enhances CLM's performance, especially in assessing structurally and logically correct ASP outputs. However, calibrating CLM with diverse calibration sets did not improve generalizability for tasks requiring much longer reasoning steps, indicating limitations in handling more complex tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 24.3 -->
                
            <!-- Medicine: 6.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.223
            </span>
            <a href="https://arxiv.org/abs/2503.08980" target="_blank" rel="noopener noreferrer">I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yuhang Liu, Dong Gong, Erdun Gao, Zhen Zhang, Biwei Huang, Mingming Gong, Anton van den Hengel, Javen Qinfeng Shi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The remarkable achievements of large language models (LLMs) have led many to conclude that they exhibit a form of intelligence. This is as opposed to explanations of their capabilities based on their ability to perform relatively simple manipulations of vast volumes of data. To illuminate the distin</span>
            
            <span class="abstract-full" style="display: none;">The remarkable achievements of large language models (LLMs) have led many to conclude that they exhibit a form of intelligence. This is as opposed to explanations of their capabilities based on their ability to perform relatively simple manipulations of vast volumes of data. To illuminate the distinction between these explanations, we introduce a novel generative model that generates tokens on the basis of human interpretable concepts represented as latent discrete variables. Under mild conditions, even when the mapping from the latent space to the observed space is non-invertible, we establish an identifiability result: the representations learned by LLMs through next-token prediction can be approximately modeled as the logarithm of the posterior probabilities of these latent discrete concepts, up to an invertible linear transformation. This theoretical finding not only provides evidence that LLMs capture underlying generative factors, but also strongly reinforces the linear representation hypothesis, which posits that LLMs learn linear representations of human-interpretable concepts. Empirically, we validate our theoretical results through evaluations on both simulation data and the Pythia, Llama, and DeepSeek model families.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 20.1 -->
                
            <!-- Reinforcement Learning: 4.4 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Medicine: 3.6 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2633
            </span>
            <a href="https://arxiv.org/abs/2504.10481" target="_blank" rel="noopener noreferrer">xVerify: Efficient Answer Verifier for Reasoning Model Evaluations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan Yang, Zhiyu Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle t</span>
            
            <span class="abstract-full" style="display: none;">With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 20.7 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Federated Learning: 2.3 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            <!-- Networks: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4137
            </span>
            <a href="https://arxiv.org/abs/2408.00672" target="_blank" rel="noopener noreferrer">ExpertAF: Expert Actionable Feedback from Video</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kumar Ashutosh, Tushar Nagarajan, Georgios Pavlakos, Kris Kitani, Kristen Grauman | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Feedback is essential for learning a new skill or improving one's current skill-level. However, current methods for skill-assessment from video only provide scores or compare demonstrations, leaving the burden of knowing what to do differently on the user. We introduce a novel method to generate act</span>
            
            <span class="abstract-full" style="display: none;">Feedback is essential for learning a new skill or improving one's current skill-level. However, current methods for skill-assessment from video only provide scores or compare demonstrations, leaving the burden of knowing what to do differently on the user. We introduce a novel method to generate actionable feedback (AF) from video of a person doing a physical activity, such as basketball or soccer. Our method takes a video demonstration and its accompanying 3D body pose and generates (1) free-form expert commentary describing what the person is doing well and what they could improve, and (2) a visual expert demonstration that incorporates the required corrections. We show how to leverage Ego-Exo4D's [29] videos of skilled activity and expert commentary together with a strong language model to create a weakly-supervised training dataset for this task, and we devise a multimodal video-language model to infer coaching feedback. Our method is able to reason across multi-modal input combinations to output full spectrum, actionable coaching-expert commentary, expert video retrieval, and expert pose generation-outperforming strong vision-language models on both established metrics and human preference studies.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.0 -->
                
            <!-- LLMs: 8.2 -->
                
            <!-- Quantum Computing: 2.9 -->
                
            <!-- Reinforcement Learning: 2.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4353
            </span>
            <a href="https://arxiv.org/abs/2504.09335" target="_blank" rel="noopener noreferrer">Efficient Implementation of Reinforcement Learning over Homomorphic Encryption</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jihoon Suh, Takashi Tanaka | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We investigate encrypted control policy synthesis over the cloud. While encrypted control implementations have been studied previously, we focus on the less explored paradigm of privacy-preserving control synthesis, which can involve heavier computations ideal for cloud outsourcing. We classify cont</span>
            
            <span class="abstract-full" style="display: none;">We investigate encrypted control policy synthesis over the cloud. While encrypted control implementations have been studied previously, we focus on the less explored paradigm of privacy-preserving control synthesis, which can involve heavier computations ideal for cloud outsourcing. We classify control policy synthesis into model-based, simulator-driven, and data-driven approaches and examine their implementation over fully homomorphic encryption (FHE) for privacy enhancements. A key challenge arises from comparison operations (min or max) in standard reinforcement learning algorithms, which are difficult to execute over encrypted data. This observation motivates our focus on Relative-Entropy-regularized reinforcement learning (RL) problems, which simplifies encrypted evaluation of synthesis algorithms due to their comparison-free structures. We demonstrate how linearly solvable value iteration, path integral control, and Z-learning can be readily implemented over FHE. We conduct a case study of our approach through numerical simulations of encrypted Z-learning in a grid world environment using the CKKS encryption scheme, showing convergence with acceptable approximation error. Our work suggests the potential for secure and efficient cloud-based reinforcement learning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.9 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 5.2 -->
                
            <!-- Reinforcement Learning: 5.1 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4602
            </span>
            <a href="https://arxiv.org/abs/2504.09192" target="_blank" rel="noopener noreferrer">Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhiyong Wang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The primary goal of my Ph.D. study is to develop provably efficient and practical algorithms for data-driven online sequential decision-making under uncertainty. My work focuses on reinforcement learning (RL), multi-armed bandits, and their applications, including recommendation systems, computer ne</span>
            
            <span class="abstract-full" style="display: none;">The primary goal of my Ph.D. study is to develop provably efficient and practical algorithms for data-driven online sequential decision-making under uncertainty. My work focuses on reinforcement learning (RL), multi-armed bandits, and their applications, including recommendation systems, computer networks, video analytics, and large language models (LLMs). Online learning methods, such as bandits and RL, have demonstrated remarkable success - ranging from outperforming human players in complex games like Atari and Go to advancing robotics, recommendation systems, and fine-tuning LLMs. Despite these successes, many established algorithms rely on idealized models that can fail under model misspecifications or adversarial perturbations, particularly in settings where accurate prior knowledge of the underlying model class is unavailable or where malicious users operate within dynamic systems. These challenges are pervasive in real-world applications, where robust and adaptive solutions are critical. Furthermore, while worst-case guarantees provide theoretical reliability, they often fail to capture instance-dependent performance, which can lead to more efficient and practical solutions. Another key challenge lies in generalizing to new, unseen environments, a crucial requirement for deploying these methods in dynamic and unpredictable settings. To address these limitations, my research aims to develop more efficient, robust, instance-adaptive, and generalizable online learning algorithms for both reinforcement learning and bandits. Towards this end, I focus on developing more efficient, robust, instance-adaptive, and generalizable for both general reinforcement learning (RL) and bandits.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.1 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- RAG: 3.0 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- HPO and AutoML: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5147
            </span>
            <a href="https://arxiv.org/abs/2412.08864" target="_blank" rel="noopener noreferrer">A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning Instructions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiankang Wang, Jianjun Xu, Xiaorui Wang, Yuxin Wang, Mengting Xing, Shancheng Fang, Zhineng Chen, Hongtao Xie, Yongdong Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Synthesizing high-quality reasoning data for continual training has been proven to be effective in enhancing the performance of Large Language Models (LLMs). However, previous synthetic approaches struggle to easily scale up data and incur high costs in the pursuit of high quality. In this paper, we</span>
            
            <span class="abstract-full" style="display: none;">Synthesizing high-quality reasoning data for continual training has been proven to be effective in enhancing the performance of Large Language Models (LLMs). However, previous synthetic approaches struggle to easily scale up data and incur high costs in the pursuit of high quality. In this paper, we propose the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable framework for high-quality reasoning data synthesis. Inspired by knowledge graphs, we extracted knowledge points from seed data and constructed a knowledge point relationships graph to explore their interconnections. By exploring the implicit relationships among knowledge, our method achieves $\times$255 data expansion. Furthermore, GSDP led by open-source models, achieves synthesis quality comparable to GPT-4-0613 while maintaining $\times$100 lower costs. To tackle the most challenging mathematical reasoning task, we present the GSDP-MATH dataset comprising over 1.91 million pairs of math problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on Mistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating the effectiveness of our method. The dataset and models will be released at https://github.com/Jayce1kk/GSDP.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 19.6 -->
                
            <!-- Medicine: 10.5 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- Math: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5645
            </span>
            <a href="https://arxiv.org/abs/2409.07703" target="_blank" rel="noopener noreferrer">DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks </span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 25.5 -->
                
            <!-- Medicine: 11.6 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- RAG: 2.5 -->
                
            <!-- Blockchain: 1.5 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6469
            </span>
            <a href="https://arxiv.org/abs/2504.08958" target="_blank" rel="noopener noreferrer">Generating Planning Feedback for Open-Ended Programming Exercises with LLMs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mehmet Arif Demirta\c{s}, Claire Zheng, Max Fowler, Kathryn Cunningham | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">To complete an open-ended programming exercise, students need to both plan a high-level solution and implement it using the appropriate syntax. However, these problems are often autograded on the correctness of the final submission through test cases, and students cannot get feedback on their planni</span>
            
            <span class="abstract-full" style="display: none;">To complete an open-ended programming exercise, students need to both plan a high-level solution and implement it using the appropriate syntax. However, these problems are often autograded on the correctness of the final submission through test cases, and students cannot get feedback on their planning process. Large language models (LLM) may be able to generate this feedback by detecting the overall code structure even for submissions with syntax errors. To this end, we propose an approach that detects which high-level goals and patterns (i.e. programming plans) exist in a student program with LLMs. We show that both the full GPT-4o model and a small variant (GPT-4o-mini) can detect these plans with remarkable accuracy, outperforming baselines inspired by conventional approaches to code analysis. We further show that the smaller, cost-effective variant (GPT-4o-mini) achieves results on par with state-of-the-art (GPT-4o) after fine-tuning, creating promising implications for smaller models for real-time grading. These smaller models can be incorporated into autograders for open-ended code-writing exercises to provide feedback for students' implicit planning skills, even when their program is syntactically incorrect. Furthermore, LLMs may be useful in providing feedback for problems in other domains where students start with a set of high-level solution steps and iteratively compute the output, such as math and physics problems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.4 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6533
            </span>
            <a href="https://arxiv.org/abs/2504.09138" target="_blank" rel="noopener noreferrer">White-Box AI Model: Next Frontier of Wireless Communications</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiayao Yang, Jiayi Zhang, Bokai Xu, Jiakang Zheng, Zhilong Liu, Ziheng Liu, Dusit Niyato, M\'erouane Debbah, Zhu Han, Bo Ai | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">White-box AI (WAI), or explainable AI (XAI) model, a novel tool to achieve the reasoning behind decisions and predictions made by the AI algorithms, makes it more understandable and transparent. It offers a new approach to address key challenges of interpretability and mathematical validation in tra</span>
            
            <span class="abstract-full" style="display: none;">White-box AI (WAI), or explainable AI (XAI) model, a novel tool to achieve the reasoning behind decisions and predictions made by the AI algorithms, makes it more understandable and transparent. It offers a new approach to address key challenges of interpretability and mathematical validation in traditional black-box models. In this paper, WAI-aided wireless communication systems are proposed and investigated thoroughly to utilize the promising capabilities. First, we introduce the fundamental principles of WAI. Then, a detailed comparison between WAI and traditional black-box model is conducted in terms of optimization objectives and architecture design, with a focus on deep neural networks (DNNs) and transformer networks. Furthermore, in contrast to the traditional black-box methods, WAI leverages theory-driven causal modeling and verifiable optimization paths, thereby demonstrating potential advantages in areas such as signal processing and resource allocation. Finally, we outline future research directions for the integration of WAI in wireless communication systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.4 -->
                
            <!-- Medicine: 6.6 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Math: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7308
            </span>
            <a href="https://arxiv.org/abs/2504.09802" target="_blank" rel="noopener noreferrer">Training Small Reasoning LLMs with Cognitive Preference Alignment</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The reasoning capabilities of large language models (LLMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need to explore strategies to train effective reasoning LLMs</span>
            
            <span class="abstract-full" style="display: none;">The reasoning capabilities of large language models (LLMs), such as OpenAI's o1 and DeepSeek-R1, have seen substantial advancements through deep thinking. However, these enhancements come with significant resource demands, underscoring the need to explore strategies to train effective reasoning LLMs with far fewer parameters. A critical challenge is that smaller models have different capacities and cognitive trajectories than their larger counterparts. Hence, direct distillation of chain-of-thought (CoT) results from large LLMs to smaller ones can be sometimes ineffective and requires a huge amount of annotated data. In this paper, we introduce a novel framework called Critique-Rethink-Verify (CRV), designed for training smaller yet powerful reasoning LLMs. Our CRV framework consists of multiple LLM agents, each specializing in unique abilities: (i) critiquing the CoTs according to the cognitive capabilities of smaller models, (ii) rethinking and refining these CoTs based on the critiques, and (iii) verifying the correctness of the refined results. We further propose the cognitive preference optimization (CogPO) algorithm to enhance the reasoning abilities of smaller models by aligning thoughts of these models with their cognitive capacities. Comprehensive evaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV and CogPO, which outperforms other training methods by a large margin.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 26.1 -->
                
            <!-- Medicine: 5.8 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7633
            </span>
            <a href="https://arxiv.org/abs/2504.08219" target="_blank" rel="noopener noreferrer">VL-UR: Vision-Language-guided Universal Restoration of Images Degraded by Adverse Weather Conditions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ziyan Liu, Yuxu Lu, Huashan Yu, Dong yang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Image restoration is critical for improving the quality of degraded images, which is vital for applications like autonomous driving, security surveillance, and digital content enhancement. However, existing methods are often tailored to specific degradation scenarios, limiting their adaptability to </span>
            
            <span class="abstract-full" style="display: none;">Image restoration is critical for improving the quality of degraded images, which is vital for applications like autonomous driving, security surveillance, and digital content enhancement. However, existing methods are often tailored to specific degradation scenarios, limiting their adaptability to the diverse and complex challenges in real-world environments. Moreover, real-world degradations are typically non-uniform, highlighting the need for adaptive and intelligent solutions. To address these issues, we propose a novel vision-language-guided universal restoration (VL-UR) framework. VL-UR leverages a zero-shot contrastive language-image pre-training (CLIP) model to enhance image restoration by integrating visual and semantic information. A scene classifier is introduced to adapt CLIP, generating high-quality language embeddings aligned with degraded images while predicting degraded types for complex scenarios. Extensive experiments across eleven diverse degradation settings demonstrate VL-UR's state-of-the-art performance, robustness, and adaptability. This positions VL-UR as a transformative solution for modern image restoration challenges in dynamic, real-world environments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 8.9 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- RAG: 1.9 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Reinforcement Learning: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7757
            </span>
            <a href="https://arxiv.org/abs/2411.17459" target="_blank" rel="noopener noreferrer">WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, Li Yuan | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes</span>
            
            <span class="abstract-full" style="display: none;">Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.1 -->
                
            <!-- Medicine: 6.1 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Math: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7786
            </span>
            <a href="https://arxiv.org/abs/2504.10231" target="_blank" rel="noopener noreferrer">A Model Zoo of Vision Transformers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Damian Falk, L\'eo Meynent, Florence Pfammatter, Konstantin Sch\"urholt, Damian Borth | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The availability of large, structured populations of neural networks - called 'model zoos' - has led to the development of a multitude of downstream tasks ranging from model analysis, to representation learning on model weights or generative modeling of neural network parameters. However, existing m</span>
            
            <span class="abstract-full" style="display: none;">The availability of large, structured populations of neural networks - called 'model zoos' - has led to the development of a multitude of downstream tasks ranging from model analysis, to representation learning on model weights or generative modeling of neural network parameters. However, existing model zoos are limited in size and architecture and neglect the transformer, which is among the currently most successful neural network architectures. We address this gap by introducing the first model zoo of vision transformers (ViT). To better represent recent training approaches, we develop a new blueprint for model zoo generation that encompasses both pre-training and fine-tuning steps, and publish 250 unique models. They are carefully generated with a large span of generating factors, and their diversity is validated using a thorough choice of weight-space and behavioral metrics. To further motivate the utility of our proposed dataset, we suggest multiple possible applications grounded in both extensive exploratory experiments and a number of examples from the existing literature. By extending previous lines of similar work, our model zoo allows researchers to push their model population-based methods from the small model regime to state-of-the-art architectures. We make our model zoo available at github.com/ModelZoos/ViTModelZoo.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 13.2 -->
                
            <!-- LLMs: 10.7 -->
                
            <!-- Reinforcement Learning: 2.9 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8533
            </span>
            <a href="https://arxiv.org/abs/2412.20211" target="_blank" rel="noopener noreferrer">Generative Regression Based Watch Time Prediction for Short-Video Recommendation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hongxu Ma, Kai Tian, Tao Zhang, Xuefeng Zhang, Han Zhou, Chunjie Chen, Han Li, Jihong Guan, Shuigeng Zhou | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Watch time prediction (WTP) has emerged as a pivotal task in short video recommendation systems, designed to quantify user engagement through continuous interaction modeling. Predicting users' watch times on videos often encounters fundamental challenges, including wide value ranges and imbalanced d</span>
            
            <span class="abstract-full" style="display: none;">Watch time prediction (WTP) has emerged as a pivotal task in short video recommendation systems, designed to quantify user engagement through continuous interaction modeling. Predicting users' watch times on videos often encounters fundamental challenges, including wide value ranges and imbalanced data distributions, which can lead to significant estimation bias when directly applying regression techniques. Recent studies have attempted to address these issues by converting the continuous watch time estimation into an ordinal regression task. While these methods demonstrate partial effectiveness, they exhibit notable limitations: (1) the discretization process frequently relies on bucket partitioning, inherently reducing prediction flexibility and accuracy and (2) the interdependencies among different partition intervals remain underutilized, missing opportunities for effective error correction.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 5.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- RAG: 2.2 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Blockchain: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- Math: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.864
            </span>
            <a href="https://arxiv.org/abs/2504.09685" target="_blank" rel="noopener noreferrer">Can LLMs Revolutionize the Design of Explainable and Efficient TinyML Models?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Christophe El Zeinaty, Wassim Hamidouche, Glenn Herrou, Daniel Menard, Merouane Debbah | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper introduces a novel framework for designing efficient neural network architectures specifically tailored to tiny machine learning (TinyML) platforms. By leveraging large language models (LLMs) for neural architecture search (NAS), a vision transformer (ViT)-based knowledge distillation (KD</span>
            
            <span class="abstract-full" style="display: none;">This paper introduces a novel framework for designing efficient neural network architectures specifically tailored to tiny machine learning (TinyML) platforms. By leveraging large language models (LLMs) for neural architecture search (NAS), a vision transformer (ViT)-based knowledge distillation (KD) strategy, and an explainability module, the approach strikes an optimal balance between accuracy, computational efficiency, and memory usage. The LLM-guided search explores a hierarchical search space, refining candidate architectures through Pareto optimization based on accuracy, multiply-accumulate operations (MACs), and memory metrics. The best-performing architectures are further fine-tuned using logits-based KD with a pre-trained ViT-B/16 model, which enhances generalization without increasing model size. Evaluated on the CIFAR-100 dataset and deployed on an STM32H7 microcontroller (MCU), the three proposed models, LMaNet-Elite, LMaNet-Core, and QwNet-Core, achieve accuracy scores of 74.50%, 74.20% and 73.00%, respectively. All three models surpass current state-of-the-art (SOTA) models, such as MCUNet-in3/in4 (69.62% / 72.86%) and XiNet (72.27%), while maintaining a low computational cost of less than 100 million MACs and adhering to the stringent 320 KB static random-access memory (SRAM) constraint. These results demonstrate the efficiency and performance of the proposed framework for TinyML platforms, underscoring the potential of combining LLM-driven search, Pareto optimization, KD, and explainability to develop accurate, efficient, and interpretable models. This approach opens new possibilities in NAS, enabling the design of efficient architectures specifically suited for TinyML.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 12.8 -->
                
            <!-- LLMs: 10.5 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Math: 1.0 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9091
            </span>
            <a href="https://arxiv.org/abs/2409.01115" target="_blank" rel="noopener noreferrer">Time series classification with random convolution kernels: pooling operators and input representations matter</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mouhamadou Mansour Lo, Gildas Morvan, Mathieu Rossi, Fabrice Morganti, David Mercier | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This article presents a new approach based on MiniRocket, called SelF-Rocket, for fast time series classification (TSC). Unlike existing approaches based on random convolution kernels, it dynamically selects the best couple of input representations and pooling operator during the training process. S</span>
            
            <span class="abstract-full" style="display: none;">This article presents a new approach based on MiniRocket, called SelF-Rocket, for fast time series classification (TSC). Unlike existing approaches based on random convolution kernels, it dynamically selects the best couple of input representations and pooling operator during the training process. SelF-Rocket achieves state-of-the-art accuracy on the University of California Riverside (UCR) TSC benchmark datasets.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 10.0 -->
                
            <!-- LLMs: 8.7 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Reinforcement Learning: 2.4 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 2.2 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Blockchain: 1.5 -->
                
            <!-- SpikingNN: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0266
            </span>
            <a href="https://arxiv.org/abs/2504.09382" target="_blank" rel="noopener noreferrer">Modeling Scrap Composition in Electric Arc and Basic Oxygen Furnaces</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yiqing Zhou, Karsten Naert, Dirk Nuyens | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This article aims to determine the composition of scrap (recycled material) used in an Electric Arc Furnace (EAF) or basic Oxygen Furnace (BOF) based on the assumption of mass balance. Accurate knowledge of this composition can increase the usage of recycled material to produce steel, reducing the n</span>
            
            <span class="abstract-full" style="display: none;">This article aims to determine the composition of scrap (recycled material) used in an Electric Arc Furnace (EAF) or basic Oxygen Furnace (BOF) based on the assumption of mass balance. Accurate knowledge of this composition can increase the usage of recycled material to produce steel, reducing the need for raw ore extraction and minimizing environmental impact by conserving natural resources and lowering carbon emissions. The study develops two models to describe the behavior of elements in the EAF or BOF process. A linear state space model is used for elements transferring completely from scrap to steel, while a non-linear state space model is applied to elements moving into both steel and slag. The Kalman filter and unscented Kalman filter are employed to approximate these models, respectively. Importantly, the models leverage only data already collected as part of the standard production process, avoiding the need for additional measurements that are often costly. This article outlines the formulation of both models, the algorithms used, and discusses the hyperparameters involved. We provide practical suggestions on how to choose appropriate hyperparameters based on expert knowledge and historical data. The models are applied to real BOF data. Cu and Cr are chosen as examples for linear and non-linear models, respectively. The results show that both models can reconstruct the composition of scrap for these elements. The findings provide valuable insights for improving process control and ensuring product quality in steelmaking.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 8.8 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Reinforcement Learning: 2.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0392
            </span>
            <a href="https://arxiv.org/abs/2504.07392" target="_blank" rel="noopener noreferrer">ID-Booth: Identity-consistent Face Generation with Diffusion Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Darian Toma\v{s}evi\'c, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir \v{S}truc, Peter Peer | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to</span>
            
            <span class="abstract-full" style="display: none;">Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at https://github.com/dariant/ID-Booth.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.0 -->
                
            <!-- Medicine: 10.6 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Quantum Computing: 2.0 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0434
            </span>
            <a href="https://arxiv.org/abs/2504.08684" target="_blank" rel="noopener noreferrer">A Dataset For Computational Reproducibility</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: L\'azaro Costa, Susana Barbosa, J\'acome Cunha | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Ensuring the reproducibility of scientific work is crucial as it allows the consistent verification of scientific claims and facilitates the advancement of knowledge by providing a reliable foundation for future research. However, scientific work based on computational artifacts, such as scripts for</span>
            
            <span class="abstract-full" style="display: none;">Ensuring the reproducibility of scientific work is crucial as it allows the consistent verification of scientific claims and facilitates the advancement of knowledge by providing a reliable foundation for future research. However, scientific work based on computational artifacts, such as scripts for statistical analysis or software prototypes, faces significant challenges in achieving reproducibility. These challenges are based on the variability of computational environments, rapid software evolution, and inadequate documentation of procedures. As a consequence, such artifacts often are not (easily) reproducible, undermining the credibility of scientific findings.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Math: 2.6 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- SpikingNN: 1.4 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0472
            </span>
            <a href="https://arxiv.org/abs/2504.08817" target="_blank" rel="noopener noreferrer">Exploring utilization of generative AI for research and education in data-driven materials science</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Takahiro Misawa, Ai Koizumi, Ryo Tamura, Kazuyoshi Yoshimi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative AI has recently had a profound impact on various fields, including daily life, research, and education. To explore its efficient utilization in data-driven materials science, we organized a hackathon -- AIMHack2024 -- in July 2024. In this hackathon, researchers from fields such as materi</span>
            
            <span class="abstract-full" style="display: none;">Generative AI has recently had a profound impact on various fields, including daily life, research, and education. To explore its efficient utilization in data-driven materials science, we organized a hackathon -- AIMHack2024 -- in July 2024. In this hackathon, researchers from fields such as materials science, information science, bioinformatics, and condensed matter physics worked together to explore how generative AI can facilitate research and education. Based on the results of the hackathon, this paper presents topics related to (1) conducting AI-assisted software trials, (2) building AI tutors for software, and (3) developing GUI applications for software. While generative AI continues to evolve rapidly, this paper provides an early record of its application in data-driven materials science and highlights strategies for integrating AI into research and education.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.3 -->
                
            <!-- Medicine: 9.0 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- RAG: 2.2 -->
                
            <!-- Blockchain: 1.7 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Reinforcement Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0712
            </span>
            <a href="https://arxiv.org/abs/2504.08747" target="_blank" rel="noopener noreferrer">GridMind: A Multi-Agent NLP Framework for Unified, Cross-Modal NFL Data Insights</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jordan Chipka, Chris Moyer, Clay Troyer, Tyler Fuelling, Jeremy Hochstedler | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The rapid growth of big data and advancements in computational techniques have significantly transformed sports analytics. However, the diverse range of data sources -- including structured statistics, semi-structured formats like sensor data, and unstructured media such as written articles, audio, </span>
            
            <span class="abstract-full" style="display: none;">The rapid growth of big data and advancements in computational techniques have significantly transformed sports analytics. However, the diverse range of data sources -- including structured statistics, semi-structured formats like sensor data, and unstructured media such as written articles, audio, and video -- creates substantial challenges in extracting actionable insights. These various formats, often referred to as multimodal data, require integration to fully leverage their potential. Conventional systems, which typically prioritize structured data, face limitations when processing and combining these diverse content types, reducing their effectiveness in real-time sports analysis.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.2 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 5.1 -->
                
            <!-- RAG: 3.2 -->
                
            <!-- Blockchain: 2.1 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0838
            </span>
            <a href="https://arxiv.org/abs/2504.10112" target="_blank" rel="noopener noreferrer">Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Andreas Happe, J\"urgen Cito | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have emerged as a powerful approach for driving offensive penetration-testing tooling. This paper analyzes the methodology and benchmarking practices used for evaluating Large Language Model (LLM)-driven attacks, focusing on offensive uses of LLMs in cybersecurity. We re</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have emerged as a powerful approach for driving offensive penetration-testing tooling. This paper analyzes the methodology and benchmarking practices used for evaluating Large Language Model (LLM)-driven attacks, focusing on offensive uses of LLMs in cybersecurity. We review 16 research papers detailing 15 prototypes and their respective testbeds.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 34.9 -->
                
            <!-- Medicine: 5.7 -->
                
            <!-- RAG: 3.3 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Blockchain: 2.6 -->
                
            <!-- 3D: 2.2 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- HPO and AutoML: 1.4 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- Datasets: 1.2 -->
                
            <!-- Fuzzy Logic: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1417
            </span>
            <a href="https://arxiv.org/abs/2406.10244" target="_blank" rel="noopener noreferrer">GLINT-RU: Gated Lightweight Intelligent Recurrent Units for Sequential Recommender Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sheng Zhang, Maolin Wang, Wanyu Wang, Jingtong Gao, Xiangyu Zhao, Yu Yang, Xuetao Wei, Zitao Liu, Tong Xu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Transformer-based models have gained significant traction in sequential recommender systems (SRSs) for their ability to capture user-item interactions effectively. However, these models often suffer from high computational costs and slow inference. Meanwhile, existing efficient SRS approaches strugg</span>
            
            <span class="abstract-full" style="display: none;">Transformer-based models have gained significant traction in sequential recommender systems (SRSs) for their ability to capture user-item interactions effectively. However, these models often suffer from high computational costs and slow inference. Meanwhile, existing efficient SRS approaches struggle to embed high-quality semantic and positional information into latent representations. To tackle these challenges, this paper introduces GLINT-RU, a lightweight and efficient SRS leveraging a single-layer dense selective Gated Recurrent Units (GRU) module to accelerate inference. By incorporating a dense selective gate, GLINT-RU adaptively captures temporal dependencies and fine-grained positional information, generating high-quality latent representations. Additionally, a parallel mixing block infuses fine-grained positional features into user-item interactions, enhancing both recommendation quality and efficiency. Extensive experiments on three datasets demonstrate that GLINT-RU achieves superior prediction accuracy and inference speed, outperforming baselines based on RNNs, Transformers, MLPs, and SSMs. These results establish GLINT-RU as a powerful and efficient solution for SRSs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 10.1 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- RAG: 3.2 -->
                
            <!-- 3D: 2.8 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Blockchain: 2.0 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2165
            </span>
            <a href="https://arxiv.org/abs/2504.08793" target="_blank" rel="noopener noreferrer">A Constraint Programming Model For Serial Batch Scheduling With Minimum Batch Size</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jorge A. Huertas, Pascal Van Hentenryck | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In serial batch (s-batch) scheduling, jobs are grouped in batches and processed sequentially within their batch. This paper considers multiple parallel machines, nonidentical job weights and release times, and sequence-dependent setup times between batches of different families. Although s-batch has</span>
            
            <span class="abstract-full" style="display: none;">In serial batch (s-batch) scheduling, jobs are grouped in batches and processed sequentially within their batch. This paper considers multiple parallel machines, nonidentical job weights and release times, and sequence-dependent setup times between batches of different families. Although s-batch has been widely studied in the literature, very few papers have taken into account a minimum batch size, typical in practical settings such as semiconductor manufacturing and the metal industry. The problem with this minimum batch size requirement has been mostly tackled with dynamic programming and meta-heuristics, and no article has ever used constraint programming (CP) to do so. This paper fills this gap by proposing, for the first time, a CP model for s-batching with minimum batch size. The computational experiments on standard cases compare the CP model with two existing mixed-integer programming (MIP) models from the literature. The results demonstrate the versatility of the proposed CP model to handle multiple variations of s-batching; and its ability to produce, in large instances, better solutions than the MIP models faster.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.3 -->
                
            <!-- Medicine: 10.6 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Math: 1.3 -->
                
            <!-- GNN: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3562
            </span>
            <a href="https://arxiv.org/abs/2408.13364" target="_blank" rel="noopener noreferrer">Reconciling Different Theories of Learning with an Agent-based Model of Procedural Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sina Rismanchian, Shayan Doroudi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Computational models of human learning can play a significant role in enhancing our knowledge about nuances in theoretical and qualitative learning theories and frameworks. There are many existing frameworks in educational settings that have shown to be verified using empirical studies, but at times</span>
            
            <span class="abstract-full" style="display: none;">Computational models of human learning can play a significant role in enhancing our knowledge about nuances in theoretical and qualitative learning theories and frameworks. There are many existing frameworks in educational settings that have shown to be verified using empirical studies, but at times we find these theories make conflicting claims or recommendations for instruction. In this study, we propose a new computational model of human learning, Procedural ABICAP, that reconciles the ICAP, Knowledge-Learning-Instruction (KLI), and cognitive load theory (CLT) frameworks for learning procedural knowledge. ICAP assumes that constructive learning generally yields better learning outcomes, while theories such as KLI and CLT claim that this is not always true. We suppose that one reason for this may be that ICAP is primarily used for conceptual learning and is underspecified as a framework for thinking about procedural learning. We show how our computational model, both by design and through simulations, can be used to reconcile different results in the literature. More generally, we position our computational model as an executable theory of learning that can be used to simulate various educational settings.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.9 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 5.4 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4612
            </span>
            <a href="https://arxiv.org/abs/2504.09978" target="_blank" rel="noopener noreferrer">New exponential law for real networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mikhail Tuzhilin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this article we have shown that the distributions of ksi satisfy an exponential law for real networks while the distributions of ksi for random networks are bell-shaped and closer to the normal distribution. The ksi distributions for Barabasi-Albert and Watts-Strogatz networks are similar to the </span>
            
            <span class="abstract-full" style="display: none;">In this article we have shown that the distributions of ksi satisfy an exponential law for real networks while the distributions of ksi for random networks are bell-shaped and closer to the normal distribution. The ksi distributions for Barabasi-Albert and Watts-Strogatz networks are similar to the ksi distributions for random networks (bell-shaped) for most parameters, but when these parameters become small enough, the Barabasi-Albert and Watts-Strogatz networks become more realistic with respect to the ksi distributions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 10.9 -->
                
            <!-- LLMs: 5.7 -->
                
            <!-- Quantum Computing: 4.9 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5586
            </span>
            <a href="https://arxiv.org/abs/2502.02777" target="_blank" rel="noopener noreferrer">Space-bounded online Kolmogorov complexity is additive</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Bruno Bauwens, Maria Marchenko | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The even online Kolmogorov complexity of a string $x = x_1 x_2 \cdots x_{n}$ is the minimal length of a program that for all $i\le n/2$, on input $x_1x_3 \cdots x_{2i-1}$ outputs $x_{2i}$. The odd complexity is defined similarly. The sum of the odd and even complexities is called the dialogue comple</span>
            
            <span class="abstract-full" style="display: none;">The even online Kolmogorov complexity of a string $x = x_1 x_2 \cdots x_{n}$ is the minimal length of a program that for all $i\le n/2$, on input $x_1x_3 \cdots x_{2i-1}$ outputs $x_{2i}$. The odd complexity is defined similarly. The sum of the odd and even complexities is called the dialogue complexity.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.6 -->
                
            <!-- LLMs: 5.4 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 3.2 -->
                
            <!-- Math: 2.8 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5621
            </span>
            <a href="https://arxiv.org/abs/2504.03767" target="_blank" rel="noopener noreferrer">MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Brandon Radosevich, John Halloran | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">To reduce development overhead and enable seamless integration between potential components comprising any given generative AI application, the Model Context Protocol (MCP) (Anthropic, 2024) has recently been released and subsequently widely adopted. The MCP is an open protocol that standardizes API</span>
            
            <span class="abstract-full" style="display: none;">To reduce development overhead and enable seamless integration between potential components comprising any given generative AI application, the Model Context Protocol (MCP) (Anthropic, 2024) has recently been released and subsequently widely adopted. The MCP is an open protocol that standardizes API calls to large language models (LLMs), data sources, and agentic tools. By connecting multiple MCP servers, each defined with a set of tools, resources, and prompts, users are able to define automated workflows fully driven by LLMs. However, we show that the current MCP design carries a wide range of security risks for end users. In particular, we demonstrate that industry-leading LLMs may be coerced into using MCP tools to compromise an AI developer's system through various attacks, such as malicious code execution, remote access control, and credential theft. To proactively mitigate these and related attacks, we introduce a safety auditing tool, MCPSafetyScanner, the first agentic tool to assess the security of an arbitrary MCP server. MCPScanner uses several agents to (a) automatically determine adversarial samples given an MCP server's tools and resources; (b) search for related vulnerabilities and remediations based on those samples; and (c) generate a security report detailing all findings. Our work highlights serious security issues with general-purpose agentic workflows while also providing a proactive tool to audit MCP server safety and address detected vulnerabilities before deployment.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.8 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- Blockchain: 1.9 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- RAG: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5968
            </span>
            <a href="https://arxiv.org/abs/2504.08296" target="_blank" rel="noopener noreferrer">Generative AI for Film Creation: A Survey of Recent Advances</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ruihan Zhang, Borou Yu, Jiajian Min, Yetong Xin, Zheng Wei, Juncheng Nemo Shi, Mingzhen Huang, Xianghao Kong, Nix Liu Xin, Shanshan Jiang, Praagya Bahuguna, Mark Chan, Khushi Hora, Lijian Yang, Yongqi Liang, Runhe Bian, Yunlei Liu, Isabela Campillo Valencia, Patricia Morales Tredinick, Ilia Kozlov, Sijia Jiang, Peiwen Huang, Na Chen, Xuanxuan Liu, Anyi Rao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative AI (GenAI) is transforming filmmaking, equipping artists with tools like text-to-image and image-to-video diffusion, neural radiance fields, avatar generation, and 3D synthesis. This paper examines the adoption of these technologies in filmmaking, analyzing workflows from recent AI-driven</span>
            
            <span class="abstract-full" style="display: none;">Generative AI (GenAI) is transforming filmmaking, equipping artists with tools like text-to-image and image-to-video diffusion, neural radiance fields, avatar generation, and 3D synthesis. This paper examines the adoption of these technologies in filmmaking, analyzing workflows from recent AI-driven films to understand how GenAI contributes to character creation, aesthetic styling, and narration. We explore key strategies for maintaining character consistency, achieving stylistic coherence, and ensuring motion continuity. Additionally, we highlight emerging trends such as the growing use of 3D generation and the integration of real footage with AI-generated elements.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.7 -->
                
            <!-- Medicine: 11.0 -->
                
            <!-- Quantum Computing: 4.7 -->
                
            <!-- RAG: 2.2 -->
                
            <!-- Blockchain: 2.0 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6462
            </span>
            <a href="https://arxiv.org/abs/2501.06019" target="_blank" rel="noopener noreferrer">BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, Weihao Xuan, Junjue Wang, Xinlei Shao, Yimin Wei, Junshi Xia, Cuiling Lan, Konrad Schindler, Naoto Yokoya | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Disaster events occur around the world and cause significant damage to human life and property. Earth observation (EO) data enables rapid and comprehensive building damage assessment (BDA), an essential capability in the aftermath of a disaster to reduce human casualties and to inform disaster relie</span>
            
            <span class="abstract-full" style="display: none;">Disaster events occur around the world and cause significant damage to human life and property. Earth observation (EO) data enables rapid and comprehensive building damage assessment (BDA), an essential capability in the aftermath of a disaster to reduce human casualties and to inform disaster relief efforts. Recent research focuses on the development of AI models to achieve accurate mapping of unseen disaster events, mostly using optical EO data. However, solutions based on optical data are limited to clear skies and daylight hours, preventing a prompt response to disasters. Integrating multimodal (MM) EO data, particularly the combination of optical and SAR imagery, makes it possible to provide all-weather, day-and-night disaster responses. Despite this potential, the development of robust multimodal AI models has been constrained by the lack of suitable benchmark datasets. In this paper, we present a BDA dataset using veRy-hIGH-resoluTion optical and SAR imagery (BRIGHT) to support AI-based all-weather disaster response. To the best of our knowledge, BRIGHT is the first open-access, globally distributed, event-diverse MM dataset specifically curated to support AI-based disaster response. It covers five types of natural disasters and two types of man-made disasters across 14 regions worldwide, with a particular focus on developing countries where external assistance is most needed. The optical and SAR imagery in BRIGHT, with a spatial resolution between 0.3-1 meters, provides detailed representations of individual buildings, making it ideal for precise BDA. In our experiments, we have tested seven advanced AI models trained with our BRIGHT to validate the transferability and robustness. The dataset and code are available at https://github.com/ChenHongruixuan/BRIGHT. BRIGHT also serves as the official dataset for the 2025 IEEE GRSS Data Fusion Contest.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 14.0 -->
                
            <!-- LLMs: 10.7 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Math: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6483
            </span>
            <a href="https://arxiv.org/abs/2504.09026" target="_blank" rel="noopener noreferrer">Detecting Instruction Fine-tuning Attack on Language Models with Influence Function</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiawei Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Instruction fine-tuning attacks pose a significant threat to large language models (LLMs) by subtly embedding poisoned data in fine-tuning datasets, which can trigger harmful or unintended responses across a range of tasks. This undermines model alignment and poses security risks in real-world deplo</span>
            
            <span class="abstract-full" style="display: none;">Instruction fine-tuning attacks pose a significant threat to large language models (LLMs) by subtly embedding poisoned data in fine-tuning datasets, which can trigger harmful or unintended responses across a range of tasks. This undermines model alignment and poses security risks in real-world deployment. In this work, we present a simple and effective approach to detect and mitigate such attacks using influence functions, a classical statistical tool adapted for machine learning interpretation. Traditionally, the high computational costs of influence functions have limited their application to large models and datasets. The recent Eigenvalue-Corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation method enables efficient influence score computation, making it feasible for large-scale analysis.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 18.5 -->
                
            <!-- Medicine: 9.6 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- RAG: 2.4 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6685
            </span>
            <a href="https://arxiv.org/abs/2411.13055" target="_blank" rel="noopener noreferrer">Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jared Fernandez, Luca Wehrstedt, Leonid Shamis, Mostafa Elhoushi, Kalyan Saladi, Yonatan Bisk, Emma Strubell, Jacob Kahn | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Dramatic increases in the capabilities of neural network models in recent years are driven by scaling model size, training data, and corresponding computational resources. To develop the exceedingly large networks required in modern applications, such as large language models (LLMs), model training </span>
            
            <span class="abstract-full" style="display: none;">Dramatic increases in the capabilities of neural network models in recent years are driven by scaling model size, training data, and corresponding computational resources. To develop the exceedingly large networks required in modern applications, such as large language models (LLMs), model training is distributed across tens of thousands of hardware accelerators (e.g. GPUs), requiring orchestration of computation and communication across large computing clusters. In this work, we demonstrate that careful consideration of hardware configuration and parallelization strategy is critical for effective (i.e. compute- and cost-efficient) scaling of model size, training data, and total computation. We conduct an extensive empirical study of the performance of large-scale LLM training workloads across model size, hardware configurations, and distributed parallelization strategies. We demonstrate that: (1) beyond certain scales, overhead incurred from certain distributed communication strategies leads parallelization strategies previously thought to be sub-optimal in fact become preferable; and (2) scaling the total number of accelerators for large model training quickly yields diminishing returns even when hardware and parallelization strategies are properly optimized, implying poor marginal performance per additional unit of power or GPU-hour.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 27.7 -->
                
            <!-- Medicine: 10.7 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- RAG: 1.6 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- Networks: 1.1 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.8294
            </span>
            <a href="https://arxiv.org/abs/2504.10419" target="_blank" rel="noopener noreferrer">Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large Language Models with CheckboxQA</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Micha{\l} Turski, Mateusz Chili\'nski, {\L}ukasz Borchmann | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Checkboxes are critical in real-world document processing where the presence or absence of ticks directly informs data extraction and decision-making processes. Yet, despite the strong performance of Large Vision and Language Models across a wide range of tasks, they struggle with interpreting check</span>
            
            <span class="abstract-full" style="display: none;">Checkboxes are critical in real-world document processing where the presence or absence of ticks directly informs data extraction and decision-making processes. Yet, despite the strong performance of Large Vision and Language Models across a wide range of tasks, they struggle with interpreting checkable content. This challenge becomes particularly pressing in industries where a single overlooked checkbox may lead to costly regulatory or contractual oversights. To address this gap, we introduce the CheckboxQA dataset, a targeted resource designed to evaluate and improve model performance on checkbox-related tasks. It reveals the limitations of current models and serves as a valuable tool for advancing document comprehension systems, with significant implications for applications in sectors such as legal tech and finance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 17.0 -->
                
            <!-- Medicine: 10.0 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.8794
            </span>
            <a href="https://arxiv.org/abs/2502.07677" target="_blank" rel="noopener noreferrer">Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered LLM Approach</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Param Kulkarni, Yingchi Liu, Hao-Ming Fu, Shaohua Yang, Isuru Gunasekara, Matt Peloquin, Noah Spitzer-Williams, Xiaotian Zhou, Xiaozhong Liu, Zhengping Ji, Yasser Ibrahim | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Achieving a delicate balance between fostering trust in law enforcement and protecting the rights of both officers and civilians continues to emerge as a pressing research and product challenge in the world today. In the pursuit of fairness and transparency, this study presents an innovative AI-driv</span>
            
            <span class="abstract-full" style="display: none;">Achieving a delicate balance between fostering trust in law enforcement and protecting the rights of both officers and civilians continues to emerge as a pressing research and product challenge in the world today. In the pursuit of fairness and transparency, this study presents an innovative AI-driven system designed to generate police report drafts from complex, noisy, and multi-role dialogue data. Our approach intelligently extracts key elements of law enforcement interactions and includes them in the draft, producing structured narratives that are not only high in quality but also reinforce accountability and procedural clarity. This framework holds the potential to transform the reporting process, ensuring greater oversight, consistency, and fairness in future policing practices. A demonstration video of our system can be accessed at https://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-Y-kpCHNO/view?usp=sharing</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.1 -->
                
            <!-- Medicine: 9.3 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Math: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.8796
            </span>
            <a href="https://arxiv.org/abs/2304.05229" target="_blank" rel="noopener noreferrer">The Big-O Problem for Max-Plus Automata is Decidable (PSPACE-Complete)</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Laure Daviaud, David Purser, Marie Tcheng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We show that the big-O problem for max-plus automata is decidable and PSPACE-complete. The big-O (or affine domination) problem asks whether, given two max-plus automata computing functions f and g, there exists a constant c such that f < cg+ c. This is a relaxation of the containment problem asking</span>
            
            <span class="abstract-full" style="display: none;">We show that the big-O problem for max-plus automata is decidable and PSPACE-complete. The big-O (or affine domination) problem asks whether, given two max-plus automata computing functions f and g, there exists a constant c such that f < cg+ c. This is a relaxation of the containment problem asking whether f < g, which is undecidable. Our decidability result uses Simon's forest factorisation theorem, and relies on detecting specific elements, that we call witnesses, in a finite semigroup closed under two special operations: stabilisation and flattening.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.2 -->
                
            <!-- LLMs: 7.8 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- GNN: 2.4 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- RAG: 1.4 -->
                
            <!-- Reinforcement Learning: 1.4 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.8814
            </span>
            <a href="https://arxiv.org/abs/2504.09846" target="_blank" rel="noopener noreferrer">GlyTwin: Digital Twin for Glucose Control in Type 1 Diabetes Through Optimal Behavioral Modifications Using Patient-Centric Counterfactuals</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Asiful Arefeen, Saman Khamesian, Maria Adela Grando, Bithika Thompson, Hassan Ghasemzadeh | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Frequent and long-term exposure to hyperglycemia (i.e., high blood glucose) increases the risk of chronic complications such as neuropathy, nephropathy, and cardiovascular disease. Current technologies like continuous subcutaneous insulin infusion (CSII) and continuous glucose monitoring (CGM) prima</span>
            
            <span class="abstract-full" style="display: none;">Frequent and long-term exposure to hyperglycemia (i.e., high blood glucose) increases the risk of chronic complications such as neuropathy, nephropathy, and cardiovascular disease. Current technologies like continuous subcutaneous insulin infusion (CSII) and continuous glucose monitoring (CGM) primarily model specific aspects of glycemic control-like hypoglycemia prediction or insulin delivery. Similarly, most digital twin approaches in diabetes management simulate only physiological processes. These systems lack the ability to offer alternative treatment scenarios that support proactive behavioral interventions. To address this, we propose GlyTwin, a novel digital twin framework that uses counterfactual explanations to simulate optimal treatments for glucose regulation. Our approach helps patients and caregivers modify behaviors like carbohydrate intake and insulin dosing to avoid abnormal glucose events. GlyTwin generates behavioral treatment suggestions that proactively prevent hyperglycemia by recommending small adjustments to daily choices, reducing both frequency and duration of these events. Additionally, it incorporates stakeholder preferences into the intervention design, making recommendations patient-centric and tailored. We evaluate GlyTwin on AZT1D, a newly constructed dataset with longitudinal data from 21 type 1 diabetes (T1D) patients on automated insulin delivery systems over 26 days. Results show GlyTwin outperforms state-of-the-art counterfactual methods, generating 76.6% valid and 86% effective interventions. These findings demonstrate the promise of counterfactual-driven digital twins in delivering personalized healthcare.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.4 -->
                
            <!-- Medicine: 8.8 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- RAG: 1.7 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Blockchain: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.8984
            </span>
            <a href="https://arxiv.org/abs/2504.10149" target="_blank" rel="noopener noreferrer">BoTTA: Benchmarking on-device Test Time Adaptation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Michal Danilowski, Soumyajit Chatterjee, Abhirup Ghosh | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The performance of deep learning models depends heavily on test samples at runtime, and shifts from the training data distribution can significantly reduce accuracy. Test-time adaptation (TTA) addresses this by adapting models during inference without requiring labeled test data or access to the ori</span>
            
            <span class="abstract-full" style="display: none;">The performance of deep learning models depends heavily on test samples at runtime, and shifts from the training data distribution can significantly reduce accuracy. Test-time adaptation (TTA) addresses this by adapting models during inference without requiring labeled test data or access to the original training set. While research has explored TTA from various perspectives like algorithmic complexity, data and class distribution shifts, model architectures, and offline versus continuous learning, constraints specific to mobile and edge devices remain underexplored. We propose BoTTA, a benchmark designed to evaluate TTA methods under practical constraints on mobile and edge devices. Our evaluation targets four key challenges caused by limited resources and usage conditions: (i) limited test samples, (ii) limited exposure to categories, (iii) diverse distribution shifts, and (iv) overlapping shifts within a sample. We assess state-of-the-art TTA methods under these scenarios using benchmark datasets and report system-level metrics on a real testbed. Furthermore, unlike prior work, we align with on-device requirements by advocating periodic adaptation instead of continuous inference-time adaptation. Experiments reveal key insights: many recent TTA algorithms struggle with small datasets, fail to generalize to unseen categories, and depend on the diversity and complexity of distribution shifts. BoTTA also reports device-specific resource use. For example, while SHOT improves accuracy by $2.25\times$ with $512$ adaptation samples, it uses $1.08\times$ peak memory on Raspberry Pi versus the base model. BoTTA offers actionable guidance for TTA in real-world, resource-constrained deployments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 13.7 -->
                
            <!-- LLMs: 10.1 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- RAG: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.9646
            </span>
            <a href="https://arxiv.org/abs/2504.08621" target="_blank" rel="noopener noreferrer">MooseAgent: A LLM Based Multi-agent Framework for Automating Moose Simulation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tao Zhang, Zhenhai Liu, Yong Xin, Yongjun Jiao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The Finite Element Method (FEM) is widely used in engineering and scientific computing, but its pre-processing, solver configuration, and post-processing stages are often time-consuming and require specialized knowledge. This paper proposes an automated solution framework, MooseAgent, for the multi-</span>
            
            <span class="abstract-full" style="display: none;">The Finite Element Method (FEM) is widely used in engineering and scientific computing, but its pre-processing, solver configuration, and post-processing stages are often time-consuming and require specialized knowledge. This paper proposes an automated solution framework, MooseAgent, for the multi-physics simulation framework MOOSE, which combines large-scale pre-trained language models (LLMs) with a multi-agent system. The framework uses LLMs to understand user-described simulation requirements in natural language and employs task decomposition and multi-round iterative verification strategies to automatically generate MOOSE input files. To improve accuracy and reduce model hallucinations, the system builds and utilizes a vector database containing annotated MOOSE input cards and function documentation. We conducted experimental evaluations on several typical cases, including heat transfer, mechanics, phase field, and multi-physics coupling. The results show that MooseAgent can automate the MOOSE simulation process to a certain extent, especially demonstrating a high success rate when dealing with relatively simple single-physics problems. The main contribution of this research is the proposal of a multi-agent automated framework for MOOSE, which validates its potential in simplifying finite element simulation processes and lowering the user barrier, providing new ideas for the development of intelligent finite element simulation software. The code for the MooseAgent framework proposed in this paper has been open-sourced and is available at https://github.com/taozhan18/MooseAgent</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 12.7 -->
                
            <!-- LLMs: 12.7 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Reinforcement Learning: 1.4 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.1001
            </span>
            <a href="https://arxiv.org/abs/2409.01990" target="_blank" rel="noopener noreferrer">Designing Large Foundation Models for Efficient Training and Inference: A Survey</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dong Liu, Yanxuan Yu, Yite Wang, Jing Wu, Zhongwei Wan, Sina Alinejad, Benjamin Lengerich, Ying Nian Wu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper focuses on modern efficient training and inference technologies on foundation models and illustrates them from two perspectives: model and system design. Model and System Design optimize LLM training and inference from different aspects to save computational resources, making LLMs more ef</span>
            
            <span class="abstract-full" style="display: none;">This paper focuses on modern efficient training and inference technologies on foundation models and illustrates them from two perspectives: model and system design. Model and System Design optimize LLM training and inference from different aspects to save computational resources, making LLMs more efficient, affordable, and more accessible. The paper list repository is available at https://github.com/NoakLiu/Efficient-Foundation-Models-Survey.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 21.4 -->
                
            <!-- Medicine: 10.2 -->
                
            <!-- RAG: 5.2 -->
                
            <!-- Blockchain: 3.2 -->
                
            <!-- 3D: 3.0 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- T2I: 2.2 -->
                
            <!-- HPO and AutoML: 1.8 -->
                
            <!-- Finance: 1.1 -->
                
            <!-- Datasets: 1.1 -->
                
            <!-- Networks: 1.1 -->
                
            <!-- Fuzzy Logic: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.1353
            </span>
            <a href="https://arxiv.org/abs/2504.08537" target="_blank" rel="noopener noreferrer">Lexical Bundle Frequency as a Construct-Relevant Candidate Feature in Automated Scoring of L2 Academic Writing</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Burak Senel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Automated scoring (AS) systems are increasingly used for evaluating L2 writing, but require ongoing refinement for construct validity. While prior work suggested lexical bundles (LBs) - recurrent multi-word sequences satisfying certain frequency criteria - could inform assessment, their empirical in</span>
            
            <span class="abstract-full" style="display: none;">Automated scoring (AS) systems are increasingly used for evaluating L2 writing, but require ongoing refinement for construct validity. While prior work suggested lexical bundles (LBs) - recurrent multi-word sequences satisfying certain frequency criteria - could inform assessment, their empirical integration into AS models needs further investigation. This study tested the impact of incorporating LB frequency features into an AS model for TOEFL independent writing tasks. Analyzing a sampled subcorpus (N=1,225 essays, 9 L1s) from the TOEFL11 corpus, scored by ETS-trained raters (Low, Medium, High), 3- to 9-word LBs were extracted, distinguishing prompt-specific from non-prompt types. A baseline Support Vector Machine (SVM) scoring model using established linguistic features (e.g., mechanics, cohesion, sophistication) was compared against an extended model including three aggregate LB frequency features (total prompt, total non-prompt, overall total). Results revealed significant, though generally small-effect, relationships between LB frequency (especially non-prompt bundles) and proficiency (p < .05). Mean frequencies suggested lower proficiency essays used more LBs overall. Critically, the LB-enhanced model improved agreement with human raters (Quadratic Cohen's Kappa +2.05%, overall Cohen's Kappa +5.63%), with notable gains for low (+10.1% exact agreement) and medium (+14.3% Cohen's Kappa) proficiency essays. These findings demonstrate that integrating aggregate LB frequency offers potential for developing more linguistically informed and accurate AS systems, particularly for differentiating developing L2 writers.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 15.2 -->
                
            <!-- LLMs: 10.6 -->
                
            <!-- Quantum Computing: 5.6 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- RAG: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Math: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.1832
            </span>
            <a href="https://arxiv.org/abs/2504.10054" target="_blank" rel="noopener noreferrer">Implementation and Performance Evaluation of TCP over QUIC Tunnels</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xuanhong Guo, Zekun Bao, Ying Chen | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">QUIC, a UDP-based transport protocol, addresses several limitations of TCP by offering built-in encryption, stream multiplexing, and improved loss recovery. To extend these benefits to legacy TCP-based applications, this paper explores the implementation and evaluation of a TCP over QUIC tunneling a</span>
            
            <span class="abstract-full" style="display: none;">QUIC, a UDP-based transport protocol, addresses several limitations of TCP by offering built-in encryption, stream multiplexing, and improved loss recovery. To extend these benefits to legacy TCP-based applications, this paper explores the implementation and evaluation of a TCP over QUIC tunneling approach. A lightweight, stream-based tunnel is constructed using the Rust-based Quinn library, enabling TCP traffic to traverse QUIC connections transparently. Performance is evaluated under varying network conditions, including packet loss, high latency, and out-of-order delivery. Results indicate that TCP over QUIC maintains significantly higher throughput than native TCP in lossy or unstable environments, with up to a high improvement under 20\% packet loss. However, under ideal network conditions, tunneling introduces modest overhead due to encryption and user-space processing. These findings provide insights into the trade-offs of TCP over QUIC tunneling and its suitability for deployment in dynamic or impaired networks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.6 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- RAG: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Reinforcement Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.2943
            </span>
            <a href="https://arxiv.org/abs/2504.09468" target="_blank" rel="noopener noreferrer">Incubation and Beyond: A Comparative Analysis of ASF Projects Sustainability Impacts on Software Quality</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Adam Alami, Steffan Klockmann, Lasse Rehder S{\o}rensen, Ra\'ul Pardo, Johan Lin\r{a}ker | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Free and Open Source Software (FOSS) communities' sustainability, meaning to remain operational without signs of weakening or interruptions to its development, is fundamental for the resilience and continuity of society's digital infrastructure. Many digital services and products either leverage or </span>
            
            <span class="abstract-full" style="display: none;">Free and Open Source Software (FOSS) communities' sustainability, meaning to remain operational without signs of weakening or interruptions to its development, is fundamental for the resilience and continuity of society's digital infrastructure. Many digital services and products either leverage or entirely rely on FOSS in their software stack. FOSS sustainability is a multifaceted concept, and the impact of its decline on community products is less known. In this study, we sought to understand how the different aspects of FOSS sustainability impact software quality from a life-cycle perspective. Specifically, we investigate whether and how support and incubation of FOSS projects or bypassing incubation correlate with software quality outcomes. We selected 342 FOSS projects from the Apache Software Foundation that have either graduated, retired, or bypassed their incubator program. We used 16 sustainability metrics to examine their impact on eight software quality metrics. Using Bayesian data analysis, we found that our selected sustainability metrics exhibit distinct relationships with software quality across different project trajectories. Graduated projects showed the strongest sustainability-software quality (SWQ) relationship, both during and post-incubation. In contrast, retired projects showed weaker relationships, despite receiving similar governance support. Bypassed projects, while not outperforming graduated ones, showed comparable sustainability-SWQ relationships. While structured incubation strengthens sustainability and SWQ in graduated projects, retired projects struggle to maintain strong sustainability-SWQ relationships, indicating that additional factors internal and specific to projects influence sustainability. This effect was evident among bypassed projects; their self-reliant sustainability practices yielded stronger sustainability-SWQ compared to the retired ones.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.2 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Blockchain: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Math: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.3464
            </span>
            <a href="https://arxiv.org/abs/2311.09245" target="_blank" rel="noopener noreferrer">Affine Invariance in Continuous-Domain Convolutional Neural Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ali Mohaddes, Johannes Lederer | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The notion of group invariance helps neural networks in recognizing patterns and features under geometric transformations. Group convolutional neural networks enhance traditional convolutional neural networks by incorporating group-based geometric structures into their design. This research studies </span>
            
            <span class="abstract-full" style="display: none;">The notion of group invariance helps neural networks in recognizing patterns and features under geometric transformations. Group convolutional neural networks enhance traditional convolutional neural networks by incorporating group-based geometric structures into their design. This research studies affine invariance on continuous-domain convolutional neural networks. Despite other research considering isometric invariance or similarity invariance, we focus on the full structure of affine transforms generated by the group of all invertible $2 \times 2$ real matrices (generalized linear group $\mathrm{GL}_2(\mathbb{R})$). We introduce a new criterion to assess the invariance of two signals under affine transformations. The input image is embedded into the affine Lie group $G_2 = \mathbb{R}^2 \ltimes \mathrm{GL}_2(\mathbb{R})$ to facilitate group convolution operations that respect affine invariance. Then, we analyze the convolution of embedded signals over $G_2$. In sum, our research could eventually extend the scope of geometrical transformations that usual deep-learning pipelines can handle.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 6.2 -->
                
            <!-- Quantum Computing: 5.6 -->
                
            <!-- GNN: 2.5 -->
                
            <!-- Reinforcement Learning: 2.4 -->
                
            <!-- Math: 2.3 -->
                
            <!-- SpikingNN: 2.0 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.6556
            </span>
            <a href="https://arxiv.org/abs/2504.01672" target="_blank" rel="noopener noreferrer">A flexible framework for early power and timing comparison of time-multiplexed CGRA kernel executions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Maxime Henri Aspros, Juan Sapriza, Giovanni Ansaloni, David Atienza | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">At the intersection between traditional CPU architectures and more specialized options such as FPGAs or ASICs lies the family of reconfigurable hardware architectures, termed Coarse-Grained Reconfigurable Arrays (CGRAs). CGRAs are composed of a 2-dimensional array of processing elements (PE), tightl</span>
            
            <span class="abstract-full" style="display: none;">At the intersection between traditional CPU architectures and more specialized options such as FPGAs or ASICs lies the family of reconfigurable hardware architectures, termed Coarse-Grained Reconfigurable Arrays (CGRAs). CGRAs are composed of a 2-dimensional array of processing elements (PE), tightly integrated with each other, each capable of performing arithmetic and logic operations. The vast design space of CGRA implementations poses a challenge, which calls for fast exploration tools to prune it in advance of time-consuming syntheses. The proposed tool aims to simplify this process by simulating kernel execution and providing a characterization framework. The estimator returns energy and latency values otherwise only available through a time-consuming post-synthesis simulation, allowing for instantaneous comparative analysis between different kernels and hardware configurations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.6 -->
                
            <!-- Medicine: 10.3 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- RAG: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.6835
            </span>
            <a href="https://arxiv.org/abs/2405.16297" target="_blank" rel="noopener noreferrer">LUCIE: A Lightweight Uncoupled ClImate Emulator with long-term stability and physical consistency for O(1000)-member ensembles</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Haiwen Guan, Troy Arcomano, Ashesh Chattopadhyay, Romit Maulik | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a lightweight, easy-to-train, low-resolution, fully data-driven climate emulator, LUCIE, that can be trained on as low as $2$ years of $6$-hourly ERA5 data. Unlike most state-of-the-art AI weather models, LUCIE remains stable and physically consistent for $100$ years of autoregressive sim</span>
            
            <span class="abstract-full" style="display: none;">We present a lightweight, easy-to-train, low-resolution, fully data-driven climate emulator, LUCIE, that can be trained on as low as $2$ years of $6$-hourly ERA5 data. Unlike most state-of-the-art AI weather models, LUCIE remains stable and physically consistent for $100$ years of autoregressive simulation with $100$ ensemble members. Long-term mean climatology from LUCIE's simulation of temperature, wind, precipitation, and humidity matches that of ERA5 data, along with the variability. We further demonstrate how well extreme weather events and their return periods can be estimated from a large ensemble of long-term simulations. We further discuss an improved training strategy with a hard-constrained first-order integrator to suppress autoregressive error growth, a novel spectral regularization strategy to better capture fine-scale dynamics, and finally an optimization algorithm that enables data-limited (as low as $2$ years of $6$-hourly data) training of the emulator without losing stability and physical consistency. Finally, we provide a scaling experiment to compare the long-term bias of LUCIE with respect to the number of training samples. Importantly, LUCIE is an easy to use model that can be trained in just $2.4$h on a single A-100 GPU, allowing for multiple experiments that can explore important scientific questions that could be answered with large ensembles of long-term simulations, e.g., the impact of different variables on the simulation, dynamic response to external forcing, and estimation of extreme weather events, amongst others.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.8 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 4.8 -->
                
            <!-- Reinforcement Learning: 2.9 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Math: 2.0 -->
                
            <!-- GNN: 1.4 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.9295
            </span>
            <a href="https://arxiv.org/abs/2504.09322" target="_blank" rel="noopener noreferrer">MedIL: Implicit Latent Spaces for Generating Heterogeneous Medical Images at Arbitrary Resolutions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tyler Spears, Shen Zhu, Yinzhu Jin, Aman Shrivastava, P. Thomas Fletcher | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this work, we introduce MedIL, a first-of-its-kind autoencoder built for encoding medical images with heterogeneous sizes and resolutions for image generation. Medical images are often large and heterogeneous, where fine details are of vital clinical importance. Image properties change drasticall</span>
            
            <span class="abstract-full" style="display: none;">In this work, we introduce MedIL, a first-of-its-kind autoencoder built for encoding medical images with heterogeneous sizes and resolutions for image generation. Medical images are often large and heterogeneous, where fine details are of vital clinical importance. Image properties change drastically when considering acquisition equipment, patient demographics, and pathology, making realistic medical image generation challenging. Recent work in latent diffusion models (LDMs) has shown success in generating images resampled to a fixed-size. However, this is a narrow subset of the resolutions native to image acquisition, and resampling discards fine anatomical details. MedIL utilizes implicit neural representations to treat images as continuous signals, where encoding and decoding can be performed at arbitrary resolutions without prior resampling. We quantitatively and qualitatively show how MedIL compresses and preserves clinically-relevant features over large multi-site, multi-resolution datasets of both T1w brain MRIs and lung CTs. We further demonstrate how MedIL can influence the quality of images generated with a diffusion model, and discuss how MedIL can enhance generative models to resemble raw clinical acquisitions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 14.1 -->
                
            <!-- LLMs: 12.4 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- RAG: 1.7 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -9.0069
            </span>
            <a href="https://arxiv.org/abs/2502.01436" target="_blank" rel="noopener noreferrer">Towards Safer Chatbots: A Framework for Policy Compliance Evaluation of Custom GPTs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: David Rodriguez, William Seymour, Jose M. Del Alamo, Jose Such | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have gained unprecedented prominence, achieving widespread adoption across diverse domains and integrating deeply into society. The capability to fine-tune general-purpose LLMs, such as Generative Pre-trained Transformers (GPT), for specific tasks has facilitated the eme</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have gained unprecedented prominence, achieving widespread adoption across diverse domains and integrating deeply into society. The capability to fine-tune general-purpose LLMs, such as Generative Pre-trained Transformers (GPT), for specific tasks has facilitated the emergence of numerous Custom GPTs. These tailored models are increasingly made available through dedicated marketplaces, such as OpenAI's GPT Store. However, their black-box nature introduces significant safety and compliance risks. In this work, we present a scalable framework for the automated evaluation of Custom GPTs against OpenAI's usage policies, which define the permissible behaviors of these systems. Our framework integrates three core components: (1) automated discovery and data collection of models from the GPT store, (2) a red-teaming prompt generator tailored to specific policy categories and the characteristics of each target GPT, and (3) an LLM-as-a-judge technique to analyze each prompt-response pair for potential policy violations. We validate our framework with a manually annotated ground truth, and evaluate it through a large-scale study with 782 Custom GPTs across three categories: Romantic, Cybersecurity, and Academic GPTs. Our manual annotation process achieved an F1 score of 0.975 in identifying policy violations, confirming the reliability of the framework's assessments. The results reveal that 58.7% of the analyzed models exhibit indications of non-compliance, exposing weaknesses in the GPT store's review and approval processes. Furthermore, our findings indicate that a model's popularity does not correlate with compliance, and non-compliance issues largely stem from behaviors inherited from base models rather than user-driven customizations. We believe this approach is extendable to other chatbot platforms and policy domains, improving LLM-based systems safety.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 23.7 -->
                
            <!-- Medicine: 9.9 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Math: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -9.1622
            </span>
            <a href="https://arxiv.org/abs/2504.09260" target="_blank" rel="noopener noreferrer">NetTAG: A Multimodal RTL-and-Layout-Aligned Netlist Foundation Model via Text-Attributed Graph</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wenji Fang, Wenkai Li, Shang Liu, Yao Lu, Hongce Zhang, Zhiyao Xie | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Circuit representation learning has shown promise in advancing Electronic Design Automation (EDA) by capturing structural and functional circuit properties for various tasks. Existing pre-trained solutions rely on graph learning with complex functional supervision, such as truth table simulation. Ho</span>
            
            <span class="abstract-full" style="display: none;">Circuit representation learning has shown promise in advancing Electronic Design Automation (EDA) by capturing structural and functional circuit properties for various tasks. Existing pre-trained solutions rely on graph learning with complex functional supervision, such as truth table simulation. However, they only handle simple and-inverter graphs (AIGs), struggling to fully encode other complex gate functionalities. While large language models (LLMs) excel at functional understanding, they lack the structural awareness for flattened netlists. To advance netlist representation learning, we present NetTAG, a netlist foundation model that fuses gate semantics with graph structure, handling diverse gate types and supporting a variety of functional and physical tasks. Moving beyond existing graph-only methods, NetTAG formulates netlists as text-attributed graphs, with gates annotated by symbolic logic expressions and physical characteristics as text attributes. Its multimodal architecture combines an LLM-based text encoder for gate semantics and a graph transformer for global structure. Pre-trained with gate and graph self-supervised objectives and aligned with RTL and layout stages, NetTAG captures comprehensive circuit intrinsics. Experimental results show that NetTAG consistently outperforms each task-specific method on four largely different functional and physical tasks and surpasses state-of-the-art AIG encoders, demonstrating its versatility.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 14.7 -->
                
            <!-- LLMs: 13.1 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- RAG: 2.8 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Blockchain: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -9.3725
            </span>
            <a href="https://arxiv.org/abs/2504.08024" target="_blank" rel="noopener noreferrer">From Speech to Summary: A Comprehensive Survey of Speech Summarization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Fabian Retkowski, Maike Z\"ufle, Andreas Sudmann, Dinah Pfau, Jan Niehues, Alexander Waibel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Speech summarization has become an essential tool for efficiently managing and accessing the growing volume of spoken and audiovisual content. However, despite its increasing importance, speech summarization is still not clearly defined and intersects with several research areas, including speech re</span>
            
            <span class="abstract-full" style="display: none;">Speech summarization has become an essential tool for efficiently managing and accessing the growing volume of spoken and audiovisual content. However, despite its increasing importance, speech summarization is still not clearly defined and intersects with several research areas, including speech recognition, text summarization, and specific applications like meeting summarization. This survey not only examines existing datasets and evaluation methodologies, which are crucial for assessing the effectiveness of summarization approaches but also synthesizes recent developments in the field, highlighting the shift from traditional systems to advanced models like fine-tuned cascaded architectures and end-to-end solutions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.7 -->
                
            <!-- Medicine: 12.1 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- RAG: 1.8 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Blockchain: 1.5 -->
                
            <!-- Reinforcement Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -9.9023
            </span>
            <a href="https://arxiv.org/abs/2210.04979" target="_blank" rel="noopener noreferrer">Label-free segmentation from cardiac ultrasound using self-supervised learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Danielle L. Ferreira, Connor Lau, Zaynaf Salaymang, Rima Arnaout | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Segmentation and measurement of cardiac chambers is critical in cardiac ultrasound but is laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations. We built a pipeline for self-supervised (no manual labels) segmentation co</span>
            
            <span class="abstract-full" style="display: none;">Segmentation and measurement of cardiac chambers is critical in cardiac ultrasound but is laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations. We built a pipeline for self-supervised (no manual labels) segmentation combining computer vision, clinical domain knowledge, and deep learning. We trained on 450 echocardiograms (93,000 images) and tested on 8,393 echocardiograms (4,476,266 images; mean 61 years, 51% female), using the resulting segmentations to calculate biometrics. We also tested against external images from an additional 10,030 patients with available manual tracings of the left ventricle. r2 between clinically measured and pipeline-predicted measurements were similar to reported inter-clinician variation and comparable to supervised learning across several different measurements (r2 0.56-0.84). Average accuracy for detecting abnormal chamber size and function was 0.85 (range 0.71-0.97) compared to clinical measurements. A subset of test echocardiograms (n=553) had corresponding cardiac MRIs, where MRI is the gold standard. Correlation between pipeline and MRI measurements was similar to that between clinical echocardiogram and MRI. Finally, the pipeline accurately segments the left ventricle with an average Dice score of 0.89 (95% CI [0.89]) in the external, manually labeled dataset. Our results demonstrate a manual-label free, clinically valid, and highly scalable method for segmentation from ultrasound, a noisy but globally important imaging modality.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 21.1 -->
                
            <!-- LLMs: 5.9 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -10.2304
            </span>
            <a href="https://arxiv.org/abs/2504.08788" target="_blank" rel="noopener noreferrer">Hub Star Modeling 2.0 for Medallion Architecture</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shahram Salami | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Data warehousing enables performant access to high-quality data integrated from dynamic data sources. The medallion architecture, a standard for data warehousing, addresses these goals by organizing data into bronze, silver and gold layers, representing raw, integrated, and fit-to-purpose data, resp</span>
            
            <span class="abstract-full" style="display: none;">Data warehousing enables performant access to high-quality data integrated from dynamic data sources. The medallion architecture, a standard for data warehousing, addresses these goals by organizing data into bronze, silver and gold layers, representing raw, integrated, and fit-to-purpose data, respectively. In terms of data modeling, bronze layer retains the structure of source data with additional metadata. The gold layer follows established modeling approaches such as star schema, snowflake, and flattened tables. The silver layer, acting as a canonical form, requires a flexible and scalable model to support continuous changes and incremental development. This paper introduces an enhanced Hub Star modeling approach tailored for the medallion architecture, simplifying silver-layer data modeling by generalizing hub and star concepts. This approach has been demonstrated using Databricks and the retail-org sample dataset, with all modeling and transformation scripts available on GitHub.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 18.6 -->
                
            <!-- LLMs: 8.9 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- RAG: 1.6 -->
                
            <!-- Blockchain: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -10.2649
            </span>
            <a href="https://arxiv.org/abs/2504.09496" target="_blank" rel="noopener noreferrer">Extending Behavioral Software Engineering: Decision-Making and Collaboration in Human-AI Teams for Responsible Software Engineering</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lekshmi Murali Rani | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The study of behavioral and social dimensions of software engineering (SE) tasks characterizes behavioral software engineering (BSE);however, the increasing significance of human-AI collaboration (HAIC) brings new directions in BSE by presenting new challenges and opportunities.This PhD research foc</span>
            
            <span class="abstract-full" style="display: none;">The study of behavioral and social dimensions of software engineering (SE) tasks characterizes behavioral software engineering (BSE);however, the increasing significance of human-AI collaboration (HAIC) brings new directions in BSE by presenting new challenges and opportunities.This PhD research focuses on decision-making (DM) for SE tasks and collaboration within human-AI teams, aiming to promote responsible software engineering through a cognitive partnership between humans and AI.The goal of the research is to identify the challenges and nuances in HAIC from a cognitive perspective, design and optimize collaboration/partnership (human-AI team) that enhance collective intelligence and promote better, responsible DM in SE through human-centered approaches. The research addresses HAIC and its impact on individual, team, and organizational level aspects of BSE.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.7 -->
                
            <!-- Medicine: 13.5 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- RAG: 1.7 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Reinforcement Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -11.1615
            </span>
            <a href="https://arxiv.org/abs/2503.14911" target="_blank" rel="noopener noreferrer">Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Siyuan Yan, Ming Hu, Yiwen Jiang, Xieji Li, Hao Fei, Philipp Tschandl, Harald Kittler, Zongyuan Ge | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datas</span>
            
            <span class="abstract-full" style="display: none;">The emergence of vision-language models has transformed medical AI, enabling unprecedented advances in diagnostic capability and clinical applications. However, progress in dermatology has lagged behind other medical domains due to the lack of standard image-text pairs. Existing dermatological datasets are limited in both scale and depth, offering only single-label annotations across a narrow range of diseases instead of rich textual descriptions, and lacking the crucial clinical context needed for real-world applications. To address these limitations, we present Derm1M, the first large-scale vision-language dataset for dermatology, comprising 1,029,761 image-text pairs. Built from diverse educational resources and structured around a standard ontology collaboratively developed by experts, Derm1M provides comprehensive coverage for over 390 skin conditions across four hierarchical levels and 130 clinical concepts with rich contextual information such as medical history, symptoms, and skin tone. To demonstrate Derm1M potential in advancing both AI research and clinical application, we pretrained a series of CLIP-like models, collectively called DermLIP, on this dataset. The DermLIP family significantly outperforms state-of-the-art foundation models on eight diverse datasets across multiple tasks, including zero-shot skin disease classification, clinical and artifacts concept identification, few-shot/full-shot learning, and cross-modal retrieval. Our dataset and code will be publicly available at https://github.com/SiyuanYan1/Derm1M upon acceptance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 22.3 -->
                
            <!-- LLMs: 10.2 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- RAG: 2.0 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Reinforcement Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -11.4612
            </span>
            <a href="https://arxiv.org/abs/2504.09004" target="_blank" rel="noopener noreferrer">Exploring Families' Use and Mediation of Generative AI: A Multi-User Perspective</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shirley Zhang, Bengisu Cagiltay, Jennica Li, Dakota Sullivan, Bilge Mutlu, Heather Kirkorian, Kassem Fawaz | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Applications of Generative AI (GenAI), such as ChatGPT, have gained popularity among the public due to their ease of access, use, and support of educational and creative activities. Despite these benefits, GenAI poses unique risks for families, such as lacking sufficient safeguards tailored to prote</span>
            
            <span class="abstract-full" style="display: none;">Applications of Generative AI (GenAI), such as ChatGPT, have gained popularity among the public due to their ease of access, use, and support of educational and creative activities. Despite these benefits, GenAI poses unique risks for families, such as lacking sufficient safeguards tailored to protect children under 16 years of age and not offering parental control features. This study explores families' use and co-use of GenAI, the perceived risks and opportunities of ChatGPT, and how parents mediate their children's use of GenAI. Through semi-structured interviews with 12 families, we identified ways families used and mediated GenAI and factors that influenced parents' GenAI mediation strategies. We contextualize our findings with a modified model of family mediation strategies, drawing from previous family media and mediation frameworks. We provide insights for future research on family-GenAI interactions and highlight the need for more robust protective measures on GenAI platforms for families.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 15.4 -->
                
            <!-- LLMs: 14.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Blockchain: 2.3 -->
                
            <!-- RAG: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -11.4863
            </span>
            <a href="https://arxiv.org/abs/2504.09480" target="_blank" rel="noopener noreferrer">Vision-Language Model for Object Detection and Segmentation: A Review and Evaluation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yongchao Feng, Yajie Liu, Shuai Yang, Wenrui Cai, Jinqing Zhang, Qiqi Zhan, Ziyue Huang, Hongxi Yan, Qiao Wan, Chenguang Liu, Junzhe Wang, Jiahui Lv, Ziqi Liu, Tengyuan Shi, Qingjie Liu, Yunhong Wang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Vision-Language Model (VLM) have gained widespread adoption in Open-Vocabulary (OV) object detection and segmentation tasks. Despite they have shown promise on OV-related tasks, their effectiveness in conventional vision tasks has thus far been unevaluated. In this work, we present the systematic re</span>
            
            <span class="abstract-full" style="display: none;">Vision-Language Model (VLM) have gained widespread adoption in Open-Vocabulary (OV) object detection and segmentation tasks. Despite they have shown promise on OV-related tasks, their effectiveness in conventional vision tasks has thus far been unevaluated. In this work, we present the systematic review of VLM-based detection and segmentation, view VLM as the foundational model and conduct comprehensive evaluations across multiple downstream tasks for the first time: 1) The evaluation spans eight detection scenarios (closed-set detection, domain adaptation, crowded objects, etc.) and eight segmentation scenarios (few-shot, open-world, small object, etc.), revealing distinct performance advantages and limitations of various VLM architectures across tasks. 2) As for detection tasks, we evaluate VLMs under three finetuning granularities: \textit{zero prediction}, \textit{visual fine-tuning}, and \textit{text prompt}, and further analyze how different finetuning strategies impact performance under varied task. 3) Based on empirical findings, we provide in-depth analysis of the correlations between task characteristics, model architectures, and training methodologies, offering insights for future VLM design. 4) We believe that this work shall be valuable to the pattern recognition experts working in the fields of computer vision, multimodal learning, and vision foundation models by introducing them to the problem, and familiarizing them with the current status of the progress while providing promising directions for future research. A project associated with this review and evaluation has been created at https://github.com/better-chao/perceptual_abilities_evaluation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 18.3 -->
                
            <!-- LLMs: 15.1 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- RAG: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Reinforcement Learning: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -13.3386
            </span>
            <a href="https://arxiv.org/abs/2503.16681" target="_blank" rel="noopener noreferrer">GauRast: Enhancing GPU Triangle Rasterizers to Accelerate 3D Gaussian Splatting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sixu Li, Ben Keller, Yingyan Celine Lin, Brucek Khailany | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">3D intelligence leverages rich 3D features and stands as a promising frontier in AI, with 3D rendering fundamental to many downstream applications. 3D Gaussian Splatting (3DGS), an emerging high-quality 3D rendering method, requires significant computation, making real-time execution on existing GPU</span>
            
            <span class="abstract-full" style="display: none;">3D intelligence leverages rich 3D features and stands as a promising frontier in AI, with 3D rendering fundamental to many downstream applications. 3D Gaussian Splatting (3DGS), an emerging high-quality 3D rendering method, requires significant computation, making real-time execution on existing GPU-equipped edge devices infeasible. Previous efforts to accelerate 3DGS rely on dedicated accelerators that require substantial integration overhead and hardware costs. This work proposes an acceleration strategy that leverages the similarities between the 3DGS pipeline and the highly optimized conventional graphics pipeline in modern GPUs. Instead of developing a dedicated accelerator, we enhance existing GPU rasterizer hardware to efficiently support 3DGS operations. Our results demonstrate a 23$\times$ increase in processing speed and a 24$\times$ reduction in energy consumption, with improvements yielding 6$\times$ faster end-to-end runtime for the original 3DGS algorithm and 4$\times$ for the latest efficiency-improved pipeline, achieving 24 FPS and 46 FPS respectively. These enhancements incur only a minimal area overhead of 0.2\% relative to the entire SoC chip area, underscoring the practicality and efficiency of our approach for enabling 3DGS rendering on resource-constrained platforms.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- 3D: 42.8 -->
                
            <!-- LLMs: 8.7 -->
                
            <!-- Medicine: 1.7 -->
                
            <!-- Blockchain: 1.5 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -19.2051
            </span>
            <a href="https://arxiv.org/abs/2504.08844" target="_blank" rel="noopener noreferrer">Artificial Intelligence Augmented Medical Imaging Reconstruction in Radiation Therapy</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Di Xu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Efficiently acquired and precisely reconstructed imaging are crucial to the success of modern radiation therapy (RT). Computed tomography (CT) and magnetic resonance imaging (MRI) are two common modalities for providing RT treatment planning and delivery guidance/monitoring. In recent decades, artif</span>
            
            <span class="abstract-full" style="display: none;">Efficiently acquired and precisely reconstructed imaging are crucial to the success of modern radiation therapy (RT). Computed tomography (CT) and magnetic resonance imaging (MRI) are two common modalities for providing RT treatment planning and delivery guidance/monitoring. In recent decades, artificial intelligence (AI) has emerged as a powerful and widely adopted technique across various fields, valued for its efficiency and convenience enabled by implicit function definition and data-driven feature representation learning. Here, we present a series of AI-driven medical imaging reconstruction frameworks for enhanced radiotherapy, designed to improve CT image reconstruction quality and speed, refine dual-energy CT (DECT) multi-material decomposition (MMD), and significantly accelerate 4D MRI acquisition.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 62.5%">
                        Medicine
                    </span>
            <!-- LLMs: 6.9 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- RAG: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- Math: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -19.4587
            </span>
            <a href="https://arxiv.org/abs/2504.08177" target="_blank" rel="noopener noreferrer">SynthFM: Training Modality-agnostic Foundation Models for Medical Image Segmentation without Real Medical Data</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sourya Sengupta, Satrajit Chakrabarty, Keerthi Sravan Ravi, Gopal Avinash, Ravi Soni | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Foundation models like the Segment Anything Model (SAM) excel in zero-shot segmentation for natural images but struggle with medical image segmentation due to differences in texture, contrast, and noise. Annotating medical images is costly and requires domain expertise, limiting large-scale annotate</span>
            
            <span class="abstract-full" style="display: none;">Foundation models like the Segment Anything Model (SAM) excel in zero-shot segmentation for natural images but struggle with medical image segmentation due to differences in texture, contrast, and noise. Annotating medical images is costly and requires domain expertise, limiting large-scale annotated data availability. To address this, we propose SynthFM, a synthetic data generation framework that mimics the complexities of medical images, enabling foundation models to adapt without real medical data. Using SAM's pretrained encoder and training the decoder from scratch on SynthFM's dataset, we evaluated our method on 11 anatomical structures across 9 datasets (CT, MRI, and Ultrasound). SynthFM outperformed zero-shot baselines like SAM and MedSAM, achieving superior results under different prompt settings and on out-of-distribution datasets.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 70.3%">
                        Medicine
                    </span>
            <!-- LLMs: 8.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -19.8354
            </span>
            <a href="https://arxiv.org/abs/2503.14049" target="_blank" rel="noopener noreferrer">A Modular Edge Device Network for Surgery Digitalization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vincent Schorp, Fr\'ed\'eric Giraud, Gianluca Parg\"atzi, Michael W\"aspe, Lorenzo von Ritter-Zahony, Marcel Wegmann, Nicola A. Cavalcanti, John Garcia Henao, Nicholas B\"unger, Dominique Cachin, Sebastiano Caprara, Philipp F\"urnstahl, Fabio Carrillo | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Future surgical care demands real-time, integrated data to drive informed decision-making and improve patient outcomes. The pressing need for seamless and efficient data capture in the OR motivates our development of a modular solution that bridges the gap between emerging machine learning technique</span>
            
            <span class="abstract-full" style="display: none;">Future surgical care demands real-time, integrated data to drive informed decision-making and improve patient outcomes. The pressing need for seamless and efficient data capture in the OR motivates our development of a modular solution that bridges the gap between emerging machine learning techniques and interventional medicine. We introduce a network of edge devices, called Data Hubs (DHs), that interconnect diverse medical sensors, imaging systems, and robotic tools via optical fiber and a centralized network switch. Built on the NVIDIA Jetson Orin NX, each DH supports multiple interfaces (HDMI, USB-C, Ethernet) and encapsulates device-specific drivers within Docker containers using the Isaac ROS framework and ROS2. A centralized user interface enables straightforward configuration and real-time monitoring, while an Nvidia DGX computer provides state-of-the-art data processing and storage. We validate our approach through an ultrasound-based 3D anatomical reconstruction experiment that combines medical imaging, pose tracking, and RGB-D data acquisition.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 66.1%">
                        Medicine
                    </span>
            <!-- LLMs: 7.1 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -22.4681
            </span>
            <a href="https://arxiv.org/abs/2504.08141" target="_blank" rel="noopener noreferrer">Variational quantum and neural quantum states algorithms for the linear complementarity problem</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Saibal De, Oliver Knitter, Rohan Kodati, Paramsothy Jayakumar, James Stokes, Shravan Veerapaneni | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of </span>
            
            <span class="abstract-full" style="display: none;">Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of concept, their practical utility in solving real-world problems -- and whether quantum-inspired classical algorithms can match their performance -- remains an open question. We present a novel application of the variational quantum linear solver (VQLS) and its classical neural quantum states-based counterpart, the variational neural linear solver (VNLS), as key components within a minimum map Newton solver for a complementarity-based rigid body contact model. We demonstrate using the VNLS that our solver accurately simulates the dynamics of rigid spherical bodies during collision events. These results suggest that quantum and quantum-inspired linear algebra algorithms can serve as viable alternatives to standard linear algebra solvers for modeling certain physical systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Quantum Computing: 31.2 -->
                
            <!-- Medicine: 4.2 -->
                
            <!-- LLMs: 4.1 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Evolutionary Algorithms: 1.7 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -23.2165
            </span>
            <a href="https://arxiv.org/abs/2411.03976" target="_blank" rel="noopener noreferrer">HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion Segmentation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ziyuan Ding, Yixiong Liang, Shichao Kan, Qing Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">High resolution is crucial for precise segmentation in fundus images, yet handling high-resolution inputs incurs considerable GPU memory costs, with diminishing performance gains as overhead increases. To address this issue while tackling the challenge of segmenting tiny objects, recent studies have</span>
            
            <span class="abstract-full" style="display: none;">High resolution is crucial for precise segmentation in fundus images, yet handling high-resolution inputs incurs considerable GPU memory costs, with diminishing performance gains as overhead increases. To address this issue while tackling the challenge of segmenting tiny objects, recent studies have explored local-global fusion methods. These methods preserve fine details using local regions and capture long-range context information from downscaled global images. However, the necessity of multiple forward passes inevitably incurs significant computational overhead, adversely affecting inference speed. In this paper, we propose HRDecoder, a simple High-Resolution Decoder network for fundus lesion segmentation. It integrates a high-resolution representation learning module to capture fine-grained local features and a high-resolution fusion module to fuse multi-scale predictions. Our method effectively improves the overall segmentation accuracy of fundus lesions while consuming reasonable memory and computational overhead, and maintaining satisfying inference speed. Experimental results on the IDRiD and DDR datasets demonstrate the effectiveness of our method. Code is available at https://github.com/CVIU-CSU/HRDecoder.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 71.3%">
                        Medicine
                    </span>
            <!-- LLMs: 6.0 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Math: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -24.5141
            </span>
            <a href="https://arxiv.org/abs/2504.08329" target="_blank" rel="noopener noreferrer">MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Junmo Kim, Namkyeong Lee, Jiwon Kim, Kwangsoo Kim | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generali</span>
            
            <span class="abstract-full" style="display: none;">Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generality of EHR foundation models and the integration of models trained with different vocabularies. To deal with this problem, we propose MedRep for EHR foundation models based on the observational medical outcome partnership (OMOP) common data model (CDM), providing the integrated medical concept representations and the basic data augmentation strategy for patient trajectories. For concept representation learning, we enrich the information of each concept with a minimal definition through large language model (LLM) prompts and enhance the text-based representations through graph ontology of OMOP vocabulary. Trajectory augmentation randomly replaces selected concepts with other similar concepts that have closely related representations to let the model practice with the concepts out-of-vocabulary. Finally, we demonstrate that EHR foundation models trained with MedRep better maintain the prediction performance in external datasets. Our code implementation is publicly available at https://github.com/kicarussays/MedRep.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #4ff278" title="Confidence: 79.7%">
                        Medicine
                    </span>
            <!-- LLMs: 5.5 -->
                
            <!-- Federated Learning: 2.9 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -26.2138
            </span>
            <a href="https://arxiv.org/abs/2504.09213" target="_blank" rel="noopener noreferrer">Spiking Neural Network for Intra-cortical Brain Signal Decoding</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Song Yang, Haotian Fu, Herui Zhang, Peng Zhang, Wei Li, Dongrui Wu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Decoding brain signals accurately and efficiently is crucial for intra-cortical brain-computer interfaces. Traditional decoding approaches based on neural activity vector features suffer from low accuracy, whereas deep learning based approaches have high computational cost. To improve both the decod</span>
            
            <span class="abstract-full" style="display: none;">Decoding brain signals accurately and efficiently is crucial for intra-cortical brain-computer interfaces. Traditional decoding approaches based on neural activity vector features suffer from low accuracy, whereas deep learning based approaches have high computational cost. To improve both the decoding accuracy and efficiency, this paper proposes a spiking neural network (SNN) for effective and energy-efficient intra-cortical brain signal decoding. We also propose a feature fusion approach, which integrates the manually extracted neural activity vector features with those extracted by a deep neural network, to further improve the decoding accuracy. Experiments in decoding motor-related intra-cortical brain signals of two rhesus macaques demonstrated that our SNN model achieved higher accuracy than traditional artificial neural networks; more importantly, it was tens or hundreds of times more efficient. The SNN model is very suitable for high precision and low power applications like intra-cortical brain-computer interfaces.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #e2e4bd" title="Confidence: 70.0%">
                        SpikingNN
                    </span>
            <!-- Medicine: 10.9 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- LLMs: 3.6 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- GNN: 2.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -26.5188
            </span>
            <a href="https://arxiv.org/abs/2504.08659" target="_blank" rel="noopener noreferrer">BowelRCNN: Region-based Convolutional Neural Network System for Bowel Sound Auscultation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Igor Matynia, Robert Nowak | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Sound events representing intestinal activity detection is a diagnostic tool with potential to identify gastrointestinal conditions. This article introduces BowelRCNN, a novel bowel sound detection system that uses audio recording, spectrogram analysys and region-based convolutional neural network (</span>
            
            <span class="abstract-full" style="display: none;">Sound events representing intestinal activity detection is a diagnostic tool with potential to identify gastrointestinal conditions. This article introduces BowelRCNN, a novel bowel sound detection system that uses audio recording, spectrogram analysys and region-based convolutional neural network (RCNN) architecture. The system was trained and validated on a real recording dataset gathered from 19 patients, comprising 60 minutes of prepared and annotated audio data. BowelRCNN achieved a classification accuracy of 96% and an F1 score of 71%. This research highlights the feasibility of using CNN architectures for bowel sound auscultation, achieving results comparable to those of recurrent-convolutional methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 71.2%">
                        Medicine
                    </span>
            <!-- LLMs: 4.6 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- SpikingNN: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -28.2707
            </span>
            <a href="https://arxiv.org/abs/2106.00571" target="_blank" rel="noopener noreferrer">A reduced 3D-0D FSI model of the aortic valve including leaflet curvature</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ivan Fumagalli, Luca Dede', Alfio Quarteroni | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We introduce an innovative lumped-parameter model of the aortic valve, designed to efficiently simulate the impact of valve dynamics on blood flow. Our reduced model includes the elastic effects associated with the leaflets' curvature and the stress exchanged with the blood flow. The introduction of</span>
            
            <span class="abstract-full" style="display: none;">We introduce an innovative lumped-parameter model of the aortic valve, designed to efficiently simulate the impact of valve dynamics on blood flow. Our reduced model includes the elastic effects associated with the leaflets' curvature and the stress exchanged with the blood flow. The introduction of a lumped-parameter model based on momentum balance entails an easier calibration of the model parameters: phenomenological-based models, on the other hand, typically have numerous parameters. This model is coupled to 3D Navier-Stokes equations describing the blood flow, where the moving valve leaflets are immersed in the fluid domain by a resistive method. A stabilized finite element method with a BDF time scheme is adopted for the discretization of the coupled problem, and the computational results show the suitability of the system in representing the leaflet motion, the blood flow in the ascending aorta, and the pressure jump across the leaflets. Both physiological and stenotic configurations are investigated, and we analyze the effects of different treatments for the leaflet velocity on the blood flow.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #4ff278" title="Confidence: 82.6%">
                        Medicine
                    </span>
            <!-- Math: 4.3 -->
                
            <!-- Reinforcement Learning: 3.9 -->
                
            <!-- Federated Learning: 2.7 -->
                
            <!-- Robotics: 2.4 -->
                
            <!-- Pathfinding: 1.6 -->
                
            <!-- LLMs: 1.3 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- Quantum Computing: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -28.2912
            </span>
            <a href="https://arxiv.org/abs/2504.08876" target="_blank" rel="noopener noreferrer">Is Productivity in Quantum Programming Equivalent to Expressiveness?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Francini Corrales-Garro, Danny Valerio-Ram\'irez, Santiago N\'u\~ez-Corrales | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The expressiveness of quantum programming languages plays a crucial role in the efficient and comprehensible representation of quantum algorithms. Unlike classical programming languages, which offer mature and well-defined abstraction mechanisms, quantum languages must integrate cognitively challeng</span>
            
            <span class="abstract-full" style="display: none;">The expressiveness of quantum programming languages plays a crucial role in the efficient and comprehensible representation of quantum algorithms. Unlike classical programming languages, which offer mature and well-defined abstraction mechanisms, quantum languages must integrate cognitively challenging concepts such as superposition, interference and entanglement while maintaining clarity and usability. However, identifying and characterizing differences in expressiveness between quantum programming paradigms remains an open area of study. Our work investigates the landscape of expressiveness through a comparative analysis of hosted quantum programming languages such as Qiskit, Cirq, Qrisp, and quAPL, and standalone languages including Q# and Qmod. We focused on evaluating how different quantum programming languages support the implementation of core quantum algorithms -- Deutsch-Jozsa, Simon, Bernstein-Vazirani, and Grover -- using expressiveness metrics: Lines of Code (LOC), Cyclomatic Complexity (CC), and Halstead Complexity (HC) metrics as proxies for developer productivity. Our findings suggest that different quantum programming paradigms offer distinct trade-offs between expressiveness and productivity, highlighting the importance of language design in quantum software development.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Quantum Computing: 25.2 -->
                
            <!-- LLMs: 10.7 -->
                
            <!-- Medicine: 8.8 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -30.0108
            </span>
            <a href="https://arxiv.org/abs/2504.08469" target="_blank" rel="noopener noreferrer">Artifact detection and localization in single-channel mobile EEG for sleep research using deep learning and attention mechanisms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Khrystyna Semkiv, Jia Zhang, Maria Laura Ferster, Walter Karlen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Artifacts in the electroencephalogram (EEG) degrade signal quality and impact the analysis of brain activity. Current methods for detecting artifacts in sleep EEG rely on simple threshold-based algorithms that require manual intervention, which is time-consuming and impractical due to the vast volum</span>
            
            <span class="abstract-full" style="display: none;">Artifacts in the electroencephalogram (EEG) degrade signal quality and impact the analysis of brain activity. Current methods for detecting artifacts in sleep EEG rely on simple threshold-based algorithms that require manual intervention, which is time-consuming and impractical due to the vast volume of data that novel mobile recording systems generate. We propose a convolutional neural network (CNN) model incorporating a convolutional block attention module (CNN-CBAM) to detect and identify the location of artifacts in the sleep EEG with attention maps. We benchmarked this model against six other machine learning and signal processing approaches. We trained/tuned all models on 72 manually annotated EEG recordings obtained during home-based monitoring from 18 healthy participants with a mean (SD) age of 68.05 y ($\pm$5.02). We tested them on 26 separate recordings from 6 healthy participants with a mean (SD) age of 68.33 y ($\pm$4.08), with contained artifacts in 4\% of epochs. CNN-CBAM achieved the highest area under the receiver operating characteristic curve (0.88), sensitivity (0.81), and specificity (0.86) when compared to the other approaches. The attention maps from CNN-CBAM localized artifacts within the epoch with a sensitivity of 0.71 and specificity of 0.67. This work demonstrates the feasibility of automating the detection and localization of artifacts in wearable sleep EEG.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #4ff278" title="Confidence: 81.7%">
                        Medicine
                    </span>
            <!-- Quantum Computing: 3.1 -->
                
            <!-- LLMs: 3.0 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -32.6393
            </span>
            <a href="https://arxiv.org/abs/2504.07822" target="_blank" rel="noopener noreferrer">DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wanna Cui, Peizheng Wang, Faliang Yin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with stati</span>
            
            <span class="abstract-full" style="display: none;">Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 68.2%">
                        GNN
                    </span>
            <!-- LLMs: 7.7 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Medicine: 3.9 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- Reinforcement Learning: 2.7 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Attention: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -32.6514
            </span>
            <a href="https://arxiv.org/abs/2504.09909" target="_blank" rel="noopener noreferrer">Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Farha Nausheen, Khandakar Ahmed, M Imad Khan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent developments, deep learning methodologies applied to Natural Language Processing (NLP) have revealed a paradox: They improve performance but demand considerable data and resources for their training. Alternatively, quantum computing exploits the principles of quantum mechanics to overcome </span>
            
            <span class="abstract-full" style="display: none;">In recent developments, deep learning methodologies applied to Natural Language Processing (NLP) have revealed a paradox: They improve performance but demand considerable data and resources for their training. Alternatively, quantum computing exploits the principles of quantum mechanics to overcome the computational limitations of current methodologies, thereby establishing an emerging field known as quantum natural language processing (QNLP). This domain holds the potential to attain a quantum advantage in the processing of linguistic structures, surpassing classical models in both efficiency and accuracy. In this paper, it is proposed to categorise QNLP models based on quantum computing principles, architecture, and computational approaches. This paper attempts to provide a survey on how quantum meets language by mapping state-of-the-art in this area, embracing quantum encoding techniques for classical data, QNLP models for prevalent NLP tasks, and quantum optimisation techniques for hyper parameter tuning. The landscape of quantum computing approaches applied to various NLP tasks is summarised by showcasing the specific QNLP methods used, and the popularity of these methods is indicated by their count. From the findings, it is observed that QNLP approaches are still limited to small data sets, with only a few models explored extensively, and there is increasing interest in the application of quantum computing to natural language processing tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Quantum Computing: 38.7 -->
                
            <!-- LLMs: 4.4 -->
                
            <!-- Medicine: 4.2 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Evolutionary Algorithms: 2.0 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Math: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -33.4147
            </span>
            <a href="https://arxiv.org/abs/2504.08310" target="_blank" rel="noopener noreferrer">DeQompile: quantum circuit decompilation using genetic programming for explainable quantum architecture search</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shubing Xie, Aritra Sarkar, Sebastian Feld | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Demonstrating quantum advantage using conventional quantum algorithms remains challenging on current noisy gate-based quantum computers. Automated quantum circuit synthesis via quantum machine learning has emerged as a promising solution, employing trainable parametric quantum circuits to alleviate </span>
            
            <span class="abstract-full" style="display: none;">Demonstrating quantum advantage using conventional quantum algorithms remains challenging on current noisy gate-based quantum computers. Automated quantum circuit synthesis via quantum machine learning has emerged as a promising solution, employing trainable parametric quantum circuits to alleviate this. The circuit ansatz in these solutions is often designed through reinforcement learning-based quantum architecture search when the domain knowledge of the problem and hardware are not effective. However, the interpretability of these synthesized circuits remains a significant bottleneck, limiting their scalability and applicability across diverse problem domains.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Quantum Computing: 47.1 -->
                
            <!-- Medicine: 4.9 -->
                
            <!-- LLMs: 3.6 -->
                
            <!-- Evolutionary Algorithms: 3.5 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -33.7916
            </span>
            <a href="https://arxiv.org/abs/2504.03175" target="_blank" rel="noopener noreferrer">Mathematical Modeling of Option Pricing with an Extended Black-Scholes Framework</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Nikhil Shivakumar Nayak | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This study investigates enhancing option pricing by extending the Black-Scholes model to include stochastic volatility and interest rate variability within the Partial Differential Equation (PDE). The PDE is solved using the finite difference method. The extended Black-Scholes model and a machine le</span>
            
            <span class="abstract-full" style="display: none;">This study investigates enhancing option pricing by extending the Black-Scholes model to include stochastic volatility and interest rate variability within the Partial Differential Equation (PDE). The PDE is solved using the finite difference method. The extended Black-Scholes model and a machine learning-based LSTM model are developed and evaluated for pricing Google stock options. Both models were backtested using historical market data. While the LSTM model exhibited higher predictive accuracy, the finite difference method demonstrated superior computational efficiency. This work provides insights into model performance under varying market conditions and emphasizes the potential of hybrid approaches for robust financial modeling.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #c48298" title="Confidence: 72.8%">
                        Finance
                    </span>
            <!-- LLMs: 16.4 -->
                
            <!-- Medicine: 3.6 -->
                
            <!-- Blockchain: 2.8 -->
                
            <!-- 3D: 2.4 -->
                
            <!-- RAG: 2.4 -->
                
            <!-- T2I: 2.2 -->
                
            <!-- HPO and AutoML: 1.8 -->
                
            <!-- Fuzzy Logic: 1.5 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Bayesian Optimization: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -35.1392
            </span>
            <a href="https://arxiv.org/abs/2412.09404" target="_blank" rel="noopener noreferrer">Opinion de-polarization of social networks with GNNs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Konstantinos Mylonas, Thrasyvoulos Spyropoulos | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Nowadays, social media is the ground for political debate and exchange of opinions. There is a significant amount of research that suggests that social media are highly polarized. A phenomenon that is commonly observed is the echo chamber structure, where users are organized in polarized communities</span>
            
            <span class="abstract-full" style="display: none;">Nowadays, social media is the ground for political debate and exchange of opinions. There is a significant amount of research that suggests that social media are highly polarized. A phenomenon that is commonly observed is the echo chamber structure, where users are organized in polarized communities and form connections only with similar-minded individuals, limiting themselves to consume specific content. In this paper we explore a way to decrease the polarization of networks with two echo chambers. Particularly, we observe that if some users adopt a moderate opinion about a topic, the polarization of the network decreases. Based on this observation, we propose an efficient algorithm to identify a good set of K users, such that if they adopt a moderate stance around a topic, the polarization is minimized. Our algorithm employs a Graph Neural Network and thus it can handle large graphs more effectively than other approaches</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 69.9%">
                        GNN
                    </span>
            <!-- LLMs: 4.9 -->
                
            <!-- Medicine: 3.9 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.7 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -40.3774
            </span>
            <a href="https://arxiv.org/abs/2504.07048" target="_blank" rel="noopener noreferrer">Context Switching for Secure Multi-programming of Near-Term Quantum Computers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Avinash Kumar, Meng Wang, Chenxu Liu, Ang Li, Prashant J. Nair, Poulami Das | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Multi-programming quantum computers improve device utilization and throughput. However, crosstalk from concurrent two-qubit CNOT gates poses security risks, compromising the fidelity and output of co-running victim programs. We design Zero Knowledge Tampering Attacks (ZKTAs), using which attackers c</span>
            
            <span class="abstract-full" style="display: none;">Multi-programming quantum computers improve device utilization and throughput. However, crosstalk from concurrent two-qubit CNOT gates poses security risks, compromising the fidelity and output of co-running victim programs. We design Zero Knowledge Tampering Attacks (ZKTAs), using which attackers can exploit crosstalk without knowledge of the hardware error profile. ZKTAs can alter victim program outputs in 40% of cases on commercial systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 68.7%">
                        Quantum Computing
                    </span>
            <!-- LLMs: 8.3 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- SpikingNN: 1.6 -->
                
            <!-- Hardware: 1.3 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -47.7128
            </span>
            <a href="https://arxiv.org/abs/2405.08190" target="_blank" rel="noopener noreferrer">Barren plateaus are amplified by the dimension of qudits</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lucas Friedrich, Tiago de Souza Farias, Jonas Maziero | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for attaining quantum advantage in diverse scientific and technological domains, notably within Quantum Neural Networks. However, despite their potential, VQAs encounter significant obstacles, chief among them being the vanishi</span>
            
            <span class="abstract-full" style="display: none;">Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for attaining quantum advantage in diverse scientific and technological domains, notably within Quantum Neural Networks. However, despite their potential, VQAs encounter significant obstacles, chief among them being the vanishing gradient problem, commonly referred to as barren plateaus. In this article, through meticulous analysis, we demonstrate that existing literature implicitly suggests the intrinsic influence of qudit dimensionality on barren plateaus. To instantiate these findings, we present numerical results that exemplify the impact of qudit dimensionality on barren plateaus. Therefore, despite the proposition of various error mitigation techniques, our results call for further scrutiny about their efficacy in the context of VQAs with qudits.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 73.8%">
                        Quantum Computing
                    </span>
            <!-- LLMs: 7.0 -->
                
            <!-- Medicine: 5.9 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- SpikingNN: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -49.7271
            </span>
            <a href="https://arxiv.org/abs/2410.03358" target="_blank" rel="noopener noreferrer">Oracle Separation Between Quantum Commitments and Quantum One-wayness</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: John Bostanci, Boyang Chen, Barak Nehoran | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We show that there exists an oracle relative to which quantum commitments exist but no (efficiently verifiable) one-way state generators exist. Both have been widely considered candidates for replacing one-way functions as the minimal assumption for cryptography: the weakest cryptographic assumption</span>
            
            <span class="abstract-full" style="display: none;">We show that there exists an oracle relative to which quantum commitments exist but no (efficiently verifiable) one-way state generators exist. Both have been widely considered candidates for replacing one-way functions as the minimal assumption for cryptography: the weakest cryptographic assumption implied by all of computational cryptography. Recent work has shown that commitments can be constructed from one-way state generators, but the other direction has remained open. Our results rule out any black-box construction, and thus settles this crucial open problem, suggesting that quantum commitments (as well as its equivalency class of EFI pairs, quantum oblivious transfer, and secure quantum multiparty computation) appear to be strictly weakest among all known cryptographic primitives.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 70.9%">
                        Quantum Computing
                    </span>
            <!-- LLMs: 7.4 -->
                
            <!-- Medicine: 5.3 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Hardware: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -53.0414
            </span>
            <a href="https://arxiv.org/abs/2504.06951" target="_blank" rel="noopener noreferrer">GLT hidden structures in mean-field quantum spin systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Christiaan J. F. van de Ven, Muhammad Faisal Khan, S. Serra-Capizzano | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work explores structured matrix sequences arising in mean-field quantum spin systems. We express these sequences within the framework of generalized locally Toeplitz (GLT) $*$-algebras, leveraging the fact that each GLT matrix sequence has a unique GLT symbol. This symbol characterizes both the</span>
            
            <span class="abstract-full" style="display: none;">This work explores structured matrix sequences arising in mean-field quantum spin systems. We express these sequences within the framework of generalized locally Toeplitz (GLT) $*$-algebras, leveraging the fact that each GLT matrix sequence has a unique GLT symbol. This symbol characterizes both the asymptotic singular value distribution and, for Hermitian or quasi-Hermitian sequences, the asymptotic spectral distribution. Specifically, we analyze two cases of real symmetric matrix sequences stemming from mean-field quantum spin systems and determine their associated distributions using GLT theory. Our study concludes with visualizations and numerical tests that validate the theoretical findings, followed by a discussion of open problems and future directions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #d37d97" title="Confidence: 75.7%">
                        Quantum Computing
                    </span>
            <!-- Medicine: 6.2 -->
                
            <!-- LLMs: 4.8 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -57.2052
            </span>
            <a href="https://arxiv.org/abs/2504.08170" target="_blank" rel="noopener noreferrer">Efficient measurement of neutral-atom qubits with matched filters</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Robert M. Kent, Linipun Phuttitarn, Chaithanya Naik Mude, Swamit Tannu, Mark Saffman, Gregory Lafyatis, Daniel J. Gauthier | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Quantum computers require high-fidelity measurement of many qubits to achieve a quantum advantage. Traditional approaches suffer from readout crosstalk for a neutral-atom quantum processor with a tightly spaced array. Although classical machine learning algorithms based on convolutional neural netwo</span>
            
            <span class="abstract-full" style="display: none;">Quantum computers require high-fidelity measurement of many qubits to achieve a quantum advantage. Traditional approaches suffer from readout crosstalk for a neutral-atom quantum processor with a tightly spaced array. Although classical machine learning algorithms based on convolutional neural networks can improve fidelity, they are computationally expensive, making it difficult to scale them to large qubit counts. We present two simpler and scalable machine learning algorithms that realize matched filters for the readout problem. One is a local model that focuses on a single qubit, and the other uses information from neighboring qubits in the array to prevent crosstalk among the qubits. We demonstrate error reductions of up to 32% and 43% for the site and array models, respectively, compared to a conventional Gaussian threshold approach. Additionally, our array model uses two orders of magnitude fewer trainable parameters and four orders of magnitude fewer multiplications and nonlinear function evaluations than a recent convolutional neural network approach, with only a minor (3.5%) increase in error across different readout times. Another strength of our approach is its physical interpretability: the learned filter can be visualized to provide insights into experimental imperfections. We also show that a convolutional neural network model for improved can be pruned to have 70x and 4000x fewer parameters, respectively, while maintaining similar errors. Our work shows that simple machine learning approaches can achieve high-fidelity qubit measurements while remaining scalable to systems with larger qubit counts.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 74.9%">
                        Quantum Computing
                    </span>
            <!-- Medicine: 6.2 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- LLMs: 3.1 -->
                
            <!-- Reinforcement Learning: 2.9 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Math: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -58.1925
            </span>
            <a href="https://arxiv.org/abs/2402.14696" target="_blank" rel="noopener noreferrer">On Schr\"odingerization based quantum algorithms for linear dynamical systems with inhomogeneous terms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shi Jin, Nana Liu, Chuwen Ma | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We analyze the Schr\"odingerization method for quantum simulation of a general class of non-unitary dynamics with inhomogeneous source terms. The Schr\"odingerization technique, introduced in [31], transforms any linear ordinary and partial differential equations with non-unitary dynamics into a sys</span>
            
            <span class="abstract-full" style="display: none;">We analyze the Schr\"odingerization method for quantum simulation of a general class of non-unitary dynamics with inhomogeneous source terms. The Schr\"odingerization technique, introduced in [31], transforms any linear ordinary and partial differential equations with non-unitary dynamics into a system under unitary dynamics via a warped phase transition that maps the equations into a higher dimension, making them suitable for quantum simulation. This technique can also be applied to these equations with inhomogeneous terms modeling source or forcing terms, or boundary and interface conditions, and discrete dynamical systems such as iterative methods in numerical linear algebra, through extra equations in the system. Difficulty arises with the presence of inhomogeneous terms since they can change the stability of the original system. In this paper, we systematically study-both theoretically and numerically-the important issue of recovering the original variables from the Schr\"odingerized equations, even when the evolution operator contains unstable modes. We show that, even with unstable modes, one can still construct a stable scheme; however, to recover the original variable, one needs to use suitable data in the extended space. We analyze and compare both the discrete and continuous Fourier transforms used in the extended dimension and derive corresponding error estimates, which allow one to use the more appropriate transform for specific equations. We also provide a smoother initialization for the Schr\"odingerized system to gain higher-order accuracy in the extended space. We homogenize the inhomogeneous terms with a stretch transformation, making it easier to recover the original variable. Our recovery technique also provides a simple and generic framework to solve general ill-posed problems in a computationally stable way.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #d37d97" title="Confidence: 78.7%">
                        Quantum Computing
                    </span>
            <!-- Reinforcement Learning: 3.7 -->
                
            <!-- Medicine: 3.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- LLMs: 1.9 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -73.8499
            </span>
            <a href="https://arxiv.org/abs/2503.10790" target="_blank" rel="noopener noreferrer">Quantum Error Detection For Early Term Fault-Tolerant Quantum Algorithms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tom Ginsberg, Vyom Patel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Quantum error detection (QED) offers a promising pathway to fault tolerance in near-term quantum devices by balancing error suppression with minimal resource overhead. However, its practical utility hinges on optimizing design parameters-such as syndrome measurement frequency-to avoid diminishing re</span>
            
            <span class="abstract-full" style="display: none;">Quantum error detection (QED) offers a promising pathway to fault tolerance in near-term quantum devices by balancing error suppression with minimal resource overhead. However, its practical utility hinges on optimizing design parameters-such as syndrome measurement frequency-to avoid diminishing returns from detection overhead. In this work, we present a comprehensive framework for fault-tolerant compilation and simulation of quantum algorithms using [[n, n-2, 2]] codes, which enable low-qubit-overhead error detection and a simple nearly fault-tolerant universal set of operations. We demonstrate and analyze our pipeline with a purely statistical interpretation and through the implementation of Grover's search algorithm. Our results are used to answer the question is quantum error detection a worthwhile avenue for early-term fault tolerance, and if so how can we get the most out of it? Simulations under the circuit-level noise model reveal that finding optimal syndrome schedules improves algorithm success probabilities by an average of 6.7x but eventual statistical limits from post-selection in noisy/resource-limited regimes constrain scalability. Furthermore, we propose a simple data-driven approach to predict fault tolerant compilation parameters, such as optimal syndrome schedules, and expected fault tolerant performance gains based on circuit and noise features. These results provide actionable guidelines for implementing QED in early-term quantum experiments and underscore its role as a pragmatic, constant-overhead error mitigation layer for shallow algorithms. To aid in further research, we release all simulation data computed for this work and provide an experimental QED compiler at https://codeqraft.xyz/qed.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 70.2%">
                        Quantum Computing
                    </span>
            <!-- Medicine: 8.4 -->
                
            <!-- LLMs: 4.5 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -83.4762
            </span>
            <a href="https://arxiv.org/abs/2504.07589" target="_blank" rel="noopener noreferrer">Copy-and-Paste? Identifying EVM-Inequivalent Code Smells in Multi-chain Reuse Contracts</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zexu Wang, Jiachi Chen, Tao Zhang, Yu Zhang, Weizhe Zhang, Yuming Feng, Zibin Zheng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As the development of Solidity contracts on Ethereum, more developers are reusing them on other compatible blockchains. However, developers may overlook the differences between the designs of the blockchain system, such as the Gas Mechanism and Consensus Protocol, leading to the same contracts on di</span>
            
            <span class="abstract-full" style="display: none;">As the development of Solidity contracts on Ethereum, more developers are reusing them on other compatible blockchains. However, developers may overlook the differences between the designs of the blockchain system, such as the Gas Mechanism and Consensus Protocol, leading to the same contracts on different blockchains not being able to achieve consistent execution as on Ethereum. This inconsistency reveals design flaws in reused contracts, exposing code smells that hinder code reusability, and we define this inconsistency as EVM-Inequivalent Code Smells. In this paper, we conducted the first empirical study to reveal the causes and characteristics of EVM-Inequivalent Code Smells. To ensure the identified smells reflect real developer concerns, we collected and analyzed 1,379 security audit reports and 326 Stack Overflow posts related to reused contracts on EVM-compatible blockchains, such as Binance Smart Chain (BSC) and Polygon. Using the open card sorting method, we defined six types of EVM-Inequivalent Code Smells. For automated detection, we developed a tool named EquivGuard. It employs static taint analysis to identify key paths from different patterns and uses symbolic execution to verify path reachability. Our analysis of 905,948 contracts across six major blockchains shows that EVM-Inequivalent Code Smells are widespread, with an average prevalence of 17.70%. While contracts with code smells do not necessarily lead to financial loss and attacks, their high prevalence and significant asset management underscore the potential threats of reusing these smelly Ethereum contracts. Thus, developers are advised to abandon Copy-and-Paste programming practices and detect EVM-Inequivalent Code Smells before reusing Ethereum contracts.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #3cc377" title="Confidence: 54.6%">
                        Blockchain
                    </span>
            <!-- LLMs: 14.3 -->
                
            <!-- Medicine: 3.1 -->
                
            <!-- Quantum Computing: 1.7 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -93.0951
            </span>
            <a href="https://arxiv.org/abs/2305.14046" target="_blank" rel="noopener noreferrer">Enhancing Smart Contract Security Analysis with Execution Property Graphs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kaihua Qin, Zhe Ye, Zhun Wang, Weilin Li, Liyi Zhou, Chao Zhang, Dawn Song, Arthur Gervais | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Smart contract vulnerabilities have led to significant financial losses, with their increasing complexity rendering outright prevention of hacks increasingly challenging. This trend highlights the crucial need for advanced forensic analysis and real-time intrusion detection, where dynamic analysis p</span>
            
            <span class="abstract-full" style="display: none;">Smart contract vulnerabilities have led to significant financial losses, with their increasing complexity rendering outright prevention of hacks increasingly challenging. This trend highlights the crucial need for advanced forensic analysis and real-time intrusion detection, where dynamic analysis plays a key role in dissecting smart contract executions. Therefore, there is a pressing need for a unified and generic representation of smart contract executions, complemented by an efficient methodology that enables the modeling and identification of a broad spectrum of emerging attacks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #3cc377" title="Confidence: 75.0%">
                        Blockchain
                    </span>
            <!-- LLMs: 16.1 -->
                
            <!-- RAG: 3.3 -->
                
            <!-- 3D: 2.9 -->
                
            <!-- T2I: 2.1 -->
                
            <!-- HPO and AutoML: 1.7 -->
                
            <!-- Medicine: 1.6 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Finance: 1.3 -->
                
            <!-- Bayesian Optimization: 1.2 -->
                
            <!-- Quality Diversity: 1.1 -->
                
            <!-- Datasets: 1.1 -->
                
            
        </div>
    </div>
    
    
    <div id="jsonPopup" class="json-popup">
        <pre id="jsonContent"></pre>
        <button onclick="copyJson()">Copy to Clipboard</button>
        <button onclick="closePopup()">Close</button>
    </div>

    <script>
        function extractPaperData(paperElement) {
            const titleElement = paperElement.querySelector('.paper-title a');
            const metaElement = paperElement.querySelector('.paper-meta');
            const abstractElement = paperElement.querySelector('.paper-abstract');
            const tagsElement = paperElement.querySelector('.paper-tags');
            
            const authorsText = metaElement.textContent.split('|')[0].replace('Authors:', '').trim();
            const dateText = metaElement.textContent.split('|')[1].replace('Date:', '').trim();
            
            const paperData = {
                title: titleElement.textContent,
                url: titleElement.href,
                authors: authorsText.split(',').map(author => author.trim()),
                created: dateText,
                abstract: abstractElement.querySelector('.abstract-full').textContent
            };
            
            return paperData;
        }

        function showJson(paperElement) {
            const popup = document.getElementById('jsonPopup');
            const content = document.getElementById('jsonContent');
            const paperData = extractPaperData(paperElement);
            content.textContent = JSON.stringify(paperData, null, null);
            popup.style.display = 'block';
            document.addEventListener('click', function closePopupOnClick(event) {
                if (!popup.contains(event.target)) {
                    popup.style.display = 'none';
                    document.removeEventListener('click', closePopupOnClick);
                }
            });
        }
        function toggleAbstract(element) {
            const abstract = element.parentElement;
            const short = abstract.querySelector('.abstract-short');
            const full = abstract.querySelector('.abstract-full');
            const lowConfidenceTags = abstract.parentElement.querySelectorAll('.tag-badge.low-confidence');
            
            if (element.textContent === '... more') {
                short.style.display = 'none';
                full.style.display = 'inline';
                element.textContent = ' less';
                lowConfidenceTags.forEach(tag => tag.style.display = 'inline-block');
            } else {
                short.style.display = 'inline';
                full.style.display = 'none';
                element.textContent = '... more';
                lowConfidenceTags.forEach(tag => tag.style.display = 'none');
            }
        }

        function closePopup() {
            document.getElementById('jsonPopup').style.display = 'none';
        }

        function copyJson() {
            const content = document.getElementById('jsonContent').textContent;
            navigator.clipboard.writeText(content).catch(() => {
                // If clipboard API is not available, just show the popup
                alert('Could not copy to clipboard. JSON is displayed in the popup.');
            });
        }

        // Close popup when clicking outside
        window.onclick = function(event) {
            const popup = document.getElementById('jsonPopup');
            if (event.target === popup) {
                popup.style.display = 'none';
            }
        }
    </script>
</body>
</html> 