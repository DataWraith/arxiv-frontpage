<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Frontpage</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .paper {
            margin-bottom: 30px;
            margin-top: 30px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .paper-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .paper-meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        .paper-abstract {
            margin-bottom: 10px;
        }
        .abstract-short {
            display: inline;
        }
        .abstract-full {
            display: none;
        }
        .more-link {
            color: blue;
            cursor: pointer;
            text-decoration: underline;
        }
        .tag-badge {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 5px;
            margin-bottom: 5px;
            border-radius: 3px;
            font-size: 0.8em;
            color: white;
        }
        .tag-badge.high-confidence {
            opacity: 1;
        }
        .tag-badge.low-confidence {
            opacity: 0.6;
            display: none;
        }
        .interestingness-score {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 10px;
            color: white;
            border-radius: 3px;
            font-weight: bold;
        }
        .interestingness-positive {
            background-color: #4CAF50;
        }
        .interestingness-negative {
            background-color: #f44336;
        }
        .interestingness-neutral {
            background-color: #9e9e9e;
        }
        .last-updated {
            text-align: right;
            color: #666;
            font-size: 0.9em;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .intro {
            text-align: center;
            max-width: 60em;
            margin: 0 auto;
            color: #888;
        }
        .copy-icon {
            display: inline-block;
            width: 16px;
            height: 16px;
            cursor: pointer;
            margin-left: 5px;
            opacity: 0.5;
        }
        .copy-icon:hover {
            opacity: 1;
        }
        .json-popup {
            display: none;
            position: fixed;
            top: 20px;
            right: 20px;
            background: white;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            max-width: 500px;
            max-height: 300px;
            overflow: auto;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        a {
            color: inherit;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h1 {
            text-align: center;
        }
        h1 a {
            text-decoration: underline;
        }
        .date-section {
            margin-bottom: 40px;
        }
        .date-header {
            color: #666;
            font-size: 1.5em;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #eee;
        }
    </style>
</head>
<body>
    <h1>
        <a href="https://github.com/DataWraith/arxiv-frontpage">DataWraith's</a> ArXiv Frontpage
    </h1>

    <div class="last-updated">
        Last updated: 2025-05-19
    </div>

    <p class="intro">
        This frontpage is made by scraping ArXiv's computer science RSS feed and tagging papers with a classifier.
    </p>

    <p class="intro">
        Each tag is weighted according to my preferences to compute a paper's <i>interestingness</i> score.
    </p>
    
    
    <div class="date-section">
        <h2 class="date-header">2025-05-19</h2>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.3945
                </span>
                <a href="https://arxiv.org/abs/2505.10954" target="_blank" rel="noopener noreferrer">Constrained Preferential Bayesian Optimization and Its Application in Banner Ad Design</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Koki Iwai, Yusuke Kumagae, Yuki Koyama, Masahiro Hamasaki, Masataka Goto
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Preferential Bayesian optimization (PBO) is a variant of Bayesian optimization that observes relative preferences (e.g., pairwise comparisons) instead of direct objective values, making it especially suitable for human-in-the-loop scenarios. However, real-world optimization tasks often involve inequ</span>
                
                <span class="abstract-full" style="display: none;">Preferential Bayesian optimization (PBO) is a variant of Bayesian optimization that observes relative preferences (e.g., pairwise comparisons) instead of direct objective values, making it especially suitable for human-in-the-loop scenarios. However, real-world optimization tasks often involve inequality constraints, which existing PBO methods have not yet addressed. To fill this gap, we propose constrained preferential Bayesian optimization (CPBO), an extension of PBO that incorporates inequality constraints for the first time. Specifically, we present a novel acquisition function for this purpose. Our technical evaluation shows that our CPBO method successfully identifies optimal solutions by focusing on exploring feasible regions. As a practical application, we also present a designer-in-the-loop system for banner ad design using CPBO, where the objective is the designer's subjective preference, and the constraint ensures a target predicted click-through rate. We conducted a user study with professional ad designers, demonstrating the potential benefits of our approach in guiding creative design under real-world constraints.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Bayesian Optimization: 10.3 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Federated Learning: 2.8 -->
                    
                <!-- Evolutionary Algorithms: 2.7 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.3739
                </span>
                <a href="https://arxiv.org/abs/2504.07742" target="_blank" rel="noopener noreferrer">Gradient-based Sample Selection for Faster Bayesian Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Qiyu Wei, Haowei Wang, Zirui Cao, Songhao Wang, Richard Allmendinger, Mauricio A \'Alvarez
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Bayesian optimization (BO) is an effective technique for black-box optimization. However, its applicability is typically limited to moderate-budget problems due to the cubic complexity in computing the Gaussian process (GP) surrogate model. In large-budget scenarios, directly employing the standard </span>
                
                <span class="abstract-full" style="display: none;">Bayesian optimization (BO) is an effective technique for black-box optimization. However, its applicability is typically limited to moderate-budget problems due to the cubic complexity in computing the Gaussian process (GP) surrogate model. In large-budget scenarios, directly employing the standard GP model faces significant challenges in computational time and resource requirements. In this paper, we propose a novel approach, gradient-based sample selection Bayesian Optimization (GSSBO), to enhance the computational efficiency of BO. The GP model is constructed on a selected set of samples instead of the whole dataset. These samples are selected by leveraging gradient information to maintain diversity and representation. We provide a theoretical analysis of the gradient-based sample selection strategy and obtain explicit sublinear regret bounds for our proposed framework. Extensive experiments on synthetic and real-world tasks demonstrate that our approach significantly reduces the computational cost of GP fitting in BO while maintaining optimization performance comparable to baseline methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Bayesian Optimization: 9.0 -->
                    
                <!-- Federated Learning: 3.5 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.3591
                </span>
                <a href="https://arxiv.org/abs/2505.10825" target="_blank" rel="noopener noreferrer">A High-Performance Thermal Infrared Object Detection Framework with Centralized Regulation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jinke Li, Yue Wu, Xiaoyan Yang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Thermal Infrared (TIR) technology involves the use of sensors to detect and measure infrared radiation emitted by objects, and it is widely utilized across a broad spectrum of applications. The advancements in object detection methods utilizing TIR images have sparked significant research interest. </span>
                
                <span class="abstract-full" style="display: none;">Thermal Infrared (TIR) technology involves the use of sensors to detect and measure infrared radiation emitted by objects, and it is widely utilized across a broad spectrum of applications. The advancements in object detection methods utilizing TIR images have sparked significant research interest. However, most traditional methods lack the capability to effectively extract and fuse local-global information, which is crucial for TIR-domain feature attention. In this study, we present a novel and efficient thermal infrared object detection framework, known as CRT-YOLO, that is based on centralized feature regulation, enabling the establishment of global-range interaction on TIR information. Our proposed model integrates efficient multi-scale attention (EMA) modules, which adeptly capture long-range dependencies while incurring minimal computational overhead. Additionally, it leverages the Centralized Feature Pyramid (CFP) network, which offers global regulation of TIR features. Extensive experiments conducted on two benchmark datasets demonstrate that our CRT-YOLO model significantly outperforms conventional methods for TIR image object detection. Furthermore, the ablation study provides compelling evidence of the effectiveness of our proposed modules, reinforcing the potential impact of our approach on advancing the field of thermal infrared object detection.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Computer Vision: 7.0 -->
                    
                <!-- Federated Learning: 5.6 -->
                    
                <!-- Medicine: 4.3 -->
                    
                <!-- Evolutionary Algorithms: 2.5 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Bayesian Optimization: 2.1 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.3555
                </span>
                <a href="https://arxiv.org/abs/2411.06780" target="_blank" rel="noopener noreferrer">SynCL: A Synergistic Training Strategy with Instance-Aware Contrastive Learning for End-to-End Multi-Camera 3D Tracking</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shubo Lin, Yutong Kou, Zirui Wu, Shaoru Wang, Bing Li, Weiming Hu, Jin Gao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While existing query-based 3D end-to-end visual trackers integrate detection and tracking via the tracking-by-attention paradigm, these two chicken-and-egg tasks encounter optimization difficulties when sharing the same parameters. Our findings reveal that these difficulties arise due to two inheren</span>
                
                <span class="abstract-full" style="display: none;">While existing query-based 3D end-to-end visual trackers integrate detection and tracking via the tracking-by-attention paradigm, these two chicken-and-egg tasks encounter optimization difficulties when sharing the same parameters. Our findings reveal that these difficulties arise due to two inherent constraints on the self-attention mechanism, i.e., over-deduplication for object queries and self-centric attention for track queries. In contrast, removing the self-attention mechanism not only minimally impacts regression predictions of the tracker, but also tends to generate more latent candidate boxes. Based on these analyses, we present SynCL, a novel plug-and-play synergistic training strategy designed to co-facilitate multi-task learning for detection and tracking. Specifically, we propose a Task-specific Hybrid Matching module for a weight-shared cross-attention-based decoder that matches the targets of track queries with multiple object queries to exploit promising candidates overlooked by the self-attention mechanism. To flexibly select optimal candidates for the one-to-many matching, we also design a Dynamic Query Filtering module controlled by model training status. Moreover, we introduce Instance-aware Contrastive Learning to break through the barrier of self-centric attention for track queries, effectively bridging the gap between detection and tracking. Without additional inference costs, SynCL consistently delivers improvements in various benchmarks and achieves state-of-the-art performance with $58.9\%$ AMOTA on the nuScenes dataset. Code and raw results will be publicly available.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Computer Vision: 6.6 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.3526
                </span>
                <a href="https://arxiv.org/abs/2505.10931" target="_blank" rel="noopener noreferrer">M4-SAR: A Multi-Resolution, Multi-Polarization, Multi-Scene, Multi-Source Dataset and Benchmark for Optical-SAR Fusion Object Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chao Wang, Wei Lu, Xiang Li, Jian Yang, Lei Luo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Single-source remote sensing object detection using optical or SAR images struggles in complex environments. Optical images offer rich textural details but are often affected by low-light, cloud-obscured, or low-resolution conditions, reducing the detection performance. SAR images are robust to weat</span>
                
                <span class="abstract-full" style="display: none;">Single-source remote sensing object detection using optical or SAR images struggles in complex environments. Optical images offer rich textural details but are often affected by low-light, cloud-obscured, or low-resolution conditions, reducing the detection performance. SAR images are robust to weather, but suffer from speckle noise and limited semantic expressiveness. Optical and SAR images provide complementary advantages, and fusing them can significantly improve the detection accuracy. However, progress in this field is hindered by the lack of large-scale, standardized datasets. To address these challenges, we propose the first comprehensive dataset for optical-SAR fusion object detection, named Multi-resolution, Multi-polarization, Multi-scene, Multi-source SAR dataset (M4-SAR). It contains 112,184 precisely aligned image pairs and nearly one million labeled instances with arbitrary orientations, spanning six key categories. To enable standardized evaluation, we develop a unified benchmarking toolkit that integrates six state-of-the-art multi-source fusion methods. Furthermore, we propose E2E-OSDet, a novel end-to-end multi-source fusion detection framework that mitigates cross-domain discrepancies and establishes a robust baseline for future studies. Extensive experiments on M4-SAR demonstrate that fusing optical and SAR data can improve $mAP$ by 5.7\% over single-source inputs, with particularly significant gains in complex environments. The dataset and code are publicly available at https://github.com/wchao0601/M4-SAR.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Computer Vision: 6.5 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- LLMs: 4.5 -->
                    
                <!-- Datasets: 3.1 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Hardware: 1.0 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.3113
                </span>
                <a href="https://arxiv.org/abs/2505.10843" target="_blank" rel="noopener noreferrer">Comparative Analysis of Black-Box Optimization Methods for Weather Intervention Design</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yuta Higuchi, Rikuto Nagai, Atsushi Okazaki, Masaki Ogura, Naoki Wakamiya
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As climate change increases the threat of weather-related disasters, research on weather control is gaining importance. The objective of weather control is to mitigate disaster risks by administering interventions with optimal timing, location, and intensity. However, the optimization process is hig</span>
                
                <span class="abstract-full" style="display: none;">As climate change increases the threat of weather-related disasters, research on weather control is gaining importance. The objective of weather control is to mitigate disaster risks by administering interventions with optimal timing, location, and intensity. However, the optimization process is highly challenging due to the vast scale and complexity of weather phenomena, which introduces two major challenges. First, obtaining accurate gradient information for optimization is difficult. In addition, numerical weather prediction (NWP) models demand enormous computational resources, necessitating parameter optimization with minimal function evaluations. To address these challenges, this study proposes a method for designing weather interventions based on black-box optimization, which enables efficient exploration without requiring gradient information. The proposed method is evaluated in two distinct control scenarios: one-shot initial value intervention and sequential intervention based on model predictive control. Furthermore, a comparative analysis is conducted among four representative black-box optimization methods in terms of total rainfall reduction. Experimental results show that Bayesian optimization achieves higher control effectiveness than the others, particularly in high-dimensional search spaces. These findings suggest that Bayesian optimization is a highly effective approach for weather intervention computation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Bayesian Optimization: 8.7 -->
                    
                <!-- Evolutionary Algorithms: 4.1 -->
                    
                <!-- Federated Learning: 3.6 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- LLMs: 2.3 -->
                    
                <!-- HPO and AutoML: 2.2 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.305
                </span>
                <a href="https://arxiv.org/abs/2403.11083" target="_blank" rel="noopener noreferrer">Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiaohao Xu, Yunkang Cao, Huaxin Zhang, Nong Sang, Xiaonan Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In t</span>
                
                <span class="abstract-full" style="display: none;">Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, our objective is to develop a generic anomaly detection model that can be applied in multiple scenarios. To achieve this, we custom-build generic visual language foundation models that possess extensive knowledge and robust reasoning abilities as anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers diverse prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling multi-modal anomaly detection and reasoning. Our preliminary studies demonstrate that combining visual and language prompts as conditions for customizing the models enhances anomaly detection performance. The customized models showcase the ability to detect anomalies across different data modalities such as images, point clouds, and videos. Qualitative case studies further highlight the anomaly detection and reasoning capabilities, particularly for multi-object scenes and temporal data. Our code is publicly available at https://github.com/Xiaohao-Xu/Customizable-VLM</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.3 -->
                    
                <!-- Computer Vision: 6.3 -->
                    
                <!-- Medicine: 4.7 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.2776
                </span>
                <a href="https://arxiv.org/abs/2505.11125" target="_blank" rel="noopener noreferrer">GraphOracle: A Foundation Model for Knowledge Graph Reasoning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Enjun Du, Siyi Liu, Yongqi Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Foundation models have demonstrated remarkable capabilities across various domains, but developing analogous models for knowledge graphs presents unique challenges due to their dynamic nature and the need for cross-domain reasoning. To address these issues, we introduce \textbf{\textsc{GraphOracle}}</span>
                
                <span class="abstract-full" style="display: none;">Foundation models have demonstrated remarkable capabilities across various domains, but developing analogous models for knowledge graphs presents unique challenges due to their dynamic nature and the need for cross-domain reasoning. To address these issues, we introduce \textbf{\textsc{GraphOracle}}, a relation-centric foundation model that unifies reasoning across knowledge graphs by converting them into Relation-Dependency Graphs (RDG), explicitly encoding compositional patterns with fewer edges than prior methods. A query-dependent attention mechanism is further developed to learn inductive representations for both relations and entities. Pre-training on diverse knowledge graphs, followed by minutes-level fine-tuning, enables effective generalization to unseen entities, relations, and entire graphs. Through comprehensive experiments on 31 diverse benchmarks spanning transductive, inductive, and cross-domain settings, we demonstrate consistent state-of-the-art performance with minimal adaptation, improving the prediction performance by up to 35\% compared to the strongest baselines.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.0 -->
                    
                <!-- Computer Vision: 5.6 -->
                    
                <!-- Medicine: 4.3 -->
                    
                <!-- GNN: 2.7 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- HPO and AutoML: 2.2 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.266
                </span>
                <a href="https://arxiv.org/abs/2407.06325" target="_blank" rel="noopener noreferrer">CONGO: Compressive Online Gradient Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jeremy Carleton, Prathik Vijaykumar, Divyanshu Saxena, Dheeraj Narasimha, Srinivas Shakkottai, Aditya Akella
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We address the challenge of zeroth-order online convex optimization where the objective function's gradient exhibits sparsity, indicating that only a small number of dimensions possess non-zero gradients. Our aim is to leverage this sparsity to obtain useful estimates of the objective function's gra</span>
                
                <span class="abstract-full" style="display: none;">We address the challenge of zeroth-order online convex optimization where the objective function's gradient exhibits sparsity, indicating that only a small number of dimensions possess non-zero gradients. Our aim is to leverage this sparsity to obtain useful estimates of the objective function's gradient even when the only information available is a limited number of function samples. Our motivation stems from the optimization of large-scale queueing networks that process time-sensitive jobs. Here, a job must be processed by potentially many queues in sequence to produce an output, and the service time at any queue is a function of the resources allocated to that queue. Since resources are costly, the end-to-end latency for jobs must be balanced with the overall cost of the resources used. While the number of queues is substantial, the latency function primarily reacts to resource changes in only a few, rendering the gradient sparse. We tackle this problem by introducing the Compressive Online Gradient Optimization framework which allows compressive sensing methods previously applied to stochastic optimization to achieve regret bounds with an optimal dependence on the time horizon without the full problem dimension appearing in the bound. For specific algorithms, we reduce the samples required per gradient estimate to scale with the gradient's sparsity factor rather than its full dimensionality. Numerical simulations and real-world microservices benchmarks demonstrate CONGO's superiority over gradient descent approaches that do not account for sparsity.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Bayesian Optimization: 5.9 -->
                    
                <!-- Federated Learning: 4.0 -->
                    
                <!-- Math: 3.5 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Cryptography: 1.8 -->
                    
                <!-- Finance: 1.4 -->
                    
                <!-- Game Theory: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.1903
                </span>
                <a href="https://arxiv.org/abs/2503.14162" target="_blank" rel="noopener noreferrer">EIAD: Explainable Industrial Anomaly Detection Via Multi-Modal Large Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zongyun Zhang, Jiacheng Ruan, Xian Gao, Ting Liu, Yuzhuo Fu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Industrial Anomaly Detection (IAD) is critical to ensure product quality during manufacturing. Although existing zero-shot defect segmentation and detection methods have shown effectiveness, they cannot provide detailed descriptions of the defects. Furthermore, the application of large multi-modal m</span>
                
                <span class="abstract-full" style="display: none;">Industrial Anomaly Detection (IAD) is critical to ensure product quality during manufacturing. Although existing zero-shot defect segmentation and detection methods have shown effectiveness, they cannot provide detailed descriptions of the defects. Furthermore, the application of large multi-modal models in IAD remains in its infancy, facing challenges in balancing question-answering (QA) performance and mask-based grounding capabilities, often owing to overfitting during the fine-tuning process. To address these challenges, we propose a novel approach that introduces a dedicated multi-modal defect localization module to decouple the dialog functionality from the core feature extraction. This decoupling is achieved through independent optimization objectives and tailored learning strategies. Additionally, we contribute to the first multi-modal industrial anomaly detection training dataset, named Defect Detection Question Answering (DDQA), encompassing a wide range of defect types and industrial scenarios. Unlike conventional datasets that rely on GPT-generated data, DDQA ensures authenticity and reliability and offers a robust foundation for model training. Experimental results demonstrate that our proposed method, Explainable Industrial Anomaly Detection Assistant (EIAD), achieves outstanding performance in defect detection and localization tasks. It not only significantly enhances accuracy but also improves interpretability. These advancements highlight the potential of EIAD for practical applications in industrial settings.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.5 -->
                    
                <!-- Computer Vision: 5.0 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.1898
                </span>
                <a href="https://arxiv.org/abs/2102.08993" target="_blank" rel="noopener noreferrer">Using Distance Correlation for Efficient Bayesian Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Takuya Kanazawa
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The need to collect data via expensive measurements of black-box functions is prevalent across science, engineering and medicine. As an example, hyperparameter tuning of a large AI model is critical to its predictive performance but is generally time-consuming and unwieldy. Bayesian optimization (BO</span>
                
                <span class="abstract-full" style="display: none;">The need to collect data via expensive measurements of black-box functions is prevalent across science, engineering and medicine. As an example, hyperparameter tuning of a large AI model is critical to its predictive performance but is generally time-consuming and unwieldy. Bayesian optimization (BO) is a collection of methods that aim to address this issue by means of Bayesian statistical inference. In this work, we put forward a BO scheme named BDC, which integrates BO with a statistical measure of association of two random variables called Distance Correlation. BDC balances exploration and exploitation automatically, and requires no manual hyperparameter tuning. We evaluate BDC on a range of benchmark tests and observe that it performs on per with popular BO methods such as the expected improvement and max-value entropy search. We also apply BDC to optimization of sequential integral observations of an unknown terrain and confirm its utility.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Bayesian Optimization: 5.6 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Medicine: 3.7 -->
                    
                <!-- Federated Learning: 2.8 -->
                    
                <!-- Evolutionary Algorithms: 2.6 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.1869
                </span>
                <a href="https://arxiv.org/abs/2405.12823" target="_blank" rel="noopener noreferrer">Chordal-NMF with Riemannian Multiplicative Update</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Flavia Esposito, Andersen Ang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Nonnegative Matrix Factorization (NMF) is the problem of approximating a given nonnegative matrix M through the product of two nonnegative low-rank matrices W and H. Traditionally NMF is tackled by optimizing a specific objective function evaluating the quality of the approximation. This assessment </span>
                
                <span class="abstract-full" style="display: none;">Nonnegative Matrix Factorization (NMF) is the problem of approximating a given nonnegative matrix M through the product of two nonnegative low-rank matrices W and H. Traditionally NMF is tackled by optimizing a specific objective function evaluating the quality of the approximation. This assessment is often done based on the Frobenius norm (F-norm). In this work, we argue that the F-norm, as the "point-to-point" distance, may not always be appropriate. Viewing from the perspective of cone, NMF may not naturally align with F-norm. So, a ray-to-ray chordal distance is proposed as an alternative way of measuring the quality of the approximation. As this measure corresponds to the Euclidean distance on the sphere, it motivates the use of manifold optimization techniques. We apply Riemannian optimization technique to solve chordal-NMF by casting it on a manifold. Unlike works on Riemannian optimization that require the manifold to be smooth, the nonnegativity in chordal-NMF defines a non-differentiable manifold. We propose a Riemannian Multiplicative Update (RMU), and showcase the effectiveness of the chordal-NMF on synthetic and real-world datasets.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Bayesian Optimization: 6.0 -->
                    
                <!-- Federated Learning: 5.8 -->
                    
                <!-- Evolutionary Algorithms: 4.5 -->
                    
                <!-- Math: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Cryptography: 1.7 -->
                    
                <!-- Medicine: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Finance: 1.1 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10567" target="_blank" rel="noopener noreferrer">M|D|$\infty$ Queue Busy Period and Busy Cycle Distributions Computational Calculus</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Manuel Alberto M. Ferreira
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Given the busy period and busy cycle major importance in queuing systems, it is crucial the knowledge of the respective distribution functions that is what allows the calculation of the important probabilities. For the M|G|$\infty$ queue system, there are no round form formulae for those distributio</span>
                
                <span class="abstract-full" style="display: none;">Given the busy period and busy cycle major importance in queuing systems, it is crucial the knowledge of the respective distribution functions that is what allows the calculation of the important probabilities. For the M|G|$\infty$ queue system, there are no round form formulae for those distribution functions. But, for the M|D|$\infty$ queue, due the fact that its busy period and busy cycle have both Laplace transform expression round forms, what does not happen for any other M|G|$\infty$ queue system, with an algorithm created by Platzman, Ammons and Bartholdi III, that allows the tail probabilities computation since the correspondent Laplace transform in round form is known, those distribution functions calculations are possible. Here, we will implement the algorithm through a FORTRAN program.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.1 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- LLMs: 1.8 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Bayesian Optimization: 1.7 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Cryptography: 1.5 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Finance: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10573" target="_blank" rel="noopener noreferrer">Measurement to Meaning: A Validity-Centered Framework for AI Evaluation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Olawale Salaudeen, Anka Reuel, Ahmed Ahmed, Suhana Bedi, Zachary Robertson, Sudharsan Sundar, Ben Domingue, Angelina Wang, Sanmi Koyejo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While the capabilities and utility of AI systems have advanced, rigorous norms for evaluating these systems have lagged. Grand claims, such as models achieving general reasoning capabilities, are supported with model performance on narrow benchmarks, like performance on graduate-level exam questions</span>
                
                <span class="abstract-full" style="display: none;">While the capabilities and utility of AI systems have advanced, rigorous norms for evaluating these systems have lagged. Grand claims, such as models achieving general reasoning capabilities, are supported with model performance on narrow benchmarks, like performance on graduate-level exam questions, which provide a limited and potentially misleading assessment. We provide a structured approach for reasoning about the types of evaluative claims that can be made given the available evidence. For instance, our framework helps determine whether performance on a mathematical benchmark is an indication of the ability to solve problems on math tests or instead indicates a broader ability to reason. Our framework is well-suited for the contemporary paradigm in machine learning, where various stakeholders provide measurements and evaluations that downstream users use to validate their claims and decisions. At the same time, our framework also informs the construction of evaluations designed to speak to the validity of the relevant claims. By leveraging psychometrics' breakdown of validity, evaluations can prioritize the most critical facets for a given claim, improving empirical utility and decision-making efficacy. We illustrate our framework through detailed case studies of vision and language model evaluations, highlighting how explicitly considering validity strengthens the connection between evaluation evidence and the claims being made.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.4 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.9 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10576" target="_blank" rel="noopener noreferrer">Robust Photo-Realistic Hand Gesture Generation: from Single View to Multiple View</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Qifan Fu, Xu Chen, Muhammad Asad, Shanxin Yuan, Changjae Oh, Gregory Slabaugh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">High-fidelity hand gesture generation represents a significant challenge in human-centric generation tasks. Existing methods typically employ single-view 3D MANO mesh-rendered images prior to enhancing gesture generation quality. However, the complexity of hand movements and the inherent limitations</span>
                
                <span class="abstract-full" style="display: none;">High-fidelity hand gesture generation represents a significant challenge in human-centric generation tasks. Existing methods typically employ single-view 3D MANO mesh-rendered images prior to enhancing gesture generation quality. However, the complexity of hand movements and the inherent limitations of single-view rendering make it difficult to capture complete 3D hand information, particularly when fingers are occluded. The fundamental contradiction lies in the loss of 3D topological relationships through 2D projection and the incomplete spatial coverage inherent to single-view representations. Diverging from single-view prior approaches, we propose a multi-view prior framework, named Multi-Modal UNet-based Feature Encoder (MUFEN), to guide diffusion models in learning comprehensive 3D hand information. Specifically, we extend conventional front-view rendering to include rear, left, right, top, and bottom perspectives, selecting the most information-rich view combination as training priors to address occlusion completion. This multi-view prior with a dedicated dual stream encoder significantly improves the model's understanding of complete hand features. Furthermore, we design a bounding box feature fusion module, which can fuse the gesture localization features and gesture multi-modal features to enhance the location-awareness of the MUFEN features to the gesture-related features. Experiments demonstrate that our method achieves state-of-the-art performance in both quantitative metrics and qualitative evaluations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.7 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Computer Vision: 2.6 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10585" target="_blank" rel="noopener noreferrer">Efficient Malicious UAV Detection Using Autoencoder-TSMamba Integration</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Azim Akhtarshenas, Ramin Toosi, David L\'opez-P\'erez, Tohid Alizadeh, Alireza Hosseini
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Malicious Unmanned Aerial Vehicles (UAVs) present a significant threat to next-generation networks (NGNs), posing risks such as unauthorized surveillance, data theft, and the delivery of hazardous materials. This paper proposes an integrated (AE)-classifier system to detect malicious UAVs. The propo</span>
                
                <span class="abstract-full" style="display: none;">Malicious Unmanned Aerial Vehicles (UAVs) present a significant threat to next-generation networks (NGNs), posing risks such as unauthorized surveillance, data theft, and the delivery of hazardous materials. This paper proposes an integrated (AE)-classifier system to detect malicious UAVs. The proposed AE, based on a 4-layer Tri-orientated Spatial Mamba (TSMamba) architecture, effectively captures complex spatial relationships crucial for identifying malicious UAV activities. The first phase involves generating residual values through the AE, which are subsequently processed by a ResNet-based classifier. This classifier leverages the residual values to achieve lower complexity and higher accuracy. Our experiments demonstrate significant improvements in both binary and multi-class classification scenarios, achieving up to 99.8 % recall compared to 96.7 % in the benchmark. Additionally, our method reduces computational complexity, making it more suitable for large-scale deployment. These results highlight the robustness and scalability of our approach, offering an effective solution for malicious UAV detection in NGN environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.7 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Computer Vision: 2.9 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10591" target="_blank" rel="noopener noreferrer">Cosmos 1.0: a multidimensional map of the emerging technology frontier</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xian Gong, Paul X. McCarthy, Colin Griffith, Claire McFarland, Marian-Andrei Rizoiu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper describes a novel methodology to map the universe of emerging technologies, utilising various source data that contain a rich diversity and breadth of contemporary knowledge to create a new dataset and multiple indices that provide new insights into these technologies. The Cosmos 1.0 data</span>
                
                <span class="abstract-full" style="display: none;">This paper describes a novel methodology to map the universe of emerging technologies, utilising various source data that contain a rich diversity and breadth of contemporary knowledge to create a new dataset and multiple indices that provide new insights into these technologies. The Cosmos 1.0 dataset is a comprehensive collection of 23,544 technologies (ET23k) structured into a hierarchical model. Each technology is categorised into three meta clusters (ET3) and seven theme clusters (ET7) enhanced by 100-dimensional embedding vectors. Within the cosmos, we manually verify 100 emerging technologies called the ET100. This dataset is enriched with additional indices specifically developed to assess the landscape of emerging technologies, including the Technology Awareness Index, Generality Index, Deeptech, and Age of Tech Index. The dataset incorporates extensive metadata sourced from Wikipedia and linked data from third-party sources such as Crunchbase, Google Books, OpenAlex and Google Scholar, which are used to validate the relevance and accuracy of the constructed indices. Moreover, we trained a classifier to identify whether they are developed "technology" or technology-related "terms".</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.0 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Datasets: 2.2 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10630" target="_blank" rel="noopener noreferrer">How many measurements are enough? Bayesian recovery in inverse problems with general distributions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ben Adcock, Nick Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the sample complexity of Bayesian recovery for solving inverse problems with general prior, forward operator and noise distributions. We consider posterior sampling according to an approximate prior $\mathcal{P}$, and establish sufficient conditions for stable and accurate recovery with hig</span>
                
                <span class="abstract-full" style="display: none;">We study the sample complexity of Bayesian recovery for solving inverse problems with general prior, forward operator and noise distributions. We consider posterior sampling according to an approximate prior $\mathcal{P}$, and establish sufficient conditions for stable and accurate recovery with high probability. Our main result is a non-asymptotic bound that shows that the sample complexity depends on (i) the intrinsic complexity of $\mathcal{P}$, quantified by its so-called approximate covering number, and (ii) concentration bounds for the forward operator and noise distributions. As a key application, we specialize to generative priors, where $\mathcal{P}$ is the pushforward of a latent distribution via a Deep Neural Network (DNN). We show that the sample complexity scales log-linearly with the latent dimension $k$, thus establishing the efficacy of DNN-based priors. Generalizing existing results on deterministic (i.e., non-Bayesian) recovery for the important problem of random sampling with an orthogonal matrix $U$, we show how the sample complexity is determined by the coherence of $U$ with respect to the support of $\mathcal{P}$. Hence, we establish that coherence plays a fundamental role in Bayesian recovery as well. Overall, our framework unifies and extends prior work, providing rigorous guarantees for the sample complexity of solving Bayesian inverse problems with arbitrary distributions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.2 -->
                    
                <!-- Bayesian Optimization: 3.0 -->
                    
                <!-- Math: 2.6 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Computer Vision: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10651" target="_blank" rel="noopener noreferrer">Evolution imposes an inductive bias that alters and accelerates learning dynamics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Benjamin Midler, Alejandro Pan Vazquez
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The learning dynamics of biological brains and artificial neural networks are of interest to both neuroscience and machine learning. A key difference between them is that neural networks are often trained from a randomly initialized state whereas each brain is the product of generations of evolution</span>
                
                <span class="abstract-full" style="display: none;">The learning dynamics of biological brains and artificial neural networks are of interest to both neuroscience and machine learning. A key difference between them is that neural networks are often trained from a randomly initialized state whereas each brain is the product of generations of evolutionary optimization, yielding innate structures that enable few-shot learning and inbuilt reflexes. Artificial neural networks, by contrast, require non-ethological quantities of training data to attain comparable performance. To investigate the effect of evolutionary optimization on the learning dynamics of neural networks, we combined algorithms simulating natural selection and online learning to produce a method for evolutionarily conditioning artificial neural networks, and applied it to both reinforcement and supervised learning contexts. We found the evolutionary conditioning algorithm, by itself, performs comparably to an unoptimized baseline. However, evolutionarily conditioned networks show signs of unique and latent learning dynamics, and can be rapidly fine-tuned to optimal performance. These results suggest evolution constitutes an inductive bias that tunes neural systems to enable rapid learning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.4 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Federated Learning: 3.1 -->
                    
                <!-- Evolutionary Algorithms: 2.7 -->
                    
                <!-- Decision Trees: 2.1 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10660" target="_blank" rel="noopener noreferrer">Surface stability of a layered magnetoelastic half-space</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Davood Shahsavari, Luis Dorfmann, Prashant Saxena
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We evaluate the conditions for surface stability of a layered magnetoelastic half-space subjected to large deformations and a magnetic field. After reviewing the fundamental measures of deformation and summarizing the magnetostatic equations in Eulerian and Lagrangian forms, we derive the constituti</span>
                
                <span class="abstract-full" style="display: none;">We evaluate the conditions for surface stability of a layered magnetoelastic half-space subjected to large deformations and a magnetic field. After reviewing the fundamental measures of deformation and summarizing the magnetostatic equations in Eulerian and Lagrangian forms, we derive the constitutive relations from a total energy function dependent on the deformation gradient and Lagrangian magnetic induction. Energy principles yield the equilibrium equations, magnetic field equations, and boundary conditions. The second variation of the energy functional provides the incremental equations and conditions for stability analysis. Surface instability is studied by linearizing increments of deformation and magnetic induction about a finitely deformed state under a magnetic field normal to the surface. Four illustrative cases are considered: (i) a layered non-magnetizable half-space with varying stiffness contrast; (ii) the critical stretch of a magnetoelastic half-space as a function of magnetic induction; (iii) surface stability of a magneto-sensitive layer atop a non-magnetizable substrate; and (iv) bifurcation conditions in a two-layered magnetoelastic solid with different stiffness ratios. Graphical results are provided throughout.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.9 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- LLMs: 1.4 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- Finance: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10664" target="_blank" rel="noopener noreferrer">CLIP Embeddings for AI-Generated Image Detection: A Few-Shot Study with Lightweight Classifier</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ziyang Ou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Verifying the authenticity of AI-generated images presents a growing challenge on social media platforms these days. While vision-language models (VLMs) like CLIP outdo in multimodal representation, their capacity for AI-generated image classification is underexplored due to the absence of such labe</span>
                
                <span class="abstract-full" style="display: none;">Verifying the authenticity of AI-generated images presents a growing challenge on social media platforms these days. While vision-language models (VLMs) like CLIP outdo in multimodal representation, their capacity for AI-generated image classification is underexplored due to the absence of such labels during the pre-training process. This work investigates whether CLIP embeddings inherently contain information indicative of AI generation. A proposed pipeline extracts visual embeddings using a frozen CLIP model, feeds its embeddings to lightweight networks, and fine-tunes only the final classifier. Experiments on the public CIFAKE benchmark show the performance reaches 95% accuracy without language reasoning. Few-shot adaptation to curated custom with 20% of the data results in performance to 85%. A closed-source baseline (Gemini-2.0) has the best zero-shot accuracy yet fails on specific styles. Notably, some specific image types, such as wide-angle photographs and oil paintings, pose significant challenges to classification. These results indicate previously unexplored difficulties in classifying certain types of AI-generated images, revealing new and more specific questions in this domain that are worth further investigation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.5 -->
                    
                <!-- Medicine: 2.9 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10678" target="_blank" rel="noopener noreferrer">System Identification and Control Using Lyapunov-Based Deep Neural Networks without Persistent Excitation: A Concurrent Learning Approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rebecca G. Hart, Omkar Sudhir Patil, Zachary I. Bell, Warren E. Dixon
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Deep Neural Networks (DNNs) are increasingly used in control applications due to their powerful function approximation capabilities. However, many existing formulations focus primarily on tracking error convergence, often neglecting the challenge of identifying the system dynamics using the DNN. Thi</span>
                
                <span class="abstract-full" style="display: none;">Deep Neural Networks (DNNs) are increasingly used in control applications due to their powerful function approximation capabilities. However, many existing formulations focus primarily on tracking error convergence, often neglecting the challenge of identifying the system dynamics using the DNN. This paper presents the first result on simultaneous trajectory tracking and online system identification using a DNN-based controller, without requiring persistent excitation. Two new concurrent learning adaptation laws are constructed for the weights of all the layers of the DNN, achieving convergence of the DNN's parameter estimates to a neighborhood of their ideal values, provided the DNN's Jacobian satisfies a finite-time excitation condition. A Lyapunov-based stability analysis is conducted to ensure convergence of the tracking error, weight estimation errors, and observer errors to a neighborhood of the origin. Simulations performed on a range of systems and trajectories, with the same initial and operating conditions, demonstrated 40.5% to 73.6% improvement in function approximation performance compared to the baseline, while maintaining a similar tracking error and control effort. Simulations evaluating function approximation capabilities on data points outside of the trajectory resulted in 58.88% and 74.75% improvement in function approximation compared to the baseline.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.0 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- LLMs: 1.1 -->
                    
                <!-- Cryptography: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10679" target="_blank" rel="noopener noreferrer">Are Spatial-Temporal Graph Convolution Networks for Human Action Recognition Over-Parameterized?</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jianyang Xie, Yitian Zhao, Yanda Meng, He Zhao, Anh Nguyen, Yalin Zheng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Spatial-temporal graph convolutional networks (ST-GCNs) showcase impressive performance in skeleton-based human action recognition (HAR). However, despite the development of numerous models, their recognition performance does not differ significantly after aligning the input settings. With this obse</span>
                
                <span class="abstract-full" style="display: none;">Spatial-temporal graph convolutional networks (ST-GCNs) showcase impressive performance in skeleton-based human action recognition (HAR). However, despite the development of numerous models, their recognition performance does not differ significantly after aligning the input settings. With this observation, we hypothesize that ST-GCNs are over-parameterized for HAR, a conjecture subsequently confirmed through experiments employing the lottery ticket hypothesis. Additionally, a novel sparse ST-GCNs generator is proposed, which trains a sparse architecture from a randomly initialized dense network while maintaining comparable performance levels to the dense components. Moreover, we generate multi-level sparsity ST-GCNs by integrating sparse structures at various sparsity levels and demonstrate that the assembled model yields a significant enhancement in HAR performance. Thorough experiments on four datasets, including NTU-RGB+D 60(120), Kinetics-400, and FineGYM, demonstrate that the proposed sparse ST-GCNs can achieve comparable performance to their dense components. Even with 95% fewer parameters, the sparse ST-GCNs exhibit a degradation of <1% in top-1 accuracy. Meanwhile, the multi-level sparsity ST-GCNs, which require only 66% of the parameters of the dense ST-GCNs, demonstrate an improvement of >1% in top-1 accuracy. The code is available at https://github.com/davelailai/Sparse-ST-GCN.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- GNN: 4.0 -->
                    
                <!-- LLMs: 2.4 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Cryptography: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10689" target="_blank" rel="noopener noreferrer">A probabilistic framework for dynamic quantization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gabriele Santini, Francesco Paissan, Elisabetta Farella
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a probabilistic framework for dynamic quantization of neural networks that allows for a computationally efficient input-adaptive rescaling of the quantization parameters. Our framework applies a probabilistic model to the network's pre-activations through a lightweight surrogate, enabling</span>
                
                <span class="abstract-full" style="display: none;">We propose a probabilistic framework for dynamic quantization of neural networks that allows for a computationally efficient input-adaptive rescaling of the quantization parameters. Our framework applies a probabilistic model to the network's pre-activations through a lightweight surrogate, enabling the adaptive adjustment of the quantization parameters on a per-input basis without significant memory overhead. We validate our approach on a set of popular computer vision tasks and models, observing only a negligible loss in performance. Our method strikes the best performance and computational overhead tradeoff compared to standard quantization strategies.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.2 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Cryptography: 1.2 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                <!-- Finance: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10690" target="_blank" rel="noopener noreferrer">Decision Making in Urban Traffic: A Game Theoretic Approach for Autonomous Vehicles Adhering to Traffic Rules</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Keqi Shu, Minghao Ning, Ahmad Alghooneh, Shen Li, Mohammad Pirani, Amir Khajepour
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">One of the primary challenges in urban autonomous vehicle decision-making and planning lies in effectively managing intricate interactions with diverse traffic participants characterized by unpredictable movement patterns. Additionally, interpreting and adhering to traffic regulations within rapidly</span>
                
                <span class="abstract-full" style="display: none;">One of the primary challenges in urban autonomous vehicle decision-making and planning lies in effectively managing intricate interactions with diverse traffic participants characterized by unpredictable movement patterns. Additionally, interpreting and adhering to traffic regulations within rapidly evolving traffic scenarios pose significant hurdles. This paper proposed a rule-based autonomous vehicle decision-making and planning framework which extracts right-of-way from traffic rules to generate behavioural parameters, integrating them to effectively adhere to and navigate through traffic regulations. The framework considers the strong interaction between traffic participants mathematically by formulating the decision-making and planning problem into a differential game. By finding the Nash equilibrium of the problem, the autonomous vehicle is able to find optimal decisions. The proposed framework was tested under simulation as well as full-size vehicle platform, the results show that the ego vehicle is able to safely interact with surrounding traffic participants while adhering to traffic rules.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 2.8 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Game Theory: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Cryptography: 1.8 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Medicine: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Finance: 1.2 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10698" target="_blank" rel="noopener noreferrer">Asymptotically-Optimal Gaussian Bandits with Side Observations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alexia Atsidakou, Orestis Papadigenopoulos, Constantine Caramanis, Sujay Sanghavi, Sanjay Shakkottai
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the problem of Gaussian bandits with general side information, as first introduced by Wu, Szepesvari, and Gyorgy. In this setting, the play of an arm reveals information about other arms, according to an arbitrary a priori known side information matrix: each element of this matrix encodes t</span>
                
                <span class="abstract-full" style="display: none;">We study the problem of Gaussian bandits with general side information, as first introduced by Wu, Szepesvari, and Gyorgy. In this setting, the play of an arm reveals information about other arms, according to an arbitrary a priori known side information matrix: each element of this matrix encodes the fidelity of the information that the ``row'' arm reveals about the ``column'' arm. In the case of Gaussian noise, this model subsumes standard bandits, full-feedback, and graph-structured feedback as special cases. In this work, we first construct an LP-based asymptotic instance-dependent lower bound on the regret. The LP optimizes the cost (regret) required to reliably estimate the suboptimality gap of each arm. This LP lower bound motivates our main contribution: the first known asymptotically optimal algorithm for this general setting.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 4.9 -->
                    
                <!-- Bayesian Optimization: 3.3 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- Evolutionary Algorithms: 2.6 -->
                    
                <!-- Medicine: 2.2 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Game Theory: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Cryptography: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10713" target="_blank" rel="noopener noreferrer">Maximum likelihood discretization of the transport equation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Brook Eyob, Florian Sch\"afer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The transport of positive quantities underlies countless physical processes, including fluid, gas, and plasma dynamics. Discretizing the associated partial differential equations with Galerkin methods can result in spurious nonpositivity of solutions. We observe that these methods amount to performi</span>
                
                <span class="abstract-full" style="display: none;">The transport of positive quantities underlies countless physical processes, including fluid, gas, and plasma dynamics. Discretizing the associated partial differential equations with Galerkin methods can result in spurious nonpositivity of solutions. We observe that these methods amount to performing statistical inference using the method of moments (MoM) and that the loss of positivity arises from MoM's susceptibility to producing estimates inconsistent with the observed data. We overcome this problem by replacing MoM with maximum likelihood estimation, introducing $\textit{maximum likelihood discretization} $(MLD). In the continuous limit, MLD simplifies to the Fisher-Rao Galerkin (FRG) semidiscretization, which replaces the $L^2$ inner product in Galerkin projection with the Fisher-Rao metric of probability distributions. We show empirically that FRG preserves positivity. We prove rigorously that it yields error bounds in the Kullback-Leibler divergence.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.8 -->
                    
                <!-- Math: 3.3 -->
                    
                <!-- Federated Learning: 2.8 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Game Theory: 1.5 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Cryptography: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10723" target="_blank" rel="noopener noreferrer">Mesh Stability Guaranteed Rigid Body Networks Using Control and Topology Co-Design</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zihao Song, Shirantha Welikala, Panos J. Antsaklis, Hai Lin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Merging and splitting are of great significance for rigid body networks in making such networks reconfigurable. The main challenges lie in simultaneously ensuring the compositionality of the distributed controllers and the mesh stability of the entire network. To this end, we propose a decentralized</span>
                
                <span class="abstract-full" style="display: none;">Merging and splitting are of great significance for rigid body networks in making such networks reconfigurable. The main challenges lie in simultaneously ensuring the compositionality of the distributed controllers and the mesh stability of the entire network. To this end, we propose a decentralized control and topology co-design method for rigid body networks, which enables flexible joining and leaving of rigid bodies without the need to redesign the controllers for the entire network after such maneuvers. We first provide a centralized linear matrix inequality (LMI)-based control and topology co-design optimization of the rigid body networks with a formal mesh stability guarantee. Then, these centralized mesh stability constraints are made decentralized by a proposed alternative set of sufficient conditions. Using these decentralized mesh stability constraints and Sylvester's criterion-based decentralization techniques, the said centralized LMI problem is equivalently broken down into a set of smaller decentralized LMI problems that can be solved at each rigid body, enabling flexible merging/splitting of rigid bodies. Finally, the effectiveness of the proposed co-design method is illustrated based on a specifically developed simulator and a comparison study with respect to a state-of-the-art method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.4 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Bayesian Optimization: 2.0 -->
                    
                <!-- LLMs: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10726" target="_blank" rel="noopener noreferrer">Learning Repetition-Invariant Representations for Polymer Informatics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yihan Zhu, Gang Liu, Eric Inae, Tengfei Luo, Meng Jiang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Polymers are large macromolecules composed of repeating structural units known as monomers and are widely applied in fields such as energy storage, construction, medicine, and aerospace. However, existing graph neural network methods, though effective for small molecules, only model the single unit </span>
                
                <span class="abstract-full" style="display: none;">Polymers are large macromolecules composed of repeating structural units known as monomers and are widely applied in fields such as energy storage, construction, medicine, and aerospace. However, existing graph neural network methods, though effective for small molecules, only model the single unit of polymers and fail to produce consistent vector representations for the true polymer structure with varying numbers of units. To address this challenge, we introduce Graph Repetition Invariance (GRIN), a novel method to learn polymer representations that are invariant to the number of repeating units in their graph representations. GRIN integrates a graph-based maximum spanning tree alignment with repeat-unit augmentation to ensure structural consistency. We provide theoretical guarantees for repetition-invariance from both model and data perspectives, demonstrating that three repeating units are the minimal augmentation required for optimal invariant representation learning. GRIN outperforms state-of-the-art baselines on both homopolymer and copolymer benchmarks, learning stable, repetition-invariant representations that generalize effectively to polymer chains of unseen sizes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.3 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- GNN: 3.6 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10736" target="_blank" rel="noopener noreferrer">Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ximing Dong, Shaowei Wang, Dayi Lin, Ahmed E. Hassan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Optimizing Large Language Model (LLM) performance requires well-crafted prompts, but manual prompt engineering is labor-intensive and often ineffective. Automated prompt optimization techniques address this challenge but the majority of them rely on randomly selected evaluation subsets, which fail t</span>
                
                <span class="abstract-full" style="display: none;">Optimizing Large Language Model (LLM) performance requires well-crafted prompts, but manual prompt engineering is labor-intensive and often ineffective. Automated prompt optimization techniques address this challenge but the majority of them rely on randomly selected evaluation subsets, which fail to represent the full dataset, leading to unreliable evaluations and suboptimal prompts. Existing coreset selection methods, designed for LLM benchmarking, are unsuitable for prompt optimization due to challenges in clustering similar samples, high data collection costs, and the unavailability of performance data for new or private datasets. To overcome these issues, we propose IPOMP, an Iterative evaluation data selection for effective Prompt Optimization using real-time Model Performance. IPOMP is a two-stage approach that selects representative and diverse samples using semantic clustering and boundary analysis, followed by iterative refinement with real-time model performance data to replace redundant samples. Evaluations on the BIG-bench dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by at least 57% compared with SOTA baselines, with minimal computational overhead below 1%. Furthermore, the results demonstrate that our real-time performance-guided refinement approach can be universally applied to enhance existing coreset selection methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.0 -->
                    
                <!-- Federated Learning: 4.2 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Computer Vision: 3.5 -->
                    
                <!-- Evolutionary Algorithms: 2.9 -->
                    
                <!-- HPO and AutoML: 2.8 -->
                    
                <!-- Decision Trees: 1.9 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10743" target="_blank" rel="noopener noreferrer">IMAGE-ALCHEMY: Advancing subject fidelity in personalised text-to-image generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Amritanshu Tiwari, Cherish Puniani, Kaustubh Sharma, Ojasva Nema
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advances in text-to-image diffusion models, particularly Stable Diffusion, have enabled the generation of highly detailed and semantically rich images. However, personalizing these models to represent novel subjects based on a few reference images remains challenging. This often leads to cata</span>
                
                <span class="abstract-full" style="display: none;">Recent advances in text-to-image diffusion models, particularly Stable Diffusion, have enabled the generation of highly detailed and semantically rich images. However, personalizing these models to represent novel subjects based on a few reference images remains challenging. This often leads to catastrophic forgetting, overfitting, or large computational overhead.We propose a two-stage pipeline that addresses these limitations by leveraging LoRA-based fine-tuning on the attention weights within the U-Net of the Stable Diffusion XL (SDXL) model. First, we use the unmodified SDXL to generate a generic scene by replacing the subject with its class label. Then, we selectively insert the personalized subject through a segmentation-driven image-to-image (Img2Img) pipeline that uses the trained LoRA weights.This framework isolates the subject encoding from the overall composition, thus preserving SDXL's broader generative capabilities while integrating the new subject in a high-fidelity manner. Our method achieves a DINO similarity score of 0.789 on SDXL, outperforming existing personalized text-to-image approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 3.1 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Cryptography: 1.8 -->
                    
                <!-- Medicine: 1.8 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Finance: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10744" target="_blank" rel="noopener noreferrer">A Virtual Admittance-Based Fault Current Limiting Method for Grid-Forming Inverters</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zaid Ibn Mahmood, Hantao Cui, Ying Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Inverter-based resources (IBRs) are a key component in the ongoing modernization of power systems, with grid-forming (GFM) inverters playing a central role. Effective fault current limiting is a major challenge to modernizing power systems through increased penetration of GFM inverters. Due to their</span>
                
                <span class="abstract-full" style="display: none;">Inverter-based resources (IBRs) are a key component in the ongoing modernization of power systems, with grid-forming (GFM) inverters playing a central role. Effective fault current limiting is a major challenge to modernizing power systems through increased penetration of GFM inverters. Due to their voltage-source nature, GFM inverters offer no direct control over the output current and, therefore, are susceptible to high fault currents. This vulnerability is especially pronounced during large phase jumps, a condition overlooked by most fault current limiting methods. This paper proposes a hybrid fault current limiting method implemented through a virtual admittance by leveraging the advantages of two virtual impedance (VI)-based methods tailored for three-phase faults and phase jump disturbances. Electromagnetic transient simulations conducted in MATLAB-Simulink demonstrate the method's effectiveness across various disturbances, validating its potential in single-loop GFM structures.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 3.3 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Cryptography: 1.8 -->
                    
                <!-- Game Theory: 1.7 -->
                    
                <!-- Medicine: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Finance: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10748" target="_blank" rel="noopener noreferrer">AutoRAC: Automated Processing-in-Memory Accelerator Design for Recommender Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Feng Cheng (Helen), Tunhou Zhang (Helen), Junyao Zhang (Helen), Jonathan Hao-Cheng Ku (Helen), Yitu Wang (Helen), Xiaoxuan Yang (Helen), Hai (Helen), Li, Yiran Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The performance bottleneck of deep-learning-based recommender systems resides in their backbone Deep Neural Networks. By integrating Processing-In-Memory~(PIM) architectures, researchers can reduce data movement and enhance energy efficiency, paving the way for next-generation recommender models. Ne</span>
                
                <span class="abstract-full" style="display: none;">The performance bottleneck of deep-learning-based recommender systems resides in their backbone Deep Neural Networks. By integrating Processing-In-Memory~(PIM) architectures, researchers can reduce data movement and enhance energy efficiency, paving the way for next-generation recommender models. Nevertheless, achieving performance and efficiency gains is challenging due to the complexity of the PIM design space and the intricate mapping of operators. In this paper, we demonstrate that automated PIM design is feasible even within the most demanding recommender model design space, spanning over $10^{54}$ possible architectures. We propose \methodname, which formulates the co-optimization of recommender models and PIM design as a combinatorial search over mixed-precision interaction operations, and parameterizes the search with a one-shot supernet encompassing all mixed-precision options. We comprehensively evaluate our approach on three Click-Through Rate benchmarks, showcasing the superiority of our automated design methodology over manual approaches. Our results indicate up to a 3.36$\times$ speedup, 1.68$\times$ area reduction, and 12.48$\times$ higher power efficiency compared to naively mapped searched designs and state-of-the-art handcrafted designs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Hardware: 4.9 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Federated Learning: 3.0 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10751" target="_blank" rel="noopener noreferrer">Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Francisco Raverta Capua, Pablo De Cristoforis
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Although the use of remote sensing technologies for monitoring forested environments has gained increasing attention, publicly available point cloud datasets remain scarce due to the high costs, sensor requirements, and time-intensive nature of their acquisition. Moreover, as far as we are aware, th</span>
                
                <span class="abstract-full" style="display: none;">Although the use of remote sensing technologies for monitoring forested environments has gained increasing attention, publicly available point cloud datasets remain scarce due to the high costs, sensor requirements, and time-intensive nature of their acquisition. Moreover, as far as we are aware, there are no public annotated datasets generated through Structure From Motion (SfM) algorithms applied to imagery, which may be due to the lack of SfM algorithms that can map semantic segmentation information into an accurate point cloud, especially in a challenging environment like forests.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.9 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10755" target="_blank" rel="noopener noreferrer">Infinigen-Sim: Procedural Generation of Articulated Simulation Assets</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Abhishek Joshi, Beining Han, Jack Nugent, Yiming Zuo, Jonathan Liu, Hongyu Wen, Stamatis Alexandropoulos, Tao Sun, Alexander Raistrick, Gaowen Liu, Yi Shao, Jia Deng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce Infinigen-Sim, a toolkit which enables users to create diverse and realistic articulated object procedural generators. These tools are composed of high-level utilities for use creating articulated assets in Blender, as well as an export pipeline to integrate the resulting assets into co</span>
                
                <span class="abstract-full" style="display: none;">We introduce Infinigen-Sim, a toolkit which enables users to create diverse and realistic articulated object procedural generators. These tools are composed of high-level utilities for use creating articulated assets in Blender, as well as an export pipeline to integrate the resulting assets into common robotics simulators. We demonstrate our system by creating procedural generators for 5 common articulated object categories. Experiments show that assets sampled from these generators are useful for movable object segmentation, training generalizable reinforcement learning policies, and sim-to-real transfer of imitation learning policies.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.5 -->
                    
                <!-- Computer Vision: 3.0 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10760" target="_blank" rel="noopener noreferrer">Counterfactual Behavior Cloning: Offline Imitation Learning from Imperfect Human Demonstrations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shahabedin Sagheb, Dylan P. Losey
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Learning from humans is challenging because people are imperfect teachers. When everyday humans show the robot a new task they want it to perform, humans inevitably make errors (e.g., inputting noisy actions) and provide suboptimal examples (e.g., overshooting the goal). Existing methods learn by mi</span>
                
                <span class="abstract-full" style="display: none;">Learning from humans is challenging because people are imperfect teachers. When everyday humans show the robot a new task they want it to perform, humans inevitably make errors (e.g., inputting noisy actions) and provide suboptimal examples (e.g., overshooting the goal). Existing methods learn by mimicking the exact behaviors the human teacher provides -- but this approach is fundamentally limited because the demonstrations themselves are imperfect. In this work we advance offline imitation learning by enabling robots to extrapolate what the human teacher meant, instead of only considering what the human actually showed. We achieve this by hypothesizing that all of the human's demonstrations are trying to convey a single, consistent policy, while the noise and sub-optimality within their behaviors obfuscates the data and introduces unintentional complexity. To recover the underlying policy and learn what the human teacher meant, we introduce Counter-BC, a generalized version of behavior cloning. Counter-BC expands the given dataset to include actions close to behaviors the human demonstrated (i.e., counterfactual actions that the human teacher could have intended, but did not actually show). During training Counter-BC autonomously modifies the human's demonstrations within this expanded region to reach a simple and consistent policy that explains the underlying trends in the human's dataset. Theoretically, we prove that Counter-BC can extract the desired policy from imperfect data, multiple users, and teachers of varying skill levels. Empirically, we compare Counter-BC to state-of-the-art alternatives in simulated and real-world settings with noisy demonstrations, standardized datasets, and real human teachers. See videos of our work here: https://youtu.be/XaeOZWhTt68</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 3.5 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- LLMs: 2.8 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Medicine: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10762" target="_blank" rel="noopener noreferrer">Deep Symbolic Optimization: Reinforcement Learning for Symbolic Mathematics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Conor F. Hayes, Felipe Leno Da Silva, Jiachen Yang, T. Nathan Mundhenk, Chak Shing Lee, Jacob F. Pettit, Claudio Santiago, Sookyung Kim, Joanne T. Kim, Ignacio Aravena Solis, Ruben Glatt, Andre R. Goncalves, Alexander Ladd, Ahmet Can Solak, Thomas Desautels, Daniel Faissol, Brenden K. Petersen, Mikel Landajuela
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Deep Symbolic Optimization (DSO) is a novel computational framework that enables symbolic optimization for scientific discovery, particularly in applications involving the search for intricate symbolic structures. One notable example is equation discovery, which aims to automatically derive mathemat</span>
                
                <span class="abstract-full" style="display: none;">Deep Symbolic Optimization (DSO) is a novel computational framework that enables symbolic optimization for scientific discovery, particularly in applications involving the search for intricate symbolic structures. One notable example is equation discovery, which aims to automatically derive mathematical models expressed in symbolic form. In DSO, the discovery process is formulated as a sequential decision-making task. A generative neural network learns a probabilistic model over a vast space of candidate symbolic expressions, while reinforcement learning strategies guide the search toward the most promising regions. This approach integrates gradient-based optimization with evolutionary and local search techniques, and it incorporates in-situ constraints, domain-specific priors, and advanced policy optimization methods. The result is a robust framework capable of efficiently exploring extensive search spaces to identify interpretable and physically meaningful models. Extensive evaluations on benchmark problems have demonstrated that DSO achieves state-of-the-art performance in both accuracy and interpretability. In this chapter, we provide a comprehensive overview of the DSO framework and illustrate its transformative potential for automating symbolic optimization in scientific discovery.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.7 -->
                    
                <!-- Medicine: 3.1 -->
                    
                <!-- Evolutionary Algorithms: 3.0 -->
                    
                <!-- Federated Learning: 2.8 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10775" target="_blank" rel="noopener noreferrer">A Systematic Analysis of Base Model Choice for Reward Modeling</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kian Ahrabian, Pegah Jandaghi, Negar Mokhberian, Sai Praneeth Karimireddy, Jay Pujara
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reinforcement learning from human feedback (RLHF) and, at its core, reward modeling have become a crucial part of training powerful large language models (LLMs). One commonly overlooked factor in training high-quality reward models (RMs) is the effect of the base model, which is becoming more challe</span>
                
                <span class="abstract-full" style="display: none;">Reinforcement learning from human feedback (RLHF) and, at its core, reward modeling have become a crucial part of training powerful large language models (LLMs). One commonly overlooked factor in training high-quality reward models (RMs) is the effect of the base model, which is becoming more challenging to choose given the rapidly growing pool of LLMs. In this work, we present a systematic analysis of the effect of base model selection on reward modeling performance. Our results show that the performance can be improved by up to 14% compared to the most common (i.e., default) choice. Moreover, we showcase the strong statistical relation between some existing benchmarks and downstream performances. We also demonstrate that the results from a small set of benchmarks could be combined to boost the model selection ($+$18% on average in the top 5-10). Lastly, we illustrate the impact of different post-training steps on the final performance and explore using estimated data distributions to reduce performance prediction error.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 4.5 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- Bayesian Optimization: 2.4 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10789" target="_blank" rel="noopener noreferrer">Bandwidth vs BFS Width in Matrix Reordering, Graph Reconstruction, and Graph Drawing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: David Eppstein (Alfred), Michael T. Goodrich (Alfred), Songyu (Alfred), Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We provide the first approximation quality guarantees for the Cuthull-McKee heuristic for reordering symmetric matrices to have low bandwidth, and we provide an algorithm for reconstructing bounded-bandwidth graphs from distance oracles with near-linear query complexity. To prove these results we in</span>
                
                <span class="abstract-full" style="display: none;">We provide the first approximation quality guarantees for the Cuthull-McKee heuristic for reordering symmetric matrices to have low bandwidth, and we provide an algorithm for reconstructing bounded-bandwidth graphs from distance oracles with near-linear query complexity. To prove these results we introduce a new width parameter, BFS width, and we prove polylogarithmic upper and lower bounds on the BFS width of graphs of bounded bandwidth. Unlike other width parameters, such as bandwidth, pathwidth, and treewidth, BFS width can easily be computed in polynomial time. Bounded BFS width implies bounded bandwidth, pathwidth, and treewidth, which in turn imply fixed-parameter tractable algorithms for many problems that are NP-hard for general graphs. In addition to their applications to matrix ordering, we also provide applications of BFS width to graph reconstruction, to reconstruct graphs from distance queries, and graph drawing, to construct arc diagrams of small height.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.8 -->
                    
                <!-- Medicine: 3.1 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10812" target="_blank" rel="noopener noreferrer">RAN Tester UE: An Automated Declarative UE Centric Security Testing Platform</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Charles Marion Ueltschey, Joshua Moore, Aly Sabri Abdalla, Vuk Marojevic
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Cellular networks require strict security procedures and measures across various network components, from core to radio access network (RAN) and end-user devices. As networks become increasingly complex and interconnected, as in O-RAN deployments, they are exposed to a numerous security threats. The</span>
                
                <span class="abstract-full" style="display: none;">Cellular networks require strict security procedures and measures across various network components, from core to radio access network (RAN) and end-user devices. As networks become increasingly complex and interconnected, as in O-RAN deployments, they are exposed to a numerous security threats. Therefore, ensuring robust security is critical for O-RAN to protect network integrity and safeguard user data. This requires rigorous testing methodologies to mitigate threats. This paper introduces an automated, adaptive, and scalable user equipment (UE) based RAN security testing framework designed to address the shortcomings of existing RAN testing solutions. Experimental results on a 5G software radio testbed built with commercial off-the-shelf hardware and open source software validate the efficiency and reproducibility of sample security test procedures developed on the RAN Tester UE framework.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.2 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Blockchain: 3.2 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10815" target="_blank" rel="noopener noreferrer">Enhancing Secrecy Energy Efficiency in RIS-Aided Aerial Mobile Edge Computing Networks: A Deep Reinforcement Learning Approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aly Sabri Abdalla, Vuk Marojevic
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper studies the problem of securing task offloading transmissions from ground users against ground eavesdropping threats. Our study introduces a reconfigurable intelligent surface (RIS)-aided unmanned aerial vehicle (UAV)-mobile edge computing (MEC) scheme to enhance the secure task offloadin</span>
                
                <span class="abstract-full" style="display: none;">This paper studies the problem of securing task offloading transmissions from ground users against ground eavesdropping threats. Our study introduces a reconfigurable intelligent surface (RIS)-aided unmanned aerial vehicle (UAV)-mobile edge computing (MEC) scheme to enhance the secure task offloading while minimizing the energy consumption of the UAV subject to task completion constraints. Leveraging a data-driven approach, we propose a comprehensive optimization strategy that jointly optimizes the aerial MEC (AMEC)'s trajectory, task offloading partitioning, UE transmission scheduling, and RIS phase shifts. Our objective centers on optimizing the secrecy energy efficiency (SEE) of UE task offloading transmissions while preserving the AMEC's energy resources and meeting the task completion time requirements. Numerical results show that the proposed solution can effectively safeguard legitimate task offloading transmissions while preserving AMEC energy.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.9 -->
                    
                <!-- Federated Learning: 3.4 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Cryptography: 2.0 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Medicine: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Finance: 1.4 -->
                    
                <!-- Game Theory: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10841" target="_blank" rel="noopener noreferrer">RefPose: Leveraging Reference Geometric Correspondences for Accurate 6D Pose Estimation of Unseen Objects</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jaeguk Kim, Jaewoo Park, Keuntek Lee, Nam Ik Cho
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Estimating the 6D pose of unseen objects from monocular RGB images remains a challenging problem, especially due to the lack of prior object-specific knowledge. To tackle this issue, we propose RefPose, an innovative approach to object pose estimation that leverages a reference image and geometric c</span>
                
                <span class="abstract-full" style="display: none;">Estimating the 6D pose of unseen objects from monocular RGB images remains a challenging problem, especially due to the lack of prior object-specific knowledge. To tackle this issue, we propose RefPose, an innovative approach to object pose estimation that leverages a reference image and geometric correspondence as guidance. RefPose first predicts an initial pose by using object templates to render the reference image and establish the geometric correspondence needed for the refinement stage. During the refinement stage, RefPose estimates the geometric correspondence of the query based on the generated references and iteratively refines the pose through a render-and-compare approach. To enhance this estimation, we introduce a correlation volume-guided attention mechanism that effectively captures correlations between the query and reference images. Unlike traditional methods that depend on pre-defined object models, RefPose dynamically adapts to new object shapes by leveraging a reference image and geometric correspondence. This results in robust performance across previously unseen objects. Extensive evaluation on the BOP benchmark datasets shows that RefPose achieves state-of-the-art results while maintaining a competitive runtime.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.9 -->
                    
                <!-- Computer Vision: 3.7 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- LLMs: 2.1 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10847" target="_blank" rel="noopener noreferrer">Robust 2D lidar-based SLAM in arboreal environments without IMU/GNSS</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Paola Nazate-Burgos, Miguel Torres-Torriti, Sergio Aguilera-Marinovic, Tito Ar\'evalo, Shoudong Huang, Fernando Auat Cheein
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Simultaneous localization and mapping (SLAM) approaches for mobile robots remains challenging in forest or arboreal fruit farming environments, where tree canopies obstruct Global Navigation Satellite Systems (GNSS) signals. Unlike indoor settings, these agricultural environments possess additional </span>
                
                <span class="abstract-full" style="display: none;">Simultaneous localization and mapping (SLAM) approaches for mobile robots remains challenging in forest or arboreal fruit farming environments, where tree canopies obstruct Global Navigation Satellite Systems (GNSS) signals. Unlike indoor settings, these agricultural environments possess additional challenges due to outdoor variables such as foliage motion and illumination variability. This paper proposes a solution based on 2D lidar measurements, which requires less processing and storage, and is more cost-effective, than approaches that employ 3D lidars. Utilizing the modified Hausdorff distance (MHD) metric, the method can solve the scan matching robustly and with high accuracy without needing sophisticated feature extraction. The method's robustness was validated using public datasets and considering various metrics, facilitating meaningful comparisons for future research. Comparative evaluations against state-of-the-art algorithms, particularly A-LOAM, show that the proposed approach achieves lower positional and angular errors while maintaining higher accuracy and resilience in GNSS-denied settings. This work contributes to the advancement of precision agriculture by enabling reliable and autonomous navigation in challenging outdoor environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.6 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Computer Vision: 2.8 -->
                    
                <!-- Blockchain: 2.4 -->
                    
                <!-- HPO and AutoML: 2.0 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Decision Trees: 1.8 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10850" target="_blank" rel="noopener noreferrer">Tracking Low-Level Cloud Systems with Topology</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mingzhe Li, Dwaipayan Chatterjee, Franziska Glassmeier, Fabian Senf, Bei Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Low-level clouds are ubiquitous in Earth's atmosphere, playing a crucial role in transporting heat, moisture, and momentum across the planet. Their evolution and interaction with other atmospheric components, such as aerosols, are essential to understanding the climate system and its sensitivity to </span>
                
                <span class="abstract-full" style="display: none;">Low-level clouds are ubiquitous in Earth's atmosphere, playing a crucial role in transporting heat, moisture, and momentum across the planet. Their evolution and interaction with other atmospheric components, such as aerosols, are essential to understanding the climate system and its sensitivity to anthropogenic influences. Advanced high-resolution geostationary satellites now resolve cloud systems with greater accuracy, establishing cloud tracking as a vital research area for studying their spatiotemporal dynamics. It enables disentangling advective and convective components driving cloud evolution. This, in turn, provides deeper insights into the structure and lifecycle of low-level cloud systems and the atmospheric processes they govern. In this paper, we propose a novel framework for tracking cloud systems using topology-driven techniques based on optimal transport. We first obtain a set of anchor points for the cloud systems based on the merge tree of the cloud optical depth field. We then apply topology-driven probabilistic feature tracking of these anchor points to guide the tracking of cloud systems. We demonstrate the utility of our framework by tracking clouds over the ocean and land to test for systematic differences in the two physically distinct settings. We further evaluate our framework through case studies and statistical analyses, comparing it against two leading cloud tracking tools and two topology-based general-purpose tracking tools. The results demonstrate that incorporating system-based tracking improves the ability to capture the evolution of low-level clouds. Our framework paves the way for detailed low-level cloud characterization studies using satellite data records.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.0 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- Computer Vision: 2.6 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10856" target="_blank" rel="noopener noreferrer">ImputeINR: Time Series Imputation via Implicit Neural Representations for Disease Diagnosis with Missing Data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mengxuan Li, Ke Liu, Jialong Guo, Jiajun Bu, Hongwei Wang, Haishuai Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Healthcare data frequently contain a substantial proportion of missing values, necessitating effective time series imputation to support downstream disease diagnosis tasks. However, existing imputation methods focus on discrete data points and are unable to effectively model sparse data, resulting i</span>
                
                <span class="abstract-full" style="display: none;">Healthcare data frequently contain a substantial proportion of missing values, necessitating effective time series imputation to support downstream disease diagnosis tasks. However, existing imputation methods focus on discrete data points and are unable to effectively model sparse data, resulting in particularly poor performance for imputing substantial missing values. In this paper, we propose a novel approach, ImputeINR, for time series imputation by employing implicit neural representations (INR) to learn continuous functions for time series. ImputeINR leverages the merits of INR in that the continuous functions are not coupled to sampling frequency and have infinite sampling frequency, allowing ImputeINR to generate fine-grained imputations even on extremely sparse observed values. Extensive experiments conducted on eight datasets with five ratios of masked values show the superior imputation performance of ImputeINR, especially for high missing ratios in time series data. Furthermore, we validate that applying ImputeINR to impute missing values in healthcare data enhances the performance of downstream disease diagnosis tasks. Codes are available.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.6 -->
                    
                <!-- Medicine: 3.7 -->
                    
                <!-- Federated Learning: 3.7 -->
                    
                <!-- GNN: 2.9 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10857" target="_blank" rel="noopener noreferrer">A high-order Newton multigrid method for steady-state shallow water equations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiaowen Wang, Chunwu Wang, Guanghan Li, Zhicheng Hu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A high-order Newton multigrid method is proposed for simulating steady-state shallow water flows in open channels with regular and irregular geometries. The method integrates two components: (1) a finite volume discretization with third-order weighted essentially non-oscillatory (WENO) reconstructio</span>
                
                <span class="abstract-full" style="display: none;">A high-order Newton multigrid method is proposed for simulating steady-state shallow water flows in open channels with regular and irregular geometries. The method integrates two components: (1) a finite volume discretization with third-order weighted essentially non-oscillatory (WENO) reconstruction for the governing shallow water equations, (2) a Newton-multigrid method with an efficient approximation of the Jacobian matrix for the resulting discrete system. Generating the full Jacobian matrix in Newton iterations causes substantial computational costs. To address this problem, we observe that the majority of the non-zero elements in the matrix exhibit negligible magnitudes. By eliminating these elements, we approximate the Jacobian matrix with fewer stencils, thereby significantly reducing calculation time. Numerical results demonstrate that the proposed simplification strategy improves computational efficiency while maintaining convergence rates comparable to those of the full Jacobian approach. Furthermore, the geometric multigrid method with a successive over-relaxation fast-sweeping smoother is employed for the linearized system to optimize performance. A variety of numerical experiments, including one-dimensional smooth subcritical flow, flows over a hump, and two-dimensional hydraulic jump over a wedge, are carried out to illustrate the third-order accuracy, efficiency and robustness of the proposed method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.6 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Math: 2.6 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Cryptography: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- LLMs: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                <!-- Finance: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10860" target="_blank" rel="noopener noreferrer">On DeepSeekMoE: Statistical Benefits of Shared Experts and Normalized Sigmoid Gating</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Huy Nguyen, Thong T. Doan, Quang Pham, Nghi D. Q. Bui, Nhat Ho, Alessandro Rinaldo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Mixture of experts (MoE) methods are a key component in most large language model architectures, including the recent series of DeepSeek models. Compared to other MoE implementations, DeepSeekMoE stands out because of two unique features: the deployment of a shared expert strategy and of the normali</span>
                
                <span class="abstract-full" style="display: none;">Mixture of experts (MoE) methods are a key component in most large language model architectures, including the recent series of DeepSeek models. Compared to other MoE implementations, DeepSeekMoE stands out because of two unique features: the deployment of a shared expert strategy and of the normalized sigmoid gating mechanism. Despite the prominent role of DeepSeekMoE in the success of the DeepSeek series of models, there have been only a few attempts to justify theoretically the value of the shared expert strategy, while its normalized sigmoid gating has remained unexplored. To bridge this gap, we undertake a comprehensive theoretical study of these two features of DeepSeekMoE from a statistical perspective. We perform a convergence analysis of the expert estimation task to highlight the gains in sample efficiency for both the shared expert strategy and the normalized sigmoid gating, offering useful insights into the design of expert and gating structures. To verify empirically our theoretical findings, we carry out several experiments on both synthetic data and real-world datasets for (vision) language modeling tasks. Finally, we conduct an extensive empirical analysis of the router behaviors, ranging from router saturation, router change rate, to expert utilization.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 4.3 -->
                    
                <!-- Medicine: 4.2 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Bayesian Optimization: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10863" target="_blank" rel="noopener noreferrer">Conversations With The Stressed Body: Facilitating Stress Self-Disclosure Among Adolescent Girls Through An Embodied Approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xinglin Sun, Caroline Claisse, Runhua Zhang, Xinyu Wu, Jialin Yuan, Qi Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Adolescent girls face significant mental health challenges during their transition to adulthood, often experiencing heightened stress from various sources. While various interactive technologies for self-disclosure had been explored to support stress relief, little is known about how to encourage st</span>
                
                <span class="abstract-full" style="display: none;">Adolescent girls face significant mental health challenges during their transition to adulthood, often experiencing heightened stress from various sources. While various interactive technologies for self-disclosure had been explored to support stress relief, little is known about how to encourage stress-related self-disclosure through an embodied approach. This study presents a co-design workshop centred on Embodied Probes, a series of artefacts and activities incorporating embodied methods and technologies. During the workshop, nine participants aged 15 to 18 engaged with their bodies, expressed bodily sensations through tangible means, and designed embodied prototypes tailored to their personal needs for stress perception and relief. The workshop revealed insights into somatic symptoms, sources, and coping strategies for stress among adolescent girls, as well as how embodied methods can support their stress self-disclosure. This paper contributes to the HCI community by offering design implications on leveraging embodied technologies to support self-disclosure for young women's mental well-being.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.6 -->
                    
                <!-- Medicine: 3.5 -->
                    
                <!-- Computer Vision: 2.8 -->
                    
                <!-- Evolutionary Algorithms: 2.5 -->
                    
                <!-- Hardware: 2.4 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Decision Trees: 1.7 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10867" target="_blank" rel="noopener noreferrer">Coordinated Inauthentic Behavior on TikTok: Challenges and Opportunities for Detection in a Video-First Ecosystem</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Luca Luceri, Tanishq Vijay Salkar, Ashwin Balasubramanian, Gabriela Pinto, Chenning Sun, Emilio Ferrara
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Detecting coordinated inauthentic behavior (CIB) is central to the study of online influence operations. However, most methods focus on text-centric platforms, leaving video-first ecosystems like TikTok largely unexplored. To address this gap, we develop and evaluate a computational framework for de</span>
                
                <span class="abstract-full" style="display: none;">Detecting coordinated inauthentic behavior (CIB) is central to the study of online influence operations. However, most methods focus on text-centric platforms, leaving video-first ecosystems like TikTok largely unexplored. To address this gap, we develop and evaluate a computational framework for detecting CIB on TikTok, leveraging a network-based approach adapted to the platform's unique content and interaction structures. Building on existing approaches, we construct user similarity networks based on shared behaviors, including synchronized posting, repeated use of similar captions, multimedia content reuse, and hashtag sequence overlap, and apply graph pruning techniques to identify dense networks of likely coordinated accounts. Analyzing a dataset of 793K TikTok videos related to the 2024 U.S. Presidential Election, we uncover a range of coordinated activities, from synchronized amplification of political narratives to semi-automated content replication using AI-generated voiceovers and split-screen video formats. Our findings show that while traditional coordination indicators generalize well to TikTok, other signals, such as those based on textual similarity of video transcripts or Duet and Stitch interactions, prove ineffective, highlighting the platform's distinct content norms and interaction mechanics. This work provides the first empirical foundation for studying and detecting CIB on TikTok, paving the way for future research into influence operations in short-form video platforms.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.6 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Evolutionary Algorithms: 2.5 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10871" target="_blank" rel="noopener noreferrer">Optimal Allocation of Privacy Budget on Hierarchical Data Release</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Joonhyuk Ko, Juba Ziani, Ferdinando Fioretto
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Releasing useful information from datasets with hierarchical structures while preserving individual privacy presents a significant challenge. Standard privacy-preserving mechanisms, and in particular Differential Privacy, often require careful allocation of a finite privacy budget across different l</span>
                
                <span class="abstract-full" style="display: none;">Releasing useful information from datasets with hierarchical structures while preserving individual privacy presents a significant challenge. Standard privacy-preserving mechanisms, and in particular Differential Privacy, often require careful allocation of a finite privacy budget across different levels and components of the hierarchy. Sub-optimal allocation can lead to either excessive noise, rendering the data useless, or to insufficient protections for sensitive information. This paper addresses the critical problem of optimal privacy budget allocation for hierarchical data release. It formulates this challenge as a constrained optimization problem, aiming to maximize data utility subject to a total privacy budget while considering the inherent trade-offs between data granularity and privacy loss. The proposed approach is supported by theoretical analysis and validated through comprehensive experiments on real hierarchical datasets. These experiments demonstrate that optimal privacy budget allocation significantly enhances the utility of the released data and improves the performance of downstream tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 4.2 -->
                    
                <!-- Medicine: 3.6 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- LLMs: 2.0 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Decision Trees: 1.7 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10873" target="_blank" rel="noopener noreferrer">Hashing for Structure-based Anomaly Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Filippo Leveni, Luca Magri, Cesare Alippi, Giacomo Boracchi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We focus on the problem of identifying samples in a set that do not conform to structured patterns represented by low-dimensional manifolds. An effective way to solve this problem is to embed data in a high dimensional space, called Preference Space, where anomalies can be identified as the most iso</span>
                
                <span class="abstract-full" style="display: none;">We focus on the problem of identifying samples in a set that do not conform to structured patterns represented by low-dimensional manifolds. An effective way to solve this problem is to embed data in a high dimensional space, called Preference Space, where anomalies can be identified as the most isolated points. In this work, we employ Locality Sensitive Hashing to avoid explicit computation of distances in high dimensions and thus improve Anomaly Detection efficiency. Specifically, we present an isolation-based anomaly detection technique designed to work in the Preference Space which achieves state-of-the-art performance at a lower computational cost. Code is publicly available at https://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.7 -->
                    
                <!-- Computer Vision: 2.6 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.7 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Game Theory: 1.6 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Medicine: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10876" target="_blank" rel="noopener noreferrer">Preference Isolation Forest for Structure-based Anomaly Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Filippo Leveni, Luca Magri, Cesare Alippi, Giacomo Boracchi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We address the problem of detecting anomalies as samples that do not conform to structured patterns represented by low-dimensional manifolds. To this end, we conceive a general anomaly detection framework called Preference Isolation Forest (PIF), that combines the benefits of adaptive isolation-base</span>
                
                <span class="abstract-full" style="display: none;">We address the problem of detecting anomalies as samples that do not conform to structured patterns represented by low-dimensional manifolds. To this end, we conceive a general anomaly detection framework called Preference Isolation Forest (PIF), that combines the benefits of adaptive isolation-based methods with the flexibility of preference embedding. The key intuition is to embed the data into a high-dimensional preference space by fitting low-dimensional manifolds, and to identify anomalies as isolated points. We propose three isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most general solution, $ii$) RuzHash-iForest, that avoids explicit computation of distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a locality prior to improve efficiency and effectiveness.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.5 -->
                    
                <!-- Federated Learning: 3.4 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Bayesian Optimization: 2.1 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10877" target="_blank" rel="noopener noreferrer">Graph and Simplicial Complex Prediction Gaussian Process via the Hodgelet Representations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mathieu Alain, So Takao, Xiaowen Dong, Bastian Rieck, Emmanuel Noutahi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Predicting the labels of graph-structured data is crucial in scientific applications and is often achieved using graph neural networks (GNNs). However, when data is scarce, GNNs suffer from overfitting, leading to poor performance. Recently, Gaussian processes (GPs) with graph-level inputs have been</span>
                
                <span class="abstract-full" style="display: none;">Predicting the labels of graph-structured data is crucial in scientific applications and is often achieved using graph neural networks (GNNs). However, when data is scarce, GNNs suffer from overfitting, leading to poor performance. Recently, Gaussian processes (GPs) with graph-level inputs have been proposed as an alternative. In this work, we extend the Gaussian process framework to simplicial complexes (SCs), enabling the handling of edge-level attributes and attributes supported on higher-order simplices. We further augment the resulting SC representations by considering their Hodge decompositions, allowing us to account for homological information, such as the number of holes, in the SC. We demonstrate that our framework enhances the predictions across various applications, paving the way for GPs to be more widely used for graph and SC-level predictions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Bayesian Optimization: 2.7 -->
                    
                <!-- Computer Vision: 2.6 -->
                    
                <!-- GNN: 2.6 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- LLMs: 2.3 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10880" target="_blank" rel="noopener noreferrer">Approximation and Generalization Abilities of Score-based Neural Network Generative Models for Sub-Gaussian Distributions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Guoji Fu, Wee Sun Lee
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper studies the approximation and generalization abilities of score-based neural network generative models (SGMs) in estimating an unknown distribution $P_0$ from $n$ i.i.d. observations in $d$ dimensions. Assuming merely that $P_0$ is $\alpha$-sub-Gaussian, we prove that for any time step $t</span>
                
                <span class="abstract-full" style="display: none;">This paper studies the approximation and generalization abilities of score-based neural network generative models (SGMs) in estimating an unknown distribution $P_0$ from $n$ i.i.d. observations in $d$ dimensions. Assuming merely that $P_0$ is $\alpha$-sub-Gaussian, we prove that for any time step $t \in [t_0, n^{O(1)}]$, where $t_0 \geq O(\alpha^2n^{-2/d}\log n)$, there exists a deep ReLU neural network with width $\leq O(\log^3n)$ and depth $\leq O(n^{3/d}\log_2n)$ that can approximate the scores with $\tilde{O}(n^{-1})$ mean square error and achieve a nearly optimal rate of $\tilde{O}(n^{-1}t_0^{-d/2})$ for score estimation, as measured by the score matching loss. Our framework is universal and can be used to establish convergence rates for SGMs under milder assumptions than previous work. For example, assuming further that the target density function $p_0$ lies in Sobolev or Besov classes, with an appropriately early stopping strategy, we demonstrate that neural network-based SGMs can attain nearly minimax convergence rates up to logarithmic factors. Our analysis removes several crucial assumptions, such as Lipschitz continuity of the score function or a strictly positive lower bound on the target density.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.7 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Networks: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10882" target="_blank" rel="noopener noreferrer">Global Convergence of Adaptive Sensing for Principal Eigenvector Estimation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alex Saad-Falcon, Brighton Ancelin, Justin Romberg
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper addresses the challenge of efficient principal component analysis (PCA) in high-dimensional spaces by analyzing a compressively sampled variant of Oja's algorithm with adaptive sensing. Traditional PCA methods incur substantial computational costs that scale poorly with data dimensionalit</span>
                
                <span class="abstract-full" style="display: none;">This paper addresses the challenge of efficient principal component analysis (PCA) in high-dimensional spaces by analyzing a compressively sampled variant of Oja's algorithm with adaptive sensing. Traditional PCA methods incur substantial computational costs that scale poorly with data dimensionality, whereas subspace tracking algorithms like Oja's offer more efficient alternatives but typically require full-dimensional observations. We analyze a variant where, at each iteration, only two compressed measurements are taken: one in the direction of the current estimate and one in a random orthogonal direction. We prove that this adaptive sensing approach achieves global convergence in the presence of noise when tracking the leading eigenvector of a datastream with eigengap $\Delta=\lambda_1-\lambda_2$. Our theoretical analysis demonstrates that the algorithm experiences two phases: (1) a warmup phase requiring $O(\lambda_1\lambda_2d^2/\Delta^2)$ iterations to achieve a constant-level alignment with the true eigenvector, followed by (2) a local convergence phase where the sine alignment error decays at a rate of $O(\lambda_1\lambda_2d^2/\Delta^2 t)$ for iterations $t$. The guarantee aligns with existing minimax lower bounds with an added factor of $d$ due to the compressive sampling. This work provides the first convergence guarantees in adaptive sensing for subspace tracking with noise. Our proof technique is also considerably simpler than those in prior works. The results have important implications for applications where acquiring full-dimensional samples is challenging or costly.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 3.0 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- Math: 2.4 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- LLMs: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Bayesian Optimization: 2.0 -->
                    
                <!-- Game Theory: 1.6 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Cryptography: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Finance: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10892" target="_blank" rel="noopener noreferrer">Multi-Objective Preference Optimization: Improving Human Alignment of Generative Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Akhil Agnihotri, Rahul Jain, Deepak Ramachandran, Zheng Wen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Post-training of LLMs with RLHF, and subsequently preference optimization algorithms such as DPO, IPO, etc., made a big difference in improving human alignment. However, all such techniques can only work with a single (human) objective. In practice, human users have multiple objectives, such as help</span>
                
                <span class="abstract-full" style="display: none;">Post-training of LLMs with RLHF, and subsequently preference optimization algorithms such as DPO, IPO, etc., made a big difference in improving human alignment. However, all such techniques can only work with a single (human) objective. In practice, human users have multiple objectives, such as helpfulness and harmlessness, and there is no natural way to aggregate them into a single objective. In this paper, we address the multi-objective preference-alignment problem, where a policy must optimize several, potentially conflicting, objectives. We introduce the Multi-Objective Preference Optimization (MOPO) algorithm, which frames alignment as a constrained KL-regularized optimization: the primary objective is maximized while secondary objectives are lower-bounded by tunable safety thresholds. Unlike prior work, MOPO operates directly on pairwise preference data, requires no point-wise reward assumption, and avoids heuristic prompt-context engineering. The method recovers policies on the Pareto front whenever the front is attainable; practically, it reduces to simple closed-form iterative updates suitable for large-scale training. On synthetic benchmarks with diverse canonical preference structures, we show that MOPO approximates the Pareto front. When fine-tuning a 1.3B-parameter language model on real-world human-preference datasets, MOPO attains higher rewards and yields policies that Pareto-dominate baselines; ablation studies confirm optimization stability and robustness to hyperparameters.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.4 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10903" target="_blank" rel="noopener noreferrer">On the Security Risks of ML-based Malware Detection Systems: A Survey</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ping He, Yuhao Mao, Changjiang Li, Lorenzo Cavallaro, Ting Wang, Shouling Ji
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Malware presents a persistent threat to user privacy and data integrity. To combat this, machine learning-based (ML-based) malware detection (MD) systems have been developed. However, these systems have increasingly been attacked in recent years, undermining their effectiveness in practice. While th</span>
                
                <span class="abstract-full" style="display: none;">Malware presents a persistent threat to user privacy and data integrity. To combat this, machine learning-based (ML-based) malware detection (MD) systems have been developed. However, these systems have increasingly been attacked in recent years, undermining their effectiveness in practice. While the security risks associated with ML-based MD systems have garnered considerable attention, the majority of prior works is limited to adversarial malware examples, lacking a comprehensive analysis of practical security risks. This paper addresses this gap by utilizing the CIA principles to define the scope of security risks. We then deconstruct ML-based MD systems into distinct operational stages, thus developing a stage-based taxonomy. Utilizing this taxonomy, we summarize the technical progress and discuss the gaps in the attack and defense proposals related to the ML-based MD systems within each stage. Subsequently, we conduct two case studies, using both inter-stage and intra-stage analyses according to the stage-based taxonomy to provide new empirical insights. Based on these analyses and insights, we suggest potential future directions from both inter-stage and intra-stage perspectives.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.1 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Medicine: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Hardware: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10942" target="_blank" rel="noopener noreferrer">Nosy Layers, Noisy Fixes: Tackling DRAs in Federated Learning Systems using Explainable AI</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Meghali Nandi, Arash Shaghaghi, Nazatul Haque Sultan, Gustavo Batista, Raymond K. Zhao, Sanjay Jha
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Federated Learning (FL) has emerged as a powerful paradigm for collaborative model training while keeping client data decentralized and private. However, it is vulnerable to Data Reconstruction Attacks (DRA) such as "LoKI" and "Robbing the Fed", where malicious models sent from the server to the cli</span>
                
                <span class="abstract-full" style="display: none;">Federated Learning (FL) has emerged as a powerful paradigm for collaborative model training while keeping client data decentralized and private. However, it is vulnerable to Data Reconstruction Attacks (DRA) such as "LoKI" and "Robbing the Fed", where malicious models sent from the server to the client can reconstruct sensitive user data. To counter this, we introduce DRArmor, a novel defense mechanism that integrates Explainable AI with targeted detection and mitigation strategies for DRA. Unlike existing defenses that focus on the entire model, DRArmor identifies and addresses the root cause (i.e., malicious layers within the model that send gradients with malicious intent) by analyzing their contribution to the output and detecting inconsistencies in gradient values. Once these malicious layers are identified, DRArmor applies defense techniques such as noise injection, pixelation, and pruning to these layers rather than the whole model, minimizing the attack surface and preserving client data privacy. We evaluate DRArmor's performance against the advanced LoKI attack across diverse datasets, including MNIST, CIFAR-10, CIFAR-100, and ImageNet, in a 200-client FL setup. Our results demonstrate DRArmor's effectiveness in mitigating data leakage, achieving high True Positive and True Negative Rates of 0.910 and 0.890, respectively. Additionally, DRArmor maintains an average accuracy of 87%, effectively protecting client privacy without compromising model performance. Compared to existing defense mechanisms, DRArmor reduces the data leakage rate by 62.5% with datasets containing 500 samples per client.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.8 -->
                    
                <!-- Medicine: 3.8 -->
                    
                <!-- Computer Vision: 3.2 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10946" target="_blank" rel="noopener noreferrer">ToDMA: Large Model-Driven Token-Domain Multiple Access for Semantic Communications</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Robert Schober, Deniz G\"und\"uz
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Token communications (TokCom) is an emerging generative semantic communication concept that reduces transmission rates by using context and multimodal large language model (MLLM)-based token processing, with tokens serving as universal semantic units across modalities. In this paper, we propose a se</span>
                
                <span class="abstract-full" style="display: none;">Token communications (TokCom) is an emerging generative semantic communication concept that reduces transmission rates by using context and multimodal large language model (MLLM)-based token processing, with tokens serving as universal semantic units across modalities. In this paper, we propose a semantic multiple access scheme in the token domain, referred to as token domain multiple access (ToDMA), where a large number of devices share a token codebook and a modulation codebook for source and channel coding, respectively. Specifically, each transmitter first tokenizes its source signal and modulate each token to a codeword. At the receiver, compressed sensing is employed first to detect active tokens and the corresponding channel state information (CSI) from the superposed signals. Then, the source token sequences are reconstructed by clustering the token-associated CSI across multiple time slots. In case of token collisions, some active tokens cannot be assigned and some positions in the reconstructed token sequences are empty. We propose to use pre-trained MLLMs to leverage the context, predict masked tokens, and thus mitigate token collisions. Simulation results demonstrate the effectiveness of the proposed ToDMA framework for both text and image transmission tasks, achieving significantly lower latency compared to context-unaware orthogonal communication schemes, while also delivering superior distortion and perceptual quality compared to state-of-the-art context-unaware non-orthogonal communication methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.0 -->
                    
                <!-- Medicine: 3.1 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10947" target="_blank" rel="noopener noreferrer">Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kehan Long, Jorge Cort\'es, Nikolay Atanasov
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the problem of certifying the stability of closed-loop systems under control policies derived from optimal control or reinforcement learning (RL). Classical Lyapunov methods require a strict step-wise decrease in the Lyapunov function but such a certificate is difficult to construct for a l</span>
                
                <span class="abstract-full" style="display: none;">We study the problem of certifying the stability of closed-loop systems under control policies derived from optimal control or reinforcement learning (RL). Classical Lyapunov methods require a strict step-wise decrease in the Lyapunov function but such a certificate is difficult to construct for a learned control policy. The value function associated with an RL policy is a natural Lyapunov function candidate but it is not clear how it should be modified. To gain intuition, we first study the linear quadratic regulator (LQR) problem and make two key observations. First, a Lyapunov function can be obtained from the value function of an LQR policy by augmenting it with a residual term related to the system dynamics and stage cost. Second, the classical Lyapunov decrease requirement can be relaxed to a generalized Lyapunov condition requiring only decrease on average over multiple time steps. Using this intuition, we consider the nonlinear setting and formulate an approach to learn generalized Lyapunov functions by augmenting RL value functions with neural network residual terms. Our approach successfully certifies the stability of RL policies trained on Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly train neural controllers and stability certificates using a multi-step Lyapunov loss, resulting in larger certified inner approximations of the region of attraction compared to the classical Lyapunov approach. Overall, our formulation enables stability certification for a broad class of systems with learned policies by making certificates easier to construct, thereby bridging classical control theory and modern learning-based methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Federated Learning: 3.3 -->
                    
                <!-- Evolutionary Algorithms: 3.2 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- LLMs: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10949" target="_blank" rel="noopener noreferrer">FP64 is All You Need: Rethinking Failure Modes in Physics-Informed Neural Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chenhui Xu, Dancheng Liu, Amir Nassereldine, Jinjun Xiong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Physics Informed Neural Networks (PINNs) often exhibit failure modes in which the PDE residual loss converges while the solution error stays large, a phenomenon traditionally blamed on local optima separated from the true solution by steep loss barriers. We challenge this understanding by demonstrat</span>
                
                <span class="abstract-full" style="display: none;">Physics Informed Neural Networks (PINNs) often exhibit failure modes in which the PDE residual loss converges while the solution error stays large, a phenomenon traditionally blamed on local optima separated from the true solution by steep loss barriers. We challenge this understanding by demonstrate that the real culprit is insufficient arithmetic precision: with standard FP32, the LBFGS optimizer prematurely satisfies its convergence test, freezing the network in a spurious failure phase. Simply upgrading to FP64 rescues optimization, enabling vanilla PINNs to solve PDEs without any failure modes. These results reframe PINN failure modes as precision induced stalls rather than inescapable local minima and expose a three stage training dynamic unconverged, failure, success whose boundaries shift with numerical precision. Our findings emphasize that rigorous arithmetic precision is the key to dependable PDE solving with neural networks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- GNN: 2.9 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Cryptography: 2.6 -->
                    
                <!-- LLMs: 2.4 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Game Theory: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Finance: 1.3 -->
                    
                <!-- Medicine: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10955" target="_blank" rel="noopener noreferrer">Tent transformed order $2$ nets and quasi-Monte Carlo rules with quadratic error decay</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bernd K\"a{\ss}emodel, Nicolas Nagel, Tino Ullrich
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We investigate the use of order $2$ digital nets for quasi-Monte Carlo quadrature of nonperiodic functions with bounded mixed second derivative over the cube. By using the so-called tent transform and its mapping properties we inherit error bounds from the periodic setting. Our analysis is based on </span>
                
                <span class="abstract-full" style="display: none;">We investigate the use of order $2$ digital nets for quasi-Monte Carlo quadrature of nonperiodic functions with bounded mixed second derivative over the cube. By using the so-called tent transform and its mapping properties we inherit error bounds from the periodic setting. Our analysis is based on decay properties of the multivariate Faber-Schauder coefficients of functions with bounded mixed second weak derivatives. As already observed by Hinrichs, Markhasin, Oettershagen, T. Ullrich (Numerische Mathematik 2016), order $2$ nets work particularly well on tensorized (periodic) Faber splines. From this we obtain a quadratic decay rate for tent transformed order $2$ nets also in the nonperiodic setting. This improves the formerly best known bound for this class of point sets by a factor of $\log N$.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- Federated Learning: 3.1 -->
                    
                <!-- Math: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- LLMs: 2.1 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.9 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Cryptography: 1.2 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Finance: 1.1 -->
                    
                <!-- Networks: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10988" target="_blank" rel="noopener noreferrer">DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Profitable Production</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Joon-Young Kim, Jecheon Yu, Heekyu Kim, Seunghwa Ryu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Plastic injection molding remains essential to modern manufacturing. However, optimizing process parameters to balance product quality and profitability under dynamic environmental and economic conditions remains a persistent challenge. This study presents a novel deep reinforcement learning (DRL)-b</span>
                
                <span class="abstract-full" style="display: none;">Plastic injection molding remains essential to modern manufacturing. However, optimizing process parameters to balance product quality and profitability under dynamic environmental and economic conditions remains a persistent challenge. This study presents a novel deep reinforcement learning (DRL)-based framework for real-time process optimization in injection molding, integrating product quality and profitability into the control objective. A profit function was developed to reflect real-world manufacturing costs, incorporating resin, mold wear, and electricity prices, including time-of-use variations. Surrogate models were constructed to predict product quality and cycle time, enabling efficient offline training of DRL agents using soft actor-critic (SAC) and proximal policy optimization (PPO) algorithms. Experimental results demonstrate that the proposed DRL framework can dynamically adapt to seasonal and operational variations, consistently maintaining product quality while maximizing profit. Compared to traditional optimization methods such as genetic algorithms, the DRL models achieved comparable economic performance with up to 135x faster inference speeds, making them well-suited for real-time applications. The framework's scalability and adaptability highlight its potential as a foundation for intelligent, data-driven decision-making in modern manufacturing environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.7 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- Computer Vision: 2.7 -->
                    
                <!-- HPO and AutoML: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Decision Trees: 2.0 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.10991" target="_blank" rel="noopener noreferrer">Most General Explanations of Tree Ensembles</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yacine Izza, Alexey Ignatiev, Joao Marques-Silva, Peter J. Stuckey
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Explainable Artificial Intelligence (XAI) is critical for attaining trust in the operation of AI systems. A key question of an AI system is ``why was this decision made this way''. Formal approaches to XAI use a formal model of the AI system to identify abductive explanations. While abductive explan</span>
                
                <span class="abstract-full" style="display: none;">Explainable Artificial Intelligence (XAI) is critical for attaining trust in the operation of AI systems. A key question of an AI system is ``why was this decision made this way''. Formal approaches to XAI use a formal model of the AI system to identify abductive explanations. While abductive explanations may be applicable to a large number of inputs sharing the same concrete values, more general explanations may be preferred for numeric inputs. So-called inflated abductive explanations give intervals for each feature ensuring that any input whose values fall withing these intervals is still guaranteed to make the same prediction. Inflated explanations cover a larger portion of the input space, and hence are deemed more general explanations. But there can be many (inflated) abductive explanations for an instance. Which is the best? In this paper, we show how to find a most general abductive explanation for an AI decision. This explanation covers as much of the input space as possible, while still being a correct formal explanation of the model's behaviour. Given that we only want to give a human one explanation for a decision, the most general explanation gives us the explanation with the broadest applicability, and hence the one most likely to seem sensible. (The paper has been accepted at IJCAI2025 conference.)</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 2.9 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Medicine: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- LLMs: 1.6 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Cryptography: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.11009" target="_blank" rel="noopener noreferrer">Concept of a System-on-Chip Research Platform Benchmarking Interaction of Memristor-based Bio-inspired Computing Paradigms</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christian Grewing, Arun Ashok, Sabitha Kusuma, Michael Schiek, Andre Zambanini, Stefan van Waasen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A system architecture is suggested for a System on Chip that will combine several different memristor-based, bio-inspired computation arrays with inter- and intra-chip communication. It will serve as a benchmark system for future developments. The architecture takes the special requirements into acc</span>
                
                <span class="abstract-full" style="display: none;">A system architecture is suggested for a System on Chip that will combine several different memristor-based, bio-inspired computation arrays with inter- and intra-chip communication. It will serve as a benchmark system for future developments. The architecture takes the special requirements into account which are caused by the memristor co-integration on commercial CMOS structures in a post processing step of the chip. The interface considers the necessary data bandwidth to monitor the internal Network on Chip at speed and provides enough flexibility to give different measurement options.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.2 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Cryptography: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Finance: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.11012" target="_blank" rel="noopener noreferrer">Doppler Resilient Complementary Sequences: Tighter Aperiodic Ambiguity Function Bound and Optimal Constructions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zheng Wang, Yang Yang, Zhengchun Zhou, Avik Ranjan Adhikary, Pingzhi Fan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Doppler-resilient complementary sequence sets (DRCSs) are crucial in modern communication and sensing systems in mobile environments. In this paper, we propose a new lower bound for the aperiodic ambiguity function (AF) of unimodular DRCSs based on weight vectors, which generalizes the existing boun</span>
                
                <span class="abstract-full" style="display: none;">Doppler-resilient complementary sequence sets (DRCSs) are crucial in modern communication and sensing systems in mobile environments. In this paper, we propose a new lower bound for the aperiodic ambiguity function (AF) of unimodular DRCSs based on weight vectors, which generalizes the existing bound as a special case. The proposed lower bound is tighter than the Shen-Yang-Zhou-Liu-Fan bound. Finally, we propose a novel class of aperiodic DRCSs with small alphabets based on quasi-Florentine rectangles and Butson-type Hadamard matrices. Interestingly, the proposed DRCSs asymptotically satisfy the proposed bound.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.2 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Medicine: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Cryptography: 1.2 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.11016" target="_blank" rel="noopener noreferrer">GoLeash: Mitigating Golang Software Supply Chain Attacks with Runtime Policy Enforcement</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Carmine Cesarano, Martin Monperrus, Roberto Natella
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modern software supply chain attacks consist of introducing new, malicious capabilities into trusted third-party software components, in order to propagate to a victim through a package dependency chain. These attacks are especially concerning for the Go language ecosystem, which is extensively used</span>
                
                <span class="abstract-full" style="display: none;">Modern software supply chain attacks consist of introducing new, malicious capabilities into trusted third-party software components, in order to propagate to a victim through a package dependency chain. These attacks are especially concerning for the Go language ecosystem, which is extensively used in critical cloud infrastructures. We present GoLeash, a novel system that applies the principle of least privilege at the package-level granularity, by enforcing distinct security policies for each package in the supply chain. This finer granularity enables GoLeash to detect malicious packages more precisely than traditional sandboxing that handles security policies at process- or container-level. Moreover, GoLeash remains effective under obfuscation, can overcome the limitations of static analysis, and incurs acceptable runtime overhead.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.5 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Game Theory: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Cryptography: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.11018" target="_blank" rel="noopener noreferrer">Rethinking the Mean Teacher Strategy from the Perspective of Self-paced Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pengchen Zhang, Alan J. X. Guo, Sipin Luo, Zhe Han, Lin Guo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Semi-supervised medical image segmentation has attracted significant attention due to its potential to reduce manual annotation costs. The mean teacher (MT) strategy, commonly understood as introducing smoothed, temporally lagged consistency regularization, has demonstrated strong performance across</span>
                
                <span class="abstract-full" style="display: none;">Semi-supervised medical image segmentation has attracted significant attention due to its potential to reduce manual annotation costs. The mean teacher (MT) strategy, commonly understood as introducing smoothed, temporally lagged consistency regularization, has demonstrated strong performance across various tasks in this field. In this work, we reinterpret the MT strategy on supervised data as a form of self-paced learning, regulated by the output agreement between the temporally lagged teacher model and the ground truth labels. This idea is further extended to incorporate agreement between a temporally lagged model and a cross-architectural model, which offers greater flexibility in regulating the learning pace and enables application to unlabeled data. Specifically, we propose dual teacher-student learning (DTSL), a framework that introduces two groups of teacher-student models with different architectures. The output agreement between the cross-group teacher and student models is used as pseudo-labels, generated via a Jensen-Shannon divergence-based consensus label generator (CLG). Extensive experiments on popular datasets demonstrate that the proposed method consistently outperforms existing state-of-the-art approaches. Ablation studies further validate the effectiveness of the proposed modules.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 3.6 -->
                    
                <!-- Medicine: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.11029" target="_blank" rel="noopener noreferrer">Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Li Ju, Max Andersson, Stina Fredriksson, Edward Gl\"ockner, Andreas Hellander, Ekta Vats, Prashant Singh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Vision-language models (VLMs) as foundation models have significantly enhanced performance across a wide range of visual and textual tasks, without requiring large-scale training from scratch for downstream tasks. However, these deterministic VLMs fail to capture the inherent ambiguity and uncertain</span>
                
                <span class="abstract-full" style="display: none;">Vision-language models (VLMs) as foundation models have significantly enhanced performance across a wide range of visual and textual tasks, without requiring large-scale training from scratch for downstream tasks. However, these deterministic VLMs fail to capture the inherent ambiguity and uncertainty in natural language and visual data. Recent probabilistic post-hoc adaptation methods address this by mapping deterministic embeddings onto probability distributions; however, existing approaches do not account for the asymmetric uncertainty structure of the modalities, and the constraint that meaningful deterministic embeddings reside on a unit hypersphere, potentially leading to suboptimal performance. In this paper, we address the asymmetric uncertainty structure inherent in textual and visual data, and propose AsymVLM to build probabilistic embeddings from pre-trained VLMs on the unit hypersphere, enabling uncertainty quantification. We validate the effectiveness of the probabilistic embeddings on established benchmarks, and present comprehensive ablation studies demonstrating the inherent nature of asymmetry in the uncertainty structure of textual and visual data.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.8 -->
                    
                <!-- Medicine: 3.6 -->
                    
                <!-- Computer Vision: 3.1 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Hardware: 2.2 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.11030" target="_blank" rel="noopener noreferrer">The heteronomy of algorithms: Traditional knowledge and computational knowledge</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: David M. Berry
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">If an active citizen should increasingly be a computationally enlightened one, replacing the autonomy of reason with the heteronomy of algorithms, then I argue in this article that we must begin teaching the principles of critiquing the computal through new notions of what we might call digital Bild</span>
                
                <span class="abstract-full" style="display: none;">If an active citizen should increasingly be a computationally enlightened one, replacing the autonomy of reason with the heteronomy of algorithms, then I argue in this article that we must begin teaching the principles of critiquing the computal through new notions of what we might call digital Bildung. Indeed, if civil society itself is mediated by computational systems and media, the public use of reason must also be complemented by skills for negotiating and using these computal forms to articulate such critique. Not only is there a need to raise the intellectual tone regarding computation and its related softwarization processes, but there is an urgent need to attend to the likely epistemic challenges from computation which, as presently constituted, tends towards justification through a philosophy of utility rather than through a philosophy of care for the territory of the intellect. We therefore need to develop an approach to this field that uses concepts and methods drawn from philosophy, politics, history, anthropology, sociology, media studies, computer science, and the humanities more generally, to try to understand these issues - particularly the way in which software and data increasingly penetrate our everyday life and the pressures and fissures that are created. We must, in other words, move to undertake a critical interdisciplinary research program to understand the way in which these systems are created, instantiated, and normatively engendered in both specific and general contexts.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.6 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.11032" target="_blank" rel="noopener noreferrer">DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yuran Wang, Ruihai Wu, Yue Chen, Jiarui Wang, Jiaqi Liang, Ziyu Zhu, Haoran Geng, Jitendra Malik, Pieter Abbeel, Hao Dong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Garment manipulation is a critical challenge due to the diversity in garment categories, geometries, and deformations. Despite this, humans can effortlessly handle garments, thanks to the dexterity of our hands. However, existing research in the field has struggled to replicate this level of dexteri</span>
                
                <span class="abstract-full" style="display: none;">Garment manipulation is a critical challenge due to the diversity in garment categories, geometries, and deformations. Despite this, humans can effortlessly handle garments, thanks to the dexterity of our hands. However, existing research in the field has struggled to replicate this level of dexterity, primarily hindered by the lack of realistic simulations of dexterous garment manipulation. Therefore, we propose DexGarmentLab, the first environment specifically designed for dexterous (especially bimanual) garment manipulation, which features large-scale high-quality 3D assets for 15 task scenarios, and refines simulation techniques tailored for garment modeling to reduce the sim-to-real gap. Previous data collection typically relies on teleoperation or training expert reinforcement learning (RL) policies, which are labor-intensive and inefficient. In this paper, we leverage garment structural correspondence to automatically generate a dataset with diverse trajectories using only a single expert demonstration, significantly reducing manual intervention. However, even extensive demonstrations cannot cover the infinite states of garments, which necessitates the exploration of new algorithms. To improve generalization across diverse garment shapes and deformations, we propose a Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies transferable affordance points to accurately locate the manipulation area, then generates generalizable trajectories to complete the task. Through extensive experiments and detailed analysis of our method and baseline, we demonstrate that HALO consistently outperforms existing methods, successfully generalizing to previously unseen instances even with significant variations in shape and deformation where others fail. Our project page is available at: https://wayrise.github.io/DexGarmentLab/.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.8 -->
                    
                <!-- Federated Learning: 4.2 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Robotics: 1.0 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.11035" target="_blank" rel="noopener noreferrer">Deep Latent Variable Model based Vertical Federated Learning with Flexible Alignment and Labeling Scenarios</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kihun Hong, Sejun Park, Ganguk Hwang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Federated learning (FL) has attracted significant attention for enabling collaborative learning without exposing private data. Among the primary variants of FL, vertical federated learning (VFL) addresses feature-partitioned data held by multiple institutions, each holding complementary information </span>
                
                <span class="abstract-full" style="display: none;">Federated learning (FL) has attracted significant attention for enabling collaborative learning without exposing private data. Among the primary variants of FL, vertical federated learning (VFL) addresses feature-partitioned data held by multiple institutions, each holding complementary information for the same set of users. However, existing VFL methods often impose restrictive assumptions such as a small number of participating parties, fully aligned data, or only using labeled data. In this work, we reinterpret alignment gaps in VFL as missing data problems and propose a unified framework that accommodates both training and inference under arbitrary alignment and labeling scenarios, while supporting diverse missingness mechanisms. In the experiments on 168 configurations spanning four benchmark datasets, six training-time missingness patterns, and seven testing-time missingness patterns, our method outperforms all baselines in 160 cases with an average gap of 9.6 percentage points over the next-best competitors. To the best of our knowledge, this is the first VFL framework to jointly handle arbitrary data alignment, unlabeled data, and multi-party collaboration all at once.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 4.5 -->
                    
                <!-- Medicine: 4.5 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Hardware: 2.4 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.11044" target="_blank" rel="noopener noreferrer">Exploration by Random Distribution Distillation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhirui Fang, Kai Yang, Jian Tao, Jiafei Lyu, Lusong Li, Li Shen, Xiu Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Exploration remains a critical challenge in online reinforcement learning, as an agent must effectively explore unknown environments to achieve high returns. Currently, the main exploration algorithms are primarily count-based methods and curiosity-based methods, with prediction-error methods being </span>
                
                <span class="abstract-full" style="display: none;">Exploration remains a critical challenge in online reinforcement learning, as an agent must effectively explore unknown environments to achieve high returns. Currently, the main exploration algorithms are primarily count-based methods and curiosity-based methods, with prediction-error methods being a prominent example. In this paper, we propose a novel method called \textbf{R}andom \textbf{D}istribution \textbf{D}istillation (RDD), which samples the output of a target network from a normal distribution. RDD facilitates a more extensive exploration by explicitly treating the difference between the prediction network and the target network as an intrinsic reward. Furthermore, by introducing randomness into the output of the target network for a given state and modeling it as a sample from a normal distribution, intrinsic rewards are bounded by two key components: a pseudo-count term ensuring proper exploration decay and a discrepancy term accounting for predictor convergence. We demonstrate that RDD effectively unifies both count-based and prediction-error approaches. It retains the advantages of prediction-error methods in high-dimensional spaces, while also implementing an intrinsic reward decay mode akin to the pseudo-count method. In the experimental section, RDD is compared with more advanced methods in a series of environments. Both theoretical analysis and experimental results confirm the effectiveness of our approach in improving online exploration for reinforcement learning tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.11047" target="_blank" rel="noopener noreferrer">User-centric Vehicle-to-Grid Optimization with an Input Convex Neural Network-based Battery Degradation Model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Arghya Mallick, Georgios Pantazis, Mohammad Khosravi, Peyman Mohajerin Esfahani, Sergio Grammatico
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a data-driven, user-centric vehicle-to-grid (V2G) methodology based on multi-objective optimization to balance battery degradation and V2G revenue according to EV user preference. Given the lack of accurate and generalizable battery degradation models, we leverage input convex neural netw</span>
                
                <span class="abstract-full" style="display: none;">We propose a data-driven, user-centric vehicle-to-grid (V2G) methodology based on multi-objective optimization to balance battery degradation and V2G revenue according to EV user preference. Given the lack of accurate and generalizable battery degradation models, we leverage input convex neural networks (ICNNs) to develop a data-driven degradation model trained on extensive experimental datasets. This approach enables our model to capture nonconvex dependencies on battery temperature and time while maintaining convexity with respect to the charging rate. Such a partial convexity property ensures that the second stage of our methodology remains computationally efficient. In the second stage, we integrate our data-driven degradation model into a multi-objective optimization framework to generate an optimal smart charging profile for each EV. This profile effectively balances the trade-off between financial benefits from V2G participation and battery degradation, controlled by a hyperparameter reflecting the user prioritization of battery health. Numerical simulations show the high accuracy of the ICNN model in predicting battery degradation for unseen data. Finally, we present a trade-off curve illustrating financial benefits from V2G versus losses from battery health degradation based on user preferences and showcase smart charging strategies under realistic scenarios.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.3 -->
                    
                <!-- Federated Learning: 4.0 -->
                    
                <!-- Evolutionary Algorithms: 2.7 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.11051" target="_blank" rel="noopener noreferrer">CAMEO: Collection of Multilingual Emotional Speech Corpora</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Iwona Christop, Maciej Czajka
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents CAMEO -- a curated collection of multilingual emotional speech datasets designed to facilitate research in emotion recognition and other speech-related tasks. The main objectives were to ensure easy access to the data, to allow reproducibility of the results, and to provide a sta</span>
                
                <span class="abstract-full" style="display: none;">This paper presents CAMEO -- a curated collection of multilingual emotional speech datasets designed to facilitate research in emotion recognition and other speech-related tasks. The main objectives were to ensure easy access to the data, to allow reproducibility of the results, and to provide a standardized benchmark for evaluating speech emotion recognition (SER) systems across different emotional states and languages. The paper describes the dataset selection criteria, the curation and normalization process, and provides performance results for several models. The collection, along with metadata, and a leaderboard, is publicly available via the Hugging Face platform.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.3 -->
                    
                <!-- LLMs: 2.4 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                <!-- GNN: 1.0 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0405
                </span>
                <a href="https://arxiv.org/abs/2505.11416" target="_blank" rel="noopener noreferrer">MID-L: Matrix-Interpolated Dropout Layer with Layer-wise Neuron Selection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pouya Shaeri, Ariane Middel
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modern neural networks often activate all neurons for every input, leading to unnecessary computation and inefficiency. We introduce Matrix-Interpolated Dropout Layer (MID-L), a novel module that dynamically selects and activates only the most informative neurons by interpolating between two transfo</span>
                
                <span class="abstract-full" style="display: none;">Modern neural networks often activate all neurons for every input, leading to unnecessary computation and inefficiency. We introduce Matrix-Interpolated Dropout Layer (MID-L), a novel module that dynamically selects and activates only the most informative neurons by interpolating between two transformation paths via a learned, input-dependent gating vector. Unlike conventional dropout or static sparsity methods, MID-L employs a differentiable Top-k masking strategy, enabling per-input adaptive computation while maintaining end-to-end differentiability. MID-L is model-agnostic and integrates seamlessly into existing architectures. Extensive experiments on six benchmarks, including MNIST, CIFAR-10, CIFAR-100, SVHN, UCI Adult, and IMDB, show that MID-L achieves up to average 55\% reduction in active neurons, 1.7$\times$ FLOPs savings, and maintains or exceeds baseline accuracy. We further validate the informativeness and selectivity of the learned neurons via Sliced Mutual Information (SMI) and observe improved robustness under overfitting and noisy data conditions. Additionally, MID-L demonstrates favorable inference latency and memory usage profiles, making it suitable for both research exploration and deployment on compute-constrained systems. These results position MID-L as a general-purpose, plug-and-play dynamic computation layer, bridging the gap between dropout regularization and efficient inference.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.3 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Decision Trees: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0532
                </span>
                <a href="https://arxiv.org/abs/2505.10994" target="_blank" rel="noopener noreferrer">Space Group Equivariant Crystal Diffusion</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rees Chang, Angela Pak, Alex Guerra, Ni Zhan, Nick Richardson, Elif Ertekin, Ryan P. Adams
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accelerating inverse design of crystalline materials with generative models has significant implications for a range of technologies. Unlike other atomic systems, 3D crystals are invariant to discrete groups of isometries called the space groups. Crucially, these space group symmetries are known to </span>
                
                <span class="abstract-full" style="display: none;">Accelerating inverse design of crystalline materials with generative models has significant implications for a range of technologies. Unlike other atomic systems, 3D crystals are invariant to discrete groups of isometries called the space groups. Crucially, these space group symmetries are known to heavily influence materials properties. We propose SGEquiDiff, a crystal generative model which naturally handles space group constraints with space group invariant likelihoods. SGEquiDiff consists of an SE(3)-invariant, telescoping discrete sampler of crystal lattices; permutation-invariant, transformer-based autoregressive sampling of Wyckoff positions, elements, and numbers of symmetrically unique atoms; and space group equivariant diffusion of atomic coordinates. We show that space group equivariant vector fields automatically live in the tangent spaces of the Wyckoff positions. SGEquiDiff achieves state-of-the-art performance on standard benchmark datasets as assessed by quantitative proxy metrics and quantum mechanical calculations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.9 -->
                    
                <!-- Medicine: 3.3 -->
                    
                <!-- Federated Learning: 3.1 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Hardware: 2.6 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0545
                </span>
                <a href="https://arxiv.org/abs/2505.10732" target="_blank" rel="noopener noreferrer">Automating Security Audit Using Large Language Model based Agent: An Exploration Experiment</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jia Hui Chin, Pu Zhang, Yu Xin Cheong, Jonathan Pan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In the current rapidly changing digital environment, businesses are under constant stress to ensure that their systems are secured. Security audits help to maintain a strong security posture by ensuring that policies are in place, controls are implemented, gaps are identified for cybersecurity risks</span>
                
                <span class="abstract-full" style="display: none;">In the current rapidly changing digital environment, businesses are under constant stress to ensure that their systems are secured. Security audits help to maintain a strong security posture by ensuring that policies are in place, controls are implemented, gaps are identified for cybersecurity risks mitigation. However, audits are usually manual, requiring much time and costs. This paper looks at the possibility of developing a framework to leverage Large Language Models (LLMs) as an autonomous agent to execute part of the security audit, namely with the field audit. password policy compliance for Windows operating system. Through the conduct of an exploration experiment of using GPT-4 with Langchain, the agent executed the audit tasks by accurately flagging password policy violations and appeared to be more efficient than traditional manual audits. Despite its potential limitations in operational consistency in complex and dynamic environment, the framework suggests possibilities to extend further to real-time threat monitoring and compliance checks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.2 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Medicine: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0615
                </span>
                <a href="https://arxiv.org/abs/2505.10590" target="_blank" rel="noopener noreferrer">Anchoring AI Capabilities in Market Valuations: The Capability Realization Rate Model and Valuation Misalignment Risk</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xinmin Fang, Lingfeng Tao, Zhengxiong Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent breakthroughs in artificial intelligence (AI) have triggered surges in market valuations for AI-related companies, often outpacing the realization of underlying capabilities. We examine the anchoring effect of AI capabilities on equity valuations and propose a Capability Realization Rate (CRR</span>
                
                <span class="abstract-full" style="display: none;">Recent breakthroughs in artificial intelligence (AI) have triggered surges in market valuations for AI-related companies, often outpacing the realization of underlying capabilities. We examine the anchoring effect of AI capabilities on equity valuations and propose a Capability Realization Rate (CRR) model to quantify the gap between AI potential and realized performance. Using data from the 2023--2025 generative AI boom, we analyze sector-level sensitivity and conduct case studies (OpenAI, Adobe, NVIDIA, Meta, Microsoft, Goldman Sachs) to illustrate patterns of valuation premium and misalignment. Our findings indicate that AI-native firms commanded outsized valuation premiums anchored to future potential, while traditional companies integrating AI experienced re-ratings subject to proof of tangible returns. We argue that CRR can help identify valuation misalignment risk-where market prices diverge from realized AI-driven value. We conclude with policy recommendations to improve transparency, mitigate speculative bubbles, and align AI innovation with sustainable market value.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.0 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0722
                </span>
                <a href="https://arxiv.org/abs/2505.00254" target="_blank" rel="noopener noreferrer">Empowering Agentic Video Analytics Systems with Video Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">AI-driven video analytics has become increasingly pivotal across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Video-Language Models (VLMs) as transformative tech</span>
                
                <span class="abstract-full" style="display: none;">AI-driven video analytics has become increasingly pivotal across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Video-Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVAS, a VLM-powered system designed for open-ended, advanced video analytics. AVAS incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively, significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an accuracy of 75.8%.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.1 -->
                    
                <!-- Medicine: 4.4 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Datasets: 2.1 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0906
                </span>
                <a href="https://arxiv.org/abs/2505.11181" target="_blank" rel="noopener noreferrer">Feasibility with Language Models for Open-World Compositional Zero-Shot Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jae Myung Kim, Stephan Alaniz, Cordelia Schmid, Zeynep Akata
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Humans can easily tell if an attribute (also called state) is realistic, i.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In Open-World Compositional Zero-Shot Learning, when all possible state-object combinations are considered as unseen classes, zero-shot predictors tend </span>
                
                <span class="abstract-full" style="display: none;">Humans can easily tell if an attribute (also called state) is realistic, i.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In Open-World Compositional Zero-Shot Learning, when all possible state-object combinations are considered as unseen classes, zero-shot predictors tend to perform poorly. Our work focuses on using external auxiliary knowledge to determine the feasibility of state-object combinations. Our Feasibility with Language Model (FLM) is a simple and effective approach that leverages Large Language Models (LLMs) to better comprehend the semantic relationships between states and objects. FLM involves querying an LLM about the feasibility of a given pair and retrieving the output logit for the positive answer. To mitigate potential misguidance of the LLM given that many of the state-object compositions are rare or completely infeasible, we observe that the in-context learning ability of LLMs is essential. We present an extensive study identifying Vicuna and ChatGPT as best performing, and we demonstrate that our FLM consistently improves OW-CZSL performance across all three benchmarks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 11.8 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0917
                </span>
                <a href="https://arxiv.org/abs/2505.10742" target="_blank" rel="noopener noreferrer">Evaluations at Work: Measuring the Capabilities of GenAI in Use</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Brandon Lepine, Gawesha Weerantunga, Juho Kim, Pamela Mishkin, Matthew Beane
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Current AI benchmarks miss the messy, multi-turn nature of human-AI collaboration. We present an evaluation framework that decomposes real-world tasks into interdependent subtasks, letting us track both LLM performance and users' strategies across a dialogue. Complementing this framework, we develop</span>
                
                <span class="abstract-full" style="display: none;">Current AI benchmarks miss the messy, multi-turn nature of human-AI collaboration. We present an evaluation framework that decomposes real-world tasks into interdependent subtasks, letting us track both LLM performance and users' strategies across a dialogue. Complementing this framework, we develop a suite of metrics, including a composite usage derived from semantic similarity, word overlap, and numerical matches; structural coherence; intra-turn diversity; and a novel measure of the "information frontier" reflecting the alignment between AI outputs and users' working knowledge. We demonstrate our methodology in a financial valuation task that mirrors real-world complexity. Our empirical findings reveal that while greater integration of LLM-generated content generally enhances output quality, its benefits are moderated by factors such as response incoherence, excessive subtask diversity, and the distance of provided information from users' existing knowledge. These results suggest that proactive dialogue strategies designed to inject novelty may inadvertently undermine task performance. Our work thus advances a more holistic evaluation of human-AI collaboration, offering both a robust methodological framework and actionable insights for developing more effective AI-augmented work processes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 11.0 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0928
                </span>
                <a href="https://arxiv.org/abs/2505.11080" target="_blank" rel="noopener noreferrer">BLEUBERI: BLEU is a surprisingly effective reward for instruction following</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yapei Chang, Yekyung Kim, Michael Krumdick, Amir Zadeh, Chuan Li, Chris Tanner, Mohit Iyyer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the quest</span>
                
                <span class="abstract-full" style="display: none;">Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 12.1 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Medicine: 2.3 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- Decision Trees: 2.2 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- HPO and AutoML: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.114
                </span>
                <a href="https://arxiv.org/abs/2505.11423" target="_blank" rel="noopener noreferrer">When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning</span>
                
                <span class="abstract-full" style="display: none;">Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 16.0 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- Decision Trees: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1339
                </span>
                <a href="https://arxiv.org/abs/2411.08881" target="_blank" rel="noopener noreferrer">Can We Trust AI Agents? A Case Study of an LLM-Based Multi-Agent System for Ethical AI</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jos\'e Antonio Siqueira de Cerqueira, Mamia Agbese, Rebekah Rousi, Nannan Xi, Juho Hamari, Pekka Abrahamsson
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">AI-based systems, including Large Language Models (LLM), impact millions by supporting diverse tasks but face issues like misinformation, bias, and misuse. AI ethics is crucial as new technologies and concerns emerge, but objective, practical guidance remains debated. This study examines the use of </span>
                
                <span class="abstract-full" style="display: none;">AI-based systems, including Large Language Models (LLM), impact millions by supporting diverse tasks but face issues like misinformation, bias, and misuse. AI ethics is crucial as new technologies and concerns emerge, but objective, practical guidance remains debated. This study examines the use of LLMs for AI ethics in practice, assessing how LLM trustworthiness-enhancing techniques affect software development in this context. Using the Design Science Research (DSR) method, we identify techniques for LLM trustworthiness: multi-agents, distinct roles, structured communication, and multiple rounds of debate. We design a multi-agent prototype LLM-MAS, where agents engage in structured discussions on real-world AI ethics issues from the AI Incident Database. We evaluate the prototype across three case scenarios using thematic analysis, hierarchical clustering, comparative (baseline) studies, and running source code. The system generates approximately 2,000 lines of code per case, compared to only 80 lines in baseline trials. Discussions reveal terms like bias detection, transparency, accountability, user consent, GDPR compliance, fairness evaluation, and EU AI Act compliance, showing this prototype ability to generate extensive source code and documentation addressing often overlooked AI ethics issues. However, practical challenges in source code integration and dependency management may limit its use by practitioners.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 16.1 -->
                    
                <!-- Medicine: 3.3 -->
                    
                <!-- Blockchain: 2.7 -->
                    
                <!-- Hardware: 2.3 -->
                    
                <!-- Datasets: 2.3 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1376
                </span>
                <a href="https://arxiv.org/abs/2504.13837" target="_blank" rel="noopener noreferrer">Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly on mathematics and programming tasks. Similar to how traditional RL helps agents explore and learn new strategies, RLVR</span>
                
                <span class="abstract-full" style="display: none;">Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly on mathematics and programming tasks. Similar to how traditional RL helps agents explore and learn new strategies, RLVR is believed to enable LLMs to continuously self-improve, thus acquiring novel reasoning abilities beyond those of the corresponding base models. In this study we critically examine the current state of RLVR by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across various model families, RL algorithms, and math, coding, and visual reasoning benchmarks, using pass@k at large k values as the evaluation metric. Surprisingly, we find that the current training setup does not elicit fundamentally new reasoning patterns. While RLVR-trained models outperform their base models at small k (e.g., k = 1), the base models achieve a higher pass@k score when k is large. Coverage and perplexity analyses show that the observed reasoning abilities originate from and are bounded by the base model. Treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in leveraging the potential of the base model. By contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model's reasoning capabilities. Overall, our findings suggest that current RLVR methods have not yet realized the potential of RL to elicit truly novel reasoning abilities in LLMs. This highlights the need for improved RL paradigms, such as continual scaling and multi-turn agent-environment interaction, to unlock this potential.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 16.5 -->
                    
                <!-- Computer Vision: 3.2 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Medicine: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1704
                </span>
                <a href="https://arxiv.org/abs/2505.10792" target="_blank" rel="noopener noreferrer">Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhan Peng Lee, Andre Lin, Calvin Tan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed dow</span>
                
                <span class="abstract-full" style="display: none;">Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose a Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 22.2 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Decision Trees: 2.1 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Medicine: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1742
                </span>
                <a href="https://arxiv.org/abs/2501.13977" target="_blank" rel="noopener noreferrer">Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rajvardhan Oak, Muhammad Haroon, Claire Jo, Magdalena Wojcieszak, Anshuman Chhabra
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated </span>
                
                <span class="abstract-full" style="display: none;">Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 22.6 -->
                    
                <!-- Federated Learning: 3.0 -->
                    
                <!-- Computer Vision: 2.8 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- Decision Trees: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.2828
                </span>
                <a href="https://arxiv.org/abs/2505.11436" target="_blank" rel="noopener noreferrer">GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrat</span>
                
                <span class="abstract-full" style="display: none;">Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs' abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at https://github.com/stan-lei/GODBench-ACL2025.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 33.9 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Medicine: 1.6 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.9585
                </span>
                <a href="https://arxiv.org/abs/2503.11020" target="_blank" rel="noopener noreferrer">Fast and Robust Localization for Humanoid Soccer Robot via Iterative Landmark Matching</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ruochen Hou, Mingzhang Zhu, Hyunwoo Nam, Gabriel I. Fernandez, Dennis W. Hong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate robot localization is essential for effective operation. Monte Carlo Localization (MCL) is commonly used with known maps but is computationally expensive due to landmark matching for each particle. Humanoid robots face additional challenges, including sensor noise from locomotion vibrations</span>
                
                <span class="abstract-full" style="display: none;">Accurate robot localization is essential for effective operation. Monte Carlo Localization (MCL) is commonly used with known maps but is computationally expensive due to landmark matching for each particle. Humanoid robots face additional challenges, including sensor noise from locomotion vibrations and a limited field of view (FOV) due to camera placement. This paper proposes a fast and robust localization method via iterative landmark matching (ILM) for humanoid robots. The iterative matching process improves the accuracy of the landmark association so that it does not need MCL to match landmarks to particles. Pose estimation with the outlier removal process enhances its robustness to measurement noise and faulty detections. Furthermore, an additional filter can be utilized to fuse inertial data from the inertial measurement unit (IMU) and pose data from localization. We compared ILM with Iterative Closest Point (ICP), which shows that ILM method is more robust towards the error in the initial guess and easier to get a correct matching. We also compared ILM with the Augmented Monte Carlo Localization (aMCL), which shows that ILM method is much faster than aMCL and even more accurate. The proposed method's effectiveness is thoroughly evaluated through experiments and validated on the humanoid robot ARTEMIS during RoboCup 2024 adult-sized soccer competition.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.1 -->
                    
                <!-- LLMs: 2.3 -->
                    
                <!-- Decision Trees: 2.2 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.0286
                </span>
                <a href="https://arxiv.org/abs/2505.10888" target="_blank" rel="noopener noreferrer">PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose Estimation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Saad Manzur, Bryan Vela, Brandon Vela, Aditya Agrawal, Lan-Anh Dang-Vu, David Li, Wayne Hayes
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reliable three-dimensional human pose estimation is becoming increasingly important for real-world applications, yet much of prior work has focused solely on the performance within a single dataset. In practice, however, systems must adapt to diverse viewpoints, environments, and camera setups -- co</span>
                
                <span class="abstract-full" style="display: none;">Reliable three-dimensional human pose estimation is becoming increasingly important for real-world applications, yet much of prior work has focused solely on the performance within a single dataset. In practice, however, systems must adapt to diverse viewpoints, environments, and camera setups -- conditions that differ significantly from those encountered during training, which is often the case in real-world scenarios. To address these challenges, we present a standardized testing environment in which each method is evaluated on a variety of datasets, ensuring consistent and fair cross-dataset comparisons -- allowing for the analysis of methods on previously unseen data. Therefore, we propose PoseBench3D, a unified framework designed to systematically re-evaluate prior and future models across four of the most widely used datasets for human pose estimation -- with the framework able to support novel and future datasets as the field progresses. Through a unified interface, our framework provides datasets in a pre-configured yet easily modifiable format, ensuring compatibility with diverse model architectures. We re-evaluated the work of 18 methods, either trained or gathered from existing literature, and reported results using both Mean Per Joint Position Error (MPJPE) and Procrustes Aligned Mean Per Joint Position Error (PA-MPJPE) metrics, yielding more than 100 novel cross-dataset evaluation results. Additionally, we analyze performance differences resulting from various pre-processing techniques and dataset preparation parameters -- offering further insight into model generalization capabilities.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.1 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.0689
                </span>
                <a href="https://arxiv.org/abs/2503.05383" target="_blank" rel="noopener noreferrer">AVA: Attentive VLM Agent for Mastering StarCraft II</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Weiyu Ma, Yuqian Fu, Zecheng Zhang, Bernard Ghanem, Guohao Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce Attentive VLM Agent (AVA), a multimodal StarCraft II agent that aligns artificial agent perception with the human gameplay experience. Traditional frameworks such as SMAC rely on abstract state representations that diverge significantly from human perception, limiting the ecological val</span>
                
                <span class="abstract-full" style="display: none;">We introduce Attentive VLM Agent (AVA), a multimodal StarCraft II agent that aligns artificial agent perception with the human gameplay experience. Traditional frameworks such as SMAC rely on abstract state representations that diverge significantly from human perception, limiting the ecological validity of agent behavior. Our agent addresses this limitation by incorporating RGB visual inputs and natural language observations that more closely simulate human cognitive processes during gameplay. The AVA architecture consists of three integrated components: (1) a vision-language model enhanced with specialized self-attention mechanisms for strategic unit targeting and battlefield assessment, (2) a retrieval-augmented generation system that leverages domain-specific StarCraft II knowledge to inform tactical decisions, and (3) a dynamic role-based task distribution system that enables coordinated multi-agent behavior. The experimental evaluation in our proposed AVACraft environment, which contains 21 multimodal StarCraft II scenarios, demonstrates that AVA powered by foundation models (specifically Qwen-VL and GPT-4o) can execute complex tactical maneuvers without explicit training, achieving comparable performance to traditional MARL methods that require substantial training iterations. This work establishes a foundation for developing human-aligned StarCraft II agents and advances the broader research agenda of multimodal game AI. Our implementation is available at https://github.com/camel-ai/VLM-Play-StarCraft2.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.8 -->
                    
                <!-- Medicine: 5.1 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.101
                </span>
                <a href="https://arxiv.org/abs/2505.10925" target="_blank" rel="noopener noreferrer">Enforced Interface Constraints for Domain Decomposition Method of Discrete Physics-Informed Neural Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jichao Yin, Mingxuan Li, Jianguang Fang, Hu Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study presents a discrete physics-informed neural network (dPINN) framework, enhanced with enforced interface constraints (EIC), for modeling physical systems using the domain decomposition method (DDM). Built upon finite element-style mesh discretization, the dPINN accurately evaluates system </span>
                
                <span class="abstract-full" style="display: none;">This study presents a discrete physics-informed neural network (dPINN) framework, enhanced with enforced interface constraints (EIC), for modeling physical systems using the domain decomposition method (DDM). Built upon finite element-style mesh discretization, the dPINN accurately evaluates system energy through Gaussian quadrature-based element-wise integration. To ensure physical field continuity across subdomain interfaces, the EIC mechanism enforces interfacial displacement constraints without requiring auxiliary sampling or loss penalties.This formulation supports independent meshing in each subdomain, simplifying preprocessing and improving computational flexibility. Additionally, by eliminating the influence of weak spatial constraints (WSC) commonly observed in traditional PINNs, the EIC-dPINN delivers more stable and physically consistent predictions.Extensive two- and three-dimensional numerical experiments validate the proposed framework's accuracy and demonstrate the computational efficiency gains achieved through parallel training. The results highlight the framework's scalability, robustness, and potential for solving large-scale, geometrically complex problems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.9 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Hardware: 2.8 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Finance: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.1118
                </span>
                <a href="https://arxiv.org/abs/2505.10930" target="_blank" rel="noopener noreferrer">Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Congcong Zhu, Xiaoyan Xu, Jiayue Han, Jingrun Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Auto-regressive partial differential equation (PDE) foundation models have shown great potential in handling time-dependent data. However, these models suffer from the shortcut problem deeply rooted in auto-regressive prediction, causing error accumulation. The challenge becomes particularly evident</span>
                
                <span class="abstract-full" style="display: none;">Auto-regressive partial differential equation (PDE) foundation models have shown great potential in handling time-dependent data. However, these models suffer from the shortcut problem deeply rooted in auto-regressive prediction, causing error accumulation. The challenge becomes particularly evident for out-of-distribution data, as the pretraining performance may approach random model initialization for downstream tasks with long-term dynamics. To deal with this problem, we propose physics-informed temporal alignment (PITA), a self-supervised learning framework inspired by inverse problem solving. Specifically, PITA aligns the physical dynamics discovered at different time steps on each given PDE trajectory by integrating physics-informed constraints into the self-supervision signal. The alignment is derived from observation data without relying on known physics priors, indicating strong generalization ability to the out-of-distribution data. Extensive experiments show that PITA significantly enhances the accuracy and robustness of existing foundation models on diverse time-dependent PDE data. The code is available at https://github.com/SCAILab-USTC/PITA.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.6 -->
                    
                <!-- Computer Vision: 3.3 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- GNN: 2.8 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Decision Trees: 1.8 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.1425
                </span>
                <a href="https://arxiv.org/abs/2505.10787" target="_blank" rel="noopener noreferrer">EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jianlin Guo, Haihong Xiao, Wenxiong Kang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Efficient scene representations are essential for many real-world applications, especially those involving spatial measurement. Although current NeRF-based methods have achieved impressive results in reconstructing building-scale scenes, they still suffer from slow training and inference speeds due </span>
                
                <span class="abstract-full" style="display: none;">Efficient scene representations are essential for many real-world applications, especially those involving spatial measurement. Although current NeRF-based methods have achieved impressive results in reconstructing building-scale scenes, they still suffer from slow training and inference speeds due to time-consuming stochastic sampling. Recently, 3D Gaussian Splatting (3DGS) has demonstrated excellent performance with its high-quality rendering and real-time speed, especially for objects and small-scale scenes. However, in outdoor scenes, its point-based explicit representation lacks an effective adjustment mechanism, and the millions of Gaussian points required often lead to memory constraints during training. To address these challenges, we propose EA-3DGS, a high-quality real-time rendering method designed for outdoor scenes. First, we introduce a mesh structure to regulate the initialization of Gaussian components by leveraging an adaptive tetrahedral mesh that partitions the grid and initializes Gaussian components on each face, effectively capturing geometric structures in low-texture regions. Second, we propose an efficient Gaussian pruning strategy that evaluates each 3D Gaussian's contribution to the view and prunes accordingly. To retain geometry-critical Gaussian points, we also present a structure-aware densification strategy that densifies Gaussian points in low-curvature regions. Additionally, we employ vector quantization for parameter quantization of Gaussian components, significantly reducing disk space requirements with only a minimal impact on rendering quality. Extensive experiments on 13 scenes, including eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \& Temples, WHU) and five self-collected scenes acquired through UAV photogrammetry measurement from SCUT-CA and plateau regions, further demonstrate the superiority of our method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.3 -->
                    
                <!-- 3D: 4.6 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.159
                </span>
                <a href="https://arxiv.org/abs/2505.11264" target="_blank" rel="noopener noreferrer">Multi-view dense image matching with similarity learning and geometry priors</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mohamed Ali Chebbi, Ewelina Rupnik, Paul Lopes, Marc Pierrot-Deseilligny
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce MV-DeepSimNets, a comprehensive suite of deep neural networks designed for multi-view similarity learning, leveraging epipolar geometry for training. Our approach incorporates an online geometry prior to characterize pixel relationships, either along the epipolar line or through homogra</span>
                
                <span class="abstract-full" style="display: none;">We introduce MV-DeepSimNets, a comprehensive suite of deep neural networks designed for multi-view similarity learning, leveraging epipolar geometry for training. Our approach incorporates an online geometry prior to characterize pixel relationships, either along the epipolar line or through homography rectification. This enables the generation of geometry-aware features from native images, which are then projected across candidate depth hypotheses using plane sweeping. Our method geometric preconditioning effectively adapts epipolar-based features for enhanced multi-view reconstruction, without requiring the laborious multi-view training dataset creation. By aggregating learned similarities, we construct and regularize the cost volume, leading to improved multi-view surface reconstruction over traditional dense matching approaches. MV-DeepSimNets demonstrates superior performance against leading similarity learning networks and end-to-end regression models, especially in terms of generalization capabilities across both aerial and satellite imagery with varied ground sampling distances. Our pipeline is integrated into MicMac software and can be readily adopted in standard multi-resolution image matching pipelines.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.2 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Bayesian Optimization: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.1657
                </span>
                <a href="https://arxiv.org/abs/2505.11294" target="_blank" rel="noopener noreferrer">Bidirectional Information Flow (BIF) -- A Sample Efficient Hierarchical Gaussian Process for Bayesian Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Juan D. Guerra (Polytechnique Montr\'eal, Mila - Quebec Artificial Intelligence Institute), Thomas Garbay (Polytechnique Montr\'eal, Mila - Quebec Artificial Intelligence Institute), Guillaume Lajoie (Universit\'e de Montr\'eal, Mila - Quebec Artificial Intelligence Institute), Marco Bonizzato (Polytechnique Montr\'eal, Mila - Quebec Artificial Intelligence Institute)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Hierarchical Gaussian Process (H-GP) models divide problems into different subtasks, allowing for different models to address each part, making them well-suited for problems with inherent hierarchical structure. However, typical H-GP models do not fully take advantage of this structure, only sending</span>
                
                <span class="abstract-full" style="display: none;">Hierarchical Gaussian Process (H-GP) models divide problems into different subtasks, allowing for different models to address each part, making them well-suited for problems with inherent hierarchical structure. However, typical H-GP models do not fully take advantage of this structure, only sending information up or down the hierarchy. This one-way coupling limits sample efficiency and slows convergence. We propose Bidirectional Information Flow (BIF), an efficient H-GP framework that establishes bidirectional information exchange between parent and child models in H-GPs for online training. BIF retains the modular structure of hierarchical models - the parent combines subtask knowledge from children GPs - while introducing top-down feedback to continually refine children models during online learning. This mutual exchange improves sample efficiency, enables robust training, and allows modular reuse of learned subtask models. BIF outperforms conventional H-GP Bayesian Optimization methods, achieving up to 85% and 5x higher $R^2$ scores for the parent and children respectively, on synthetic and real-world neurostimulation optimization tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.3 -->
                    
                <!-- Medicine: 5.2 -->
                    
                <!-- Computer Vision: 2.9 -->
                    
                <!-- Bayesian Optimization: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Decision Trees: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.1773
                </span>
                <a href="https://arxiv.org/abs/2505.11053" target="_blank" rel="noopener noreferrer">Conceptual framework for the application of deep neural networks to surface composition reconstruction from Mercury's exospheric data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Adrian Kazakov (INAF-IAPS, Rome, Italy), Anna Milillo (INAF-IAPS, Rome, Italy), Alessandro Mura (INAF-IAPS, Rome, Italy), Stavro Ivanovski (INAF-Osservatorio Astronomico di Trieste, Trieste, Italy), Valeria Mangano (INAF-IAPS, Rome, Italy), Alessandro Aronica (INAF-IAPS, Rome, Italy), Elisabetta De Angelis (INAF-IAPS, Rome, Italy), Pier Paolo Di Bartolomeo (INAF-IAPS, Rome, Italy), Alessandro Brin (INAF-IAPS, Rome, Italy), Luca Colasanti (INAF-IAPS, Rome, Italy), Miguel Escalona-Moran (Augmented Intelligence Lab, Salceda de Caselas, Spain), Francesco Lazzarotto (INAF-Osservatorio Astronomico di Padova, Padova, Italy), Stefano Massetti (INAF-IAPS, Rome, Italy), Martina Moroni (INAF-IAPS, Rome, Italy), Raffaella Noschese (INAF-IAPS, Rome, Italy), Fabrizio Nuccilli (INAF-IAPS, Rome, Italy), Stefano Orsini (INAF-IAPS, Rome, Italy), Christina Plainaki (ASI - Italian Space Agency, Rome, Italy), Rosanna Rispoli (INAF-IAPS, Rome, Italy), Roberto Sordini (INAF-IAPS, Rome, Italy), Mirko Stumpo (INAF-IAPS, Rome, Italy), Nello Vertolli (INAF-IAPS, Rome, Italy)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Surface information derived from exospheric measurements at planetary bodies complements surface mapping provided by dedicated imagers, offering critical insights into surface release processes, interactions within the planetary environment, space weathering, and planetary evolution. This study expl</span>
                
                <span class="abstract-full" style="display: none;">Surface information derived from exospheric measurements at planetary bodies complements surface mapping provided by dedicated imagers, offering critical insights into surface release processes, interactions within the planetary environment, space weathering, and planetary evolution. This study explores the feasibility of deriving Mercury's regolith elemental composition from in-situ measurements of its neutral exosphere using deep neural networks (DNNs). We present a supervised feed-forward DNN architecture - a multilayer perceptron (MLP) - that, starting from exospheric densities and proton precipitation fluxes, predicts the chemical elements of the surface regolith below. It serves as an estimator for the surface-exosphere interaction and the processes leading to exosphere formation. Because the DNN requires a comprehensive exospheric dataset not available from previous missions, this study uses simulated exosphere components and simulated drivers. Extensive training and testing campaigns demonstrate the MLP's ability to accurately predict and reconstruct surface composition maps from these simulated measurements. Although this initial version does not aim to reproduce Mercury's actual surface composition, it provides a proof of concept, showcasing the algorithm's robustness and capacity for handling complex datasets to create estimators for exospheric generation models. Moreover, our tests reveal substantial potential for further development, suggesting that this method could significantly enhance the analysis of complex surface-exosphere interactions and complement planetary exosphere models. This work anticipates applying the approach to data from the BepiColombo mission, specifically the SERENA package, whose nominal phase begins in 2027.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.9 -->
                    
                <!-- LLMs: 2.4 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Robotics: 1.0 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                <!-- Bayesian Optimization: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2011
                </span>
                <a href="https://arxiv.org/abs/2505.11311" target="_blank" rel="noopener noreferrer">Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ardian Selmonaj, Alessandro Antonucci, Adrian Schneider, Michael R\"uegsegger, Matthias Sommer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Artificial intelligence (AI) is reshaping strategic planning, with Multi-Agent Reinforcement Learning (MARL) enabling coordination among autonomous agents in complex scenarios. However, its practical deployment in sensitive military contexts is constrained by the lack of explainability, which is an </span>
                
                <span class="abstract-full" style="display: none;">Artificial intelligence (AI) is reshaping strategic planning, with Multi-Agent Reinforcement Learning (MARL) enabling coordination among autonomous agents in complex scenarios. However, its practical deployment in sensitive military contexts is constrained by the lack of explainability, which is an essential factor for trust, safety, and alignment with human strategies. This work reviews and assesses current advances in explainability methods for MARL with a focus on simulated air combat scenarios. We proceed by adapting various explainability techniques to different aerial combat scenarios to gain explanatory insights about the model behavior. By linking AI-generated tactics with human-understandable reasoning, we emphasize the need for transparency to ensure reliable deployment and meaningful human-machine interaction. By illuminating the crucial importance of explainability in advancing MARL for operational defense, our work supports not only strategic planning but also the training of military personnel with insightful and comprehensible analyses.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.2 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- HPO and AutoML: 2.1 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2298
                </span>
                <a href="https://arxiv.org/abs/2505.10575" target="_blank" rel="noopener noreferrer">Robust Emotion Recognition via Bi-Level Self-Supervised Continual Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Adnan Ahmad, Bahareh Nakisa, Mohammad Naim Rastgoo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Emotion recognition through physiological signals such as electroencephalogram (EEG) has become an essential aspect of affective computing and provides an objective way to capture human emotions. However, physiological data characterized by cross-subject variability and noisy labels hinder the perfo</span>
                
                <span class="abstract-full" style="display: none;">Emotion recognition through physiological signals such as electroencephalogram (EEG) has become an essential aspect of affective computing and provides an objective way to capture human emotions. However, physiological data characterized by cross-subject variability and noisy labels hinder the performance of emotion recognition models. Existing domain adaptation and continual learning methods struggle to address these issues, especially under realistic conditions where data is continuously streamed and unlabeled. To overcome these limitations, we propose a novel bi-level self-supervised continual learning framework, SSOCL, based on a dynamic memory buffer. This bi-level architecture iteratively refines the dynamic buffer and pseudo-label assignments to effectively retain representative samples, enabling generalization from continuous, unlabeled physiological data streams for emotion recognition. The assigned pseudo-labels are subsequently leveraged for accurate emotion prediction. Key components of the framework, including a fast adaptation module and a cluster-mapping module, enable robust learning and effective handling of evolving data streams. Experimental validation on two mainstream EEG tasks demonstrates the framework's ability to adapt to continuous data streams while maintaining strong generalization across subjects, outperforming existing approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.7 -->
                    
                <!-- Federated Learning: 2.8 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Decision Trees: 1.7 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2303
                </span>
                <a href="https://arxiv.org/abs/2505.00169" target="_blank" rel="noopener noreferrer">GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Filipp Nikitin, Ian Dunn, David Ryan Koes, Olexandr Isayev
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Deep generative models have shown significant promise in generating valid 3D molecular structures, with the GEOM-Drugs dataset serving as a key benchmark. However, current evaluation protocols suffer from critical flaws, including incorrect valency definitions, bugs in bond order calculations, and r</span>
                
                <span class="abstract-full" style="display: none;">Deep generative models have shown significant promise in generating valid 3D molecular structures, with the GEOM-Drugs dataset serving as a key benchmark. However, current evaluation protocols suffer from critical flaws, including incorrect valency definitions, bugs in bond order calculations, and reliance on force fields inconsistent with the reference data. In this work, we revisit GEOM-Drugs and propose a corrected evaluation framework: we identify and fix issues in data preprocessing, construct chemically accurate valency tables, and introduce a GFN2-xTB-based geometry and energy benchmark. We retrain and re-evaluate several leading models under this framework, providing updated performance metrics and practical recommendations for future benchmarking. Our results underscore the need for chemically rigorous evaluation practices in 3D molecular generation. Our recommended evaluation methods and GEOM-Drugs processing scripts are available at https://github.com/isayevlab/geom-drugs-3dgen-evaluation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.5 -->
                    
                <!-- Medicine: 5.5 -->
                    
                <!-- Computer Vision: 2.8 -->
                    
                <!-- Hardware: 2.3 -->
                    
                <!-- Datasets: 2.2 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2428
                </span>
                <a href="https://arxiv.org/abs/2505.10941" target="_blank" rel="noopener noreferrer">Privacy-Aware Lifelong Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ozan \"Ozdenizci, Elmar Rueckert, Robert Legenstein
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Lifelong learning algorithms enable models to incrementally acquire new knowledge without forgetting previously learned information. Contrarily, the field of machine unlearning focuses on explicitly forgetting certain previous knowledge from pretrained models when requested, in order to comply with </span>
                
                <span class="abstract-full" style="display: none;">Lifelong learning algorithms enable models to incrementally acquire new knowledge without forgetting previously learned information. Contrarily, the field of machine unlearning focuses on explicitly forgetting certain previous knowledge from pretrained models when requested, in order to comply with data privacy regulations on the right-to-be-forgotten. Enabling efficient lifelong learning with the capability to selectively unlearn sensitive information from models presents a critical and largely unaddressed challenge with contradicting objectives. We address this problem from the perspective of simultaneously preventing catastrophic forgetting and allowing forward knowledge transfer during task-incremental learning, while ensuring exact task unlearning and minimizing memory requirements, based on a single neural network model to be adapted. Our proposed solution, privacy-aware lifelong learning (PALL), involves optimization of task-specific sparse subnetworks with parameter sharing within a single architecture. We additionally utilize an episodic memory rehearsal mechanism to facilitate exact unlearning without performance degradations. We empirically demonstrate the scalability of PALL across various architectures in image classification, and provide a state-of-the-art solution that uniquely integrates lifelong learning and privacy-aware unlearning mechanisms for responsible AI applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.4 -->
                    
                <!-- Federated Learning: 3.8 -->
                    
                <!-- LLMs: 3.6 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.3 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2528
                </span>
                <a href="https://arxiv.org/abs/2505.11390" target="_blank" rel="noopener noreferrer">IISE PG&E Energy Analytics Challenge 2025: Hourly-Binned Regression Models Beat Transformers in Load Forecasting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Millend Roy, Vladimir Pyltsov, Yinbo Hu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate electricity load forecasting is essential for grid stability, resource optimization, and renewable energy integration. While transformer-based deep learning models like TimeGPT have gained traction in time-series forecasting, their effectiveness in long-term electricity load prediction rema</span>
                
                <span class="abstract-full" style="display: none;">Accurate electricity load forecasting is essential for grid stability, resource optimization, and renewable energy integration. While transformer-based deep learning models like TimeGPT have gained traction in time-series forecasting, their effectiveness in long-term electricity load prediction remains uncertain. This study evaluates forecasting models ranging from classical regression techniques to advanced deep learning architectures using data from the ESD 2025 competition. The dataset includes two years of historical electricity load data, alongside temperature and global horizontal irradiance (GHI) across five sites, with a one-day-ahead forecasting horizon. Since actual test set load values remain undisclosed, leveraging predicted values would accumulate errors, making this a long-term forecasting challenge. We employ (i) Principal Component Analysis (PCA) for dimensionality reduction and (ii) frame the task as a regression problem, using temperature and GHI as covariates to predict load for each hour, (iii) ultimately stacking 24 models to generate yearly forecasts.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.8 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- HPO and AutoML: 2.1 -->
                    
                <!-- Datasets: 2.0 -->
                    
                <!-- Decision Trees: 2.0 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.258
                </span>
                <a href="https://arxiv.org/abs/2505.10869" target="_blank" rel="noopener noreferrer">A Convolution-Based Gait Asymmetry Metric for Inter-Limb Synergistic Coordination</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Go Fukino, Kanta Tachibana
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study focuses on the velocity patterns of various body parts during walking and proposes a method for evaluating gait symmetry. Traditional motion analysis studies have assessed gait symmetry based on differences in electromyographic (EMG) signals or acceleration between the left and right side</span>
                
                <span class="abstract-full" style="display: none;">This study focuses on the velocity patterns of various body parts during walking and proposes a method for evaluating gait symmetry. Traditional motion analysis studies have assessed gait symmetry based on differences in electromyographic (EMG) signals or acceleration between the left and right sides. In contrast, this paper models intersegmental coordination using an LTI system and proposes a dissimilarity metric to evaluate symmetry. The method was tested on five subjects with both symmetric and asymmetric gait.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.5 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Computer Vision: 2.7 -->
                    
                <!-- Blockchain: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Hardware: 2.2 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- 3D: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2789
                </span>
                <a href="https://arxiv.org/abs/2411.06581" target="_blank" rel="noopener noreferrer">HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yang Su, Na Yan, Yansha Deng, Mischa Dohler, Robert Schober
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Federated fine-tuning of pre-trained Large Language Models (LLMs) enables task-specific adaptation across diverse datasets while preserving privacy. However, challenges such as high computational and memory demands, heterogeneous client resources, bandwidth constraints, and ineffective global aggreg</span>
                
                <span class="abstract-full" style="display: none;">Federated fine-tuning of pre-trained Large Language Models (LLMs) enables task-specific adaptation across diverse datasets while preserving privacy. However, challenges such as high computational and memory demands, heterogeneous client resources, bandwidth constraints, and ineffective global aggregation hinder its efficiency. To address these issues, we propose HAFLQ (Heterogeneous Adaptive Federated Low-Rank Adaptation Fine-tuned LLM with Quantization), a novel framework for efficient and scalable federated fine-tuning of LLMs in heterogeneous environments. To reduce memory and computation demands, we propose a salience-driven adaptive LLM quantization framework that evaluates the importance of transformer blocks using a salience metric and applies adaptive block-wise quantization accordingly. To handle heterogeneous computational capabilities, we propose an importance-based parameter truncation and freezing scheme. To address communication bottlenecks, we propose an importance-aware bandwidth-adaptive quantization method, which dynamically adjusts parameter precision based on importance and bandwidth constraints. To improve global model aggregation, we propose an adaptive rank-1 matrix-level aggregation strategy, which prevents information dilution and accelerates convergence by aggregating only updated rank-1 matrices from clients. Experimental results on the text classification task demonstrate that HAFLQ reduces memory usage by 31%, lowers communication cost by 49%, improves accuracy by 50%, and achieves faster convergence compared to the baseline method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 16.5 -->
                    
                <!-- Medicine: 5.4 -->
                    
                <!-- Federated Learning: 4.0 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2805
                </span>
                <a href="https://arxiv.org/abs/2505.10696" target="_blank" rel="noopener noreferrer">TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Manthan Patel, Fan Yang, Yuheng Qiu, Cesar Cadena, Sebastian Scherer, Marco Hutter, Wenshan Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present TartanGround, a large-scale, multi-modal dataset to advance the perception and autonomy of ground robots operating in diverse environments. This dataset, collected in various photorealistic simulation environments includes multiple RGB stereo cameras for 360-degree coverage, along with de</span>
                
                <span class="abstract-full" style="display: none;">We present TartanGround, a large-scale, multi-modal dataset to advance the perception and autonomy of ground robots operating in diverse environments. This dataset, collected in various photorealistic simulation environments includes multiple RGB stereo cameras for 360-degree coverage, along with depth, optical flow, stereo disparity, LiDAR point clouds, ground truth poses, semantic segmented images, and occupancy maps with semantic labels. Data is collected using an integrated automatic pipeline, which generates trajectories mimicking the motion patterns of various ground robot platforms, including wheeled and legged robots. We collect 910 trajectories across 70 environments, resulting in 1.5 million samples. Evaluations on occupancy prediction and SLAM tasks reveal that state-of-the-art methods trained on existing datasets struggle to generalize across diverse scenes. TartanGround can serve as a testbed for training and evaluation of a broad range of learning-based tasks, including occupancy prediction, SLAM, neural scene representation, perception-based navigation, and more, enabling advancements in robotic perception and autonomy towards achieving robust models generalizable to more diverse scenarios. The dataset and codebase for data collection will be made publicly available upon acceptance. Webpage: https://tartanair.org/tartanground</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.5 -->
                    
                <!-- Medicine: 5.8 -->
                    
                <!-- Datasets: 4.2 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.291
                </span>
                <a href="https://arxiv.org/abs/2403.19310" target="_blank" rel="noopener noreferrer">MRNaB: Mixed Reality-based Robot Navigation Interface using Optical-see-through MR-beacons</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Eduardo Iglesius, Masato Kobayashi, Yuki Uranishi, Haruo Takemura
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advancements in robotics have led to the development of numerous interfaces to enhance the intuitiveness of robot navigation. However, the reliance on traditional 2D displays imposes limitations on the simultaneous visualization of information. Mixed Reality (MR) technology addresses this iss</span>
                
                <span class="abstract-full" style="display: none;">Recent advancements in robotics have led to the development of numerous interfaces to enhance the intuitiveness of robot navigation. However, the reliance on traditional 2D displays imposes limitations on the simultaneous visualization of information. Mixed Reality (MR) technology addresses this issue by enhancing the dimensionality of information visualization, allowing users to perceive multiple pieces of information concurrently. This paper proposes the Mixed Reality-based Robot Navigation Interface using an Optical-see-through MR-beacons (MRNaB), a novel approach that uses MR-beacons created with an ``air tap'', situated in the real world. This beacon is persistent, enabling multi-destination visualization and functioning as a signal transmitter for robot navigation, eliminating the need for repeated navigation inputs. Our system is mainly constructed into four primary functions: ``Add'', ``Move'', ``Delete'', and ``Select''. These allow for the addition of MR-beacons, location movement, its deletion, and the selection of MR-beacons for navigation purposes, respectively. To validate the effectiveness, we conducted comprehensive experiments comparing MRNaB with traditional 2D navigation systems. The results show significant improvements in user performance, both objectively and subjectively, confirming that the MRNaB enhances navigation efficiency and user experience. For additional material, please check: https://mertcookimg.github.io/mrnab</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.2 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3019
                </span>
                <a href="https://arxiv.org/abs/2505.10718" target="_blank" rel="noopener noreferrer">AI-enhanced semantic feature norms for 786 concepts</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Siddharth Suresh, Kushin Mukherjee, Tyler Giallanza, Xizheng Yu, Mia Patil, Jonathan D. Cohen, Timothy T. Rogers
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Semantic feature norms have been foundational in the study of human conceptual knowledge, yet traditional methods face trade-offs between concept/feature coverage and verifiability of quality due to the labor-intensive nature of norming studies. Here, we introduce a novel approach that augments a da</span>
                
                <span class="abstract-full" style="display: none;">Semantic feature norms have been foundational in the study of human conceptual knowledge, yet traditional methods face trade-offs between concept/feature coverage and verifiability of quality due to the labor-intensive nature of norming studies. Here, we introduce a novel approach that augments a dataset of human-generated feature norms with responses from large language models (LLMs) while verifying the quality of norms against reliable human judgments. We find that our AI-enhanced feature norm dataset, NOVA: Norms Optimized Via AI, shows much higher feature density and overlap among concepts while outperforming a comparable human-only norm dataset and word-embedding models in predicting people's semantic similarity judgments. Taken together, we demonstrate that human conceptual knowledge is richer than captured in previous norm datasets and show that, with proper validation, LLMs can serve as powerful tools for cognitive science research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 15.9 -->
                    
                <!-- Medicine: 5.1 -->
                    
                <!-- Datasets: 2.1 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3246
                </span>
                <a href="https://arxiv.org/abs/2505.10827" target="_blank" rel="noopener noreferrer">NeuSEditor: From Multi-View Images to Text-Guided Neural Surface Edits</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nail Ibrahimli, Julian F. P. Kooij, Liangliang Nan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Implicit surface representations are valued for their compactness and continuity, but they pose significant challenges for editing. Despite recent advancements, existing methods often fail to preserve identity and maintain geometric consistency during editing. To address these challenges, we present</span>
                
                <span class="abstract-full" style="display: none;">Implicit surface representations are valued for their compactness and continuity, but they pose significant challenges for editing. Despite recent advancements, existing methods often fail to preserve identity and maintain geometric consistency during editing. To address these challenges, we present NeuSEditor, a novel method for text-guided editing of neural implicit surfaces derived from multi-view images. NeuSEditor introduces an identity-preserving architecture that efficiently separates scenes into foreground and background, enabling precise modifications without altering the scene-specific elements. Our geometry-aware distillation loss significantly enhances rendering and geometric quality. Our method simplifies the editing workflow by eliminating the need for continuous dataset updates and source prompting. NeuSEditor outperforms recent state-of-the-art methods like PDS and InstructNeRF2NeRF, delivering superior quantitative and qualitative results. For more visual results, visit: neuseditor.github.io.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.2 -->
                    
                <!-- LLMs: 4.5 -->
                    
                <!-- Computer Vision: 3.0 -->
                    
                <!-- 3D: 2.7 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Decision Trees: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3492
                </span>
                <a href="https://arxiv.org/abs/2505.11468" target="_blank" rel="noopener noreferrer">PSDiffusion: Harmonized Multi-Layer Image Generation via Layout and Appearance Alignment</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dingbang Huang, Wenbo Li, Yifei Zhao, Xinyu Pan, Yanhong Zeng, Bo Dai
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Diffusion models have made remarkable advancements in generating high-quality images from textual descriptions. Recent works like LayerDiffuse have extended the previous single-layer, unified image generation paradigm to transparent image layer generation. However, existing multi-layer generation me</span>
                
                <span class="abstract-full" style="display: none;">Diffusion models have made remarkable advancements in generating high-quality images from textual descriptions. Recent works like LayerDiffuse have extended the previous single-layer, unified image generation paradigm to transparent image layer generation. However, existing multi-layer generation methods fail to handle the interactions among multiple layers such as rational global layout, physics-plausible contacts and visual effects like shadows and reflections while maintaining high alpha quality. To solve this problem, we propose PSDiffusion, a unified diffusion framework for simultaneous multi-layer text-to-image generation. Our model can automatically generate multi-layer images with one RGB background and multiple RGBA foregrounds through a single feed-forward process. Unlike existing methods that combine multiple tools for post-decomposition or generate layers sequentially and separately, our method introduces a global-layer interactive mechanism that generates layered-images concurrently and collaboratively, ensuring not only high quality and completeness for each layer, but also spatial and visual interactions among layers for global coherence.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.1 -->
                    
                <!-- Medicine: 6.0 -->
                    
                <!-- Computer Vision: 3.0 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- T2I: 2.0 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Decision Trees: 1.7 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3799
                </span>
                <a href="https://arxiv.org/abs/2505.11177" target="_blank" rel="noopener noreferrer">Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hrishit Madhavi, Jacob Cherian, Yuvraj Khamkar, Dhananjay Bhagat
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents an end-to-end suite for multilingual information extraction and processing from image-based documents. The system uses Optical Character Recognition (Tesseract) to extract text in languages such as English, Hindi, and Tamil, and then a pipeline involving large language model APIs</span>
                
                <span class="abstract-full" style="display: none;">This paper presents an end-to-end suite for multilingual information extraction and processing from image-based documents. The system uses Optical Character Recognition (Tesseract) to extract text in languages such as English, Hindi, and Tamil, and then a pipeline involving large language model APIs (Gemini) for cross-lingual translation, abstractive summarization, and re-translation into a target language. Additional modules add sentiment analysis (TensorFlow), topic classification (Transformers), and date extraction (Regex) for better document comprehension. Made available in an accessible Gradio interface, the current research shows a real-world application of libraries, models, and APIs to close the language gap and enhance access to information in image media across different linguistic environments</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.5 -->
                    
                <!-- Medicine: 5.7 -->
                    
                <!-- Hardware: 2.6 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.401
                </span>
                <a href="https://arxiv.org/abs/2407.20194" target="_blank" rel="noopener noreferrer">Radiance Fields for Robotic Teleoperation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Maximum Wilder-Smith, Vaishakh Patil, Marco Hutter
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS), have revolutionized graphics and novel view synthesis. Their ability to synthesize new viewpoints with photo-realistic quality, as well as capture complex volumetric and specular scenes, makes them an idea</span>
                
                <span class="abstract-full" style="display: none;">Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian Splatting (3DGS), have revolutionized graphics and novel view synthesis. Their ability to synthesize new viewpoints with photo-realistic quality, as well as capture complex volumetric and specular scenes, makes them an ideal visualization for robotic teleoperation setups. Direct camera teleoperation provides high-fidelity operation at the cost of maneuverability, while reconstruction-based approaches offer controllable scenes with lower fidelity. With this in mind, we propose replacing the traditional reconstruction-visualization components of the robotic teleoperation pipeline with online Radiance Fields, offering highly maneuverable scenes with photorealistic quality. As such, there are three main contributions to state of the art: (1) online training of Radiance Fields using live data from multiple cameras, (2) support for a variety of radiance methods including NeRF and 3DGS, (3) visualization suite for these methods including a virtual reality scene. To enable seamless integration with existing setups, these components were tested with multiple robots in multiple configurations and were displayed using traditional tools as well as the VR headset. The results across methods and robots were compared quantitatively to a baseline of mesh reconstruction, and a user study was conducted to compare the different visualization methods. For videos and code, check out https://rffr.leggedrobotics.com/works/teleoperation/.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.1 -->
                    
                <!-- LLMs: 2.8 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4338
                </span>
                <a href="https://arxiv.org/abs/2505.11283" target="_blank" rel="noopener noreferrer">SubROC: AUC-Based Discovery of Exceptional Subgroup Performance for Binary Classifiers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tom Siegl, Kutalm{\i}\c{s} Co\c{s}kun, Bjarne Hiller, Amin Mirzaei, Florian Lemmerich, Martin Becker
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Machine learning (ML) is increasingly employed in real-world applications like medicine or economics, thus, potentially affecting large populations. However, ML models often do not perform homogeneously across such populations resulting in subgroups of the population (e.g., sex=female AND marital_st</span>
                
                <span class="abstract-full" style="display: none;">Machine learning (ML) is increasingly employed in real-world applications like medicine or economics, thus, potentially affecting large populations. However, ML models often do not perform homogeneously across such populations resulting in subgroups of the population (e.g., sex=female AND marital_status=married) where the model underperforms or, conversely, is particularly accurate. Identifying and describing such subgroups can support practical decisions on which subpopulation a model is safe to deploy or where more training data is required. The potential of identifying and analyzing such subgroups has been recognized, however, an efficient and coherent framework for effective search is missing. Consequently, we introduce SubROC, an open-source, easy-to-use framework based on Exceptional Model Mining for reliably and efficiently finding strengths and weaknesses of classification models in the form of interpretable population subgroups. SubROC incorporates common evaluation measures (ROC and PR AUC), efficient search space pruning for fast exhaustive subgroup search, control for class imbalance, adjustment for redundant patterns, and significance testing. We illustrate the practical benefits of SubROC in case studies as well as in comparative analyses across multiple datasets.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.6 -->
                    
                <!-- LLMs: 5.1 -->
                    
                <!-- Hardware: 3.1 -->
                    
                <!-- Datasets: 2.3 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.437
                </span>
                <a href="https://arxiv.org/abs/2505.10686" target="_blank" rel="noopener noreferrer">NeoLightning: A Modern Reimagination of Gesture-Based Sound Design</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yonghyun Kim, Sangheon Park, Marcus Parker, Donghoon Seu, Alexandria Smith
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces NeoLightning, a modern reinterpretation of the Buchla Lightning. NeoLightning preserves the innovative spirit of Don Buchla's "Buchla Lightning" (introduced in the 1990s) while making its gesture-based interaction accessible to contemporary users. While the original Buchla Ligh</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces NeoLightning, a modern reinterpretation of the Buchla Lightning. NeoLightning preserves the innovative spirit of Don Buchla's "Buchla Lightning" (introduced in the 1990s) while making its gesture-based interaction accessible to contemporary users. While the original Buchla Lightning and many other historical instruments were groundbreaking in their time, they are now largely unsupported, limiting user interaction to indirect experiences. To address this, NeoLightning leverages MediaPipe for deep learning-based gesture recognition and employs Max/MSP and Processing for real-time multimedia processing. The redesigned system offers precise, low-latency gesture recognition and immersive 3D interaction. By merging the creative spirit of the original Lightning with modern advancements, NeoLightning redefines gesture-based musical interaction, expanding possibilities for expressive performance and interactive sound design.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.3 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Hardware: 3.0 -->
                    
                <!-- Blockchain: 2.4 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4416
                </span>
                <a href="https://arxiv.org/abs/2505.10579" target="_blank" rel="noopener noreferrer">Bias and Generalizability of Foundation Models across Datasets in Breast Mammography</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Germani Elodie, Selin T\"urk Ilayda, Zeineddine Fatima, Mourad Charbel, Albarqouni Shadi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Over the past decades, computer-aided diagnosis tools for breast cancer have been developed to enhance screening procedures, yet their clinical adoption remains challenged by data variability and inherent biases. Although foundation models (FMs) have recently demonstrated impressive generalizability</span>
                
                <span class="abstract-full" style="display: none;">Over the past decades, computer-aided diagnosis tools for breast cancer have been developed to enhance screening procedures, yet their clinical adoption remains challenged by data variability and inherent biases. Although foundation models (FMs) have recently demonstrated impressive generalizability and transfer learning capabilities by leveraging vast and diverse datasets, their performance can be undermined by spurious correlations that arise from variations in image quality, labeling uncertainty, and sensitive patient attributes. In this work, we explore the fairness and bias of FMs for breast mammography classification by leveraging a large pool of datasets from diverse sources-including data from underrepresented regions and an in-house dataset. Our extensive experiments show that while modality-specific pre-training of FMs enhances performance, classifiers trained on features from individual datasets fail to generalize across domains. Aggregating datasets improves overall performance, yet does not fully mitigate biases, leading to significant disparities across under-represented subgroups such as extreme breast densities and age groups. Furthermore, while domain-adaptation strategies can reduce these disparities, they often incur a performance trade-off. In contrast, fairness-aware techniques yield more stable and equitable performance across subgroups. These findings underscore the necessity of incorporating rigorous fairness evaluations and mitigation strategies into FM-based models to foster inclusive and generalizable AI.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 11.7 -->
                    
                <!-- Medicine: 5.9 -->
                    
                <!-- Computer Vision: 2.6 -->
                    
                <!-- Decision Trees: 2.3 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4601
                </span>
                <a href="https://arxiv.org/abs/2505.10643" target="_blank" rel="noopener noreferrer">Artificial Intelligence Bias on English Language Learners in Automatic Scoring</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shuchen Guo, Yun Wang, Jichao Yu, Xuansheng Wu, Bilgehan Ayik, Field M. Watts, Ehsan Latif, Ninghao Liu, Lei Liu, Xiaoming Zhai
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study investigated potential scoring biases and disparities toward English Language Learners (ELLs) when using automatic scoring systems for middle school students' written responses to science assessments. We specifically focus on examining how unbalanced training data with ELLs contributes to</span>
                
                <span class="abstract-full" style="display: none;">This study investigated potential scoring biases and disparities toward English Language Learners (ELLs) when using automatic scoring systems for middle school students' written responses to science assessments. We specifically focus on examining how unbalanced training data with ELLs contributes to scoring bias and disparities. We fine-tuned BERT with four datasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting the real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced mixed dataset with equal representation of both groups. The study analyzed 21 assessment items: 10 items with about 30,000 ELL responses, five items with about 1,000 ELL responses, and six items with about 200 ELL responses. Scoring accuracy (Acc) was calculated and compared to identify bias using Friedman tests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and then calculated the differences in MSGs generated through both the human and AI models to identify the scoring disparities. We found that no AI bias and distorted disparities between ELLs and non-ELLs were found when the training dataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could exist if the sample size is limited (ELL = 200).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.6 -->
                    
                <!-- LLMs: 6.0 -->
                    
                <!-- Computer Vision: 2.6 -->
                    
                <!-- Decision Trees: 2.0 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Datasets: 1.8 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4607
                </span>
                <a href="https://arxiv.org/abs/2505.10799" target="_blank" rel="noopener noreferrer">Cell Library Characterization for Composite Current Source Models Based on Gaussian Process Regression and Active Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tao Bai, Junzhuo Zhou, Zeyuan Deng, Peng Cao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The composite current source (CCS) model has been adopted as an advanced timing model that represents the current behavior of cells for improved accuracy and better capability than traditional non-linear delay models (NLDM) to model complex dynamic effects and interactions under advanced process nod</span>
                
                <span class="abstract-full" style="display: none;">The composite current source (CCS) model has been adopted as an advanced timing model that represents the current behavior of cells for improved accuracy and better capability than traditional non-linear delay models (NLDM) to model complex dynamic effects and interactions under advanced process nodes. However, the high accuracy requirement, large amount of data and extensive simulation cost pose severe challenges to CCS characterization. To address these challenges, we introduce a novel Gaussian Process Regression(GPR) model with active learning(AL) to establish the characterization framework efficiently and accurately. Our approach significantly outperforms conventional commercial tools as well as learning based approaches by achieving an average absolute error of 2.05 ps and a relative error of 2.27% for current waveform of 57 cells under 9 process, voltage, temperature (PVT) corners with TSMC 22nm process. Additionally, our model drastically reduces the runtime to 27% and the storage by up to 19.5x compared with that required by commercial tools.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.6 -->
                    
                <!-- LLMs: 5.1 -->
                    
                <!-- Federated Learning: 3.1 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4737
                </span>
                <a href="https://arxiv.org/abs/2505.10919" target="_blank" rel="noopener noreferrer">A Physics-Informed Convolutional Long Short Term Memory Statistical Model for Fluid Thermodynamics Simulations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Luca Menicali, Andrew Grace, David H. Richter, Stefano Castruccio
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Fluid thermodynamics underpins atmospheric dynamics, climate science, industrial applications, and energy systems. However, direct numerical simulations (DNS) of such systems are computationally prohibitive. To address this, we present a novel physics-informed spatio-temporal surrogate model for Ray</span>
                
                <span class="abstract-full" style="display: none;">Fluid thermodynamics underpins atmospheric dynamics, climate science, industrial applications, and energy systems. However, direct numerical simulations (DNS) of such systems are computationally prohibitive. To address this, we present a novel physics-informed spatio-temporal surrogate model for Rayleigh-B\'enard convection (RBC), a canonical example of convective fluid flow. Our approach combines convolutional neural networks for spatial feature extraction with an innovative recurrent architecture inspired by large language models, comprising a context builder and a sequence generator to capture temporal dynamics. Inference is penalized with respect to the governing partial differential equations to ensure physical interpretability. Given the sensitivity of turbulent convection to initial conditions, we quantify uncertainty using a conformal prediction framework. This model replicates key features of RBC dynamics while significantly reducing computational cost, offering a scalable alternative to DNS for long-term simulations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.9 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Federated Learning: 3.0 -->
                    
                <!-- Evolutionary Algorithms: 2.9 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4897
                </span>
                <a href="https://arxiv.org/abs/2505.11465" target="_blank" rel="noopener noreferrer">The Dilemma Between Euphoria and Freedom in Recommendation Algorithms</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: James Brusseau (Department of Philosophy, Pace University, NYC, Department of Information Engineering, Computer Science, University of Trento, Italy)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Today's AI recommendation algorithms produce a human dilemma between euphoria and freedom. To elaborate, four ways that recommenders reshape experience are delineated. First, the human experience of convenience is tuned to euphoric perfection. Second, a kind of personal authenticity becomes capturab</span>
                
                <span class="abstract-full" style="display: none;">Today's AI recommendation algorithms produce a human dilemma between euphoria and freedom. To elaborate, four ways that recommenders reshape experience are delineated. First, the human experience of convenience is tuned to euphoric perfection. Second, a kind of personal authenticity becomes capturable with algorithms and data. Third, a conception of human freedom emerges, one that promotes unfamiliar interests for users instead of satisfying those that already exist. Finally, a new human dilemma is posed between two types of personal identity. On one side, there are recommendation algorithms that locate a user's core preferences, and then reinforce that identity with options designed to resemble those that have already proved satisfying. The result is an algorithmic production of euphoria and authenticity. On the other side, there are recommenders that provoke unfamiliar interests and curiosities. These proposals deny the existence of an authentic self and instead promote new preferences and experiences. The result is a human freedom of new personal identity.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.8 -->
                    
                <!-- Medicine: 5.7 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Datasets: 2.3 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4981
                </span>
                <a href="https://arxiv.org/abs/2505.11008" target="_blank" rel="noopener noreferrer">Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ye Kyaw Thu, Thazin Myint Oo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper explores syllable sequence prediction in Abugida languages using Transformer-based models, focusing on six languages: Bengali, Hindi, Khmer, Lao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We investigate the reconstruction of complete syllable sequences from variou</span>
                
                <span class="abstract-full" style="display: none;">This paper explores syllable sequence prediction in Abugida languages using Transformer-based models, focusing on six languages: Bengali, Hindi, Khmer, Lao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We investigate the reconstruction of complete syllable sequences from various incomplete input types, including consonant sequences, vowel sequences, partial syllables (with random character deletions), and masked syllables (with fixed syllable deletions). Our experiments reveal that consonant sequences play a critical role in accurate syllable prediction, achieving high BLEU scores, while vowel sequences present a significantly greater challenge. The model demonstrates robust performance across tasks, particularly in handling partial and masked syllable reconstruction, with strong results for tasks involving consonant information and syllable masking. This study advances the understanding of sequence prediction for Abugida languages and provides practical insights for applications such as text prediction, spelling correction, and data augmentation in these scripts.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.7 -->
                    
                <!-- Medicine: 6.3 -->
                    
                <!-- Blockchain: 2.4 -->
                    
                <!-- Hardware: 2.2 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4985
                </span>
                <a href="https://arxiv.org/abs/2505.11232" target="_blank" rel="noopener noreferrer">AW-GATCN: Adaptive Weighted Graph Attention Convolutional Network for Event Camera Data Joint Denoising and Object Recognition</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Haiyu Li, Charith Abhayaratne
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Event cameras, which capture brightness changes with high temporal resolution, inherently generate a significant amount of redundant and noisy data beyond essential object structures. The primary challenge in event-based object recognition lies in effectively removing this noise without losing criti</span>
                
                <span class="abstract-full" style="display: none;">Event cameras, which capture brightness changes with high temporal resolution, inherently generate a significant amount of redundant and noisy data beyond essential object structures. The primary challenge in event-based object recognition lies in effectively removing this noise without losing critical spatial-temporal information. To address this, we propose an Adaptive Graph-based Noisy Data Removal framework for Event-based Object Recognition. Specifically, our approach integrates adaptive event segmentation based on normalized density analysis, a multifactorial edge-weighting mechanism, and adaptive graph-based denoising strategies. These innovations significantly enhance the integration of spatiotemporal information, effectively filtering noise while preserving critical structural features for robust recognition. Experimental evaluations on four challenging datasets demonstrate that our method achieves superior recognition accuracies of 83.77%, 76.79%, 99.30%, and 96.89%, surpassing existing graph-based methods by up to 8.79%, and improving noise reduction performance by up to 19.57%, with an additional accuracy gain of 6.26% compared to traditional Euclidean-based techniques.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.8 -->
                    
                <!-- LLMs: 5.1 -->
                    
                <!-- Computer Vision: 3.6 -->
                    
                <!-- Federated Learning: 2.5 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5104
                </span>
                <a href="https://arxiv.org/abs/2505.11244" target="_blank" rel="noopener noreferrer">A Review of Tools and Techniques for Optimization of Workload Mapping and Scheduling in Heterogeneous HPC System</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aasish Kumar Sharma, Julian Kunkel
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents a systematic review of mapping and scheduling strategies within the High-Performance Computing (HPC) compute continuum, with a particular emphasis on heterogeneous systems. It introduces a prototype workflow to establish foundational concepts in workload characterization and reso</span>
                
                <span class="abstract-full" style="display: none;">This paper presents a systematic review of mapping and scheduling strategies within the High-Performance Computing (HPC) compute continuum, with a particular emphasis on heterogeneous systems. It introduces a prototype workflow to establish foundational concepts in workload characterization and resource allocation. Building on this, a thorough analysis of 66 selected research papers - spanning the period from 2017 to 2024 - is conducted, evaluating contemporary tools and techniques used for workload mapping and scheduling.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.0 -->
                    
                <!-- LLMs: 4.8 -->
                    
                <!-- Hardware: 2.6 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Blockchain: 2.3 -->
                    
                <!-- Datasets: 2.0 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5237
                </span>
                <a href="https://arxiv.org/abs/2505.10598" target="_blank" rel="noopener noreferrer">Enhancing Collaboration Through Google Workspace: Assessing and Strengthening Current Practices</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alexander Pahayahay
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study investigates the effectiveness of Google Workspace in fostering collaboration within academic settings, specifically at the University of Makati. The aim is to evaluate its role in enhancing blended learning practices and identify areas for improvement among faculty, staff, and students. </span>
                
                <span class="abstract-full" style="display: none;">This study investigates the effectiveness of Google Workspace in fostering collaboration within academic settings, specifically at the University of Makati. The aim is to evaluate its role in enhancing blended learning practices and identify areas for improvement among faculty, staff, and students. A survey was conducted with 50 participants, including academic staff, faculty, and students at the University of Makati who regularly use Google Workspace for academic and collaborative activities. Participants were selected through purposive sampling to ensure familiarity with the platform. The study employed a quantitative research design using structured surveys to assess user experiences with key features such as real-time document editing, communication tools, etc. The study found that Google Workspace and rated as "Very Effective" (mean score of 4.61) in promoting teamwork. Key advantages included improved collaboration, enhanced communication, and efficient management of group projects. However, several challenges were also noted, including low user adoption rates, limited Google Drive storage capacity, the need for better technical support, and limited offline functionality. Google Workspace significantly supports academic collaboration in the normal practices within the University of Makati, however, it faces challenges that impact its overall effectiveness. Addressing these issues could improve user experience and platform efficiency in educational contexts. It is recommended to enhance user adoption through targeted training and improve offline capabilities. Additionally, providing more advanced technical support could mitigate existing challenges.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.9 -->
                    
                <!-- LLMs: 5.8 -->
                    
                <!-- Hardware: 2.7 -->
                    
                <!-- Blockchain: 2.3 -->
                    
                <!-- Datasets: 1.8 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5514
                </span>
                <a href="https://arxiv.org/abs/2503.23108" target="_blank" rel="noopener noreferrer">SupertonicTTS: Towards Highly Scalable and Efficient Text-to-Speech System</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hyeongju Kim, Jinhyeok Yang, Yechan Yu, Seunghun Ji, Jacob Morton, Frederik Bous, Joon Byun, Juheon Lee
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a novel text-to-speech (TTS) system, namely SupertonicTTS, for improved scalability and efficiency in speech synthesis. SupertonicTTS comprises three components: a speech autoencoder for continuous latent representation, a text-to-latent module leveraging flow-matching for text-to-latent </span>
                
                <span class="abstract-full" style="display: none;">We present a novel text-to-speech (TTS) system, namely SupertonicTTS, for improved scalability and efficiency in speech synthesis. SupertonicTTS comprises three components: a speech autoencoder for continuous latent representation, a text-to-latent module leveraging flow-matching for text-to-latent mapping, and an utterance-level duration predictor. To enable a lightweight architecture, we employ a low-dimensional latent space, temporal compression of latents, and ConvNeXt blocks. We further simplify the TTS pipeline by operating directly on raw character-level text and employing cross-attention for text-speech alignment, thus eliminating the need for grapheme-to-phoneme (G2P) modules and external aligners. In addition, we introduce context-sharing batch expansion that accelerates loss convergence and stabilizes text-speech alignment. Experimental results demonstrate that SupertonicTTS achieves competitive performance while significantly reducing architectural complexity and computational overhead compared to contemporary TTS models. Audio samples demonstrating the capabilities of SupertonicTTS are available at: https://supertonictts.github.io/.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.2 -->
                    
                <!-- LLMs: 4.7 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5641
                </span>
                <a href="https://arxiv.org/abs/2505.10884" target="_blank" rel="noopener noreferrer">Estimating Deformable-Rigid Contact Interactions for a Deformable Tool via Learning and Model-Based Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mark Van der Merwe, Miquel Oller, Dmitry Berenson, Nima Fazeli
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Dexterous manipulation requires careful reasoning over extrinsic contacts. The prevalence of deforming tools in human environments, the use of deformable sensors, and the increasing number of soft robots yields a need for approaches that enable dexterous manipulation through contact reasoning where </span>
                
                <span class="abstract-full" style="display: none;">Dexterous manipulation requires careful reasoning over extrinsic contacts. The prevalence of deforming tools in human environments, the use of deformable sensors, and the increasing number of soft robots yields a need for approaches that enable dexterous manipulation through contact reasoning where not all contacts are well characterized by classical rigid body contact models. Here, we consider the case of a deforming tool dexterously manipulating a rigid object. We propose a hybrid learning and first-principles approach to the modeling of simultaneous motion and force transfer of tools and objects. The learned module is responsible for jointly estimating the rigid object's motion and the deformable tool's imparted contact forces. We then propose a Contact Quadratic Program to recover forces between the environment and object subject to quasi-static equilibrium and Coulomb friction. The results is a system capable of modeling both intrinsic and extrinsic motions, contacts, and forces during dexterous deformable manipulation. We train our method in simulation and show that our method outperforms baselines under varying block geometries and physical properties, during pushing and pivoting manipulations, and demonstrate transfer to real world interactions. Video results can be found at https://deform-rigid-contact.github.io/.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.1 -->
                    
                <!-- LLMs: 5.1 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5654
                </span>
                <a href="https://arxiv.org/abs/2505.10649" target="_blank" rel="noopener noreferrer">Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xianrui Li, Yufei Cui, Jun Li, Antoni B. Chan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Advances in medical imaging and deep learning have propelled progress in whole slide image (WSI) analysis, with multiple instance learning (MIL) showing promise for efficient and accurate diagnostics. However, conventional MIL models often lack adaptability to evolving datasets, as they rely on stat</span>
                
                <span class="abstract-full" style="display: none;">Advances in medical imaging and deep learning have propelled progress in whole slide image (WSI) analysis, with multiple instance learning (MIL) showing promise for efficient and accurate diagnostics. However, conventional MIL models often lack adaptability to evolving datasets, as they rely on static training that cannot incorporate new information without extensive retraining. Applying continual learning (CL) to MIL models is a possible solution, but often sees limited improvements. In this paper, we analyze CL in the context of attention MIL models and find that the model forgetting is mainly concentrated in the attention layers of the MIL model. Using the results of this analysis we propose two components for improving CL on MIL: Attention Knowledge Distillation (AKD) and the Pseudo-Bag Memory Pool (PMP). AKD mitigates catastrophic forgetting by focusing on retaining attention layer knowledge between learning sessions, while PMP reduces the memory footprint by selectively storing only the most informative patches, or ``pseudo-bags'' from WSIs. Experimental evaluations demonstrate that our method significantly improves both accuracy and memory efficiency on diverse WSI datasets, outperforming current state-of-the-art CL methods. This work provides a foundation for CL in large-scale, weakly annotated clinical datasets, paving the way for more adaptable and resilient diagnostic models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.8 -->
                    
                <!-- LLMs: 4.5 -->
                    
                <!-- Computer Vision: 3.1 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5794
                </span>
                <a href="https://arxiv.org/abs/2505.11474" target="_blank" rel="noopener noreferrer">REACT: Runtime-Enabled Active Collision-avoidance Technique for Autonomous Driving</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Heye Huang, Hao Cheng, Zhiyuan Zhou, Zijin Wang, Qichao Liu, Xiaopeng Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Achieving rapid and effective active collision avoidance in dynamic interactive traffic remains a core challenge for autonomous driving. This paper proposes REACT (Runtime-Enabled Active Collision-avoidance Technique), a closed-loop framework that integrates risk assessment with active avoidance con</span>
                
                <span class="abstract-full" style="display: none;">Achieving rapid and effective active collision avoidance in dynamic interactive traffic remains a core challenge for autonomous driving. This paper proposes REACT (Runtime-Enabled Active Collision-avoidance Technique), a closed-loop framework that integrates risk assessment with active avoidance control. By leveraging energy transfer principles and human-vehicle-road interaction modeling, REACT dynamically quantifies runtime risk and constructs a continuous spatial risk field. The system incorporates physically grounded safety constraints such as directional risk and traffic rules to identify high-risk zones and generate feasible, interpretable avoidance behaviors. A hierarchical warning trigger strategy and lightweight system design enhance runtime efficiency while ensuring real-time responsiveness. Evaluations across four representative high-risk scenarios including car-following braking, cut-in, rear-approaching, and intersection conflict demonstrate REACT's capability to accurately identify critical risks and execute proactive avoidance. Its risk estimation aligns closely with human driver cognition (i.e., warning lead time < 0.4 s), achieving 100% safe avoidance with zero false alarms or missed detections. Furthermore, it exhibits superior real-time performance (< 50 ms latency), strong foresight, and generalization. The lightweight architecture achieves state-of-the-art accuracy, highlighting its potential for real-time deployment in safety-critical autonomous systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.1 -->
                    
                <!-- LLMs: 7.0 -->
                    
                <!-- Computer Vision: 3.6 -->
                    
                <!-- Hardware: 2.8 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- HPO and AutoML: 2.2 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5849
                </span>
                <a href="https://arxiv.org/abs/2505.11160" target="_blank" rel="noopener noreferrer">Protecting Young Users on Social Media: Evaluating the Effectiveness of Content Moderation and Legal Safeguards on Video Sharing Platforms</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fatmaelzahraa Eltaher, Rahul Krishna Gajula, Luis Miralles-Pechu\'an, Patrick Crotty, Juan Mart\'inez-Otero, Christina Thorpe, Susan McKeever
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Video-sharing social media platforms, such as TikTok, YouTube, and Instagram, implement content moderation policies aimed at reducing exposure to harmful videos among minor users. As video has become the dominant and most immersive form of online content, understanding how effectively this medium is</span>
                
                <span class="abstract-full" style="display: none;">Video-sharing social media platforms, such as TikTok, YouTube, and Instagram, implement content moderation policies aimed at reducing exposure to harmful videos among minor users. As video has become the dominant and most immersive form of online content, understanding how effectively this medium is moderated for younger audiences is urgent. In this study, we evaluated the effectiveness of video moderation for different age groups on three of the main video-sharing platforms: TikTok, YouTube, and Instagram. We created experimental accounts for the children assigned ages 13 and 18. Using these accounts, we evaluated 3,000 videos served up by the social media platforms, in passive scrolling and search modes, recording the frequency and speed at which harmful videos were encountered. Each video was manually assessed for level and type of harm, using definitions from a unified framework of harmful content.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.8 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Hardware: 2.8 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Blockchain: 2.4 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Datasets: 1.6 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5895
                </span>
                <a href="https://arxiv.org/abs/2505.11026" target="_blank" rel="noopener noreferrer">StRuCom: A Novel Dataset of Structured Code Comments in Russian</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Maria Dziuba, Valentin Malykh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Structured code comments in docstring format are essential for code comprehension and maintenance, but existing machine learning models for their generation perform poorly for Russian compared to English. To bridge this gap, we present StRuCom - the first large-scale dataset (153K examples) specific</span>
                
                <span class="abstract-full" style="display: none;">Structured code comments in docstring format are essential for code comprehension and maintenance, but existing machine learning models for their generation perform poorly for Russian compared to English. To bridge this gap, we present StRuCom - the first large-scale dataset (153K examples) specifically designed for Russian code documentation. Unlike machine-translated English datasets that distort terminology (e.g., technical loanwords vs. literal translations) and docstring structures, StRuCom combines human-written comments from Russian GitHub repositories with synthetically generated ones, ensuring compliance with Python, Java, JavaScript, C#, and Go standards through automated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom shows statistically significant improvements of chrf++ and BERTScore over baseline models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.2 -->
                    
                <!-- LLMs: 6.6 -->
                    
                <!-- Datasets: 4.2 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6097
                </span>
                <a href="https://arxiv.org/abs/2505.10589" target="_blank" rel="noopener noreferrer">Super-Resolution Generative Adversarial Networks based Video Enhancement</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ka\u{g}an \c{C}ET\.IN
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study introduces an enhanced approach to video super-resolution by extending ordinary Single-Image Super-Resolution (SISR) Super-Resolution Generative Adversarial Network (SRGAN) structure to handle spatio-temporal data. While SRGAN has proven effective for single-image enhancement, its design </span>
                
                <span class="abstract-full" style="display: none;">This study introduces an enhanced approach to video super-resolution by extending ordinary Single-Image Super-Resolution (SISR) Super-Resolution Generative Adversarial Network (SRGAN) structure to handle spatio-temporal data. While SRGAN has proven effective for single-image enhancement, its design does not account for the temporal continuity required in video processing. To address this, a modified framework that incorporates 3D Non-Local Blocks is proposed, which is enabling the model to capture relationships across both spatial and temporal dimensions. An experimental training pipeline is developed, based on patch-wise learning and advanced data degradation techniques, to simulate real-world video conditions and learn from both local and global structures and details. This helps the model generalize better and maintain stability across varying video content while maintaining the general structure besides the pixel-wise correctness. Two model variants-one larger and one more lightweight-are presented to explore the trade-offs between performance and efficiency. The results demonstrate improved temporal coherence, sharper textures, and fewer visual artifacts compared to traditional single-image methods. This work contributes to the development of practical, learning-based solutions for video enhancement tasks, with potential applications in streaming, gaming, and digital restoration.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.4 -->
                    
                <!-- Computer Vision: 3.9 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- Decision Trees: 1.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6468
                </span>
                <a href="https://arxiv.org/abs/2412.09758" target="_blank" rel="noopener noreferrer">Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yunfei Luo, Yuliang Chen, Asif Salekin, Tauhidur Rahman
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Time-series foundation models excel at tasks like forecasting across diverse data types by leveraging informative waveform representations. Wearable sensing data, however, pose unique challenges due to their variability in patterns and frequency bands, especially for healthcare-related outcomes. The</span>
                
                <span class="abstract-full" style="display: none;">Time-series foundation models excel at tasks like forecasting across diverse data types by leveraging informative waveform representations. Wearable sensing data, however, pose unique challenges due to their variability in patterns and frequency bands, especially for healthcare-related outcomes. The main obstacle lies in crafting generalizable representations that adapt efficiently across heterogeneous sensing configurations and applications. To address this, we propose NormWear, the first multi-modal and ubiquitous foundation model designed to extract generalized and informative representations from wearable sensing data. Specifically, we design a channel-aware attention mechanism with a shared special liaison [CLS] token to detect signal patterns in both intra-sensor and inter-sensors. This helps the model to extract more meaningful information considering both time series themselves and the relationships between input sensors. This helps the model to be widely compatible with various sensors settings. NormWear is pretrained on a diverse set of physiological signals, including PPG, ECG, EEG, GSR, and IMU, from various public datasets. Our model shows exceptional generalizability across 11 public wearable sensing datasets, spanning 18 applications in mental health, body state inference, vital sign estimation, and disease risk evaluation. It consistently outperforms competitive baselines under zero-shot, partial-shot, and full-shot settings, indicating broad applicability in real-world health applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.6 -->
                    
                <!-- Medicine: 7.4 -->
                    
                <!-- Computer Vision: 3.4 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6646
                </span>
                <a href="https://arxiv.org/abs/2505.10937" target="_blank" rel="noopener noreferrer">Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The emergence of large reasoning models (LRMs) has transformed Natural Language Processing by excelling in complex tasks such as mathematical problem-solving and code generation. These models leverage chain-of-thought (CoT) processes, enabling them to emulate human-like reasoning strategies. However</span>
                
                <span class="abstract-full" style="display: none;">The emergence of large reasoning models (LRMs) has transformed Natural Language Processing by excelling in complex tasks such as mathematical problem-solving and code generation. These models leverage chain-of-thought (CoT) processes, enabling them to emulate human-like reasoning strategies. However, the advancement of LRMs is hindered by the lack of comprehensive CoT datasets. Current resources often fail to provide extensive reasoning problems with coherent CoT processes distilled from multiple teacher models and do not account for multifaceted properties describing the internal characteristics of CoTs. To address these challenges, we introduce OmniThought, a large-scale dataset featuring 2 million CoT processes generated and validated by two powerful LRMs as teacher models. Each CoT process in OmniThought is annotated with novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which describe the appropriateness of CoT verbosity and cognitive difficulty level for models to comprehend these reasoning processes. We further establish a self-reliant pipeline to curate this dataset. Extensive experiments using Qwen2.5 models of various sizes demonstrate the positive impact of our proposed scores on LRM training effectiveness. Based on the proposed OmniThought dataset, we further train and release a series of high-performing LRMs, specifically equipped with stronger reasoning abilities and optimal CoT output length and difficulty level. Our contributions significantly enhance the development and training of LRMs for solving complex tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.0 -->
                    
                <!-- Medicine: 5.3 -->
                    
                <!-- Datasets: 2.6 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.677
                </span>
                <a href="https://arxiv.org/abs/2505.09160" target="_blank" rel="noopener noreferrer">A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Berkay Guler, Giovanni Geraci, Hamid Jafarkhani
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Current applications of self-supervised learning to wireless channel representation often borrow paradigms developed for text and image processing, without fully addressing the unique characteristics and constraints of wireless communications. Aiming to fill this gap, we first propose WiMAE (Wireles</span>
                
                <span class="abstract-full" style="display: none;">Current applications of self-supervised learning to wireless channel representation often borrow paradigms developed for text and image processing, without fully addressing the unique characteristics and constraints of wireless communications. Aiming to fill this gap, we first propose WiMAE (Wireless Masked Autoencoder), a transformer-based encoder-decoder foundation model pretrained on a realistic open-source multi-antenna wireless channel dataset. Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by incorporating a contrastive learning objective alongside the reconstruction task in a unified multi-task framework. By warm-starting from pretrained WiMAE weights and generating positive pairs via noise injection, the contrastive component enables the model to capture both structural and discriminative features, enhancing representation quality beyond what reconstruction alone can achieve. Through extensive evaluation on unseen scenarios, we demonstrate the effectiveness of both approaches across multiple downstream tasks, with ContraWiMAE showing further improvements in linear separability and adaptability in diverse wireless environments. Comparative evaluations against a state-of-the-art wireless channel foundation model confirm the superior performance and data efficiency of our models, highlighting their potential as powerful baselines for future research in self-supervised wireless channel representation learning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.3 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Computer Vision: 2.8 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.678
                </span>
                <a href="https://arxiv.org/abs/2502.06830" target="_blank" rel="noopener noreferrer">OrderFusion: Encoding Orderbook for End-to-End Probabilistic Intraday Electricity Price Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Runyao Yu, Yuchen Tao, Fabian Leimgruber, Tara Esterl, Jochen L. Cremer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate and reliable probabilistic prediction of intraday electricity prices is essential to manage market uncertainties and support robust trading strategies. However, current methods rely heavily on domain feature extraction and fail to capture the dynamics between buy and sell orders, limiting t</span>
                
                <span class="abstract-full" style="display: none;">Accurate and reliable probabilistic prediction of intraday electricity prices is essential to manage market uncertainties and support robust trading strategies. However, current methods rely heavily on domain feature extraction and fail to capture the dynamics between buy and sell orders, limiting the ability to form rich representations of the orderbook. Furthermore, these methods often require training separate models for different quantiles and introduce additional procedures-such as post-hoc quantile sorting or loss-based penalties-to address the quantile crossing issue, where predicted upper quantiles fall below lower ones. These steps are either decoupled from model training or introduce extra tuning complexity. To address these challenges, we propose an encoding method called OrderFusion and design a hierarchical multi-quantile head. OrderFusion encodes the orderbook into a 2.5D representation and employs a tailored jump cross-attention to model buy-sell dynamics without the need for domain feature extraction. The multi-quantile head anchors on the median quantile and hierarchically estimates other quantiles through constrained residuals, ensuring monotonicity without post-processing or additional tuning. We conduct extensive experiments and ablation studies on three key price indices (ID1, ID2, and ID3) using three years of orderbook data from the German and Austrian markets. The results demonstrate that our approach provides an accurate, reliable, and unified end-to-end framework for probabilistic intraday price prediction.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.3 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Computer Vision: 2.7 -->
                    
                <!-- Hardware: 2.3 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6879
                </span>
                <a href="https://arxiv.org/abs/2505.10586" target="_blank" rel="noopener noreferrer">Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Poli A. Nemkova, Suleyman O. Polat, Rafid I. Jahan, Sagnik Ray Choudhury, Sun-joo Lee, Shouryadipta Sarkar, Mark V. Albert
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Timely and accurate situation awareness is vital for decision-making in humanitarian response, conflict monitoring, and early warning and early action. However, the manual analysis of vast and heterogeneous data sources often results in delays, limiting the effectiveness of interventions. This paper</span>
                
                <span class="abstract-full" style="display: none;">Timely and accurate situation awareness is vital for decision-making in humanitarian response, conflict monitoring, and early warning and early action. However, the manual analysis of vast and heterogeneous data sources often results in delays, limiting the effectiveness of interventions. This paper introduces a dynamic Retrieval-Augmented Generation (RAG) system that autonomously generates situation awareness reports by integrating real-time data from diverse sources, including news articles, conflict event databases, and economic indicators. Our system constructs query-specific knowledge bases on demand, ensuring timely, relevant, and accurate insights.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.1 -->
                    
                <!-- LLMs: 6.8 -->
                    
                <!-- Blockchain: 2.7 -->
                    
                <!-- Hardware: 2.2 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- Datasets: 2.0 -->
                    
                <!-- HPO and AutoML: 1.8 -->
                    
                <!-- Decision Trees: 1.8 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7447
                </span>
                <a href="https://arxiv.org/abs/2408.11518" target="_blank" rel="noopener noreferrer">EmoFace: Emotion-Content Disentangled Speech-Driven 3D Talking Face Animation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yihong Lin, Liang Peng, Zhaoxin Fan, Xianjia Wu, Jianqiao Hu, Xiandong Li, Wenxiong Kang, Songju Lei
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The creation of increasingly vivid 3D talking face has become a hot topic in recent years. Currently, most speech-driven works focus on lip synchronisation but neglect to effectively capture the correlations between emotions and facial motions. To address this problem, we propose a two-stream networ</span>
                
                <span class="abstract-full" style="display: none;">The creation of increasingly vivid 3D talking face has become a hot topic in recent years. Currently, most speech-driven works focus on lip synchronisation but neglect to effectively capture the correlations between emotions and facial motions. To address this problem, we propose a two-stream network called EmoFace, which consists of an emotion branch and a content branch. EmoFace employs a novel Mesh Attention mechanism to analyse and fuse the emotion features and content features. Particularly, a newly designed spatio-temporal graph-based convolution, SpiralConv3D, is used in Mesh Attention to learn potential temporal and spatial feature dependencies between mesh vertices. In addition, to the best of our knowledge, it is the first time to introduce a new self-growing training scheme with intermediate supervision to dynamically adjust the ratio of groundtruth adopted in the 3D face animation task. Comprehensive quantitative and qualitative evaluations on our high-quality 3D emotional facial animation dataset, 3D-RAVDESS ($4.8863\times 10^{-5}$mm for LVE and $0.9509\times 10^{-5}$mm for EVE), together with the public dataset VOCASET ($2.8669\times 10^{-5}$mm for LVE and $0.4664\times 10^{-5}$mm for EVE), demonstrate that our approach achieves state-of-the-art performance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.8 -->
                    
                <!-- LLMs: 2.7 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7447
                </span>
                <a href="https://arxiv.org/abs/2505.11314" target="_blank" rel="noopener noreferrer">CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christoph Leiter, Yuki M. Asano, Margret Keuper, Steffen Eger
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The assessment of evaluation metrics (meta-evaluation) is crucial for determining the suitability of existing metrics in text-to-image (T2I) generation tasks. Human-based meta-evaluation is costly and time-intensive, and automated alternatives are scarce. We address this gap and propose CROC: a scal</span>
                
                <span class="abstract-full" style="display: none;">The assessment of evaluation metrics (meta-evaluation) is crucial for determining the suitability of existing metrics in text-to-image (T2I) generation tasks. Human-based meta-evaluation is costly and time-intensive, and automated alternatives are scarce. We address this gap and propose CROC: a scalable framework for automated Contrastive Robustness Checks that systematically probes and quantifies metric robustness by synthesizing contrastive test cases across a comprehensive taxonomy of image properties. With CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one million contrastive prompt-image pairs to enable a fine-grained comparison of evaluation metrics. We also use the dataset to train CROCScore, a new metric that achieves state-of-the-art performance among open-source methods, demonstrating an additional key application of our framework. To complement this dataset, we introduce a human-supervised benchmark (CROC$^{hum}$) targeting especially challenging categories. Our results highlight robustness issues in existing metrics: for example, many fail on prompts involving negation, and all tested open-source metrics fail on at least 25% of cases involving correct identification of body parts.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.5 -->
                    
                <!-- Medicine: 6.2 -->
                    
                <!-- Datasets: 3.0 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7517
                </span>
                <a href="https://arxiv.org/abs/2504.06232" target="_blank" rel="noopener noreferrer">HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jiazi Bu, Pengyang Ling, Yujie Zhou, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. Recent appro</span>
                
                <span class="abstract-full" style="display: none;">Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. Recent approaches have investigated training-free strategies to enable high-resolution image synthesis with pre-trained models. However, these techniques often struggle with generating high-quality visuals and tend to exhibit artifacts or low-fidelity details, as they typically rely solely on the endpoint of the low-resolution sampling trajectory while neglecting intermediate states that are critical for preserving structure and synthesizing finer detail. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging such flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's capability in achieving superior high-resolution image quality over state-of-the-art methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.4 -->
                    
                <!-- LLMs: 4.5 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Hardware: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.762
                </span>
                <a href="https://arxiv.org/abs/2505.11282" target="_blank" rel="noopener noreferrer">MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shrutarv Awasthi, Anas Gouda, Sven Franke, J\'er\^ome Rutinowski, Frank Hoffmann, Moritz Roidl
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Mobile robots are reaching unprecedented speeds, with platforms like Unitree B2, and Fraunhofer O3dyn achieving maximum speeds between 5 and 10 m/s. However, effectively utilizing such speeds remains a challenge due to the limitations of RGB cameras, which suffer from motion blur and fail to provide</span>
                
                <span class="abstract-full" style="display: none;">Mobile robots are reaching unprecedented speeds, with platforms like Unitree B2, and Fraunhofer O3dyn achieving maximum speeds between 5 and 10 m/s. However, effectively utilizing such speeds remains a challenge due to the limitations of RGB cameras, which suffer from motion blur and fail to provide real-time responsiveness. Event cameras, with their asynchronous operation, and low-latency sensing, offer a promising alternative for high-speed robotic perception. In this work, we introduce MTevent, a dataset designed for 6D pose estimation and moving object detection in highly dynamic environments with large detection distances. Our setup consists of a stereo-event camera and an RGB camera, capturing 75 scenes, each on average 16 seconds, and featuring 16 unique objects under challenging conditions such as extreme viewing angles, varying lighting, and occlusions. MTevent is the first dataset to combine high-speed motion, long-range perception, and real-world object interactions, making it a valuable resource for advancing event-based vision in robotics. To establish a baseline, we evaluate the task of 6D pose estimation using NVIDIA's FoundationPose on RGB images, achieving an Average Recall of 0.22 with ground-truth masks, highlighting the limitations of RGB-based approaches in such dynamic settings. With MTevent, we provide a novel resource to improve perception models and foster further research in high-speed robotic vision. The dataset is available for download https://huggingface.co/datasets/anas-gouda/MTevent</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.6 -->
                    
                <!-- Computer Vision: 4.1 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Datasets: 2.8 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8165
                </span>
                <a href="https://arxiv.org/abs/2505.10923" target="_blank" rel="noopener noreferrer">GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian Splats</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Simeon Adebola, Shuangyu Xie, Chung Min Kim, Justin Kerr, Bart M. van Marrewijk, Mieke van Vlaardingen, Tim van Daalen, Robert van Loo, Jose Luis Susa Rincon, Eugen Solowjow, Rick van de Zedde, Ken Goldberg
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate temporal reconstructions of plant growth are essential for plant phenotyping and breeding, yet remain challenging due to complex geometries, occlusions, and non-rigid deformations of plants. We present a novel framework for building temporal digital twins of plants by combining 3D Gaussian </span>
                
                <span class="abstract-full" style="display: none;">Accurate temporal reconstructions of plant growth are essential for plant phenotyping and breeding, yet remain challenging due to complex geometries, occlusions, and non-rigid deformations of plants. We present a novel framework for building temporal digital twins of plants by combining 3D Gaussian Splatting with a robust sample alignment pipeline. Our method begins by reconstructing Gaussian Splats from multi-view camera data, then leverages a two-stage registration approach: coarse alignment through feature-based matching and Fast Global Registration, followed by fine alignment with Iterative Closest Point. This pipeline yields a consistent 4D model of plant development in discrete time steps. We evaluate the approach on data from the Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at https://berkeleyautomation.github.io/GrowSplat/</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.3 -->
                    
                <!-- LLMs: 5.6 -->
                    
                <!-- 3D: 2.7 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Datasets: 1.8 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8211
                </span>
                <a href="https://arxiv.org/abs/2505.11394" target="_blank" rel="noopener noreferrer">From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alexander Oberstrass, Esteban Vaca, Eric Upschulte, Meiqi Niu, Nicola Palomero-Gallagher, David Graessel, Christian Schiffer, Markus Axer, Katrin Amunts, Timo Dickscheid
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Comprehensive assessment of the various aspects of the brain's microstructure requires the use of complementary imaging techniques. This includes measuring the spatial distribution of cell bodies (cytoarchitecture) and nerve fibers (myeloarchitecture). The gold standard for cytoarchitectonic analysi</span>
                
                <span class="abstract-full" style="display: none;">Comprehensive assessment of the various aspects of the brain's microstructure requires the use of complementary imaging techniques. This includes measuring the spatial distribution of cell bodies (cytoarchitecture) and nerve fibers (myeloarchitecture). The gold standard for cytoarchitectonic analysis is light microscopic imaging of cell-body stained tissue sections. To reveal the 3D orientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been introduced as a reliable technique providing a resolution in the micrometer range while allowing processing of series of complete brain sections. 3D-PLI acquisition is label-free and allows subsequent staining of sections after measurement. By post-staining for cell bodies, a direct link between fiber- and cytoarchitecture can potentially be established within the same section. However, inevitable distortions introduced during the staining process make a nonlinear and cross-modal registration necessary in order to study the detailed relationships between cells and fibers in the images. In addition, the complexity of processing histological sections for post-staining only allows for a limited number of samples. In this work, we take advantage of deep learning methods for image-to-image translation to generate a virtual staining of 3D-PLI that is spatially aligned at the cellular level. In a supervised setting, we build on a unique dataset of brain sections, to which Cresyl violet staining has been applied after 3D-PLI measurement. To ensure high correspondence between both modalities, we address the misalignment of training data using Fourier-based registration methods. In this way, registration can be efficiently calculated during training for local image patches of target and predicted staining. We demonstrate that the proposed method enables prediction of a Cresyl violet staining from 3D-PLI, matching individual cell instances.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.3 -->
                    
                <!-- Federated Learning: 4.1 -->
                    
                <!-- Evolutionary Algorithms: 2.7 -->
                    
                <!-- Bayesian Optimization: 2.5 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- LLMs: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8283
                </span>
                <a href="https://arxiv.org/abs/2505.10975" target="_blank" rel="noopener noreferrer">Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xinlu He, Jacob Whitehill
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Monaural multi-speaker automatic speech recognition (ASR) remains challenging due to data scarcity and the intrinsic difficulty of recognizing and attributing words to individual speakers, particularly in overlapping speech. Recent advances have driven the shift from cascade systems to end-to-end (E</span>
                
                <span class="abstract-full" style="display: none;">Monaural multi-speaker automatic speech recognition (ASR) remains challenging due to data scarcity and the intrinsic difficulty of recognizing and attributing words to individual speakers, particularly in overlapping speech. Recent advances have driven the shift from cascade systems to end-to-end (E2E) architectures, which reduce error propagation and better exploit the synergy between speech content and speaker identity. Despite rapid progress in E2E multi-speaker ASR, the field lacks a comprehensive review of recent developments. This survey provides a systematic taxonomy of E2E neural approaches for multi-speaker ASR, highlighting recent advances and comparative analysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO) for pre-segmented audio, analyzing their distinct characteristics and trade-offs; (2) recent architectural and algorithmic improvements based on these two paradigms; (3) extensions to long-form speech, including segmentation strategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate and compare methods across standard benchmarks. We conclude with a discussion of open challenges and future research directions towards building robust and scalable multi-speaker ASR.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.1 -->
                    
                <!-- LLMs: 5.4 -->
                    
                <!-- Hardware: 2.5 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8422
                </span>
                <a href="https://arxiv.org/abs/2408.15852" target="_blank" rel="noopener noreferrer">chemtrain: Learning Deep Potential Models via Automatic Differentiation and Statistical Physics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Paul Fuchs, Stephan Thaler, Sebastien R\"ocken, Julija Zavadlav
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Neural Networks (NNs) are effective models for refining the accuracy of molecular dynamics, opening up new fields of application. Typically trained bottom-up, atomistic NN potential models can reach first-principle accuracy, while coarse-grained implicit solvent NN potentials surpass classical conti</span>
                
                <span class="abstract-full" style="display: none;">Neural Networks (NNs) are effective models for refining the accuracy of molecular dynamics, opening up new fields of application. Typically trained bottom-up, atomistic NN potential models can reach first-principle accuracy, while coarse-grained implicit solvent NN potentials surpass classical continuum solvent models. However, overcoming the limitations of costly generation of accurate reference data and data inefficiency of common bottom-up training demands efficient incorporation of data from many sources. This paper introduces the framework chemtrain to learn sophisticated NN potential models through customizable training routines and advanced training algorithms. These routines can combine multiple top-down and bottom-up algorithms, e.g., to incorporate both experimental and simulation data or pre-train potentials with less costly algorithms. chemtrain provides an object-oriented high-level interface to simplify the creation of custom routines. On the lower level, chemtrain relies on JAX to compute gradients and scale the computations to use available resources. We demonstrate the simplicity and importance of combining multiple algorithms in the examples of parametrizing an all-atomistic model of titanium and a coarse-grained implicit solvent model of alanine dipeptide.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.8 -->
                    
                <!-- LLMs: 6.5 -->
                    
                <!-- Federated Learning: 3.7 -->
                    
                <!-- Hardware: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Computer Vision: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8651
                </span>
                <a href="https://arxiv.org/abs/2502.00262" target="_blank" rel="noopener noreferrer">INSIGHT: Enhancing Autonomous Driving Safety through Vision-Language Models on Context-Aware Hazard Detection and Edge Case Evaluation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dianwei Chen, Zifan Zhang, Yuchen Liu, Xianfeng Terry Yang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Autonomous driving systems face significant challenges in handling unpredictable edge-case scenarios, such as adversarial pedestrian movements, dangerous vehicle maneuvers, and sudden environmental changes. Current end-to-end driving models struggle with generalization to these rare events due to li</span>
                
                <span class="abstract-full" style="display: none;">Autonomous driving systems face significant challenges in handling unpredictable edge-case scenarios, such as adversarial pedestrian movements, dangerous vehicle maneuvers, and sudden environmental changes. Current end-to-end driving models struggle with generalization to these rare events due to limitations in traditional detection and prediction approaches. To address this, we propose INSIGHT (Integration of Semantic and Visual Inputs for Generalized Hazard Tracking), a hierarchical vision-language model (VLM) framework designed to enhance hazard detection and edge-case evaluation. By using multimodal data fusion, our approach integrates semantic and visual representations, enabling precise interpretation of driving scenarios and accurate forecasting of potential dangers. Through supervised fine-tuning of VLMs, we optimize spatial hazard localization using attention-based mechanisms and coordinate regression techniques. Experimental results on the BDD100K dataset demonstrate a substantial improvement in hazard prediction straightforwardness and accuracy over existing models, achieving a notable increase in generalization performance. This advancement enhances the robustness and safety of autonomous driving systems, ensuring improved situational awareness and potential decision-making in complex real-world scenarios.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.0 -->
                    
                <!-- Medicine: 7.8 -->
                    
                <!-- Computer Vision: 3.7 -->
                    
                <!-- Datasets: 2.3 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8679
                </span>
                <a href="https://arxiv.org/abs/2505.11424" target="_blank" rel="noopener noreferrer">Improving Object Detection Performance through YOLOv8: A Comprehensive Training and Evaluation Study</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rana Poureskandar, Shiva Razzagzadeh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study evaluated the performance of a YOLOv8-based segmentation model for detecting and segmenting wrinkles in facial images.</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.9 -->
                    
                <!-- LLMs: 6.0 -->
                    
                <!-- Computer Vision: 3.5 -->
                    
                <!-- Blockchain: 2.5 -->
                    
                <!-- Hardware: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9938
                </span>
                <a href="https://arxiv.org/abs/2505.11344" target="_blank" rel="noopener noreferrer">Dynamic Base model Shift for Delta Compression</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chenyu Huang, Peng Ye, Shenghe Zheng, Xiaohui Wang, Lei Bai, Tao Chen, Wanli Ouyang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Transformer-based models with the pretrain-finetune paradigm bring about significant progress, along with the heavy storage and deployment costs of finetuned models on multiple tasks. Delta compression attempts to lower the costs by reducing the redundancy of delta parameters (i.e., the difference b</span>
                
                <span class="abstract-full" style="display: none;">Transformer-based models with the pretrain-finetune paradigm bring about significant progress, along with the heavy storage and deployment costs of finetuned models on multiple tasks. Delta compression attempts to lower the costs by reducing the redundancy of delta parameters (i.e., the difference between the finetuned and pre-trained model weights) through pruning or quantization. However, existing methods by default employ the pretrained model as the base model and compress the delta parameters for every task, which may causes significant performance degradation, especially when the compression rate is extremely high. To tackle this issue, we investigate the impact of different base models on the performance of delta compression and find that the pre-trained base model can hardly be optimal. To this end, we propose Dynamic Base Model Shift (DBMS), which dynamically adapts the base model to the target task before performing delta compression. Specifically, we adjust two parameters, which respectively determine the magnitude of the base model shift and the overall scale of delta compression, to boost the compression performance on each task. Through low-cost learning of these two parameters, our DBMS can maintain most of the finetuned model's performance even under an extremely high compression ratio setting, significantly surpassing existing methods. Moreover, our DBMS is orthogonal and can be integrated with a variety of other methods, and it has been evaluated across different types of models including language, vision transformer, and multi-modal models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Federated Learning: 6.7 -->
                    
                <!-- Medicine: 6.4 -->
                    
                <!-- Computer Vision: 3.9 -->
                    
                <!-- Hardware: 2.3 -->
                    
                <!-- LLMs: 2.2 -->
                    
                <!-- Bayesian Optimization: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.0325
                </span>
                <a href="https://arxiv.org/abs/2505.10600" target="_blank" rel="noopener noreferrer">Enhancing IoT Cyber Attack Detection in the Presence of Highly Imbalanced Data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Md. Ehsanul Haque, Md. Saymon Hosen Polash, Md Al-Imran Sanjida Simla, Md Alomgir Hossain, Sarwar Jahan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Due to the rapid growth in the number of Internet of Things (IoT) networks, the cyber risk has increased exponentially, and therefore, we have to develop effective IDS that can work well with highly imbalanced datasets. A high rate of missed threats can be the result, as traditional machine learning</span>
                
                <span class="abstract-full" style="display: none;">Due to the rapid growth in the number of Internet of Things (IoT) networks, the cyber risk has increased exponentially, and therefore, we have to develop effective IDS that can work well with highly imbalanced datasets. A high rate of missed threats can be the result, as traditional machine learning models tend to struggle in identifying attacks when normal data volume is much higher than the volume of attacks. For example, the dataset used in this study reveals a strong class imbalance with 94,659 instances of the majority class and only 28 instances of the minority class, making it quite challenging to determine rare attacks accurately. The challenges presented in this research are addressed by hybrid sampling techniques designed to improve data imbalance detection accuracy in IoT domains. After applying these techniques, we evaluate the performance of several machine learning models such as Random Forest, Soft Voting, Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), Multi-Layer Perceptron (MLP), and Logistic Regression with respect to the classification of cyber-attacks. The obtained results indicate that the Random Forest model achieved the best performance with a Kappa score of 0.9903, test accuracy of 0.9961, and AUC of 0.9994. Strong performance is also shown by the Soft Voting model, with an accuracy of 0.9952 and AUC of 0.9997, indicating the benefits of combining model predictions. Overall, this work demonstrates the value of hybrid sampling combined with robust model and feature selection for significantly improving IoT security against cyber-attacks, especially in highly imbalanced data environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.5 -->
                    
                <!-- Federated Learning: 4.9 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Bayesian Optimization: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Computer Vision: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.0783
                </span>
                <a href="https://arxiv.org/abs/2411.07019" target="_blank" rel="noopener noreferrer">UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhiqiang Liu, Yin Hua, Mingyang Chen, Zhuo Chen, Ziqi Liu, Lei Liang, Huajun Chen, Wen Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Beyond-triple fact representations including hyper-relational facts with auxiliary key-value pairs, temporal facts with additional timestamps, and nested facts implying relationships between facts, are gaining significant attention. However, constrained by complex fact representation forms, existing</span>
                
                <span class="abstract-full" style="display: none;">Beyond-triple fact representations including hyper-relational facts with auxiliary key-value pairs, temporal facts with additional timestamps, and nested facts implying relationships between facts, are gaining significant attention. However, constrained by complex fact representation forms, existing link prediction models for beyond-triple facts have difficulty achieving hierarchical fact modeling and generalizing the modules for one specific facts to other fact types. To overcome this limitation, we propose a Unified Hierarchical Representation learning framework (UniHR) for unified knowledge graph link prediction. It consists of a unified Hierarchical Data Representation (HiDR) module and a unified Hierarchical Structure Learning (HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested factual KGs into triple-based representations. Then HiSL incorporates intra-fact and inter-fact message passing, focusing on enhancing the semantic information within individual facts and enriching the structural information between facts. Empirical results demonstrate the effectiveness of UniHR and highlight the strong potential of unified representations. Code and data are available at https://github.com/Lza12a/UniHR.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.0 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- GNN: 3.0 -->
                    
                <!-- Hardware: 3.0 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1105
                </span>
                <a href="https://arxiv.org/abs/2505.11413" target="_blank" rel="noopener noreferrer">CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sijia Chen, Xiaomin Li, Mengxue Zhang, Eric Hanchen Jiang, Qingcheng Zeng, Chen-Hsiang Yu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language models (LLMs) are increasingly deployed in medical contexts, raising critical concerns about safety, alignment, and susceptibility to adversarial manipulation. While prior benchmarks assess model refusal capabilities for harmful prompts, they often lack clinical specificity, graded ha</span>
                
                <span class="abstract-full" style="display: none;">Large language models (LLMs) are increasingly deployed in medical contexts, raising critical concerns about safety, alignment, and susceptibility to adversarial manipulation. While prior benchmarks assess model refusal capabilities for harmful prompts, they often lack clinical specificity, graded harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES (Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for evaluating LLM safety in healthcare. CARES includes over 18,000 prompts spanning eight medical safety principles, four harm levels, and four prompting styles: direct, indirect, obfuscated, and role-play, to simulate both malicious and benign use cases. We propose a three-way response evaluation protocol (Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess model behavior. Our analysis reveals that many state-of-the-art LLMs remain vulnerable to jailbreaks that subtly rephrase harmful prompts, while also over-refusing safe but atypically phrased queries. Finally, we propose a mitigation strategy using a lightweight classifier to detect jailbreak attempts and steer models toward safer behavior via reminder-based conditioning. CARES provides a rigorous framework for testing and improving medical LLM safety under adversarial and ambiguous conditions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 34.3 -->
                    
                <!-- Medicine: 7.3 -->
                    
                <!-- Decision Trees: 1.8 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1202
                </span>
                <a href="https://arxiv.org/abs/2505.11024" target="_blank" rel="noopener noreferrer">Leveraging Real-Time Data Analysis and Multiple Kernel Learning for Manufacturing of Innovative Steels</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wolfgang Rannetbauer, Simon Hubmer, Carina Hambrock, Ronny Ramlau
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The implementation of thermally sprayed components in steel manufacturing presents challenges for production and plant maintenance. While enhancing performance through specialized surface properties, these components may encounter difficulties in meeting modified requirements due to standardization </span>
                
                <span class="abstract-full" style="display: none;">The implementation of thermally sprayed components in steel manufacturing presents challenges for production and plant maintenance. While enhancing performance through specialized surface properties, these components may encounter difficulties in meeting modified requirements due to standardization in the refurbishment process. This article proposes updating the established coating process for thermally spray coated components for steel manufacturing (TCCSM) by integrating real-time data analytics and predictive quality management. Two essential components--the data aggregator and the quality predictor--are designed through continuous process monitoring and the application of data-driven methodologies to meet the dynamic demands of the evolving steel landscape. The quality predictor is powered by the simple and effective multiple kernel learning strategy with the goal of realizing predictive quality. The data aggregator, designed with sensors, flow meters, and intelligent data processing for the thermal spray coating process, is proposed to facilitate real-time analytics. The performance of this combination was verified using small-scale tests that enabled not only the accurate prediction of coating quality based on the collected data but also proactive notification to the operator as soon as significant deviations are identified.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.5 -->
                    
                <!-- Hardware: 3.4 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- LLMs: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1293
                </span>
                <a href="https://arxiv.org/abs/2505.10578" target="_blank" rel="noopener noreferrer">ExploreGS: a vision-based low overhead framework for 3D scene reconstruction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yunji Feng, Chengpu Yu, Fengrui Ran, Zhi Yang, Yinni Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper proposes a low-overhead, vision-based 3D scene reconstruction framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces traditional lidar-based point cloud acquisition process with a vision model, achieving a high-quality reconstruction at a lower cost. The framework </span>
                
                <span class="abstract-full" style="display: none;">This paper proposes a low-overhead, vision-based 3D scene reconstruction framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces traditional lidar-based point cloud acquisition process with a vision model, achieving a high-quality reconstruction at a lower cost. The framework integrates scene exploration and model reconstruction, and leverags a Bag-of-Words(BoW) model to enable real-time processing capabilities, therefore, the 3D Gaussian Splatting (3DGS) training can be executed on-board. Comprehensive experiments in both simulation and real-world environments demonstrate the efficiency and applicability of the ExploreGS framework on resource-constrained devices, while maintaining reconstruction quality comparable to state-of-the-art methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- 3D: 8.2 -->
                    
                <!-- Medicine: 6.6 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                <!-- Networks: 1.0 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1376
                </span>
                <a href="https://arxiv.org/abs/2504.06552" target="_blank" rel="noopener noreferrer">Understanding Users' Security and Privacy Concerns and Attitudes Towards Conversational AI Platforms</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mutahar Ali, Arjun Arunasalam, Habiba Farrukh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The widespread adoption of conversational AI platforms has introduced new security and privacy risks. While these risks and their mitigation strategies have been extensively researched from a technical perspective, users' perceptions of these platforms' security and privacy remain largely unexplored</span>
                
                <span class="abstract-full" style="display: none;">The widespread adoption of conversational AI platforms has introduced new security and privacy risks. While these risks and their mitigation strategies have been extensively researched from a technical perspective, users' perceptions of these platforms' security and privacy remain largely unexplored. In this paper, we conduct a large-scale analysis of over 2.5M user posts from the r/ChatGPT Reddit community to understand users' security and privacy concerns and attitudes toward conversational AI platforms. Our qualitative analysis reveals that users are concerned about each stage of the data lifecycle (i.e., collection, usage, and retention). They seek mitigations for security vulnerabilities, compliance with privacy regulations, and greater transparency and control in data handling. We also find that users exhibit varied behaviors and preferences when interacting with these platforms. Some users proactively safeguard their data and adjust privacy settings, while others prioritize convenience over privacy risks, dismissing privacy concerns in favor of benefits, or feel resigned to inevitable data sharing. Through qualitative content and regression analysis, we discover that users' concerns evolve over time with the evolving AI landscape and are influenced by technological developments and major events. Based on our findings, we provide recommendations for users, platforms, enterprises, and policymakers to enhance transparency, improve data controls, and increase user trust and adoption.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.5 -->
                    
                <!-- Medicine: 6.8 -->
                    
                <!-- Blockchain: 3.5 -->
                    
                <!-- Datasets: 2.6 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2033
                </span>
                <a href="https://arxiv.org/abs/2504.03737" target="_blank" rel="noopener noreferrer">PrediHealth: Telemedicine and Predictive Algorithms for the Care and Prevention of Patients with Chronic Heart Failure</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pietro Cassieri, Giuseppe De Filippo, Simranjit Singh, Gianpiero Sisto, Marco Mazzotta, Gianvito Mitrano, Claudio Pascarelli, Gianluca Fimiani, Simone Romano, Mariangela Lazoi, Marina Garofano, Alessia Bramanti, Giuseppe Scanniello
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The management of chronic heart failure presents significant challenges in modern healthcare, requiring continuous monitoring, early detection of exacerbations, and personalized treatment strategies. This paper presents the preliminary results of the PrediHealth research project conducted in this co</span>
                
                <span class="abstract-full" style="display: none;">The management of chronic heart failure presents significant challenges in modern healthcare, requiring continuous monitoring, early detection of exacerbations, and personalized treatment strategies. This paper presents the preliminary results of the PrediHealth research project conducted in this context. Specifically, it aims to address the challenges above by integrating telemedicine, mobile health solutions, and predictive analytics into a unified digital healthcare platform. We leveraged a web-based IoT platform, a telemonitoring kit with medical devices and environmental sensors, and AI-driven predictive models to support clinical decision-making. The project follows a structured methodology comprising research on emerging CPS/IoT technologies, system prototyping, predictive model development, and empirical validation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.6 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Blockchain: 2.3 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Datasets: 1.8 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- HPO and AutoML: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2267
                </span>
                <a href="https://arxiv.org/abs/2402.09447" target="_blank" rel="noopener noreferrer">Wavelet Analysis of Noninvasive EEG Signals Discriminates Complex and Natural Grasp Types</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ali Rabiee, Sima Ghafoori, Anna Cetera, Reza Abiri
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This research aims to decode hand grasps from Electroencephalograms (EEGs) for dexterous neuroprosthetic development and Brain-Computer Interface (BCI) applications, especially for patients with motor disorders. Particularly, it focuses on distinguishing two complex natural power and precision grasp</span>
                
                <span class="abstract-full" style="display: none;">This research aims to decode hand grasps from Electroencephalograms (EEGs) for dexterous neuroprosthetic development and Brain-Computer Interface (BCI) applications, especially for patients with motor disorders. Particularly, it focuses on distinguishing two complex natural power and precision grasps in addition to a neutral condition as a no-movement condition using a new EEG-based BCI platform and wavelet signal processing. Wavelet analysis involved generating time-frequency and topographic maps from wavelet power coefficients. Then, by using machine learning techniques with novel wavelet features, we achieved high average accuracies: 85.16% for multiclass, 95.37% for No-Movement vs Power, 95.40% for No-Movement vs Precision, and 88.07% for Power vs Precision, demonstrating the effectiveness of these features in EEG-based grasp differentiation. In contrast to previous studies, a critical part of our study was permutation feature importance analysis, which highlighted key features for grasp classification. It revealed that the most crucial brain activities during grasping occur in the motor cortex, within the alpha and beta frequency bands. These insights demonstrate the potential of wavelet features in real-time neuroprosthetic technology and BCI applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.9 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2328
                </span>
                <a href="https://arxiv.org/abs/2505.11178" target="_blank" rel="noopener noreferrer">CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yixin Wan, Kai-Wei Chang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emph</span>
                
                <span class="abstract-full" style="display: none;">State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on compositional image generation. CompAlign consists of 900 complex multi-subject image generation prompts that combine numerical and 3D-spatial relationships with varied attribute bindings. Our benchmark is remarkably challenging, incorporating generation tasks with 3+ generation subjects with complex 3D-spatial relationships. Additionally, we propose CompQuest, an interpretable and accurate evaluation framework that decomposes complex prompts into atomic sub-questions, then utilizes a MLLM to provide fine-grained binary feedback on the correctness of each aspect of generation elements in model-generated images. This enables precise quantification of alignment between generated images and compositional prompts. Furthermore, we propose an alignment framework that uses CompQuest's feedback as preference signals to improve diffusion models' compositional image generation abilities. Using adjustable per-image preferences, our method is easily scalable and flexible for different tasks. Evaluation of 9 T2I models reveals that: (1) models remarkable struggle more with compositional tasks with more complex 3D-spatial configurations, and (2) a noticeable performance gap exists between open-source accessible models and closed-source commercial models. Further empirical study on using CompAlign for model alignment yield promising results: post-alignment diffusion models achieve remarkable improvements in compositional accuracy, especially on complex generation tasks, outperforming previous approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 11.4 -->
                    
                <!-- Medicine: 10.0 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- T2I: 2.2 -->
                    
                <!-- Datasets: 2.0 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Hardware: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.265
                </span>
                <a href="https://arxiv.org/abs/2505.11175" target="_blank" rel="noopener noreferrer">Real-Time Verification of Embodied Reasoning for Generative Skill Acquisition</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bo Yue, Shuqi Guo, Kaiyu Hu, Chujiao Wang, Benyou Wang, Kui Jia, Guiliang Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generative skill acquisition enables embodied agents to actively learn a scalable and evolving repertoire of control skills, crucial for the advancement of large decision models. While prior approaches often rely on supervision signals from generalist agents (e.g., LLMs), their effectiveness in comp</span>
                
                <span class="abstract-full" style="display: none;">Generative skill acquisition enables embodied agents to actively learn a scalable and evolving repertoire of control skills, crucial for the advancement of large decision models. While prior approaches often rely on supervision signals from generalist agents (e.g., LLMs), their effectiveness in complex 3D environments remains unclear; exhaustive evaluation incurs substantial computational costs, significantly hindering the efficiency of skill learning. Inspired by recent successes in verification models for mathematical reasoning, we propose VERGSA (Verifying Embodied Reasoning in Generative Skill Acquisition), a framework that systematically integrates real-time verification principles into embodied skill learning. VERGSA establishes 1) a seamless extension from verification of mathematical reasoning into embodied learning by dynamically incorporating contextually relevant tasks into prompts and defining success metrics for both subtasks and overall tasks, and 2) an automated, scalable reward labeling scheme that synthesizes dense reward signals by iteratively finalizing the contribution of scene configuration and subtask learning to overall skill acquisition. To the best of our knowledge, this approach constitutes the first comprehensive training dataset for verification-driven generative skill acquisition, eliminating arduous manual reward engineering. Experiments validate the efficacy of our approach: 1) the exemplar task pool improves the average task success rates by 21%, 2) our verification model boosts success rates by 24% for novel tasks and 36% for encountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification quality.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.9 -->
                    
                <!-- LLMs: 6.1 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Datasets: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2751
                </span>
                <a href="https://arxiv.org/abs/2505.11439" target="_blank" rel="noopener noreferrer">SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Utsav Rai, Haozheng Xu, Stamatia Giannarou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate pose estimation of surgical tools in Robot-assisted Minimally Invasive Surgery (RMIS) is essential for surgical navigation and robot control. While traditional marker-based methods offer accuracy, they face challenges with occlusions, reflections, and tool-specific designs. Similarly, super</span>
                
                <span class="abstract-full" style="display: none;">Accurate pose estimation of surgical tools in Robot-assisted Minimally Invasive Surgery (RMIS) is essential for surgical navigation and robot control. While traditional marker-based methods offer accuracy, they face challenges with occlusions, reflections, and tool-specific designs. Similarly, supervised learning methods require extensive training on annotated datasets, limiting their adaptability to new tools. Despite their success in other domains, zero-shot pose estimation models remain unexplored in RMIS for pose estimation of surgical instruments, creating a gap in generalising to unseen surgical tools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation pipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D models like the FoundationPose and SAM-6D. We advanced these models by incorporating vision-based depth estimation using the RAFT-Stereo method, for robust depth estimation in reflective and textureless environments. Additionally, we enhanced SAM-6D by replacing its instance segmentation module, Segment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly boosting segmentation accuracy in occluded and complex conditions. Extensive validation reveals that our enhanced SAM-6D surpasses FoundationPose in zero-shot pose estimation of unseen surgical instruments, setting a new benchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the generalisability of pose estimation for unseen objects and pioneers the application of RGB-D zero-shot methods in RMIS.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.7 -->
                    
                <!-- LLMs: 5.8 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Datasets: 2.1 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Hardware: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2862
                </span>
                <a href="https://arxiv.org/abs/2505.11298" target="_blank" rel="noopener noreferrer">Graph Representational Learning: When Does More Expressivity Hurt Generalization?</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sohir Maskey, Raffaele Paolino, Fabian Jogl, Gitta Kutyniok, Johannes F. Lutzeyer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph Neural Networks (GNNs) are powerful tools for learning on structured data, yet the relationship between their expressivity and predictive performance remains unclear. We introduce a family of premetrics that capture different degrees of structural similarity between graphs and relate these sim</span>
                
                <span class="abstract-full" style="display: none;">Graph Neural Networks (GNNs) are powerful tools for learning on structured data, yet the relationship between their expressivity and predictive performance remains unclear. We introduce a family of premetrics that capture different degrees of structural similarity between graphs and relate these similarities to generalization, and consequently, the performance of expressive GNNs. By considering a setting where graph labels are correlated with structural features, we derive generalization bounds that depend on the distance between training and test graphs, model complexity, and training set size. These bounds reveal that more expressive GNNs may generalize worse unless their increased complexity is balanced by a sufficiently large training set or reduced distance between training and test graphs. Our findings relate expressivity and generalization, offering theoretical insights supported by empirical results.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- GNN: 5.8 -->
                    
                <!-- LLMs: 5.0 -->
                    
                <!-- Medicine: 3.7 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- Blockchain: 2.2 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Computer Vision: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3161
                </span>
                <a href="https://arxiv.org/abs/2405.01225" target="_blank" rel="noopener noreferrer">CoolWalks for active mobility in urban street networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Henrik Wolf, Ane Rahbek Vier{\o}, Michael Szell
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Walking is the most sustainable form of urban mobility, but is compromised by uncomfortable or unhealthy sun exposure, which is an increasing problem due to global warming. Shade from buildings can provide cooling and protection for pedestrians, but the extent of this potential benefit is unknown. H</span>
                
                <span class="abstract-full" style="display: none;">Walking is the most sustainable form of urban mobility, but is compromised by uncomfortable or unhealthy sun exposure, which is an increasing problem due to global warming. Shade from buildings can provide cooling and protection for pedestrians, but the extent of this potential benefit is unknown. Here we explore the potential for shaded walking, using building footprints and street networks from both synthetic and real cities. We introduce a route choice model with a sun avoidance parameter $\alpha$ and define the CoolWalkability metric to measure opportunities for walking in shade. We derive analytically that on a regular grid with constant building heights, CoolWalkability is independent of $\alpha$, and that the grid provides no CoolWalkability benefit for shade-seeking individuals compared to the shortest path. However, variations in street geometry and building heights create such benefits. We further uncover that the potential for shaded routing differs between grid-like and irregular street networks, forms local clusters, and is sensitive to the mapped network geometry. Our research identifies the limitations and potential of shade for cool, active travel, and is a first step towards a rigorous understanding of shade provision for sustainable mobility in cities.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.7 -->
                    
                <!-- LLMs: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.342
                </span>
                <a href="https://arxiv.org/abs/2505.11151" target="_blank" rel="noopener noreferrer">STEP: A Unified Spiking Transformer Evaluation Platform for Fair and Reproducible Benchmarking</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sicheng Shen, Dongcheng Zhao, Linghao Feng, Zeyang Yue, Jindong Li, Tenglong Li, Guobin Shen, Yi Zeng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Spiking Transformers have recently emerged as promising architectures for combining the efficiency of spiking neural networks with the representational power of self-attention. However, the lack of standardized implementations, evaluation pipelines, and consistent design choices has hindered fair co</span>
                
                <span class="abstract-full" style="display: none;">Spiking Transformers have recently emerged as promising architectures for combining the efficiency of spiking neural networks with the representational power of self-attention. However, the lack of standardized implementations, evaluation pipelines, and consistent design choices has hindered fair comparison and principled analysis. In this paper, we introduce \textbf{STEP}, a unified benchmark framework for Spiking Transformers that supports a wide range of tasks, including classification, segmentation, and detection across static, event-based, and sequential datasets. STEP provides modular support for diverse components such as spiking neurons, input encodings, surrogate gradients, and multiple backends (e.g., SpikingJelly, BrainCog). Using STEP, we reproduce and evaluate several representative models, and conduct systematic ablation studies on attention design, neuron types, encoding schemes, and temporal modeling capabilities. We also propose a unified analytical model for energy estimation, accounting for spike sparsity, bitwidth, and memory access, and show that quantized ANNs may offer comparable or better energy efficiency. Our results suggest that current Spiking Transformers rely heavily on convolutional frontends and lack strong temporal modeling, underscoring the need for spike-native architectural innovations. The full code is available at: https://github.com/Fancyssc/STEP</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.2 -->
                    
                <!-- LLMs: 5.9 -->
                    
                <!-- Hardware: 3.9 -->
                    
                <!-- Computer Vision: 2.1 -->
                    
                <!-- Datasets: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3472
                </span>
                <a href="https://arxiv.org/abs/2505.10691" target="_blank" rel="noopener noreferrer">Predicting Risk of Pulmonary Fibrosis Formation in PASC Patients</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wanying Dou, Gorkem Durak, Koushik Biswas, Ziliang Hong, Andrea Mia Bejar, Elif Keles, Kaan Akin, Sukru Mehmet Erturk, Alpay Medetalibeyoglu, Marc Sala, Alexander Misharin, Hatice Savas, Mary Salvatore, Sachin Jambawalikar, Drew Torigian, Jayaram K. Udupa, Ulas Bagci
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While the acute phase of the COVID-19 pandemic has subsided, its long-term effects persist through Post-Acute Sequelae of COVID-19 (PASC), commonly known as Long COVID. There remains substantial uncertainty regarding both its duration and optimal management strategies. PASC manifests as a diverse ar</span>
                
                <span class="abstract-full" style="display: none;">While the acute phase of the COVID-19 pandemic has subsided, its long-term effects persist through Post-Acute Sequelae of COVID-19 (PASC), commonly known as Long COVID. There remains substantial uncertainty regarding both its duration and optimal management strategies. PASC manifests as a diverse array of persistent or newly emerging symptoms--ranging from fatigue, dyspnea, and neurologic impairments (e.g., brain fog), to cardiovascular, pulmonary, and musculoskeletal abnormalities--that extend beyond the acute infection phase. This heterogeneous presentation poses substantial challenges for clinical assessment, diagnosis, and treatment planning. In this paper, we focus on imaging findings that may suggest fibrotic damage in the lungs, a critical manifestation characterized by scarring of lung tissue, which can potentially affect long-term respiratory function in patients with PASC. This study introduces a novel multi-center chest CT analysis framework that combines deep learning and radiomics for fibrosis prediction. Our approach leverages convolutional neural networks (CNNs) and interpretable feature extraction, achieving 82.2% accuracy and 85.5% AUC in classification tasks. We demonstrate the effectiveness of Grad-CAM visualization and radiomics-based feature analysis in providing clinically relevant insights for PASC-related lung fibrosis prediction. Our findings highlight the potential of deep learning-driven computational methods for early detection and risk assessment of PASC-related lung fibrosis--presented for the first time in the literature.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.6 -->
                    
                <!-- LLMs: 5.4 -->
                    
                <!-- Hardware: 2.3 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Datasets: 1.8 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- GNN: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4049
                </span>
                <a href="https://arxiv.org/abs/2505.11119" target="_blank" rel="noopener noreferrer">Predicting Student Dropout Risk With A Dual-Modal Abrupt Behavioral Changes Approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jiabei Cheng, Zhen-Qun Yang, Jiannong Cao, Yu Yang, Xinzhe Zheng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Timely prediction of students at high risk of dropout is critical for early intervention and improving educational outcomes. However, in offline educational settings, poor data quality, limited scale, and high heterogeneity often hinder the application of advanced machine learning models. Furthermor</span>
                
                <span class="abstract-full" style="display: none;">Timely prediction of students at high risk of dropout is critical for early intervention and improving educational outcomes. However, in offline educational settings, poor data quality, limited scale, and high heterogeneity often hinder the application of advanced machine learning models. Furthermore, while educational theories provide valuable insights into dropout phenomena, the lack of quantifiable metrics for key indicators limits their use in data-driven modeling. Through data analysis and a review of educational literature, we identified abrupt changes in student behavior as key early signals of dropout risk. To address this, we propose the Dual-Modal Multiscale Sliding Window (DMSW) Model, which integrates academic performance and behavioral data to dynamically capture behavior patterns using minimal data. The DMSW model improves prediction accuracy by 15% compared to traditional methods, enabling educators to identify high-risk students earlier, provide timely support, and foster a more inclusive learning environment. Our analysis highlights key behavior patterns, offering practical insights for preventive strategies and tailored support. These findings bridge the gap between theory and practice in dropout prediction, giving educators an innovative tool to enhance student retention and outcomes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.4 -->
                    
                <!-- LLMs: 6.4 -->
                    
                <!-- Blockchain: 2.3 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Datasets: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.6 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4172
                </span>
                <a href="https://arxiv.org/abs/2505.11308" target="_blank" rel="noopener noreferrer">Reinforcement Learning Closures for Underresolved Partial Differential Equations using Synthetic Data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lothar Heimbach, Sebastian Kaltenbach, Petr Karnakov, Francis J. Alexander, Petros Koumoutsakos
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Partial Differential Equations (PDEs) describe phenomena ranging from turbulence and epidemics to quantum mechanics and financial markets. Despite recent advances in computational science, solving such PDEs for real-world applications remains prohibitively expensive because of the necessity of resol</span>
                
                <span class="abstract-full" style="display: none;">Partial Differential Equations (PDEs) describe phenomena ranging from turbulence and epidemics to quantum mechanics and financial markets. Despite recent advances in computational science, solving such PDEs for real-world applications remains prohibitively expensive because of the necessity of resolving a broad range of spatiotemporal scales. In turn, practitioners often rely on coarse-grained approximations of the original PDEs, trading off accuracy for reduced computational resources. To mitigate the loss of detail inherent in such approximations, closure models are employed to represent unresolved spatiotemporal interactions. We present a framework for developing closure models for PDEs using synthetic data acquired through the method of manufactured solutions. These data are used in conjunction with reinforcement learning to provide closures for coarse-grained PDEs. We illustrate the efficacy of our method using the one-dimensional and two-dimensional Burgers' equations and the two-dimensional advection equation. Moreover, we demonstrate that closure models trained for inhomogeneous PDEs can be effectively generalized to homogeneous PDEs. The results demonstrate the potential for developing accurate and computationally efficient closure models for systems with scarce data.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.9 -->
                    
                <!-- LLMs: 2.7 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Bayesian Optimization: 2.1 -->
                    
                <!-- Evolutionary Algorithms: 2.1 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Datasets: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4309
                </span>
                <a href="https://arxiv.org/abs/2505.10687" target="_blank" rel="noopener noreferrer">ROIsGAN: A Region Guided Generative Adversarial Framework for Murine Hippocampal Subregion Segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sayed Mehedi Azim, Brian Corbett, Iman Dehzangi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The hippocampus, a critical brain structure involved in memory processing and various neurodegenerative and psychiatric disorders, comprises three key subregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3 (CA3). Accurate segmentation of these subregions from histological tis</span>
                
                <span class="abstract-full" style="display: none;">The hippocampus, a critical brain structure involved in memory processing and various neurodegenerative and psychiatric disorders, comprises three key subregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3 (CA3). Accurate segmentation of these subregions from histological tissue images is essential for advancing our understanding of disease mechanisms, developmental dynamics, and therapeutic interventions. However, no existing methods address the automated segmentation of hippocampal subregions from tissue images, particularly from immunohistochemistry (IHC) images. To bridge this gap, we introduce a novel set of four comprehensive murine hippocampal IHC datasets featuring distinct staining modalities: cFos, NeuN, and multiplexed stains combining cFos, NeuN, and either {\Delta}FosB or GAD67, capturing structural, neuronal activity, and plasticity associated information. Additionally, we propose ROIsGAN, a region-guided U-Net-based generative adversarial network tailored for hippocampal subregion segmentation. By leveraging adversarial learning, ROIsGAN enhances boundary delineation and structural detail refinement through a novel region-guided discriminator loss combining Dice and binary cross-entropy loss. Evaluated across DG, CA1, and CA3 subregions, ROIsGAN consistently outperforms conventional segmentation models, achieving performance gains ranging from 1-10% in Dice score and up to 11% in Intersection over Union (IoU), particularly under challenging staining conditions. Our work establishes foundational datasets and methods for automated hippocampal segmentation, enabling scalable, high-precision analysis of tissue images in neuroscience research. Our generated datasets, proposed model as a standalone tool, and its corresponding source code are publicly available at: https://github.com/MehediAzim/ROIsGAN</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.3 -->
                    
                <!-- LLMs: 7.2 -->
                    
                <!-- Datasets: 3.4 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Quantum Computing: 1.1 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.432
                </span>
                <a href="https://arxiv.org/abs/2505.11335" target="_blank" rel="noopener noreferrer">The Final Layer Holds the Key: A Unified and Efficient GNN Calibration Framework</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jincheng Huang, Jie Xu, Xiaoshuang Shi, Ping Hu, Lei Feng, Xiaofeng Zhu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on graph-based tasks. However, their predictive confidence is often miscalibrated, typically exhibiting under-confidence, which harms the reliability of their decisions. Existing calibration methods for GNNs normally introduce a</span>
                
                <span class="abstract-full" style="display: none;">Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on graph-based tasks. However, their predictive confidence is often miscalibrated, typically exhibiting under-confidence, which harms the reliability of their decisions. Existing calibration methods for GNNs normally introduce additional calibration components, which fail to capture the intrinsic relationship between the model and the prediction confidence, resulting in limited theoretical guarantees and increased computational overhead. To address this issue, we propose a simple yet efficient graph calibration method. We establish a unified theoretical framework revealing that model confidence is jointly governed by class-centroid-level and node-level calibration at the final layer. Based on this insight, we theoretically show that reducing the weight decay of the final-layer parameters alleviates GNN under-confidence by acting on the class-centroid level, while node-level calibration acts as a finer-grained complement to class-centroid level calibration, which encourages each test node to be closer to its predicted class centroid at the final-layer representations. Extensive experiments validate the superiority of our method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- GNN: 5.4 -->
                    
                <!-- Federated Learning: 3.8 -->
                    
                <!-- Computer Vision: 3.0 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Medicine: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Hardware: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4574
                </span>
                <a href="https://arxiv.org/abs/2505.10680" target="_blank" rel="noopener noreferrer">Generalization of Repetitiveness Measures for Two-Dimensional Strings</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lorenzo Carfagna, Giovanni Manzini, Giuseppe Romana, Marinella Sciortino, Cristian Urbina
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The problem of detecting and measuring the repetitiveness of one-dimensional strings has been extensively studied in data compression and text indexing. Our understanding of these issues has been significantly improved by the introduction of the notion of string attractor [Kempa and Prezza, STOC 201</span>
                
                <span class="abstract-full" style="display: none;">The problem of detecting and measuring the repetitiveness of one-dimensional strings has been extensively studied in data compression and text indexing. Our understanding of these issues has been significantly improved by the introduction of the notion of string attractor [Kempa and Prezza, STOC 2018] and by the results showing the relationship between attractors and other measures of compressibility.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.2 -->
                    
                <!-- LLMs: 5.9 -->
                    
                <!-- Blockchain: 4.6 -->
                    
                <!-- Hardware: 3.6 -->
                    
                <!-- Federated Learning: 3.5 -->
                    
                <!-- Evolutionary Algorithms: 2.6 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4683
                </span>
                <a href="https://arxiv.org/abs/2505.11050" target="_blank" rel="noopener noreferrer">Halting Recurrent GNNs and the Graded $\mu$-Calculus</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jeroen Bollen, Jan Van den Bussche, Stijn Vansummeren, Jonni Virtema
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph Neural Networks (GNNs) are a class of machine-learning models that operate on graph-structured data. Their expressive power is intimately related to logics that are invariant under graded bisimilarity. Current proposals for recurrent GNNs either assume that the graph size is given to the model</span>
                
                <span class="abstract-full" style="display: none;">Graph Neural Networks (GNNs) are a class of machine-learning models that operate on graph-structured data. Their expressive power is intimately related to logics that are invariant under graded bisimilarity. Current proposals for recurrent GNNs either assume that the graph size is given to the model, or suffer from a lack of termination guarantees. In this paper, we propose a halting mechanism for recurrent GNNs. We prove that our halting model can express all node classifiers definable in graded modal mu-calculus, even for the standard GNN variant that is oblivious to the graph size. A recent breakthrough in the study of the expressivity of graded modal mu-calculus in the finite suggests that conversely, restricted to node classifiers definable in monadic second-order logic, recurrent GNNs can express only node classifiers definable in graded modal mu-calculus. To prove our main result, we develop a new approximate semantics for graded mu-calculus, which we believe to be of independent interest. We leverage this new semantics into a new model-checking algorithm, called the counting algorithm, which is oblivious to the graph size. In a final step we show that the counting algorithm can be implemented on a halting recurrent GNN.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- GNN: 6.9 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- LLMs: 2.4 -->
                    
                <!-- Bayesian Optimization: 2.4 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Game Theory: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Cryptography: 1.2 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Medicine: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.478
                </span>
                <a href="https://arxiv.org/abs/2505.10609" target="_blank" rel="noopener noreferrer">Agent Name Service (ANS): A Universal Directory for Secure AI Agent Discovery and Interoperability</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ken Huang, Vineeth Sai Narajala, Idan Habler, Akram Sheriff
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The proliferation of AI agents requires robust mechanisms for secure discovery. This paper introduces the Agent Name Service (ANS), a novel architecture based on DNS addressing the lack of a public agent discovery framework. ANS provides a protocol-agnostic registry infrastructure that leverages Pub</span>
                
                <span class="abstract-full" style="display: none;">The proliferation of AI agents requires robust mechanisms for secure discovery. This paper introduces the Agent Name Service (ANS), a novel architecture based on DNS addressing the lack of a public agent discovery framework. ANS provides a protocol-agnostic registry infrastructure that leverages Public Key Infrastructure (PKI) certificates for verifiable agent identity and trust. The architecture features several key innovations: a formalized agent registration and renewal mechanism for lifecycle management; DNS-inspired naming conventions with capability-aware resolution; a modular Protocol Adapter Layer supporting diverse communication standards (A2A, MCP, ACP etc.); and precisely defined algorithms for secure resolution. We implement structured communication using JSON Schema and conduct a comprehensive threat analysis of our proposal. The result is a foundational directory service addressing the core challenges of secured discovery and interaction in multi-agent systems, paving the way for future interoperable, trustworthy, and scalable agent ecosystems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.1 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Hardware: 2.6 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Evolutionary Algorithms: 1.9 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Computer Vision: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4949
                </span>
                <a href="https://arxiv.org/abs/2502.06876" target="_blank" rel="noopener noreferrer">Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jinluan Yang, Dingnan Jin, Anke Tang, Li Shen, Didi Zhu, Zhengyu Chen, Ziyu Zhao, Daixin Wang, Qing Cui, Zhiqiang Zhang, Jun Zhou, Fei Wu, Kun Kuang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conflicting </span>
                
                <span class="abstract-full" style="display: none;">Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conflicting optimization signals. While model merging offers parameter-level conflict-resolution strategies through integrating specialized models' parameters, its potential for 3H optimization remains underexplored. This paper systematically compares the effectiveness of model merging and data mixture methods in constructing 3H-aligned LLMs for the first time, revealing previously overlooked collaborative and conflict relationships among the 3H dimensions and discussing the advantages and drawbacks of data mixture (\textit{data-level}) and model merging (\textit{parameter-level}) methods in mitigating the conflict for balanced 3H optimization. Specially, we propose a novel \textbf{R}eweighting \textbf{E}nhanced task \textbf{S}ingular \textbf{M}erging method, \textbf{RESM}, through outlier weighting and sparsity-aware rank selection strategies to address the challenges of preference noise accumulation and layer sparsity adaptation inherent in 3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and robustness of RESM compared to previous data mixture (2\%-5\% gain) and model merging (1\%-3\% gain) methods in achieving balanced LLM alignment. We release our models through \href{https://huggingface.co/Jinluan}{3H\_Merging} for further investigations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 17.9 -->
                    
                <!-- Medicine: 7.7 -->
                    
                <!-- Computer Vision: 2.3 -->
                    
                <!-- Hardware: 2.3 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7937
                </span>
                <a href="https://arxiv.org/abs/2410.06883" target="_blank" rel="noopener noreferrer">Degree-Conscious Spiking Graph for Cross-Domain Adaptation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yingxu Wang, Mengzhu Wang, Siwei Liu, Houcheng Su, Nan Yin, James Kwok
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Spiking Graph Networks (SGNs) have demonstrated significant potential in graph classification by emulating brain-inspired neural dynamics to achieve energy-efficient computation. However, existing SGNs are generally constrained to in-distribution scenarios and struggle with distribution shifts. In t</span>
                
                <span class="abstract-full" style="display: none;">Spiking Graph Networks (SGNs) have demonstrated significant potential in graph classification by emulating brain-inspired neural dynamics to achieve energy-efficient computation. However, existing SGNs are generally constrained to in-distribution scenarios and struggle with distribution shifts. In this paper, we first propose the domain adaptation problem in SGNs, and introduce a novel framework named Degree-Consicious Spiking Graph for Cross-Domain Adaptation. DeSGraDA enhances generalization across domains with three key components. First, we introduce the degree-conscious spiking representation module by adapting spike thresholds based on node degrees, enabling more expressive and structure-aware signal encoding. Then, we perform temporal distribution alignment by adversarially matching membrane potentials between domains, ensuring effective performance under domain shift while preserving energy efficiency. Additionally, we extract consistent predictions across two spaces to create reliable pseudo-labels, effectively leveraging unlabeled data to enhance graph classification performance. Furthermore, we establish the first generalization bound for SGDA, providing theoretical insights into its adaptation performance. Extensive experiments on benchmark datasets validate that DeSGraDA consistently outperforms state-of-the-art methods in both classification accuracy and energy efficiency.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- GNN: 7.9 -->
                    
                <!-- LLMs: 6.1 -->
                    
                <!-- Computer Vision: 5.6 -->
                    
                <!-- Medicine: 2.3 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- HPO and AutoML: 2.0 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8685
                </span>
                <a href="https://arxiv.org/abs/2409.20204" target="_blank" rel="noopener noreferrer">Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aditi Dutta, Susan Banducci, Chico Q. Camargo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Several computational tools have been developed to detect and identify sexism, misogyny, and gender-based hate speech, particularly on online platforms. These tools draw on insights from both social science and computer science. Given the increasing concern over gender-based discrimination in digita</span>
                
                <span class="abstract-full" style="display: none;">Several computational tools have been developed to detect and identify sexism, misogyny, and gender-based hate speech, particularly on online platforms. These tools draw on insights from both social science and computer science. Given the increasing concern over gender-based discrimination in digital spaces, the contested definitions and measurements of sexism, and the rise of interdisciplinary efforts to understand its online manifestations, a systematic literature review is essential for capturing the current state and trajectory of this evolving field. In this review, we make four key contributions: (1) we synthesize the literature into five core themes: definitions of sexism and misogyny, disciplinary divergences, automated detection methods, associated challenges, and design-based interventions; (2) we adopt an interdisciplinary lens, bridging theoretical and methodological divides across disciplines; (3) we highlight critical gaps, including the need for intersectional approaches, the under-representation of non-Western languages and perspectives, and the limited focus on proactive design strategies beyond text classification; and (4) we offer a methodological contribution by applying a rigorous semi-automated systematic review process guided by PRISMA, establishing a replicable standard for future work in this domain. Our findings reveal a clear disciplinary divide in how sexism and misogyny are conceptualized and measured. Through an evidence-based synthesis, we examine how existing studies have attempted to bridge this gap through interdisciplinary collaboration. Drawing on both social science theories and computational modeling practices, we assess the strengths and limitations of current methodologies. Finally, we outline key challenges and future directions for advancing research on the detection and mitigation of online sexism and misogyny.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.5 -->
                    
                <!-- LLMs: 5.4 -->
                    
                <!-- Computer Vision: 3.0 -->
                    
                <!-- Datasets: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.937
                </span>
                <a href="https://arxiv.org/abs/2505.10717" target="_blank" rel="noopener noreferrer">A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jean-Philippe Corbeil, Amin Dada, Jean-Michel Attendu, Asma Ben Abacha, Alessandro Sordoni, Lucas Caccia, Fran\c{c}ois Beaulieu, Thomas Lin, Jens Kleesiek, Paul Vozila
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">High computation costs and latency of large language models such as GPT-4 have limited their deployment in clinical settings. Small language models (SLMs) offer a cost-effective alternative, but their limited capacity requires biomedical domain adaptation, which remains challenging. An additional bo</span>
                
                <span class="abstract-full" style="display: none;">High computation costs and latency of large language models such as GPT-4 have limited their deployment in clinical settings. Small language models (SLMs) offer a cost-effective alternative, but their limited capacity requires biomedical domain adaptation, which remains challenging. An additional bottleneck is the unavailability and high sensitivity of clinical data. To address these challenges, we propose a novel framework for adapting SLMs into high-performing clinical models. We introduce the MediPhi collection of 3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning of experts on relevant medical and clinical corpora (PMC, Medical Guideline, MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our expert models deliver relative improvements on this benchmark over the base model without any task-specific fine-tuning: 64.3% on medical entities, 49.5% on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by 14%). We unify the expert models into MediPhi via model merging, preserving gains across benchmarks. Furthermore, we built the MediFlow collection, a synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP tasks, 98 fine-grained document types, and JSON format support. Alignment of MediPhi using supervised fine-tuning and direct preference optimization achieves further gains of 18.9% on average.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.5 -->
                    
                <!-- LLMs: 7.1 -->
                    
                <!-- Computer Vision: 3.2 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.9438
                </span>
                <a href="https://arxiv.org/abs/2505.04258" target="_blank" rel="noopener noreferrer">RGB-Event Fusion with Self-Attention for Collision Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pietro Bonazzi, Christian Vogt, Michael Jost, Haotong Qin, Lyes Khacef, Federico Paredes-Valles, Michele Magno
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Ensuring robust and real-time obstacle avoidance is critical for the safe operation of autonomous robots in dynamic, real-world environments. This paper proposes a neural network framework for predicting the time and collision position of an unmanned aerial vehicle with a dynamic object, using RGB a</span>
                
                <span class="abstract-full" style="display: none;">Ensuring robust and real-time obstacle avoidance is critical for the safe operation of autonomous robots in dynamic, real-world environments. This paper proposes a neural network framework for predicting the time and collision position of an unmanned aerial vehicle with a dynamic object, using RGB and event-based vision sensors. The proposed architecture consists of two separate encoder branches, one for each modality, followed by fusion by self-attention to improve prediction accuracy. To facilitate benchmarking, we leverage the ABCD [8] dataset collected that enables detailed comparisons of single-modality and fusion-based approaches. At the same prediction throughput of 50Hz, the experimental results show that the fusion-based model offers an improvement in prediction accuracy over single-modality approaches of 1% on average and 10% for distances beyond 0.5m, but comes at the cost of +71% in memory and + 105% in FLOPs. Notably, the event-based model outperforms the RGB model by 4% for position and 26% for time error at a similar computational cost, making it a competitive alternative. Additionally, we evaluate quantized versions of the event-based models, applying 1- to 8-bit quantization to assess the trade-offs between predictive performance and computational efficiency. These findings highlight the trade-offs of multi-modal perception using RGB and event-based cameras in robotic applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.8 -->
                    
                <!-- Hardware: 3.2 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Computer Vision: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 1.8 -->
                    
                <!-- Datasets: 1.4 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0879
                </span>
                <a href="https://arxiv.org/abs/2505.10829" target="_blank" rel="noopener noreferrer">Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chen-Chi Chang, Chong-Fu Li, Chu-Hsuan Lee, Hung-Shin Lee
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study investigates the challenges of translating low-resource languages by integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG). Various model configurations were tested on Hakka translations, with BLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gem</span>
                
                <span class="abstract-full" style="display: none;">This study investigates the challenges of translating low-resource languages by integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG). Various model configurations were tested on Hakka translations, with BLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0). The best-performing model (Model 4) combined retrieval and advanced language modeling, improving lexical coverage, particularly for specialized or culturally nuanced terms, and enhancing grammatical coherence. A two-stage method (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU score of 26%, highlighting iterative correction's value and the challenges of domain-specific expressions. Static dictionary-based approaches struggled with context-sensitive content, demonstrating the limitations of relying solely on predefined resources. These results emphasize the need for curated resources, domain knowledge, and ethical collaboration with local communities, offering a framework that improves translation accuracy and fluency while supporting cultural preservation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 14.1 -->
                    
                <!-- Medicine: 11.9 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Evolutionary Algorithms: 2.5 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- Computer Vision: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.29
                </span>
                <a href="https://arxiv.org/abs/2505.11191" target="_blank" rel="noopener noreferrer">Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kasra Borazjani, Payam Abdisarabshali, Fardis Nadimi, Naji Khosravan, Minghui Liwang, Xianbin Wang, Yiguang Hong, Seyyedali Hosseinalipour
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As embodied AI systems become increasingly multi-modal, personalized, and interactive, they must learn effectively from diverse sensory inputs, adapt continually to user preferences, and operate safely under resource and privacy constraints. These challenges expose a pressing need for machine learni</span>
                
                <span class="abstract-full" style="display: none;">As embodied AI systems become increasingly multi-modal, personalized, and interactive, they must learn effectively from diverse sensory inputs, adapt continually to user preferences, and operate safely under resource and privacy constraints. These challenges expose a pressing need for machine learning models capable of swift, context-aware adaptation while balancing model generalization and personalization. Here, two methods emerge as suitable candidates, each offering parts of these capabilities: Foundation Models (FMs) provide a pathway toward generalization across tasks and modalities, whereas Federated Learning (FL) offers the infrastructure for distributed, privacy-preserving model updates and user-level model personalization. However, when used in isolation, each of these approaches falls short of meeting the complex and diverse capability requirements of real-world embodied environments. In this vision paper, we introduce Federated Foundation Models (FFMs) for embodied AI, a new paradigm that unifies the strengths of multi-modal multi-task (M3T) FMs with the privacy-preserving distributed nature of FL, enabling intelligent systems at the wireless edge. We collect critical deployment dimensions of FFMs in embodied AI ecosystems under a unified framework, which we name "EMBODY": Embodiment heterogeneity, Modality richness and imbalance, Bandwidth and compute constraints, On-device continual learning, Distributed control and autonomy, and Yielding safety, privacy, and personalization. For each, we identify concrete challenges and envision actionable research directions. We also present an evaluation framework for deploying FFMs in embodied AI systems, along with the associated trade-offs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.6 -->
                    
                <!-- LLMs: 8.6 -->
                    
                <!-- Hardware: 2.6 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Datasets: 2.0 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3
                </span>
                <a href="https://arxiv.org/abs/2505.03049" target="_blank" rel="noopener noreferrer">34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yoel Zimmermann, Adib Bazgir, Alexander Al-Feghali, Mehrad Ansari, Joshua Bocarsly, L. Catherine Brinson, Yuan Chiang, Defne Circi, Min-Hsueh Chiu, Nathan Daelman, Matthew L. Evans, Abhijeet S. Gangan, Janine George, Hassan Harb, Ghazal Khalighinejad, Sartaaj Takrim Khan, Sascha Klawohn, Magdalena Lederbauer, Soroush Mahjoubi, Bernadette Mohr, Seyed Mohamad Moosavi, Aakash Naik, Aleyna Beste Ozhan, Dieter Plessers, Aritra Roy, Fabian Sch\"oppach, Philippe Schwaller, Carla Terboven, Katharina Ueltzen, Yue Wu, Shang Zhu, Jan Janssen, Calvin Li, Ian Foster, Ben Blaiszik
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large Language Models (LLMs) are reshaping many aspects of materials science and chemistry research, enabling advances in molecular property prediction, materials design, scientific automation, knowledge extraction, and more. Recent developments demonstrate that the latest class of models are able t</span>
                
                <span class="abstract-full" style="display: none;">Large Language Models (LLMs) are reshaping many aspects of materials science and chemistry research, enabling advances in molecular property prediction, materials design, scientific automation, knowledge extraction, and more. Recent developments demonstrate that the latest class of models are able to integrate structured and unstructured data, assist in hypothesis generation, and streamline research workflows. To explore the frontier of LLM capabilities across the research lifecycle, we review applications of LLMs through 34 total projects developed during the second annual Large Language Model Hackathon for Applications in Materials Science and Chemistry, a global hybrid event. These projects spanned seven key research areas: (1) molecular and material property prediction, (2) molecular and material design, (3) automation and novel interfaces, (4) scientific communication and education, (5) research data management and automation, (6) hypothesis generation and evaluation, and (7) knowledge extraction and reasoning from the scientific literature. Collectively, these applications illustrate how LLMs serve as versatile predictive models, platforms for rapid prototyping of domain-specific tools, and much more. In particular, improvements in both open source and proprietary LLM performance through the addition of reasoning, additional training data, and new techniques have expanded effectiveness, particularly in low-data environments and interdisciplinary research. As LLMs continue to improve, their integration into scientific workflows presents both new opportunities and new challenges, requiring ongoing exploration, continued refinement, and further research to address reliability, interpretability, and reproducibility.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 40.9 -->
                    
                <!-- Medicine: 6.3 -->
                    
                <!-- Datasets: 2.7 -->
                    
                <!-- Blockchain: 2.3 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.4065
                </span>
                <a href="https://arxiv.org/abs/2505.10806" target="_blank" rel="noopener noreferrer">RapidGNN: Communication Efficient Large-Scale Distributed Training of Graph Neural Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Arefin Niam, M S Q Zulkar Nine
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA) performance in diverse domains. However, training GNNs on large-scale graphs poses significant challenges due to high memory demands and significant communication overhead in distributed settings. Traditional sampling-based approache</span>
                
                <span class="abstract-full" style="display: none;">Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA) performance in diverse domains. However, training GNNs on large-scale graphs poses significant challenges due to high memory demands and significant communication overhead in distributed settings. Traditional sampling-based approaches mitigate computation load to some extent but often fail to address communication inefficiencies inherent in distributed environments. This paper presents RapidGNN that introduces a deterministic sampling strategy to precompute mini-batches. By leveraging the sampling strategy, RapidGNN accurately anticipates feature access patterns, enabling optimal cache construction and timely prefetching of remote features. This reduces the frequency and latency of remote data transfers without compromising the stochastic nature of training. Evaluations on Reddit and OGBN-Products datasets demonstrate that RapidGNN achieves significant reductions in training time and remote feature fetches, outperforming existing models in both communication efficiency and throughput. Our findings highlight RapidGNN's potential for scalable, high-performance GNN training across large, real-world graph datasets along with improving energy efficiency. Our model improves end-to-end training throughput by 2.10x on average over SOTA model GraphSAGE-METIS (up to 2.45x in some settings), while cutting remote feature fetches by over 4x. It also reduces energy consumption up to 23%.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- GNN: 8.7 -->
                    
                <!-- LLMs: 7.7 -->
                    
                <!-- Computer Vision: 2.8 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Datasets: 1.8 -->
                    
                <!-- Hardware: 1.7 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.5235
                </span>
                <a href="https://arxiv.org/abs/2505.10584" target="_blank" rel="noopener noreferrer">Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Huafeng Shi, Jianzhong Liang, Rongchang Xie, Xian Wu, Cheng Chen, Chang Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This report introduces Aquarius, a family of industry-level video generation models for marketing scenarios designed for thousands-xPU clusters and models with hundreds of billions of parameters. Leveraging efficient engineering architecture and algorithmic innovation, Aquarius demonstrates exceptio</span>
                
                <span class="abstract-full" style="display: none;">This report introduces Aquarius, a family of industry-level video generation models for marketing scenarios designed for thousands-xPU clusters and models with hundreds of billions of parameters. Leveraging efficient engineering architecture and algorithmic innovation, Aquarius demonstrates exceptional performance in high-fidelity, multi-aspect-ratio, and long-duration video synthesis. By disclosing the framework's design details, we aim to demystify industrial-scale video generation systems and catalyze advancements in the generative video community. The Aquarius framework consists of five components: Distributed Graph and Video Data Processing Pipeline: Manages tens of thousands of CPUs and thousands of xPUs via automated task distribution, enabling efficient video data processing. Additionally, we are about to open-source the entire data processing framework named "Aquarius-Datapipe". Model Architectures for Different Scales: Include a Single-DiT architecture for 2B models and a Multimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios, multi-resolution, and multi-duration video generation. High-Performance infrastructure designed for video generation model training: Incorporating hybrid parallelism and fine-grained memory optimization strategies, this infrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference Acceleration: Utilizes diffusion cache and attention optimization to achieve a 2.35x inference speedup. Multiple marketing-scenarios applications: Including image-to-video, text-to-video (avatar), video inpainting and video personalization, among others. More downstream applications and multi-dimensional evaluation metrics will be added in the upcoming version updates.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.3 -->
                    
                <!-- LLMs: 7.1 -->
                    
                <!-- Hardware: 3.7 -->
                    
                <!-- Datasets: 2.6 -->
                    
                <!-- Computer Vision: 1.8 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.637
                </span>
                <a href="https://arxiv.org/abs/2505.10960" target="_blank" rel="noopener noreferrer">Relational Graph Transformer</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vijay Prakash Dwivedi, Sri Jaladi, Yangyi Shen, Federico L\'opez, Charilaos I. Kanatsoulis, Rishi Puri, Matthias Fey, Jure Leskovec
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Relational Deep Learning (RDL) is a promising approach for building state-of-the-art predictive models on multi-table relational data by representing it as a heterogeneous temporal graph. However, commonly used Graph Neural Network models suffer from fundamental limitations in capturing complex stru</span>
                
                <span class="abstract-full" style="display: none;">Relational Deep Learning (RDL) is a promising approach for building state-of-the-art predictive models on multi-table relational data by representing it as a heterogeneous temporal graph. However, commonly used Graph Neural Network models suffer from fundamental limitations in capturing complex structural patterns and long-range dependencies that are inherent in relational data. While Graph Transformers have emerged as powerful alternatives to GNNs on general graphs, applying them to relational entity graphs presents unique challenges: (i) Traditional positional encodings fail to generalize to massive, heterogeneous graphs; (ii) existing architectures cannot model the temporal dynamics and schema constraints of relational data; (iii) existing tokenization schemes lose critical structural information. Here we introduce the Relational Graph Transformer (RelGT), the first graph transformer architecture designed specifically for relational tables. RelGT employs a novel multi-element tokenization strategy that decomposes each node into five components (features, type, hop distance, time, and local structure), enabling efficient encoding of heterogeneity, temporality, and topology without expensive precomputation. Our architecture combines local attention over sampled subgraphs with global attention to learnable centroids, incorporating both local and database-wide representations. Across 21 tasks from the RelBench benchmark, RelGT consistently matches or outperforms GNN baselines by up to 18%, establishing Graph Transformers as a powerful architecture for Relational Deep Learning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- GNN: 9.5 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- HPO and AutoML: 1.6 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Datasets: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.7546
                </span>
                <a href="https://arxiv.org/abs/2505.11168" target="_blank" rel="noopener noreferrer">CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xinran Li, Yu Liu, Xiujuan Xu, Xiaowei Zhao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The automatic diagnosis of chest diseases is a popular and challenging task. Most current methods are based on convolutional neural networks (CNNs), which focus on local features while neglecting global features. Recently, self-attention mechanisms have been introduced into the field of computer vis</span>
                
                <span class="abstract-full" style="display: none;">The automatic diagnosis of chest diseases is a popular and challenging task. Most current methods are based on convolutional neural networks (CNNs), which focus on local features while neglecting global features. Recently, self-attention mechanisms have been introduced into the field of computer vision, demonstrating superior performance. Therefore, this paper proposes an effective model, CheX-DS, for classifying long-tail multi-label data in the medical field of chest X-rays. The model is based on the excellent CNN model DenseNet for medical imaging and the newly popular Swin Transformer model, utilizing ensemble deep learning techniques to combine the two models and leverage the advantages of both CNNs and Transformers. The loss function of CheX-DS combines weighted binary cross-entropy loss with asymmetric loss, effectively addressing the issue of data imbalance. The NIH ChestX-ray14 dataset is selected to evaluate the model's effectiveness. The model outperforms previous studies with an excellent average AUC score of 83.76\%, demonstrating its superior performance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.3 -->
                    
                <!-- Federated Learning: 3.8 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Computer Vision: 2.0 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Datasets: 1.7 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Bayesian Optimization: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.7964
                </span>
                <a href="https://arxiv.org/abs/2301.09192" target="_blank" rel="noopener noreferrer">Lower Bounds on Learning Pauli Channels with Individual Measurements</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Omar Fawzi, Aadil Oufkir, Daniel Stilck Fran\c{c}a
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Understanding the noise affecting a quantum device is of fundamental importance for scaling quantum technologies. A particularly important class of noise models is that of Pauli channels, as randomized compiling techniques can effectively bring any quantum channel to this form and are significantly </span>
                
                <span class="abstract-full" style="display: none;">Understanding the noise affecting a quantum device is of fundamental importance for scaling quantum technologies. A particularly important class of noise models is that of Pauli channels, as randomized compiling techniques can effectively bring any quantum channel to this form and are significantly more structured than general quantum channels. In this paper, we show fundamental lower bounds on the sample complexity for learning Pauli channels in diamond norm. We consider strategies that may not use auxiliary systems entangled with the input to the unknown channel and have to perform a measurement before reusing the channel. For non-adaptive algorithms, we show a lower bound of $\Omega(2^{3n}\varepsilon^{-2})$ to learn an $n$-qubit Pauli channel. In particular, this shows that the recently introduced learning procedure by Flammia and Wallman is essentially optimal. In the adaptive setting, we show a lower bound of $\Omega(2^{2.5n}\varepsilon^{-2})$ for $\varepsilon=\mathcal{O}(2^{-n})$, and a lower bound of $\Omega(2^{2n}\varepsilon^{-2} )$ for any $\varepsilon> 0$. This last lower bound holds even in a stronger model where in each step, before performing the measurement, the unknown channel may be used arbitrarily many times sequentially interspersed with unital operations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 5.7 -->
                    
                <!-- LLMs: 2.4 -->
                    
                <!-- Medicine: 2.3 -->
                    
                <!-- Federated Learning: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Hardware: 1.1 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Networks: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8212
                </span>
                <a href="https://arxiv.org/abs/2505.10823" target="_blank" rel="noopener noreferrer">From Embeddings to Accuracy: Comparing Foundation Models for Radiographic Classification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xue Li, Jameson Merkow, Noel C. F. Codella, Alberto Santamaria-Pang, Naiteek Sangani, Alexander Ersoy, Christopher Burt, John W. Garrett, Richard J. Bruce, Joshua D. Warner, Tyler Bradshaw, Ivan Tarapov, Matthew P. Lungren, Alan B. McMillan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Foundation models, pretrained on extensive datasets, have significantly advanced machine learning by providing robust and transferable embeddings applicable to various domains, including medical imaging diagnostics. This study evaluates the utility of embeddings derived from both general-purpose and</span>
                
                <span class="abstract-full" style="display: none;">Foundation models, pretrained on extensive datasets, have significantly advanced machine learning by providing robust and transferable embeddings applicable to various domains, including medical imaging diagnostics. This study evaluates the utility of embeddings derived from both general-purpose and medical domain-specific foundation models for training lightweight adapter models in multi-class radiography classification, focusing specifically on tube placement assessment. A dataset comprising 8842 radiographs classified into seven distinct categories was employed to extract embeddings using six foundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight, Rad-DINO, and CXR-Foundation. Adapter models were subsequently trained using classical machine learning algorithms. Among these combinations, MedImageInsight embeddings paired with an support vector machine adapter yielded the highest mean area under the curve (mAUC) at 93.8%, followed closely by Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP and DenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%, respectively, whereas Med-Flamingo delivered the lowest performance at 75.1%. Notably, most adapter models demonstrated computational efficiency, achieving training within one minute and inference within seconds on CPU, underscoring their practicality for clinical applications. Furthermore, fairness analyses on adapters trained on MedImageInsight-derived embeddings indicated minimal disparities, with gender differences in performance within 2% and standard deviations across age groups not exceeding 3%. These findings confirm that foundation model embeddings-especially those from MedImageInsight-facilitate accurate, computationally efficient, and equitable diagnostic classification using lightweight adapters for radiographic image analysis.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.6 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Computer Vision: 4.3 -->
                    
                <!-- HPO and AutoML: 2.3 -->
                    
                <!-- Decision Trees: 2.1 -->
                    
                <!-- Hardware: 1.9 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Datasets: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8359
                </span>
                <a href="https://arxiv.org/abs/2505.04207" target="_blank" rel="noopener noreferrer">An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mustafa Yurdakul, \c{S}akir Tasdemir
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Potholes cause vehicle damage and traffic accidents, creating serious safety and economic problems. Therefore, early and accurate detection of potholes is crucial. Existing detection methods are usually only based on 2D RGB images and cannot accurately analyze the physical characteristics of pothole</span>
                
                <span class="abstract-full" style="display: none;">Potholes cause vehicle damage and traffic accidents, creating serious safety and economic problems. Therefore, early and accurate detection of potholes is crucial. Existing detection methods are usually only based on 2D RGB images and cannot accurately analyze the physical characteristics of potholes. In this paper, a publicly available dataset of RGB-D images (PothRGBD) is created and an improved YOLOv8-based model is proposed for both pothole detection and pothole physical features analysis. The Intel RealSense D415 depth camera was used to collect RGB and depth data from the road surfaces, resulting in a PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg architecture, which is structurally improved with Dynamic Snake Convolution (DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit (GELU). The proposed model segmented potholes with irregular edge structure more accurately, and performed perimeter and depth measurements on depth maps with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision, 85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to 93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model performs pothole detection as well as perimeter and depth measurement with high accuracy and is suitable for real-time applications due to its low model complexity. In this way, a lightweight and effective model that can be used in deep learning-based intelligent transportation solutions has been acquired.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.8 -->
                    
                <!-- Computer Vision: 5.7 -->
                    
                <!-- Datasets: 3.5 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- Decision Trees: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8933
                </span>
                <a href="https://arxiv.org/abs/2505.11356" target="_blank" rel="noopener noreferrer">Fractal Graph Contrastive Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nero Z. Li, Xuehao Zhai, Zhichao Shi, Boshen Shi, Xuhui Jiang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While Graph Contrastive Learning (GCL) has attracted considerable attention in the field of graph self-supervised learning, its performance heavily relies on data augmentations that are expected to generate semantically consistent positive pairs. Existing strategies typically resort to random pertur</span>
                
                <span class="abstract-full" style="display: none;">While Graph Contrastive Learning (GCL) has attracted considerable attention in the field of graph self-supervised learning, its performance heavily relies on data augmentations that are expected to generate semantically consistent positive pairs. Existing strategies typically resort to random perturbations or local structure preservation, yet lack explicit control over global structural consistency between augmented views. To address this limitation, we propose Fractal Graph Contrastive Learning (FractalGCL), a theory-driven framework that leverages fractal self-similarity to enforce global topological coherence. FractalGCL introduces two key innovations: a renormalisation-based augmentation that generates structurally aligned positive views via box coverings; and a fractal-dimension-aware contrastive loss that aligns graph embeddings according to their fractal dimensions. While combining the two innovations markedly boosts graph-representation quality, it also adds non-trivial computational overhead. To mitigate the computational overhead of fractal dimension estimation, we derive a one-shot estimator by proving that the dimension discrepancy between original and renormalised graphs converges weakly to a centred Gaussian distribution. This theoretical insight enables a reduction in dimension computation cost by an order of magnitude, cutting overall training time by approximately 61%. The experiments show that FractalGCL not only delivers state-of-the-art results on standard benchmarks but also outperforms traditional baselines on traffic networks by an average margin of about remarkably 7%. Codes are available at (https://anonymous.4open.science/r/FractalGCL-0511).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- GNN: 10.1 -->
                    
                <!-- Federated Learning: 4.6 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.0 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Medicine: 1.5 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.0746
                </span>
                <a href="https://arxiv.org/abs/2505.10603" target="_blank" rel="noopener noreferrer">Toward a Public and Secure Generative AI: A Comparative Analysis of Open and Closed LLMs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jorge Machado
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generative artificial intelligence (Gen AI) systems represent a critical technology with far-reaching implications across multiple domains of society. However, their deployment entails a range of risks and challenges that require careful evaluation. To date, there has been a lack of comprehensive, i</span>
                
                <span class="abstract-full" style="display: none;">Generative artificial intelligence (Gen AI) systems represent a critical technology with far-reaching implications across multiple domains of society. However, their deployment entails a range of risks and challenges that require careful evaluation. To date, there has been a lack of comprehensive, interdisciplinary studies offering a systematic comparison between open-source and proprietary (closed) generative AI systems, particularly regarding their respective advantages and drawbacks. This study aims to: i) critically evaluate and compare the characteristics, opportunities, and challenges of open and closed generative AI models; and ii) propose foundational elements for the development of an Open, Public, and Safe Gen AI framework. As a methodology, we adopted a combined approach that integrates three methods: literature review, critical analysis, and comparative analysis. The proposed framework outlines key dimensions, openness, public governance, and security, as essential pillars for shaping the future of trustworthy and inclusive Gen AI. Our findings reveal that open models offer greater transparency, auditability, and flexibility, enabling independent scrutiny and bias mitigation. In contrast, closed systems often provide better technical support and ease of implementation, but at the cost of unequal access, accountability, and ethical oversight. The research also highlights the importance of multi-stakeholder governance, environmental sustainability, and regulatory frameworks in ensuring responsible development.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 13.0 -->
                    
                <!-- Medicine: 11.8 -->
                    
                <!-- Blockchain: 2.9 -->
                    
                <!-- Hardware: 2.8 -->
                    
                <!-- Datasets: 2.6 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                <!-- Quantum Computing: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.0791
                </span>
                <a href="https://arxiv.org/abs/2503.24121" target="_blank" rel="noopener noreferrer">IMPACT: A Generic Semantic Loss for Multimodal Medical Image Registration</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Valentin Boussot, C\'edric H\'emon, Jean-Claude Nunes, Jason Dowling, Simon Rouz\'e, Caroline Lafond, Ana\"is Barateau, Jean-Louis Dillenseger
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Image registration is fundamental in medical imaging, enabling precise alignment of anatomical structures for diagnosis, treatment planning, image-guided interventions, and longitudinal monitoring. This work introduces IMPACT (Image Metric with Pretrained model-Agnostic Comparison for Transmodality </span>
                
                <span class="abstract-full" style="display: none;">Image registration is fundamental in medical imaging, enabling precise alignment of anatomical structures for diagnosis, treatment planning, image-guided interventions, and longitudinal monitoring. This work introduces IMPACT (Image Metric with Pretrained model-Agnostic Comparison for Transmodality registration), a novel similarity metric designed for robust multimodal image registration. Rather than relying on raw intensities, handcrafted descriptors, or task-specific training, IMPACT defines a semantic similarity measure based on the comparison of deep features extracted from large-scale pretrained segmentation models. By leveraging representations from models such as TotalSegmentator, Segment Anything (SAM), and other foundation networks, IMPACT provides a task-agnostic, training-free solution that generalizes across imaging modalities. These features, originally trained for segmentation, offer strong spatial correspondence and semantic alignment capabilities, making them naturally suited for registration. The method integrates seamlessly into both algorithmic (Elastix) and learning-based (VoxelMorph) frameworks, leveraging the strengths of each. IMPACT was evaluated on five challenging 3D registration tasks involving thoracic CT/CBCT and pelvic MR/CT datasets. Quantitative metrics, including Target Registration Error and Dice Similarity Coefficient, demonstrated consistent improvements in anatomical alignment over baseline methods. Qualitative analyses further highlighted the robustness of the proposed metric in the presence of noise, artifacts, and modality variations. With its versatility, efficiency, and strong performance across diverse tasks, IMPACT offers a powerful solution for advancing multimodal image registration in both clinical and research settings.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.7 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Hardware: 2.5 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- HPO and AutoML: 1.2 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.0939
                </span>
                <a href="https://arxiv.org/abs/2505.10902" target="_blank" rel="noopener noreferrer">Patient-Specific Dynamic Digital-Physical Twin for Coronary Intervention Training: An Integrated Mixed Reality Approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shuo Wang, Tong Ren, Nan Cheng, Rong Wang, Li Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Background and Objective: Precise preoperative planning and effective physician training for coronary interventions are increasingly important. Despite advances in medical imaging technologies, transforming static or limited dynamic imaging data into comprehensive dynamic cardiac models remains chal</span>
                
                <span class="abstract-full" style="display: none;">Background and Objective: Precise preoperative planning and effective physician training for coronary interventions are increasingly important. Despite advances in medical imaging technologies, transforming static or limited dynamic imaging data into comprehensive dynamic cardiac models remains challenging. Existing training systems lack accurate simulation of cardiac physiological dynamics. This study develops a comprehensive dynamic cardiac model research framework based on 4D-CTA, integrating digital twin technology, computer vision, and physical model manufacturing to provide precise, personalized tools for interventional cardiology. Methods: Using 4D-CTA data from a 60-year-old female with three-vessel coronary stenosis, we segmented cardiac chambers and coronary arteries, constructed dynamic models, and implemented skeletal skinning weight computation to simulate vessel deformation across 20 cardiac phases. Transparent vascular physical models were manufactured using medical-grade silicone. We developed cardiac output analysis and virtual angiography systems, implemented guidewire 3D reconstruction using binocular stereo vision, and evaluated the system through angiography validation and CABG training applications. Results: Morphological consistency between virtual and real angiography reached 80.9%. Dice similarity coefficients for guidewire motion ranged from 0.741-0.812, with mean trajectory errors below 1.1 mm. The transparent model demonstrated advantages in CABG training, allowing direct visualization while simulating beating heart challenges. Conclusion: Our patient-specific digital-physical twin approach effectively reproduces both anatomical structures and dynamic characteristics of coronary vasculature, offering a dynamic environment with visual and tactile feedback valuable for education and clinical planning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 18.9 -->
                    
                <!-- LLMs: 6.4 -->
                    
                <!-- Computer Vision: 2.6 -->
                    
                <!-- Hardware: 2.2 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- HPO and AutoML: 1.7 -->
                    
                <!-- Decision Trees: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.1105
                </span>
                <a href="https://arxiv.org/abs/2505.10592" target="_blank" rel="noopener noreferrer">LizAI XT -- Artificial Intelligence-Powered Platform for Healthcare Data Management: A Study on Clinical Data Mega-Structure, Semantic Search, and Insights of Sixteen Diseases</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Trung Tin Nguyen, Salomon M. Stemmer, David R. Elmaleh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">AI-powered LizAI XT ensures real-time and accurate mega-structure of different clinical datasets and largely inaccessible and fragmented sources, into one comprehensive table or any designated forms, based on diseases, clinical variables, and/or other defined parameters. We evaluate the platform's p</span>
                
                <span class="abstract-full" style="display: none;">AI-powered LizAI XT ensures real-time and accurate mega-structure of different clinical datasets and largely inaccessible and fragmented sources, into one comprehensive table or any designated forms, based on diseases, clinical variables, and/or other defined parameters. We evaluate the platform's performance on a cluster of 4x NVIDIA A30 GPU 24GB, with 16 diseases -- from deathly cancer and COPD, to conventional ones -- ear infections, including a total 16,000 patients, $\sim$115,000 medical files, and $\sim$800 clinical variables. LizAI XT structures data from thousands of files into sets of variables for each disease in one file, achieving >95.0% overall accuracy, while providing exceptional outputs in complicated cases of cancers (99.1%), COPD (98.89%), and asthma (98.12%), without model-overfitting. Data retrieval is sub-second for a variable per patient with a minimal GPU power, which can significantly be improved on more powerful GPUs. LizAI XT uniquely enables fully client-controlled data, complying with strict data security and privacy regulations per region/nation. Our advances complement the existing EMR/EHR, AWS HealthLake, and Google Vertex AI platforms, for healthcare data management and AI development, with large-scalability and expansion at any levels of HMOs, clinics, pharma, and government.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.4 -->
                    
                <!-- LLMs: 4.2 -->
                    
                <!-- Hardware: 3.2 -->
                    
                <!-- Datasets: 2.4 -->
                    
                <!-- Blockchain: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Decision Trees: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- HPO and AutoML: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.6212
                </span>
                <a href="https://arxiv.org/abs/2505.11269" target="_blank" rel="noopener noreferrer">Driving Mechanisms and Forecasting of China's Pet Population-An ARIMA-RF-HW Hybrid Approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shengjia Chang, Xianshuo Yue
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study proposes a dynamically weighted ARIMA-RF-HW hybrid model integrating ARIMA for seasonality and trends, Random Forest for nonlinear features, and Holt-Winters smoothing for seasonal adjustment to improve China's pet population forecasting accuracy. Using 2005-2023 data with nine economic, </span>
                
                <span class="abstract-full" style="display: none;">This study proposes a dynamically weighted ARIMA-RF-HW hybrid model integrating ARIMA for seasonality and trends, Random Forest for nonlinear features, and Holt-Winters smoothing for seasonal adjustment to improve China's pet population forecasting accuracy. Using 2005-2023 data with nine economic, social, and policy indicators (urban income, consumption, aging ratio, policy quantity, new veterinary drug approvals), data were preprocessed via Z-score normalization and missing value imputation. The results show that key drivers of pet populations include urban income (19.48% for cats, 17.15% for dogs), consumption (17.99% for cats), and policy quantity (13.33% for cats, 14.02% for dogs), with aging (12.81% for cats, 13.27% for dogs) and urbanization amplifying the demand for pets. Forecasts show steady cat growth and fluctuating dog numbers, reflecting cats' adaptability to urban environments. This research supports policymakers in optimizing pet health management and guides enterprises in developing differentiated services, advancing sustainable industry growth.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 18.5 -->
                    
                <!-- Hardware: 4.0 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Datasets: 2.2 -->
                    
                <!-- Evolutionary Algorithms: 2.2 -->
                    
                <!-- HPO and AutoML: 1.9 -->
                    
                <!-- Decision Trees: 1.9 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.7933
                </span>
                <a href="https://arxiv.org/abs/2505.10735" target="_blank" rel="noopener noreferrer">From noisy observables to accurate ground state energies: a quantum classical signal subspace approach with denoising</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hardeep Bassi, Yizhi Shen, Harish S. Bhat, Roel Van Beeumen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a hybrid quantum-classical algorithm for ground state energy (GSE) estimation that remains robust to highly noisy data and exhibits low sensitivity to hyperparameter tuning. Our approach -- Fourier Denoising Observable Dynamic Mode Decomposition (FDODMD) -- combines Fourier-based denoisin</span>
                
                <span class="abstract-full" style="display: none;">We propose a hybrid quantum-classical algorithm for ground state energy (GSE) estimation that remains robust to highly noisy data and exhibits low sensitivity to hyperparameter tuning. Our approach -- Fourier Denoising Observable Dynamic Mode Decomposition (FDODMD) -- combines Fourier-based denoising thresholding to suppress spurious noise modes with observable dynamic mode decomposition (ODMD), a quantum-classical signal subspace method. By applying ODMD to an ensemble of denoised time-domain trajectories, FDODMD reliably estimates the system's eigenfrequencies. We also provide an error analysis of FDODMD. Numerical experiments on molecular systems demonstrate that FDODMD achieves convergence in high-noise regimes inaccessible to baseline methods under a limited quantum computational budget, while accelerating spectral estimation in intermediate-noise regimes. Importantly, this performance gain is entirely classical, requiring no additional quantum overhead and significantly reducing overall quantum resource demands.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 8.4 -->
                    
                <!-- LLMs: 4.8 -->
                    
                <!-- Evolutionary Algorithms: 2.5 -->
                    
                <!-- Federated Learning: 2.4 -->
                    
                <!-- HPO and AutoML: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Decision Trees: 1.8 -->
                    
                <!-- Medicine: 1.7 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Computer Vision: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.2821
                </span>
                <a href="https://arxiv.org/abs/2505.11003" target="_blank" rel="noopener noreferrer">ForensicHub: A Unified Benchmark & Codebase for All-Domain Fake Image Detection and Localization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bo Du, Xuekang Zhu, Xiaochen Ma, Chenfan Qu, Kaiwen Feng, Zhe Yang, Chi-Man Pun, Jian Liu, Jizhe Zhou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The field of Fake Image Detection and Localization (FIDL) is highly fragmented, encompassing four domains: deepfake detection (Deepfake), image manipulation detection and localization (IMDL), artificial intelligence-generated image detection (AIGC), and document image manipulation localization (Doc)</span>
                
                <span class="abstract-full" style="display: none;">The field of Fake Image Detection and Localization (FIDL) is highly fragmented, encompassing four domains: deepfake detection (Deepfake), image manipulation detection and localization (IMDL), artificial intelligence-generated image detection (AIGC), and document image manipulation localization (Doc). Although individual benchmarks exist in some domains, a unified benchmark for all domains in FIDL remains blank. The absence of a unified benchmark results in significant domain silos, where each domain independently constructs its datasets, models, and evaluation protocols without interoperability, preventing cross-domain comparisons and hindering the development of the entire FIDL field. To close the domain silo barrier, we propose ForensicHub, the first unified benchmark & codebase for all-domain fake image detection and localization. Considering drastic variations on dataset, model, and evaluation configurations across all domains, as well as the scarcity of open-sourced baseline models and the lack of individual benchmarks in some domains, ForensicHub: i) proposes a modular and configuration-driven architecture that decomposes forensic pipelines into interchangeable components across datasets, transforms, models, and evaluators, allowing flexible composition across all domains; ii) fully implements 10 baseline models, 6 backbones, 2 new benchmarks for AIGC and Doc, and integrates 2 existing benchmarks of DeepfakeBench and IMDLBenCo through an adapter-based design; iii) conducts indepth analysis based on the ForensicHub, offering 8 key actionable insights into FIDL model architecture, dataset characteristics, and evaluation standards. ForensicHub represents a significant leap forward in breaking the domain silos in the FIDL field and inspiring future breakthroughs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.4 -->
                    
                <!-- LLMs: 6.1 -->
                    
                <!-- Computer Vision: 4.5 -->
                    
                <!-- Datasets: 2.7 -->
                    
                <!-- Blockchain: 2.7 -->
                    
                <!-- Hardware: 2.7 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.7382
                </span>
                <a href="https://arxiv.org/abs/2505.10855" target="_blank" rel="noopener noreferrer">Pretrained hybrid transformer for generalizable cardiac substructures segmentation from contrast and non-contrast CTs in lung and breast cancers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aneesh Rangnekar, Nikhil Mankuzhy, Jonas Willmann, Chloe Choi, Abraham Wu, Maria Thor, Andreas Rimner, Harini Veeraraghavan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">AI automated segmentations for radiation treatment planning (RTP) can deteriorate when applied in clinical cases with different characteristics than training dataset. Hence, we refined a pretrained transformer into a hybrid transformer convolutional network (HTN) to segment cardiac substructures lun</span>
                
                <span class="abstract-full" style="display: none;">AI automated segmentations for radiation treatment planning (RTP) can deteriorate when applied in clinical cases with different characteristics than training dataset. Hence, we refined a pretrained transformer into a hybrid transformer convolutional network (HTN) to segment cardiac substructures lung and breast cancer patients acquired with varying imaging contrasts and patient scan positions. Cohort I, consisting of 56 contrast-enhanced (CECT) and 124 non-contrast CT (NCCT) scans from patients with non-small cell lung cancers acquired in supine position, was used to create oracle with all 180 training cases and balanced (CECT: 32, NCCT: 32 training) HTN models. Models were evaluated on a held-out validation set of 60 cohort I patients and 66 patients with breast cancer from cohort II acquired in supine (n=45) and prone (n=21) positions. Accuracy was measured using DSC, HD95, and dose metrics. Publicly available TotalSegmentator served as the benchmark. The oracle and balanced models were similarly accurate (DSC Cohort I: 0.80 \pm 0.10 versus 0.81 \pm 0.10; Cohort II: 0.77 \pm 0.13 versus 0.80 \pm 0.12), outperforming TotalSegmentator. The balanced model, using half the training cases as oracle, produced similar dose metrics as manual delineations for all cardiac substructures. This model was robust to CT contrast in 6 out of 8 substructures and patient scan position variations in 5 out of 8 substructures and showed low correlations of accuracy to patient size and age. A HTN demonstrated robustly accurate (geometric and dose metrics) cardiac substructures segmentation from CTs with varying imaging and patient characteristics, one key requirement for clinical use. Moreover, the model combining pretraining with balanced distribution of NCCT and CECT scans was able to provide reliably accurate segmentations under varied conditions with far fewer labeled datasets compared to an oracle model.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 22.8 -->
                    
                <!-- LLMs: 5.9 -->
                    
                <!-- Datasets: 2.4 -->
                    
                <!-- Computer Vision: 1.9 -->
                    
                <!-- Hardware: 1.8 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- HPO and AutoML: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Decision Trees: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.0212
                </span>
                <a href="https://arxiv.org/abs/2505.03743" target="_blank" rel="noopener noreferrer">Implementation of Shor Algorithm: Factoring a 4096-Bit Integer Under Specific Constraints</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Abel C. H. Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In recent years, advancements in quantum chip technology, such as Willow, have contributed to reducing quantum computation error rates, potentially accelerating the practical adoption of quantum computing. As a result, the design of quantum algorithms suitable for real-world applications has become </span>
                
                <span class="abstract-full" style="display: none;">In recent years, advancements in quantum chip technology, such as Willow, have contributed to reducing quantum computation error rates, potentially accelerating the practical adoption of quantum computing. As a result, the design of quantum algorithms suitable for real-world applications has become a crucial research direction. This study focuses on the implementation of Shor algorithm, aiming to improve modular computation efficiency and demonstrate the factorization of a 4096-bit integer under specific constraints. Experimental results, when compared with state-of-the-art (SOTA) methods, indicate a significant improvement in efficiency while enabling the factorization of longer integers.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 10.5 -->
                    
                <!-- Evolutionary Algorithms: 3.8 -->
                    
                <!-- Federated Learning: 3.0 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- Blockchain: 2.4 -->
                    
                <!-- Hardware: 2.2 -->
                    
                <!-- Bayesian Optimization: 2.0 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Game Theory: 1.1 -->
                    
                <!-- Cryptography: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.7522
                </span>
                <a href="https://arxiv.org/abs/2408.11894" target="_blank" rel="noopener noreferrer">Automated Synthesis of Fault-Tolerant State Preparation Circuits for Quantum Error Correction Codes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tom Peham, Ludwig Schmid, Lucas Berent, Markus M\"uller, Robert Wille
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A central ingredient in fault-tolerant quantum algorithms is the initialization of a logical state for a given quantum error-correcting code from a set of noisy qubits. A scheme that has demonstrated promising results for small code instances that are realizable on currently available hardware compo</span>
                
                <span class="abstract-full" style="display: none;">A central ingredient in fault-tolerant quantum algorithms is the initialization of a logical state for a given quantum error-correcting code from a set of noisy qubits. A scheme that has demonstrated promising results for small code instances that are realizable on currently available hardware composes a non-fault-tolerant state preparation step with a verification step that checks for spreading errors. Known circuit constructions of this scheme are mostly obtained manually, and no algorithmic techniques for constructing depth- or gate-optimal circuits exist. As a consequence, the current state of the art exploits this scheme only for specific code instances and mostly for the special case of distance 3 codes. In this work, we propose an automated approach for synthesizing fault-tolerant state preparation circuits for arbitrary CSS codes. We utilize methods based on satisfiability solving (SAT) techniques to construct fault-tolerant state preparation circuits consisting of depth- and gate-optimal preparation and verification circuits. We also provide heuristics that can synthesize fault-tolerant state preparation circuits for code instances where no optimal solution can be obtained in an adequate timeframe. Moreover, we give a general construction for non-deterministic state preparation circuits beyond distance 3. Numerical evaluations using $d=3$ and $d=5$ codes confirm that the generated circuits exhibit the desired scaling of the logical error rates. The resulting methods are publicly available as part of the Munich Quantum Toolkit (MQT) at https://github.com/cda-tum/mqt-qecc. Such methods are an important step in providing fault-tolerant circuit constructions that can aid in near-term demonstration of fault-tolerant quantum computing.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 7.6 -->
                    
                <!-- Medicine: 4.5 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- LLMs: 2.2 -->
                    
                <!-- Hardware: 2.1 -->
                    
                <!-- Datasets: 1.9 -->
                    
                <!-- Bayesian Optimization: 1.8 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.5305
                </span>
                <a href="https://arxiv.org/abs/2504.16116" target="_blank" rel="noopener noreferrer">DMind Benchmark: Toward a Holistic Assessment of LLM Capabilities across the Web3 Domain</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Enhao Huang, Pengyu Sun, Zixin Lin, Alex Chen, Joey Ouyang, Hobert Wang, Dong Dong, Gang Zhao, James Yi, Frank Li, Ziang Ling, Lowes Yang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large Language Models (LLMs) have achieved impressive performance in diverse natural language processing tasks, but specialized domains such as Web3 present new challenges and require more tailored evaluation. Despite the significant user base and capital flows in Web3, encompassing smart contracts,</span>
                
                <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have achieved impressive performance in diverse natural language processing tasks, but specialized domains such as Web3 present new challenges and require more tailored evaluation. Despite the significant user base and capital flows in Web3, encompassing smart contracts, decentralized finance (DeFi), non-fungible tokens (NFTs), decentralized autonomous organizations (DAOs), on-chain governance, and novel token-economics, no comprehensive benchmark has systematically assessed LLM performance in this domain. To address this gap, we introduce the DMind Benchmark, a holistic Web3-oriented evaluation suite covering nine critical subfields: fundamental blockchain concepts, blockchain infrastructure, smart contract, DeFi mechanisms, DAOs, NFTs, token economics, meme concept, and security vulnerabilities. Beyond multiple-choice questions, DMind Benchmark features domain-specific tasks such as contract debugging and on-chain numeric reasoning, mirroring real-world scenarios. We evaluated 26 models, including ChatGPT, Claude, DeepSeek, Gemini, Grok, and Qwen, uncovering notable performance gaps in specialized areas like token economics and security-critical contract analysis. While some models excel in blockchain infrastructure tasks, advanced subfields remain challenging. Our benchmark dataset and evaluation pipeline are open-sourced on https://huggingface.co/datasets/DMindAI/DMind_Benchmark, reaching number one in Hugging Face's trending dataset charts within a week of release.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 21.3 -->
                    
                <!-- Blockchain: 5.9 -->
                    
                <!-- Datasets: 2.7 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- Medicine: 2.2 -->
                    
                <!-- Hardware: 2.0 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- HPO and AutoML: 1.4 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Decision Trees: 1.2 -->
                    
                <!-- Quantum Computing: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -8.3565
                </span>
                <a href="https://arxiv.org/abs/2505.11362" target="_blank" rel="noopener noreferrer">Channel coding against quantum jammers via minimax</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Michael X. Cao, Yongsheng Yao, Mario Berta
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce a minimax approach for characterizing the capacities of fully quantum arbitrarily varying channels (FQAVCs) under different shared resource models. In contrast to previous methods, our technique avoids de Finetti-type reductions, allowing us to treat quantum jammers with infinite-dimens</span>
                
                <span class="abstract-full" style="display: none;">We introduce a minimax approach for characterizing the capacities of fully quantum arbitrarily varying channels (FQAVCs) under different shared resource models. In contrast to previous methods, our technique avoids de Finetti-type reductions, allowing us to treat quantum jammers with infinite-dimensional systems. Consequently, we show that the entanglement-assisted and shared-randomness-assisted capacities of FQAVCs match those of the corresponding compound channels, even in the presence of general quantum adversaries.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 13.6 -->
                    
                <!-- LLMs: 4.6 -->
                    
                <!-- Federated Learning: 2.8 -->
                    
                <!-- Evolutionary Algorithms: 2.6 -->
                    
                <!-- Bayesian Optimization: 2.3 -->
                    
                <!-- Medicine: 2.2 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Hardware: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Computer Vision: 1.1 -->
                    
                <!-- Decision Trees: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -8.4028
                </span>
                <a href="https://arxiv.org/abs/2504.10400" target="_blank" rel="noopener noreferrer">Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pietro Bonazzi, Christian Vogt, Michael Jost, Lyes Khacef, Federico Paredes-Vall\'es, Michele Magno
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This work quantitatively evaluates the performance of event-based vision systems (EVS) against conventional RGB-based models for action prediction in collision avoidance on an FPGA accelerator. Our experiments demonstrate that the EVS model achieves a significantly higher effective frame rate (1 kHz</span>
                
                <span class="abstract-full" style="display: none;">This work quantitatively evaluates the performance of event-based vision systems (EVS) against conventional RGB-based models for action prediction in collision avoidance on an FPGA accelerator. Our experiments demonstrate that the EVS model achieves a significantly higher effective frame rate (1 kHz) and lower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the RGB-based model, particularly when tested on out-of-distribution data. The EVS model also exhibits superior robustness in selecting optimal evasion maneuvers. In particular, in distinguishing between movement and stationary states, it achieves a 59 percentage point advantage in precision (78% vs. 19%) and a substantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility of the RGB model to overfitting. Further analysis in different combinations of spatial classes confirms the consistent performance of the EVS model in both test data sets. Finally, we evaluated the system end-to-end and achieved a latency of approximately 2.14 ms, with event aggregation (1 ms) and inference on the processing unit (0.94 ms) accounting for the largest components. These results underscore the advantages of event-based vision for real-time collision avoidance and demonstrate its potential for deployment in resource-constrained environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #3c02a3" title="Confidence: 74.9%">
                            Hardware
                        </span>
                <!-- Medicine: 5.8 -->
                    
                <!-- Computer Vision: 2.5 -->
                    
                <!-- LLMs: 2.2 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Bayesian Optimization: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -14.5681
                </span>
                <a href="https://arxiv.org/abs/2411.00105" target="_blank" rel="noopener noreferrer">A Universal Quantum Computer From Relativistic Motion</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Philip A. LeMaitre, T. Rick Perche, Marius Krumm, Hans J. Briegel
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present an explicit construction of a relativistic quantum computing architecture using a variational quantum circuit approach that is shown to allow for universal quantum computing. The variational quantum circuit consists of tunable single-qubit rotations and entangling gates that are implement</span>
                
                <span class="abstract-full" style="display: none;">We present an explicit construction of a relativistic quantum computing architecture using a variational quantum circuit approach that is shown to allow for universal quantum computing. The variational quantum circuit consists of tunable single-qubit rotations and entangling gates that are implemented successively. The single qubit rotations are parameterized by the proper time intervals of the qubits' trajectories and can be tuned by varying their relativistic motion in spacetime. The entangling layer is mediated by a relativistic quantum field instead of through direct coupling between the qubits. Within this setting, we give a prescription for how to use quantum field-mediated entanglement and manipulation of the relativistic motion of qubits to obtain a universal gate set, for which compact non-perturbative expressions that are valid for general spacetimes are also obtained. We also derive a lower bound on the channel fidelity that shows the existence of parameter regimes in which all entangling operations are effectively unitary, despite the noise generated from the presence of a mediating quantum field. Finally, we consider an explicit implementation of the quantum Fourier transform with relativistic qubits.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 23.7 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Medicine: 2.2 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Bayesian Optimization: 1.7 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -16.7679
                </span>
                <a href="https://arxiv.org/abs/2505.11025" target="_blank" rel="noopener noreferrer">Generalization Bounds for Quantum Learning via R\'enyi Divergences</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Naqueeb Ahmad Warsi, Ayanava Dasgupta, Masahito Hayashi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This work advances the theoretical understanding of quantum learning by establishing a new family of upper bounds on the expected generalization error of quantum learning algorithms, leveraging the framework introduced by Caro et al. (2024) and a new definition for the expected true loss. Our primar</span>
                
                <span class="abstract-full" style="display: none;">This work advances the theoretical understanding of quantum learning by establishing a new family of upper bounds on the expected generalization error of quantum learning algorithms, leveraging the framework introduced by Caro et al. (2024) and a new definition for the expected true loss. Our primary contribution is the derivation of these bounds in terms of quantum and classical R\'enyi divergences, utilizing a variational approach for evaluating quantum R\'enyi divergences, specifically the Petz and a newly introduced modified sandwich quantum R\'enyi divergence. Analytically and numerically, we demonstrate the superior performance of the bounds derived using the modified sandwich quantum R\'enyi divergence compared to those based on the Petz divergence. Furthermore, we provide probabilistic generalization error bounds using two distinct techniques: one based on the modified sandwich quantum R\'enyi divergence and classical R\'enyi divergence, and another employing smooth max R\'enyi divergence.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 21.0 -->
                    
                <!-- Medicine: 4.0 -->
                    
                <!-- Federated Learning: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Evolutionary Algorithms: 2.3 -->
                    
                <!-- Hardware: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Computer Vision: 1.5 -->
                    
                <!-- Bayesian Optimization: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -20.9702
                </span>
                <a href="https://arxiv.org/abs/2503.21686" target="_blank" rel="noopener noreferrer">Molecular Quantum Transformer</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yuichi Kamata, Quoc Hoan Tran, Yasuhiro Endo, Hirotaka Oshima
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The Transformer model, renowned for its powerful attention mechanism, has achieved state-of-the-art performance in various artificial intelligence tasks but faces challenges such as high computational cost and memory usage. Researchers are exploring quantum computing to enhance the Transformer's des</span>
                
                <span class="abstract-full" style="display: none;">The Transformer model, renowned for its powerful attention mechanism, has achieved state-of-the-art performance in various artificial intelligence tasks but faces challenges such as high computational cost and memory usage. Researchers are exploring quantum computing to enhance the Transformer's design, though it still shows limited success with classical data. With a growing focus on leveraging quantum machine learning for quantum data, particularly in quantum chemistry, we propose the Molecular Quantum Transformer (MQT) for modeling interactions in molecular quantum systems. By utilizing quantum circuits to implement the attention mechanism on the molecular configurations, MQT can efficiently calculate ground-state energies for all configurations. Numerical demonstrations show that in calculating ground-state energies for H2, LiH, BeH2, and H4, MQT outperforms the classical Transformer, highlighting the promise of quantum effects in Transformer structures. Furthermore, its pretraining capability on diverse molecular data facilitates the efficient learning of new molecules, extending its applicability to complex molecular systems with minimal additional effort. Our method offers an alternative to existing quantum algorithms for estimating ground-state energies, opening new avenues in quantum chemistry and materials science.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 31.5 -->
                    
                <!-- Computer Vision: 2.4 -->
                    
                <!-- Evolutionary Algorithms: 2.4 -->
                    
                <!-- Medicine: 2.3 -->
                    
                <!-- Hardware: 2.2 -->
                    
                <!-- LLMs: 1.9 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- HPO and AutoML: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Decision Trees: 1.0 -->
                    
                
            </div>
        </div>
        
    </div>
    
    
    <div id="jsonPopup" class="json-popup">
        <pre id="jsonContent"></pre>
        <button onclick="copyJson()">Copy to Clipboard</button>
        <button onclick="closePopup()">Close</button>
    </div>

    <script>
        function extractPaperData(paperElement) {
            const titleElement = paperElement.querySelector('.paper-title a');
            const metaElement = paperElement.querySelector('.paper-meta');
            const abstractElement = paperElement.querySelector('.paper-abstract');
            const tagsElement = paperElement.querySelector('.paper-tags');
            
            // Get the date from the parent date-section header
            const dateSection = paperElement.closest('.date-section');
            const dateText = dateSection.querySelector('.date-header').textContent.trim();
            
            const authorsText = metaElement.textContent.replace('Authors:', '').trim();
            
            const paperData = {
                title: titleElement.textContent,
                url: titleElement.href,
                authors: authorsText.split(',').map(author => author.trim()),
                created: dateText,
                abstract: abstractElement.querySelector('.abstract-full').textContent
            };
            
            return paperData;
        }

        function showJson(paperElement) {
            const popup = document.getElementById('jsonPopup');
            const content = document.getElementById('jsonContent');
            const paperData = extractPaperData(paperElement);
            content.textContent = JSON.stringify(paperData, null, null);
            popup.style.display = 'block';
            document.addEventListener('click', function closePopupOnClick(event) {
                if (!popup.contains(event.target)) {
                    popup.style.display = 'none';
                    document.removeEventListener('click', closePopupOnClick);
                }
            });
        }
        function toggleAbstract(element) {
            const abstract = element.parentElement;
            const short = abstract.querySelector('.abstract-short');
            const full = abstract.querySelector('.abstract-full');
            const lowConfidenceTags = abstract.parentElement.querySelectorAll('.tag-badge.low-confidence');
            
            if (element.textContent === '... more') {
                short.style.display = 'none';
                full.style.display = 'inline';
                element.textContent = ' less';
                lowConfidenceTags.forEach(tag => tag.style.display = 'inline-block');
            } else {
                short.style.display = 'inline';
                full.style.display = 'none';
                element.textContent = '... more';
                lowConfidenceTags.forEach(tag => tag.style.display = 'none');
            }
        }

        function closePopup() {
            document.getElementById('jsonPopup').style.display = 'none';
        }

        function copyJson() {
            const content = document.getElementById('jsonContent').textContent;
            navigator.clipboard.writeText(content).catch(() => {
                // If clipboard API is not available, just show the popup
                alert('Could not copy to clipboard. JSON is displayed in the popup.');
            });
        }

        // Close popup when clicking outside
        window.onclick = function(event) {
            const popup = document.getElementById('jsonPopup');
            if (event.target === popup) {
                popup.style.display = 'none';
            }
        }
    </script>
</body>
</html> 