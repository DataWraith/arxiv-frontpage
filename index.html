<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Frontpage</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .paper {
            margin-bottom: 30px;
            margin-top: 30px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .paper-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .paper-meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        .paper-abstract {
            margin-bottom: 10px;
        }
        .abstract-short {
            display: inline;
        }
        .abstract-full {
            display: none;
        }
        .more-link {
            color: blue;
            cursor: pointer;
            text-decoration: underline;
        }
        .tag-badge {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 5px;
            margin-bottom: 5px;
            border-radius: 3px;
            font-size: 0.8em;
            color: white;
        }
        .tag-badge.high-confidence {
            opacity: 1;
        }
        .tag-badge.low-confidence {
            opacity: 0.6;
            display: none;
        }
        .interestingness-score {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 10px;
            color: white;
            border-radius: 3px;
            font-weight: bold;
        }
        .interestingness-positive {
            background-color: #4CAF50;
        }
        .interestingness-negative {
            background-color: #f44336;
        }
        .last-updated {
            text-align: right;
            color: #666;
            font-size: 0.9em;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .intro {
            text-align: center;
            max-width: 60em;
            margin: 0 auto;
            color: #888;
        }
        .copy-icon {
            display: inline-block;
            width: 16px;
            height: 16px;
            cursor: pointer;
            margin-left: 5px;
            opacity: 0.5;
        }
        .copy-icon:hover {
            opacity: 1;
        }
        .json-popup {
            display: none;
            position: fixed;
            top: 20px;
            right: 20px;
            background: white;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            max-width: 500px;
            max-height: 300px;
            overflow: auto;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        a {
            color: inherit;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h1 {
            text-align: center;
        }
        h1 a {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>
        <a href="https://github.com/DataWraith/arxiv-frontpage">DataWraith's</a> ArXiv Frontpage
    </h1>

    <div class="last-updated">
        Last updated: 2025-04-16
    </div>

    <p class="intro">
        This frontpage is made by scraping ArXiv's computer science RSS feed and tagging papers with a classifier.
    </p>

    <p class="intro">
        Each tag is weighted according to my preferences to compute a paper's <i>interestingness</i> score.
    </p>
    
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                49.9425
            </span>
            <a href="https://arxiv.org/abs/2504.08359" target="_blank" rel="noopener noreferrer">Kernel-Level Energy-Efficient Neural Architecture Search for Tabular Dataset</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hoang-Loc La, Phuong Hoai Ha | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neur</span>
            
            <span class="abstract-full" style="display: none;">Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neural Architecture Search (NAS) method that directly focuses on identifying architectures that minimize energy consumption while maintaining acceptable accuracy. Unlike previous methods that primarily target vision and language tasks, the approach proposed here specifically addresses tabular datasets. Remarkably, the optimal architecture suggested by this method can reduce energy consumption by up to 92% compared to architectures recommended by conventional NAS.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #19d3a1" title="Confidence: 58.2%">
                        HPO and AutoML
                    </span>
            <!-- LLMs: 17.8 -->
                
            <!-- RAG: 2.6 -->
                
            <!-- 3D: 2.3 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Finance: 1.2 -->
                
            <!-- Bayesian Optimization: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                48.323
            </span>
            <a href="https://arxiv.org/abs/2504.08057" target="_blank" rel="noopener noreferrer">Vector Quantized-Elites: Unsupervised and Problem-Agnostic Quality-Diversity Optimization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Constantinos Tsakonas, Konstantinos Chatzilygeroudis | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Quality-Diversity algorithms have transformed optimization by prioritizing the discovery of diverse, high-performing solutions over a single optimal result. However, traditional Quality-Diversity methods, such as MAP-Elites, rely heavily on predefined behavioral descriptors and complete prior knowle</span>
            
            <span class="abstract-full" style="display: none;">Quality-Diversity algorithms have transformed optimization by prioritizing the discovery of diverse, high-performing solutions over a single optimal result. However, traditional Quality-Diversity methods, such as MAP-Elites, rely heavily on predefined behavioral descriptors and complete prior knowledge of the task to define the behavioral space grid, limiting their flexibility and applicability. In this work, we introduce Vector Quantized-Elites (VQ-Elites), a novel Quality-Diversity algorithm that autonomously constructs a structured behavioral space grid using unsupervised learning, eliminating the need for prior task-specific knowledge. At the core of VQ-Elites is the integration of Vector Quantized Variational Autoencoders, which enables the dynamic learning of behavioral descriptors and the generation of a structured, rather than unstructured, behavioral space grid - a significant advancement over existing unsupervised Quality-Diversity approaches. This design establishes VQ-Elites as a flexible, robust, and task-agnostic optimization framework. To further enhance the performance of unsupervised Quality-Diversity algorithms, we introduce two key components: behavioral space bounding and cooperation mechanisms, which significantly improve convergence and performance. We validate VQ-Elites on robotic arm pose-reaching and mobile robot space-covering tasks. The results demonstrate its ability to efficiently generate diverse, high-quality solutions, emphasizing its adaptability, scalability, robustness to hyperparameters, and potential to extend Quality-Diversity optimization to complex, previously inaccessible domains.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Quality Diversity: 48.9 -->
                
            <!-- LLMs: 11.3 -->
                
            <!-- Medicine: 4.2 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Blockchain: 1.7 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                13.2827
            </span>
            <a href="https://arxiv.org/abs/2504.09415" target="_blank" rel="noopener noreferrer">Nash Equilibrium Between Consumer Electronic Devices and DoS Attacker for Distributed IoT-enabled RSE Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Gengcan Chen, Donghong Cai, Zahid Khan, Jawad Ahmad, Wadii Boulila | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between cons</span>
            
            <span class="abstract-full" style="display: none;">In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between consumer electronic devices and DoS attackers is critical. This paper focuses on the adversarial strategy between consumer electronic devices and DoS attackers in IoT-enabled RSE Systems. We first propose a remote joint estimation model for distributed measurements to effectively reduce consumer electronic device workload and minimize data leakage risks. The Kalman filter is deployed on the remote estimator, and the DoS attacks with open-loop as well as closed-loop are considered. We further introduce advanced reinforcement learning techniques, including centralized and distributed Minimax-DQN, to address high-dimensional decision-making challenges in both open-loop and closed-loop scenarios. Especially, the Q-network instead of the Q-table is used in the proposed approaches, which effectively solves the challenge of Q-learning. Moreover, the proposed distributed Minimax-DQN reduces the action space to expedite the search for Nash Equilibrium (NE). The experimental results validate that the proposed model can expeditiously restore the RSE error covariance to a stable state in the presence of DoS attacks, exhibiting notable attack robustness. The proposed centralized and distributed Minimax-DQN effectively resolves the NE in both open and closed-loop case, showcasing remarkable performance in terms of convergence. It reveals that substantial advantages in both efficiency and stability are achieved compared with the state-of-the-art methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 77.4%">
                        Reinforcement Learning
                    </span>
            <span class="tag-badge high-confidence" style="background-color: #546bc5" title="Confidence: 75.9%">
                        Game Theory
                    </span>
            <!-- Medicine: 6.0 -->
                
            <!-- LLMs: 3.0 -->
                
            <!-- Robotics: 2.9 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                11.6369
            </span>
            <a href="https://arxiv.org/abs/2504.01332" target="_blank" rel="noopener noreferrer">When to Truncate the Archive? On the Effect of the Truncation Frequency in Multi-Objective Optimisation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhiji Cui, Zimin Liang, Lie Meng Pang, Hisao Ishibuchi, Miqing Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Using an archive to store nondominated solutions found during the search of a multi-objective evolutionary algorithm (MOEA) is a useful practice. However, as nondominated solutions of a multi-objective optimisation problem can be enormous or infinitely many, it is desirable to provide the decision-m</span>
            
            <span class="abstract-full" style="display: none;">Using an archive to store nondominated solutions found during the search of a multi-objective evolutionary algorithm (MOEA) is a useful practice. However, as nondominated solutions of a multi-objective optimisation problem can be enormous or infinitely many, it is desirable to provide the decision-maker with only a small, representative portion of all the nondominated solutions in the archive, thus entailing a truncation operation. Then, an important issue is when to truncate the archive. This can be done once a new solution generated, a batch of new solutions generated, or even using an unbounded archive to keep all nondominated solutions generated and truncate it later. Intuitively, the last approach may lead to a better result since we have all the information in hand before performing the truncation. In this paper, we study this issue and investigate the effect of the timing of truncating the archive. We apply well-established truncation criteria that are commonly used in the population maintenance procedure of MOEAs (e.g., crowding distance, hypervolume indicator, and decomposition). We show that, interestingly, truncating the archive once a new solution generated tends to be the best, whereas considering an unbounded archive is often the worst. We analyse and discuss this phenomenon. Our results highlight the importance of developing effective subset selection techniques (rather than employing the population maintenance methods in MOEAs) when using a large archive.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #ae668e" title="Confidence: 76.2%">
                        Evolutionary Algorithms
                    </span>
            <!-- Reinforcement Learning: 3.9 -->
                
            <!-- Math: 3.2 -->
                
            <!-- LLMs: 2.8 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Medicine: 2.1 -->
                
            <!-- Quantum Computing: 1.8 -->
                
            <!-- Pathfinding: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                7.1057
            </span>
            <a href="https://arxiv.org/abs/2409.11267" target="_blank" rel="noopener noreferrer">Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Caio Fabio Oliveira da Silva, Azita Dabiri, Bart De Schutter | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work proposes an approach that integrates reinforcement learning and model predictive control (MPC) to solve finite-horizon optimal control problems in mixed-logical dynamical systems efficiently. Optimization-based control of such systems with discrete and continuous decision variables entails</span>
            
            <span class="abstract-full" style="display: none;">This work proposes an approach that integrates reinforcement learning and model predictive control (MPC) to solve finite-horizon optimal control problems in mixed-logical dynamical systems efficiently. Optimization-based control of such systems with discrete and continuous decision variables entails the online solution of mixed-integer linear programs, which suffer from the curse of dimensionality. Our approach aims to mitigate this issue by decoupling the decision on the discrete variables from the decision on the continuous variables. In the proposed approach, reinforcement learning determines the discrete decision variables and simplifies the online optimization problem of the MPC controller from a mixed-integer linear program to a linear program, significantly reducing the computational time. A fundamental contribution of this work is the definition of the decoupled Q-function, which plays a crucial role in making the learning problem tractable in a combinatorial action space. We motivate the use of recurrent neural networks to approximate the decoupled Q-function and show how they can be employed in a reinforcement learning setting. Simulation experiments on a microgrid system using real-world data demonstrate that the proposed method substantially reduces the online computation time of MPC while maintaining high feasibility and low suboptimality.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 80.6%">
                        Reinforcement Learning
                    </span>
            <!-- Medicine: 3.3 -->
                
            <!-- Math: 2.6 -->
                
            <!-- LLMs: 2.0 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                7.1007
            </span>
            <a href="https://arxiv.org/abs/2504.09035" target="_blank" rel="noopener noreferrer">InterQ: A DQN Framework for Optimal Intermittent Control</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shubham Aggarwal, Dipankar Maity, Tamer Ba\c{s}ar | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this letter, we explore the communication-control co-design of discrete-time stochastic linear systems through reinforcement learning. Specifically, we examine a closed-loop system involving two sequential decision-makers: a scheduler and a controller. The scheduler continuously monitors the syst</span>
            
            <span class="abstract-full" style="display: none;">In this letter, we explore the communication-control co-design of discrete-time stochastic linear systems through reinforcement learning. Specifically, we examine a closed-loop system involving two sequential decision-makers: a scheduler and a controller. The scheduler continuously monitors the system's state but transmits it to the controller intermittently to balance the communication cost and control performance. The controller, in turn, determines the control input based on the intermittently received information. Given the partially nested information structure, we show that the optimal control policy follows a certainty-equivalence form. Subsequently, we analyze the qualitative behavior of the scheduling policy. To develop the optimal scheduling policy, we propose InterQ, a deep reinforcement learning algorithm which uses a deep neural network to approximate the Q-function. Through extensive numerical evaluations, we analyze the scheduling landscape and further compare our approach against two baseline strategies: (a) a multi-period periodic scheduling policy, and (b) an event-triggered policy. The results demonstrate that our proposed method outperforms both baselines. The open source implementation can be found at https://github.com/AC-sh/InterQ.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 76.2%">
                        Reinforcement Learning
                    </span>
            <!-- Medicine: 4.2 -->
                
            <!-- LLMs: 2.6 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Federated Learning: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Multi-armed Bandit: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                6.9174
            </span>
            <a href="https://arxiv.org/abs/2504.08534" target="_blank" rel="noopener noreferrer">Genetic Algorithm Design Exploration for On-Device Training on FPGAs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alaa Mazouz, Van-Tam Nguyen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose an automated Design Space Exploration (DSE) workflow for generating adaptive and reconfigurable deep learning models on FPGA hardware. The workflow consists of two main components: Offline Design Exploration (ODE) and Online Design Reconfiguration (ODR). ODE applies a multi-objective gene</span>
            
            <span class="abstract-full" style="display: none;">We propose an automated Design Space Exploration (DSE) workflow for generating adaptive and reconfigurable deep learning models on FPGA hardware. The workflow consists of two main components: Offline Design Exploration (ODE) and Online Design Reconfiguration (ODR). ODE applies a multi-objective genetic algorithm to explore CNN-based hardware configurations, optimizing for latency and resource utilization by leveraging intra-layer parallelism. Given a CNN architecture and user-defined constraints, the hardware model is generated automatically. ODR enables runtime hardware adaptability by dynamically selecting between partial or full reconfigurable designs based on application requirements. This flexibility is essential for time-critical, autonomous onboard systems. We demonstrate the proposed workflow on the Xilinx Zynq-7100 FPGA operating at 200 MHz, using CNN models trained on MNIST, SVHN, and CIFAR-10. ODE-generated designs show latency improvements of up to 95 times for MNIST, 71 times for CIFAR-10, and 18 times for SVHN. Resource utilization in DSP slices was improved by up to 44 times for MNIST, 52 times for SVHN, and 24 times for CIFAR-10. The ODR approach achieved trade-offs between accuracy and performance, such as a 0.7 percent accuracy drop for a 13 times speedup and 25 percent power reduction on MNIST, a 2 percent drop for 14 times speedup and 28 percent power savings on SVHN, and a 4 percent drop for 50 times speedup with 32.5 percent power reduction on CIFAR-10.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #ae668e" title="Confidence: 72.0%">
                        Evolutionary Algorithms
                    </span>
            <!-- Medicine: 15.5 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- LLMs: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                6.4345
            </span>
            <a href="https://arxiv.org/abs/2504.08943" target="_blank" rel="noopener noreferrer">Investigating the Treacherous Turn in Deep Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chace Ashcraft, Kiran Karra, Josh Carney, Nathan Drenkow | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by </span>
            
            <span class="abstract-full" style="display: none;">The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by the human supervisor, but when deployed to perform its task, it performs an alternate behavior without the supervisor there to prevent it. Initial experiments applying DRL to an implementation of the A Link to the Past example do not produce the treacherous turn effect naturally, despite various modifications to the environment intended to produce it. However, in this work, we find the treacherous behavior to be reproducible in a DRL agent when using other trojan injection strategies. This approach deviates from the prototypical treacherous turn behavior since the behavior is explicitly trained into the agent, rather than occurring as an emergent consequence of environmental complexity or poor objective specification. Nonetheless, these experiments provide new insights into the challenges of producing agents capable of true treacherous turn behavior.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #44f899" title="Confidence: 77.3%">
                        Reinforcement Learning
                    </span>
            <!-- LLMs: 4.7 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Medicine: 2.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Attention: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                5.8703
            </span>
            <a href="https://arxiv.org/abs/2504.08667" target="_blank" rel="noopener noreferrer">Faster shortest-path algorithms using the acyclic-connected tree</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Elis Stefansson, Oliver Biggar, Karl H. Johansson | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper gives a fixed-parameter linear algorithm for the single-source shortest path problem (SSSP) on directed graphs. The parameter in question is the nesting width, a measure of the extent to which a graph can be represented as a nested collection of graphs. We present a novel directed graph d</span>
            
            <span class="abstract-full" style="display: none;">This paper gives a fixed-parameter linear algorithm for the single-source shortest path problem (SSSP) on directed graphs. The parameter in question is the nesting width, a measure of the extent to which a graph can be represented as a nested collection of graphs. We present a novel directed graph decomposition called the acyclic-connected tree (A-C tree), which breaks the graph into a recursively nested sequence of strongly connected components in topological order. We prove that the A-C tree is optimal in the sense that its width, the size of the largest nested graph, is equal to the nesting width of the graph. We then provide a linear-time algorithm for constructing the A-C tree of any graph. Finally, we show how the A-C tree allows us to construct a simple variant of Dijkstra's algorithm which achieves a time complexity of $O(e+n\log w)$, where $n$ ($e$) is the number of nodes (arcs) in the graph and $w$ is the nesting width. The idea is to apply the shortest path algorithm separately to each component in the order dictated by the A-C tree. We obtain an asymptotic improvement over Dijkstra's algorithm: when $w=n$, our algorithm reduces to Dijkstra's algorithm, but it is faster when $w \in o(n)$, and linear-time for classes of graphs with bounded width, such as directed acyclic graphs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #5caaa5" title="Confidence: 80.7%">
                        Pathfinding
                    </span>
            <!-- Reinforcement Learning: 4.1 -->
                
            <!-- Math: 4.1 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Quantum Computing: 1.8 -->
                
            <!-- Medicine: 1.3 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                4.9372
            </span>
            <a href="https://arxiv.org/abs/2504.05108" target="_blank" rel="noopener noreferrer">Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, Caglar Gulcehre | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating t</span>
            
            <span class="abstract-full" style="display: none;">Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #ae668e" title="Confidence: 69.8%">
                        Evolutionary Algorithms
                    </span>
            <!-- LLMs: 38.1 -->
                
            <!-- Medicine: 4.7 -->
                
            <!-- Reinforcement Learning: 3.8 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Federated Learning: 2.5 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                4.1852
            </span>
            <a href="https://arxiv.org/abs/2504.03784" target="_blank" rel="noopener noreferrer">Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, Chengchung Shi | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preference</span>
            
            <span class="abstract-full" style="display: none;">Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #44f899" title="Confidence: 69.6%">
                        Reinforcement Learning
                    </span>
            <!-- LLMs: 11.4 -->
                
            <!-- Medicine: 5.8 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Federated Learning: 2.1 -->
                
            <!-- Math: 2.1 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                3.8057
            </span>
            <a href="https://arxiv.org/abs/2412.14865" target="_blank" rel="noopener noreferrer">Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anthony Kobanda, R\'emy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Su</span>
            
            <span class="abstract-full" style="display: none;">We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Such issues prevail in autonomous robotics and video game simulations, notably for navigation tasks prone to topological or kinematic changes. To address these issues, we introduce HiSPO, a novel hierarchical framework designed specifically for continual learning in navigation settings from offline data. Our method leverages distinct policy subspaces of neural networks to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like navigation simulations, showcasing competitive performances and satisfying adaptability with respect to classical continual learning metrics, in particular regarding the memory usage and efficiency.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #44f899" title="Confidence: 70.6%">
                        Reinforcement Learning
                    </span>
            <!-- Medicine: 7.7 -->
                
            <!-- LLMs: 6.4 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Math: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                2.7683
            </span>
            <a href="https://arxiv.org/abs/2504.09716" target="_blank" rel="noopener noreferrer">Dominated Actions in Imperfect-Information Games</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sam Ganzfried | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Na</span>
            
            <span class="abstract-full" style="display: none;">Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Nash equilibrium. For imperfect-information games in extensive form, we could convert the game to strategic form and then iteratively remove dominated strategies in the same way; however, this conversion may cause an exponential blowup in game size. In this paper we define and study the concept of dominated actions in imperfect-information games. Our main result is a polynomial-time algorithm for determining whether an action is dominated (strictly or weakly) by any mixed strategy in n-player games, which can be extended to an algorithm for iteratively removing dominated actions. This allows us to efficiently reduce the size of the game tree as a preprocessing step for Nash equilibrium computation. We explore the role of dominated actions empirically in the "All In or Fold" No-Limit Texas Hold'em poker variant.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #546bc5" title="Confidence: 74.7%">
                        Game Theory
                    </span>
            <!-- Medicine: 3.4 -->
                
            <!-- LLMs: 3.4 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Pathfinding: 2.2 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Evolutionary Algorithms: 1.9 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -1.6481
            </span>
            <a href="https://arxiv.org/abs/2409.04663" target="_blank" rel="noopener noreferrer">On pattern formation in the thermodynamically-consistent variational Gray-Scott model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wenrui Hao, Chun Liu, Yiwei Wang, Yahong Yang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we explore pattern formation in a four-species variational Gary-Scott model, which includes all reverse reactions and introduces a virtual species to describe the birth-death process in the classical Gray-Scott model. This modification transforms the classical Gray-Scott model into a </span>
            
            <span class="abstract-full" style="display: none;">In this paper, we explore pattern formation in a four-species variational Gary-Scott model, which includes all reverse reactions and introduces a virtual species to describe the birth-death process in the classical Gray-Scott model. This modification transforms the classical Gray-Scott model into a thermodynamically consistent closed system. The classical two-species Gray-Scott model can be viewed as a subsystem of the variational model in the limiting case when the small parameter $\epsilon$, related to the reaction rate of the reverse reactions, approaches zero. We numerically explore pattern formation in this physically more complete Gray-Scott model in one spatial dimension, using non-uniform steady states of the classical model as initial conditions. By decreasing $\epsilon$, we observed that the stationary pattern in the classical Gray-Scott model can be stabilized as the transient state in the variational model for a significantly small $\epsilon$. Additionally, the variational model admits oscillating and traveling-wave-like patterns for small $\epsilon$. The persistent time of these patterns is on the order of $O(\epsilon^{-1})$. We also analyze the energy stability of two uniform steady states in the variational Gary-Scott model for fixed $\epsilon$. Although both states are stable in a certain sense, the gradient flow type dynamics of the variational model exhibit a selection effect based on the initial conditions, with pattern formation occurring only if the initial condition does not converge to the boundary steady state, which corresponds to the trivial uniform steady state in the classical Gray-Scott model.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.6 -->
                
            <!-- Math: 3.7 -->
                
            <!-- Federated Learning: 3.4 -->
                
            <!-- Medicine: 3.0 -->
                
            <!-- Robotics: 2.8 -->
                
            <!-- Pathfinding: 2.8 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Quantum Computing: 1.5 -->
                
            <!-- LLMs: 1.2 -->
                
            <!-- Game Theory: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -1.6727
            </span>
            <a href="https://arxiv.org/abs/2411.09525" target="_blank" rel="noopener noreferrer">Data-driven parameterization refinement for the structural optimization of cruise ship hulls</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lorenzo Fabris, Marco Tezzele, Ciro Busiello, Mauro Sicchiero, Gianluigi Rozza | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this work, we focus on the early design phase of cruise ship hulls, where the designers are tasked with ensuring the structural resilience of the ship against extreme waves while reducing steel usage and respecting safety and manufacturing constraints. At this stage the geometry of the ship is al</span>
            
            <span class="abstract-full" style="display: none;">In this work, we focus on the early design phase of cruise ship hulls, where the designers are tasked with ensuring the structural resilience of the ship against extreme waves while reducing steel usage and respecting safety and manufacturing constraints. At this stage the geometry of the ship is already finalized and the designer choose the thickness of the primary structural elements, such as decks, bulkheads, and the shell. Reduced order modeling and black-box optimization techniques reduce the use of expensive finite element analysis to only validate the most promising configurations, thanks to the efficient exploration of the domain of decision variables. However, the quality of the final results heavily relies on the problem formulation, and on how the structural elements are assigned to the decision variables. With the increased request for alternative fuels and engine technologies, the designers are often faced with novel configurations and risk producing ill-suited parameterizations. To address this issue, we enhanced a structural optimization pipeline for cruise ships developed in collaboration with Fincantieri S.p.A. with a novel data-driven hierarchical reparameterization procedure, based on the optimization of a series of sub-problems. Moreover, we implemented a multi-objective optimization module to provide the designers with insights into the efficient trade-offs between competing quantities of interest and enhanced the single-objective Bayesian optimization module. The new pipeline is tested on a simplified midship section and a full ship hull, comparing the automated reparameterization to a baseline model provided by the designers. The tests show that the iterative refinement outperforms the baseline, thus streamlining the initial design phase and helping tackle more innovative projects.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.1 -->
                
            <!-- Reinforcement Learning: 5.7 -->
                
            <!-- Math: 4.3 -->
                
            <!-- Federated Learning: 2.3 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- LLMs: 1.6 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Networks: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -1.7236
            </span>
            <a href="https://arxiv.org/abs/2504.10850" target="_blank" rel="noopener noreferrer">How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Meiqi Liu, Zhuoqun Huang, Yue Xing | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">With the rise of powerful foundation models, a pre-training-fine-tuning paradigm becomes increasingly popular these days: A foundation model is pre-trained using a huge amount of data from various sources, and then the downstream users only need to fine-tune and adapt it to specific downstream tasks</span>
            
            <span class="abstract-full" style="display: none;">With the rise of powerful foundation models, a pre-training-fine-tuning paradigm becomes increasingly popular these days: A foundation model is pre-trained using a huge amount of data from various sources, and then the downstream users only need to fine-tune and adapt it to specific downstream tasks. However, due to the high computation complexity of adversarial training, it is not feasible to fine-tune the foundation model to improve its robustness on the downstream task. Observing the above challenge, we want to improve the downstream robustness without updating/accessing the weights in the foundation model. Inspired from existing literature in robustness inheritance (Kim et al., 2020), through theoretical investigation, we identify a close relationship between robust contrastive learning with the adversarial robustness of supervised learning. To further validate and utilize this theoretical insight, we design a simple-yet-effective robust auto-encoder as a data pre-processing method before feeding the data into the foundation model. The proposed approach has zero access to the foundation model when training the robust auto-encoder. Extensive experiments demonstrate the effectiveness of the proposed method in improving the robustness of downstream tasks, verifying the connection between the feature robustness (implied by small adversarial contrastive loss) and the robustness of the downstream task.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.5 -->
                
            <!-- Medicine: 4.3 -->
                
            <!-- Federated Learning: 3.5 -->
                
            <!-- LLMs: 3.4 -->
                
            <!-- Math: 2.6 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Quantum Computing: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -1.9428
            </span>
            <a href="https://arxiv.org/abs/2501.01383" target="_blank" rel="noopener noreferrer">Electrical networks and data analysis in phylogenetics</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: V. Gorbounov, A. Kazakov | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A classic problem in data analysis is studying the systems of subsets defined by either a similarity or a dissimilarity function on $X$ which is either observed directly or derived from a data set. For an electrical network there are two functions on the set of the nodes defined by the resistance ma</span>
            
            <span class="abstract-full" style="display: none;">A classic problem in data analysis is studying the systems of subsets defined by either a similarity or a dissimilarity function on $X$ which is either observed directly or derived from a data set. For an electrical network there are two functions on the set of the nodes defined by the resistance matrix and the response matrix either of which defines the network completely. We argue that these functions should be viewed as a similarity and a dissimilarity function on the set of the nodes moreover they are related via the covariance mapping also known as the Farris transform or the Gromov product. We will explore the properties of electrical networks from this point of view. It has been known for a while that the resistance matrix defines a metric on the nodes of the electrical networks. Moreover for a circular electrical network this metric obeys the Kalmanson property as it was shown recently. We will call such a metric an electrical Kalmanson metric. The main results of this paper is a complete description of the electrical Kalmanson metrics in the set of all Kalmanson metrics in terms of the geometry of the positive Isotropic Grassmannian whose connection to the theory of electrical networks was discovered earlier. One important area of applications where Kalmanson metrics are actively used is the theory of phylogenetic networks which are a generalization of phylogenetic trees. Our results allow us to use in phylogenetics the powerful methods of reconstruction of the minimal graphs of electrical networks and possibly open the door into data analysis for the methods of the theory of cluster algebras.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 7.9 -->
                
            <!-- Reinforcement Learning: 4.7 -->
                
            <!-- Medicine: 2.4 -->
                
            <!-- Pathfinding: 2.4 -->
                
            <!-- Robotics: 2.3 -->
                
            <!-- Quantum Computing: 1.4 -->
                
            <!-- Networks: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- LLMs: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.0166
            </span>
            <a href="https://arxiv.org/abs/2407.17182" target="_blank" rel="noopener noreferrer">A DeepONet for inverting the Neumann-to-Dirichlet Operator in Electrical Impedance Tomography: An approximation theoretic perspective and numerical results</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anuj Abhishek, Thilo Strauss | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this work, we consider the non-invasive medical imaging modality of Electrical Impedance Tomography, where the problem is to recover the conductivity in a medium from a set of data that arises out of a current-to-voltage map (Neumann-to-Dirichlet operator) defined on the boundary of the medium. W</span>
            
            <span class="abstract-full" style="display: none;">In this work, we consider the non-invasive medical imaging modality of Electrical Impedance Tomography, where the problem is to recover the conductivity in a medium from a set of data that arises out of a current-to-voltage map (Neumann-to-Dirichlet operator) defined on the boundary of the medium. We formulate this inverse problem as an operator-learning problem where the goal is to learn the implicitly defined operator-to-function map between the space of Neumann-to-Dirichlet operators to the space of admissible conductivities. Subsequently, we use an operator-learning architecture, popularly called DeepONets, to learn this operator-to-function map. Thus far, most of the operator learning architectures have been implemented to learn operators between function spaces. In this work, we generalize the earlier works and use a DeepONet to actually {learn an operator-to-function} map. We provide a Universal Approximation Theorem type result which guarantees that this implicitly defined operator-to-function map between the space of Neumann-to-Dirichlet operator to the space of conductivity function can be approximated to an arbitrary degree using such a DeepONet. Furthermore, we provide a computational implementation of our proposed approach and compare it against a standard baseline. We show that the proposed approach achieves good reconstructions and outperforms the baseline method in our experiments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.5 -->
                
            <!-- LLMs: 4.4 -->
                
            <!-- Medicine: 2.8 -->
                
            <!-- Math: 2.6 -->
                
            <!-- Quantum Computing: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.0313
            </span>
            <a href="https://arxiv.org/abs/2404.10550" target="_blank" rel="noopener noreferrer">Analytical Approximation of the ELBO Gradient in the Context of the Clutter Problem</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Roumen Nikolaev Popov | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose an analytical solution for approximating the gradient of the Evidence Lower Bound (ELBO) in variational inference problems where the statistical model is a Bayesian network consisting of observations drawn from a mixture of a Gaussian distribution embedded in unrelated clutter, known as t</span>
            
            <span class="abstract-full" style="display: none;">We propose an analytical solution for approximating the gradient of the Evidence Lower Bound (ELBO) in variational inference problems where the statistical model is a Bayesian network consisting of observations drawn from a mixture of a Gaussian distribution embedded in unrelated clutter, known as the clutter problem. The method employs the reparameterization trick to move the gradient operator inside the expectation and relies on the assumption that, because the likelihood factorizes over the observed data, the variational distribution is generally more compactly supported than the Gaussian distribution in the likelihood factors. This allows efficient local approximation of the individual likelihood factors, which leads to an analytical solution for the integral defining the gradient expectation. We integrate the proposed gradient approximation as the expectation step in an EM (Expectation Maximization) algorithm for maximizing ELBO and test against classical deterministic approaches in Bayesian inference, such as the Laplace approximation, Expectation Propagation and Mean-Field Variational Inference. The proposed method demonstrates good accuracy and rate of convergence together with linear computational complexity.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 4.5 -->
                
            <!-- Medicine: 4.3 -->
                
            <!-- Reinforcement Learning: 4.3 -->
                
            <!-- Robotics: 2.6 -->
                
            <!-- Pathfinding: 2.6 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- LLMs: 1.8 -->
                
            <!-- Quantum Computing: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.0869
            </span>
            <a href="https://arxiv.org/abs/2504.11056" target="_blank" rel="noopener noreferrer">A study of troubled-cell indicators applied to finite volume methods using a novel monotonicity parameter</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: R. Shivananda Rao, M. Ramakrishna | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We adapt a troubled-cell indicator developed for discontinuous Galerkin (DG) methods to the finite volume method (FVM) framework for solving hyperbolic conservation laws. This indicator depends solely on the cell-average data of the target cell and its immediate neighbours. Once the troubled-cells a</span>
            
            <span class="abstract-full" style="display: none;">We adapt a troubled-cell indicator developed for discontinuous Galerkin (DG) methods to the finite volume method (FVM) framework for solving hyperbolic conservation laws. This indicator depends solely on the cell-average data of the target cell and its immediate neighbours. Once the troubled-cells are identified, we apply the limiter only in these cells instead of applying in all computational cells. We introduce a novel technique to quantify the quality of the solution in the neighbourhood of the shock by defining a monotonicity parameter $\mu$. Numerical results from various two-dimensional simulations on the hyperbolic systems of Euler equations using a finite volume solver employing MUSCL reconstruction validate the performance of the troubled-cell indicator and the approach of limiting only in the troubled-cells. These results show that limiting only in the troubled-cells is preferable to limiting everywhere as it improves convergence without compromising on the solution accuracy.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.6 -->
                
            <!-- Math: 3.7 -->
                
            <!-- Medicine: 3.4 -->
                
            <!-- LLMs: 3.0 -->
                
            <!-- Robotics: 2.3 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Quantum Computing: 2.0 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Pathfinding: 1.9 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.1339
            </span>
            <a href="https://arxiv.org/abs/2504.08801" target="_blank" rel="noopener noreferrer">Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Andrew Kiruluta, Priscilla Burity, Samantha Williams | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Transformer architectures, underpinned by the self-attention mechanism, have achieved state-of-the-art results across numerous natural language processing (NLP) tasks by effectively modeling long-range dependencies. However, the computational complexity of self-attention, scaling quadratically with </span>
            
            <span class="abstract-full" style="display: none;">Transformer architectures, underpinned by the self-attention mechanism, have achieved state-of-the-art results across numerous natural language processing (NLP) tasks by effectively modeling long-range dependencies. However, the computational complexity of self-attention, scaling quadratically with input sequence length, presents significant challenges for processing very long sequences or operating under resource constraints. This paper introduces the Learnable Multi-Scale Wavelet Transformer (LMWT), a novel architecture that replaces the standard dot-product self-attention with a learnable multi-scale Haar wavelet transform module. Leveraging the intrinsic multi-resolution properties of wavelets, the LMWT efficiently captures both local details and global context. Crucially, the parameters of the wavelet transform, including scale-specific coefficients, are learned end-to-end during training, allowing the model to adapt its decomposition strategy to the data and task. We present the detailed mathematical formulation of the learnable Haar wavelet module and its integration into the transformer framework, supplemented by an architectural diagram. We conduct a comprehensive experimental evaluation on a standard machine translation benchmark (WMT16 En-De), comparing the LMWT against a baseline self-attention transformer using metrics like BLEU score, perplexity, and token accuracy. Furthermore, we analyze the computational complexity, highlighting the linear scaling of our approach, discuss its novelty in the context of related work, and explore the interpretability offered by visualizing the learned Haar coefficients. Our results indicate that the LMWT achieves competitive performance while offering substantial computational advantages, positioning it as a promising and novel alternative for efficient sequence modeling.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #2aa97e" title="Confidence: 73.3%">
                        Attention
                    </span>
            <!-- Medicine: 5.2 -->
                
            <!-- Reinforcement Learning: 3.6 -->
                
            <!-- LLMs: 2.9 -->
                
            <!-- Quantum Computing: 2.9 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Federated Learning: 2.3 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.1577
            </span>
            <a href="https://arxiv.org/abs/2503.23600" target="_blank" rel="noopener noreferrer">Online Convex Optimization and Integral Quadratic Constraints: A new approach to regret analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Fabian Jakob, Andrea Iannelli | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose a novel approach for analyzing dynamic regret of first-order constrained online convex optimization algorithms for strongly convex and Lipschitz-smooth objectives. Crucially, we provide a general analysis that is applicable to a wide range of first-order algorithms that can be expressed a</span>
            
            <span class="abstract-full" style="display: none;">We propose a novel approach for analyzing dynamic regret of first-order constrained online convex optimization algorithms for strongly convex and Lipschitz-smooth objectives. Crucially, we provide a general analysis that is applicable to a wide range of first-order algorithms that can be expressed as an interconnection of a linear dynamical system in feedback with a first-order oracle. By leveraging Integral Quadratic Constraints (IQCs), we derive a semi-definite program which, when feasible, provides a regret guarantee for the online algorithm. For this, the concept of variational IQCs is introduced as the generalization of IQCs to time-varying monotone operators. Our bounds capture the temporal rate of change of the problem in the form of the path length of the time-varying minimizer and the objective function variation. In contrast to standard results in OCO, our results do not require nerither the assumption of gradient boundedness, nor that of a bounded feasible set. Numerical analyses showcase the ability of the approach to capture the dependence of the regret on the function class condition number.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 5.5 -->
                
            <!-- LLMs: 4.2 -->
                
            <!-- Math: 3.3 -->
                
            <!-- Medicine: 3.1 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Pathfinding: 1.8 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.1804
            </span>
            <a href="https://arxiv.org/abs/2409.01097" target="_blank" rel="noopener noreferrer">Nested Bregman Iterations for Decomposition Problems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tobias Wolf, Derek Driggs, Kostas Papafitsoros, Elena Resmerita, Carola-Bibiane Sch\"onlieb | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We consider the task of image reconstruction while simultaneously decomposing the reconstructed image into components with different features. A commonly used tool for this is a variational approach with an infimal convolution of appropriate functions as a regularizer. Especially for noise corrupted</span>
            
            <span class="abstract-full" style="display: none;">We consider the task of image reconstruction while simultaneously decomposing the reconstructed image into components with different features. A commonly used tool for this is a variational approach with an infimal convolution of appropriate functions as a regularizer. Especially for noise corrupted observations, incorporating these functionals into the classical method of Bregman iterations provides a robust method for obtaining an overall good approximation of the true image, by stopping early the iteration according to a discrepancy principle. However, crucially, the quality of the separate components depends further on the proper choice of the regularization weights associated to the infimally convoluted functionals. Here, we propose the method of Nested Bregman iterations to improve a decomposition in a structured way. This allows to transform the task of choosing the weights into the problem of stopping the iteration according to a meaningful criterion based on normalized cross-correlation. We discuss the well-definedness and the convergence behavior of the proposed method, and illustrate its strength numerically with various image decomposition tasks employing infimal convolution functionals.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.7 -->
                
            <!-- Medicine: 4.7 -->
                
            <!-- Math: 3.6 -->
                
            <!-- LLMs: 2.2 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.2046
            </span>
            <a href="https://arxiv.org/abs/2504.08698" target="_blank" rel="noopener noreferrer">Performance Evaluation of Trajectory Tracking Controllers for a Quadruped Robot Leg</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hossein Shojaei, Hamid Rahmanei, Seyed Hossein Sadati | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The complexities in the dynamic model of the legged robots make it necessary to utilize model-free controllers in the task of trajectory tracking. In This paper, an adaptive transpose Jacobian approach is proposed to deal with the dynamic model complexity, which utilizes an adaptive PI-algorithm to </span>
            
            <span class="abstract-full" style="display: none;">The complexities in the dynamic model of the legged robots make it necessary to utilize model-free controllers in the task of trajectory tracking. In This paper, an adaptive transpose Jacobian approach is proposed to deal with the dynamic model complexity, which utilizes an adaptive PI-algorithm to adjust the control gains. The performance of the proposed control algorithm is compared with the conventional transpose Jacobian and sliding mode control algorithms and evaluated by the root mean square of the errors and control input energy criteria. In order to appraise the effectiveness of the proposed control system, simulations are carried out in MATLAB/Simulink software for a quadruped robot leg for semi-elliptical path tracking. The obtained results show that the proposed adaptive transpose Jacobian reduces the overshoot and root mean square of the errors and at the same time, decreases the control input energy. Moreover, transpose Jacobin and adaptive transpose Jacobian are more robust to changes in initial conditions compared to the conventional sliding mode control. Furthermore, sliding mode control performs well up to 20% uncertainties in the parameters due to its model-based nature, whereas the transpose Jacobin and the proposed adaptive transpose Jacobian algorithms show promising results even in higher mass uncertainties.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.7 -->
                
            <!-- Medicine: 4.2 -->
                
            <!-- LLMs: 3.3 -->
                
            <!-- Robotics: 2.8 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Quantum Computing: 1.7 -->
                
            <!-- Pathfinding: 1.6 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.2305
            </span>
            <a href="https://arxiv.org/abs/2504.09273" target="_blank" rel="noopener noreferrer">Arnold diffusion in the full three-body problem</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Maciej J. Capinski, Marian Gidea | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We show the existence of Arnold diffusion in the planar full three-body problem, which is expressed as a perturbation of a Kepler problem and a planar circular restricted three-body problem, with the perturbation parameter being the mass of the smallest body. In this context, we obtain Arnold diffus</span>
            
            <span class="abstract-full" style="display: none;">We show the existence of Arnold diffusion in the planar full three-body problem, which is expressed as a perturbation of a Kepler problem and a planar circular restricted three-body problem, with the perturbation parameter being the mass of the smallest body. In this context, we obtain Arnold diffusion in terms of a transfer of energy, in an amount independent of the perturbation parameter, between the Kepler problem and the restricted three-body problem. Our argument is based on a topological method based on correctly aligned windows which is implemented into a computer assisted proof. This approach can be applied to physically relevant masses of the bodies, such as those in a Neptune-Triton-asteroid system. In this case, we obtain explicit estimates for the range of the perturbation parameter and for the diffusion time.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 4.2 -->
                
            <!-- Medicine: 3.9 -->
                
            <!-- Reinforcement Learning: 3.5 -->
                
            <!-- LLMs: 2.5 -->
                
            <!-- Robotics: 2.4 -->
                
            <!-- Pathfinding: 2.2 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Quantum Computing: 1.8 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.2669
            </span>
            <a href="https://arxiv.org/abs/2504.09273" target="_blank" rel="noopener noreferrer">Arnold Diffusion in the Full Three-Body Problem</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Maciej J. Capinski, Marian Gidea | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We show the existence of Arnold diffusion in the planar full three-body problem, which is expressed as a perturbation of a Kepler problem and a planar circular restricted three-body problem, with the perturbation parameter being the mass of the smallest body. In this context, we obtain Arnold diffus</span>
            
            <span class="abstract-full" style="display: none;">We show the existence of Arnold diffusion in the planar full three-body problem, which is expressed as a perturbation of a Kepler problem and a planar circular restricted three-body problem, with the perturbation parameter being the mass of the smallest body. In this context, we obtain Arnold diffusion in terms of a transfer of energy, in an amount independent of the perturbation parameter, between the Kepler problem and the restricted three-body problem. Our argument is based on a topological method based on correctly aligned windows which is implemented into a computer assisted proof. This approach can be applied to physically relevant masses of the bodies, such as those in a Neptune-Triton-asteroid system. In this case, we obtain explicit estimates for the range of the perturbation parameter and for the diffusion time.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 4.1 -->
                
            <!-- Medicine: 3.9 -->
                
            <!-- Reinforcement Learning: 3.5 -->
                
            <!-- LLMs: 2.5 -->
                
            <!-- Robotics: 2.4 -->
                
            <!-- Pathfinding: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Quantum Computing: 1.8 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.3086
            </span>
            <a href="https://arxiv.org/abs/2504.11250" target="_blank" rel="noopener noreferrer">A Rollout-Based Algorithm and Reward Function for Efficient Resource Allocation in Business Processes</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jeroen Middelhuis, Zaharah Bukhsh, Ivo Adan, Remco Dijkman | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Resource allocation plays a critical role in minimizing cycle time and improving the efficiency of business processes. Recently, Deep Reinforcement Learning (DRL) has emerged as a powerful tool to optimize resource allocation policies in business processes. In the DRL framework, an agent learns a po</span>
            
            <span class="abstract-full" style="display: none;">Resource allocation plays a critical role in minimizing cycle time and improving the efficiency of business processes. Recently, Deep Reinforcement Learning (DRL) has emerged as a powerful tool to optimize resource allocation policies in business processes. In the DRL framework, an agent learns a policy through interaction with the environment, guided solely by reward signals that indicate the quality of its decisions. However, existing algorithms are not suitable for dynamic environments such as business processes. Furthermore, existing DRL-based methods rely on engineered reward functions that approximate the desired objective, but a misalignment between reward and objective can lead to undesired decisions or suboptimal policies. To address these issues, we propose a rollout-based DRL algorithm and a reward function to optimize the objective directly. Our algorithm iteratively improves the policy by evaluating execution trajectories following different actions. Our reward function directly decomposes the objective function of minimizing the mean cycle time. Maximizing our reward function guarantees that the objective function is minimized without requiring extensive reward engineering. The results show that our method consistently learns the optimal policy in all six evaluated business processes, outperforming the state-of-the-art algorithm that can only learn the optimal policy in two of the evaluated processes.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.7 -->
                
            <!-- LLMs: 4.2 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Medicine: 2.9 -->
                
            <!-- Math: 2.7 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Multi-armed Bandit: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.4465
            </span>
            <a href="https://arxiv.org/abs/2504.08626" target="_blank" rel="noopener noreferrer">Task-conditioned Ensemble of Expert Models for Continuous Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Renu Sharma, Debasmita Pal, Arun Ross | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed mode</span>
            
            <span class="abstract-full" style="display: none;">One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed model with new data could be one remedy. However, the question arises as to how we should update the model with new training data so that it retains its accuracy on the old data while adapting to the new data. In this work, we propose a task-conditioned ensemble of models to maintain the performance of the existing model. The method involves an ensemble of expert models based on task membership information. The in-domain models-based on the local outlier concept (different from the expert models) provide task membership information dynamically at run-time to each probe sample. To evaluate the proposed method, we experiment with three setups: the first represents distribution shift between tasks (LivDet-Iris-2017), the second represents distribution shift both between and within tasks (LivDet-Iris-2020), and the third represents disjoint distribution between tasks (Split MNIST). The experiments highlight the benefits of the proposed method. The source code is available at https://github.com/iPRoBe-lab/Continuous_Learning_FE_DM.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.1 -->
                
            <!-- Medicine: 5.5 -->
                
            <!-- LLMs: 4.7 -->
                
            <!-- Federated Learning: 4.2 -->
                
            <!-- Math: 3.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Quantum Computing: 1.5 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.4465
            </span>
            <a href="https://arxiv.org/abs/2504.08626" target="_blank" rel="noopener noreferrer">Task-conditioned Ensemble of Expert Models for Continuous Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Renu Sharma, Debasmita Pal, Arun Ross | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed mode</span>
            
            <span class="abstract-full" style="display: none;">One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed model with new data could be one remedy. However, the question arises as to how we should update the model with new training data so that it retains its accuracy on the old data while adapting to the new data. In this work, we propose a task-conditioned ensemble of models to maintain the performance of the existing model. The method involves an ensemble of expert models based on task membership information. The in-domain models-based on the local outlier concept (different from the expert models) provide task membership information dynamically at run-time to each probe sample. To evaluate the proposed method, we experiment with three setups: the first represents distribution shift between tasks (LivDet-Iris-2017), the second represents distribution shift both between and within tasks (LivDet-Iris-2020), and the third represents disjoint distribution between tasks (Split MNIST). The experiments highlight the benefits of the proposed method. The source code is available at https://github.com/iPRoBe-lab/Continuous_Learning_FE_DM.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.1 -->
                
            <!-- Medicine: 5.5 -->
                
            <!-- LLMs: 4.7 -->
                
            <!-- Federated Learning: 4.2 -->
                
            <!-- Math: 3.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Quantum Computing: 1.5 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.5456
            </span>
            <a href="https://arxiv.org/abs/2408.00379" target="_blank" rel="noopener noreferrer">Over-the-Air Diagnosis of Defective Elements in Intelligent Reflecting Surface</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ziyi Zhao, Zhaorui Wang, Lin Zhou, Chunsong Sun, Shuowen Zhang, Naofal Al-Dhahir, Liang Liu | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Due to circuit failures, defective elements that cannot adaptively adjust the phase shifts of their impinging signals in a desired manner may exist on an intelligent reflecting surface (IRS). Traditional way to locate these defective IRS elements requires a thorough diagnosis of all the circuits bel</span>
            
            <span class="abstract-full" style="display: none;">Due to circuit failures, defective elements that cannot adaptively adjust the phase shifts of their impinging signals in a desired manner may exist on an intelligent reflecting surface (IRS). Traditional way to locate these defective IRS elements requires a thorough diagnosis of all the circuits belonging to a huge number of IRS elements, which is practically challenging. In this paper, we will devise novel approaches under which a transmitter sends known pilot signals and a receiver localizes all the defective IRS elements just based on its over-the-air measurements reflected from the IRS. Specifically, given any set of IRS elements, we propose an efficient method to process the received signals to determine whether this cluster contains defective elements or not with a very high accuracy probability. Based on this method, we show that the over-the-air diagnosis problem belongs to the 20 questions problem, where we can adaptively change the query set at the IRS so as to localize all the defective elements as quickly as possible. Along this line, we first propose a sorted posterior matching (sortPM) based method according to the noisy 20 questions technique, which enables accurate diagnosis even if the answers about the existence of defective elements in some sets of interest are wrong at certain question and answer (Q&amp;A) rounds due to the noisy received signals. Next, to reduce the complexity, we propose a bisection based method according to the noiseless 20 questions technique, which totally trusts the answer at each Q&amp;A round and keeps removing half of the remaining region based on such answers. Via numerical results, we show that our proposed methods can exploit the over-the-air measurements to localize all the defective IRS elements quickly and accurately.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 7.2 -->
                
            <!-- LLMs: 4.5 -->
                
            <!-- Math: 2.7 -->
                
            <!-- Federated Learning: 2.6 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Medicine: 1.4 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.5526
            </span>
            <a href="https://arxiv.org/abs/2504.08421" target="_blank" rel="noopener noreferrer">Poisson multi-Bernoulli mixture filter for trajectory measurements</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Marco Fontana, \'Angel F. Garc\'ia-Fern\'andez, Simon Maskell | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper presents a Poisson multi-Bernoulli mixture (PMBM) filter for multi-target filtering based on sensor measurements that are sets of trajectories in the last two-time step window. The proposed filter, the trajectory measurement PMBM (TM-PMBM) filter, propagates a PMBM density on the set of t</span>
            
            <span class="abstract-full" style="display: none;">This paper presents a Poisson multi-Bernoulli mixture (PMBM) filter for multi-target filtering based on sensor measurements that are sets of trajectories in the last two-time step window. The proposed filter, the trajectory measurement PMBM (TM-PMBM) filter, propagates a PMBM density on the set of target states. In prediction, the filter obtains the PMBM density on the set of trajectories over the last two time steps. This density is then updated with the set of trajectory measurements. After the update step, the PMBM posterior on the set of two-step trajectories is marginalised to obtain a PMBM density on the set of target states. The filter provides a closed-form solution for multi-target filtering based on sets of trajectory measurements, estimating the set of target states at the end of each time window. Additionally, the paper proposes computationally lighter alternatives to the TM-PMBM filter by deriving a Poisson multi-Bernoulli (PMB) density through Kullback-Leibler divergence minimisation in an augmented space with auxiliary variables. The performance of the proposed filters are evaluated in a simulation study.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 5.1 -->
                
            <!-- Reinforcement Learning: 4.7 -->
                
            <!-- Medicine: 4.4 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- LLMs: 2.1 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Pathfinding: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.5661
            </span>
            <a href="https://arxiv.org/abs/2504.10932" target="_blank" rel="noopener noreferrer">Multi-scale DeepOnet (Mscale-DeepOnet) for Mitigating Spectral Bias in Learning High Frequency Operators of Oscillatory Functions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Bo Wang, Lizuo Liu, Wei Cai | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, a multi-scale DeepOnet (Mscale-DeepOnet) is proposed to reduce the spectral bias of the DeepOnet in learning high-frequency mapping between highly oscillatory functions, with an application to the nonlinear mapping between the coefficient of the Helmholtz equation and its solution. Th</span>
            
            <span class="abstract-full" style="display: none;">In this paper, a multi-scale DeepOnet (Mscale-DeepOnet) is proposed to reduce the spectral bias of the DeepOnet in learning high-frequency mapping between highly oscillatory functions, with an application to the nonlinear mapping between the coefficient of the Helmholtz equation and its solution. The Mscale-DeepOnet introduces the multiscale neural network in the branch and trunk networks of the original DeepOnet, the resulting Mscale-DeepOnet is shown to be able to capture various high-frequency components of the mapping itself and its image. Numerical results demonstrate the substantial improvement of the Mscale-DeepOnet for the problem of wave scattering in the high-frequency regime over the normal DeepOnet with a similar number of network parameters.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.0 -->
                
            <!-- Reinforcement Learning: 3.9 -->
                
            <!-- Math: 3.4 -->
                
            <!-- LLMs: 3.1 -->
                
            <!-- Robotics: 2.3 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Pathfinding: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Quantum Computing: 1.7 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.5666
            </span>
            <a href="https://arxiv.org/abs/2504.09804" target="_blank" rel="noopener noreferrer">BO-SA-PINNs: Self-adaptive physics-informed neural networks based on Bayesian optimization for automatically designing PDE solvers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rui Zhang, Liang Li, St\'ephane Lanteri, Hao Kang, Jiaqi Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Physics-informed neural networks (PINNs) is becoming a popular alternative method for solving partial differential equations (PDEs). However, they require dedicated manual modifications to the hyperparameters of the network, the sampling methods and loss function weights for different PDEs, which re</span>
            
            <span class="abstract-full" style="display: none;">Physics-informed neural networks (PINNs) is becoming a popular alternative method for solving partial differential equations (PDEs). However, they require dedicated manual modifications to the hyperparameters of the network, the sampling methods and loss function weights for different PDEs, which reduces the efficiency of the solvers. In this paper, we pro- pose a general multi-stage framework, i.e. BO-SA-PINNs to alleviate this issue. In the first stage, Bayesian optimization (BO) is used to select hyperparameters for the training process, and based on the results of the pre-training, the network architecture, learning rate, sampling points distribution and loss function weights suitable for the PDEs are automatically determined. The proposed hyperparameters search space based on experimental results can enhance the efficiency of BO in identifying optimal hyperparameters. After selecting the appropriate hyperparameters, we incorporate a global self-adaptive (SA) mechanism the second stage. Using the pre-trained model and loss information in the second-stage training, the exponential moving average (EMA) method is employed to optimize the loss function weights, and residual-based adaptive refinement with distribution (RAR-D) is used to optimize the sampling points distribution. In the third stage, L-BFGS is used for stable training. In addition, we introduce a new activation function that enables BO-SA-PINNs to achieve higher accuracy. In numerical experiments, we conduct comparative and ablation experiments to verify the performance of the model on Helmholtz, Maxwell, Burgers and high-dimensional Poisson equations. The comparative experiment results show that our model can achieve higher accuracy and fewer iterations in test cases, and the ablation experiments demonstrate the positive impact of every improvement.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.7 -->
                
            <!-- Reinforcement Learning: 4.3 -->
                
            <!-- LLMs: 3.9 -->
                
            <!-- Math: 3.3 -->
                
            <!-- Federated Learning: 2.9 -->
                
            <!-- Quantum Computing: 1.7 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.5708
            </span>
            <a href="https://arxiv.org/abs/2011.11692" target="_blank" rel="noopener noreferrer">NOMA-Based Cooperative Relaying with Receive Diversity in Nakagami-m Fading Channels</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vaibhav Kumar, Barry Cardiff, Mark F Flanagan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Non-orthogonal multiple access (NOMA) is being widely considered as a potential candidate to enhance the spectrum utilization in beyond fifth-generation (B5G) communications. In this paper, we derive closed-form expressions for the ergodic rate and outage probability of a multiple-antenna-assisted N</span>
            
            <span class="abstract-full" style="display: none;">Non-orthogonal multiple access (NOMA) is being widely considered as a potential candidate to enhance the spectrum utilization in beyond fifth-generation (B5G) communications. In this paper, we derive closed-form expressions for the ergodic rate and outage probability of a multiple-antenna-assisted NOMA-based cooperative relaying system (CRS-NOMA). We present the performance analysis of the system for two different receive diversity schemes - selection combining (SC) and maximal-ratio combining (MRC), in Nakagami-m fading. We also evaluate the asymptotic behavior of the CRS-NOMA to determine the slope of the ergodic rate and diversity order. Our results show that in contrast to the existing CRS-NOMA systems, the CRS-NOMA with receive diversity outperforms its orthogonal multiple access (OMA) based counterpart even in the low-SNR regime, by achieving higher ergodic rate. Diversity analysis confirms that the CRS-NOMA achieves full diversity order using both SC and MRC schemes, and this diversity order depends on both the shape parameter m and the number of receive antennas. We also discuss the problem of optimal power allocation for the minimization of the outage probability of the system, and subsequently use this optimal value to obtain the ergodic rate. An excellent match is observed between the numerical and the analytical results, confirming the correctness of the derived analytical expressions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.9 -->
                
            <!-- Reinforcement Learning: 4.0 -->
                
            <!-- LLMs: 3.5 -->
                
            <!-- Math: 3.4 -->
                
            <!-- Robotics: 2.5 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Pathfinding: 1.7 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.5881
            </span>
            <a href="https://arxiv.org/abs/2504.08341" target="_blank" rel="noopener noreferrer">Deep learning-based moment closure for multi-phase computation of semiclassical limit of the Schr\"odinger equation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jin Woo Jang, Jae Yong Lee, Liu Liu, Zhenyi Zhu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a deep learning approach for computing multi-phase solutions to the semiclassical limit of the Schr\"odinger equation. Traditional methods require deriving a multi-phase ansatz to close the moment system of the Liouville equation, a process that is often computationally intensive and impr</span>
            
            <span class="abstract-full" style="display: none;">We present a deep learning approach for computing multi-phase solutions to the semiclassical limit of the Schr\"odinger equation. Traditional methods require deriving a multi-phase ansatz to close the moment system of the Liouville equation, a process that is often computationally intensive and impractical. Our method offers an efficient alternative by introducing a novel two-stage neural network framework to close the $2N\times 2N$ moment system, where $N$ represents the number of phases in the solution ansatz. In the first stage, we train neural networks to learn the mapping between higher-order moments and lower-order moments (along with their derivatives). The second stage incorporates physics-informed neural networks (PINNs), where we substitute the learned higher-order moments to systematically close the system. We provide theoretical guarantees for the convergence of both the loss functions and the neural network approximations. Numerical experiments demonstrate the effectiveness of our method for one- and two-dimensional problems with various phase numbers $N$ in the multi-phase solutions. The results confirm the accuracy and computational efficiency of the proposed approach compared to conventional techniques.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.9 -->
                
            <!-- Medicine: 5.8 -->
                
            <!-- Math: 2.6 -->
                
            <!-- LLMs: 2.6 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.6041
            </span>
            <a href="https://arxiv.org/abs/2504.09458" target="_blank" rel="noopener noreferrer">The Whitney method of fundamental solutions with Lusin wavelets</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jakob Jonsson, Andreas Ros\'en, Emil Timlin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We establish the theoretical foundation for a variant of the method of fundamental solutions (MFS), where the source points $\{q_j\}_{j=1}^\infty$ accumulate towards the domain in a Whitney fashion, meaning that their separation is proportional to the distance to the domain. We prove that the normal</span>
            
            <span class="abstract-full" style="display: none;">We establish the theoretical foundation for a variant of the method of fundamental solutions (MFS), where the source points $\{q_j\}_{j=1}^\infty$ accumulate towards the domain in a Whitney fashion, meaning that their separation is proportional to the distance to the domain. We prove that the normalized Lusin wavelets $\psi_j(w) = b_j(w-q_j)^{-2}$ constitute a generalized basis, known as a frame, for the Hardy subspace of $L_2$-traces of holomorphic functions on the domain. Consequently, our method, where $\psi_j$ are used as basis functions in the MFS, enables a numerically stable approximation of solutions to Laplace boundary value problems, even when the solutions lack analytic continuation across the boundary. Despite the source points accumulating towards the domain, our computations show no loss of accuracy near the boundary, in contrast to the boundary integral equation method.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Math: 5.8 -->
                
            <!-- Reinforcement Learning: 5.4 -->
                
            <!-- LLMs: 2.7 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Medicine: 2.2 -->
                
            <!-- Pathfinding: 2.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.623
            </span>
            <a href="https://arxiv.org/abs/2504.10435" target="_blank" rel="noopener noreferrer">What metric to optimize for suppressing instability in a Vlasov-Poisson system?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Martin Guerra, Qin Li, Yukun Yue | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Stabilizing plasma dynamics is an important task in green energy generation via nuclear fusion. One common strategy is to introduce an external field to prevent the plasma distribution from developing turbulence. However, finding such external fields efficiently remains an open question, even for si</span>
            
            <span class="abstract-full" style="display: none;">Stabilizing plasma dynamics is an important task in green energy generation via nuclear fusion. One common strategy is to introduce an external field to prevent the plasma distribution from developing turbulence. However, finding such external fields efficiently remains an open question, even for simplified models such as the Vlasov-Poisson (VP) system. In this work, we leverage two different approaches to build such fields: for the first approach, we use an analytical derivation of the dispersion relation of the VP system to find a range of reasonable fields that can potentially suppress instability, providing a qualitative suggestion. For the second approach, we leverage PDE-constrained optimization to obtain a locally optimal field using different loss functions. As the stability of the system can be characterized in several different ways, the objective functions need to be tailored accordingly. We show, through extensive numerical tests, that objective functions such as the relative entropy (KL divergence) and the $L^{2}$ norm result in a highly non-convex problem, rendering the global minimum difficult to find. However, we show that using the electric energy of the system as a loss function is advantageous, as it has a large convex basin close to the global minimum. Unfortunately, outside the basin, the electric energy landscape consists of unphysical flat local minima, thus rendering a good initial guess key for the overall convergence of the optimization problem, particularly for solvers with adaptive steps.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.1 -->
                
            <!-- Math: 2.9 -->
                
            <!-- Robotics: 2.7 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- LLMs: 2.3 -->
                
            <!-- Medicine: 2.1 -->
                
            <!-- Federated Learning: 2.1 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.6777
            </span>
            <a href="https://arxiv.org/abs/2504.09768" target="_blank" rel="noopener noreferrer">Robust Output-Feedback MPC for Nonlinear Systems with Applications to Robotic Exploration</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Scott Brown, Mohammad Khajenejad, Aamodh Suresh, Sonia Martinez | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper introduces a novel method for robust output-feedback model predictive control (MPC) for a class of nonlinear discrete-time systems. We propose a novel interval-valued predictor which, given an initial estimate of the state, produces intervals which are guaranteed to contain the future tra</span>
            
            <span class="abstract-full" style="display: none;">This paper introduces a novel method for robust output-feedback model predictive control (MPC) for a class of nonlinear discrete-time systems. We propose a novel interval-valued predictor which, given an initial estimate of the state, produces intervals which are guaranteed to contain the future trajectory of the system. By parameterizing the control input with an initial stabilizing feedback term, we are able to reduce the width of the predicted state intervals compared to existing methods. We demonstrate this through a numerical comparison where we show that our controller performs better in the presence of large amounts of noise. Finally, we present a simulation study of a robot navigation scenario, where we incorporate a time-varying entropy term into the cost function in order to autonomously explore an uncertain area.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 5.9 -->
                
            <!-- LLMs: 5.7 -->
                
            <!-- Medicine: 3.6 -->
                
            <!-- Networks: 3.3 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Attention: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.6969
            </span>
            <a href="https://arxiv.org/abs/2410.19631" target="_blank" rel="noopener noreferrer">Efficient Biological Data Acquisition through Inference Set Design</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ihor Neporozhnii, Julien Roy, Emmanuel Bengio, Jason Hartford | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so one might hope to reduce their cost by only experimenting on a subset of the compounds, and predicting the outcomes of the</span>
            
            <span class="abstract-full" style="display: none;">In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so one might hope to reduce their cost by only experimenting on a subset of the compounds, and predicting the outcomes of the remaining experiments. In this work, we model this scenario as a sequential subset selection problem: we aim to select the smallest set of candidates in order to achieve some desired level of accuracy for the system as a whole. Our key observation is that, if there is heterogeneity in the difficulty of the prediction problem across the input space, selectively obtaining the labels for the hardest examples in the acquisition pool will leave only the relatively easy examples to remain in the inference set, leading to better overall system performance. We call this mechanism inference set design, and propose the use of a confidence-based active learning solution to prune out these challenging examples. Our algorithm includes an explicit stopping criterion that interrupts the acquisition loop when it is sufficiently confident that the system has reached the target performance. Our empirical studies on image and molecular datasets, as well as a real-world large-scale biological assay, show that active learning for inference set design leads to significant reduction in experimental cost while retaining high system performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 3.7 -->
                
            <!-- LLMs: 3.6 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Medicine: 2.6 -->
                
            <!-- Robotics: 2.6 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.7584
            </span>
            <a href="https://arxiv.org/abs/2504.11096" target="_blank" rel="noopener noreferrer">A fully variational numerical method for structural topology optimization based on a Cahn-Hilliard model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Edmund Bell-Navas, David Portillo, Ignacio Romero | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We formulate a novel numerical method suitable for the solution of topology optimization problems in solid mechanics. The most salient feature of the new approach is that the space and time discrete equations of the numerical method can be obtained as the optimality conditions of a single incrementa</span>
            
            <span class="abstract-full" style="display: none;">We formulate a novel numerical method suitable for the solution of topology optimization problems in solid mechanics. The most salient feature of the new approach is that the space and time discrete equations of the numerical method can be obtained as the optimality conditions of a single incremental potential. The governing equations define a gradient flow of the mass in the domain that maximizes the stiffness of the proposed solid, while exactly preserving the mass of the allocated material. Moreover, we propose a change of variables in the model equations that constrains the value of the density within admissible bounds and a continuation strategy that speeds up the evolution of the flow. The proposed strategy results in a robust and efficient topology optimization method that is exactly mass-preserving, does not employ Lagrange multipliers, and is fully variational.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 5.7 -->
                
            <!-- Math: 5.1 -->
                
            <!-- Reinforcement Learning: 4.5 -->
                
            <!-- Medicine: 4.2 -->
                
            <!-- Pathfinding: 2.2 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Federated Learning: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.7881
            </span>
            <a href="https://arxiv.org/abs/2504.10499" target="_blank" rel="noopener noreferrer">Graph-based Approaches and Functionalities in Retrieval-Augmented Generation: A Comprehensive Survey</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zulun Zhu, Tiancheng Huang, Kai Wang, Junda Ye, Xinghe Chen, Siqiang Luo | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) struggle with the factual error during inference due to the lack of sufficient training data and the most updated knowledge, leading to the hallucination problem. Retrieval-Augmented Generation (RAG) has gained attention as a promising solution to address the limitation </span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) struggle with the factual error during inference due to the lack of sufficient training data and the most updated knowledge, leading to the hallucination problem. Retrieval-Augmented Generation (RAG) has gained attention as a promising solution to address the limitation of LLMs, by retrieving relevant information from external source to generate more accurate answers to the questions. Given the pervasive presence of structured knowledge in the external source, considerable strides in RAG have been made to employ the techniques related to graphs and achieve more complex reasoning based on the topological information between knowledge entities. However, there is currently neither unified review examining the diverse roles of graphs in RAG, nor a comprehensive resource to help researchers navigate and contribute to this evolving field. This survey offers a novel perspective on the functionality of graphs within RAG and their impact on enhancing performance across a wide range of graph-structured data. It provides a detailed breakdown of the roles that graphs play in RAG, covering database construction, algorithms, pipelines, and tasks. Finally, it identifies current challenges and outline future research directions, aiming to inspire further developments in this field. Our graph-centered analysis highlights the commonalities and differences in existing methods, setting the stage for future researchers in areas such as graph learning, database systems, and natural language processing.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.9 -->
                
            <!-- Medicine: 3.4 -->
                
            <!-- Reinforcement Learning: 2.8 -->
                
            <!-- Robotics: 2.5 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- GNN: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.7905
            </span>
            <a href="https://arxiv.org/abs/2503.19609" target="_blank" rel="noopener noreferrer">Nanopass Back-Translation of Call-Return Trees for Mechanized Secure Compilation Proofs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: J\'er\'emy Thibault, Joseph Lenormand, Catalin Hritcu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Researchers aim to build secure compilation chains enforcing that if there is no attack a source context can mount against a source program then there is also no attack an adversarial target context can mount against the compiled program. Proving that these compilation chains are secure is, however,</span>
            
            <span class="abstract-full" style="display: none;">Researchers aim to build secure compilation chains enforcing that if there is no attack a source context can mount against a source program then there is also no attack an adversarial target context can mount against the compiled program. Proving that these compilation chains are secure is, however, challenging, and involves a non-trivial back-translation step: for any attack a target context mounts against the compiled program one has to exhibit a source context mounting the same attack against the source program. We describe a novel back-translation technique, which results in simpler proofs that can be more easily mechanized in a proof assistant. Given a finite set of finite trace prefixes, capturing the interaction recorded during an attack between a target context and the compiled program, we build a call-return tree that we back-translate into a source context producing the same trace prefixes. We use state in the generated source context to record the current location in the call-return tree. The back-translation is done in several small steps, each adding to the tree new information describing how the location should change depending on how the context regains control. To prove this back-translation correct we give semantics to every intermediate call-return tree language, using ghost state to store information and explicitly enforce execution invariants. We prove several small forward simulations, basically seeing the back-translation as a verified nanopass compiler. Thanks to this modular structure, we are able to mechanize this complex back-translation and its correctness proof in the Rocq prover without too much effort.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.2 -->
                
            <!-- LLMs: 3.3 -->
                
            <!-- Networks: 3.2 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Medicine: 2.2 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Attention: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.795
            </span>
            <a href="https://arxiv.org/abs/2402.19172" target="_blank" rel="noopener noreferrer">Point Processes and spatial statistics in time-frequency analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Barbara Pascal, R\'emi Bardenet | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A finite-energy signal is represented by a square-integrable, complex-valued function $t\mapsto s(t)$ of a real variable $t$, interpreted as time. Similarly, a noisy signal is represented by a random process. Time-frequency analysis, a subfield of signal processing, amounts to describing the tempora</span>
            
            <span class="abstract-full" style="display: none;">A finite-energy signal is represented by a square-integrable, complex-valued function $t\mapsto s(t)$ of a real variable $t$, interpreted as time. Similarly, a noisy signal is represented by a random process. Time-frequency analysis, a subfield of signal processing, amounts to describing the temporal evolution of the frequency content of a signal. Loosely speaking, if $s$ is the audio recording of a musical piece, time-frequency analysis somehow consists in writing the musical score of the piece. Mathematically, the operation is performed through a transform $\mathcal{V}$, mapping $s \in L^2(\mathbb{R})$ onto a complex-valued function $\mathcal{V}s \in L^2(\mathbb{R}^2)$ of time $t$ and angular frequency $\omega$. The squared modulus $(t, \omega) \mapsto \vert\mathcal{V}s(t,\omega)\vert^2$ of the time-frequency representation is known as the spectrogram of $s$; in the musical score analogy, a peaked spectrogram at $(t_0,\omega_0)$ corresponds to a musical note at angular frequency $\omega_0$ localized at time $t_0$. More generally, the intuition is that upper level sets of the spectrogram contain relevant information about in the original signal. Hence, many signal processing algorithms revolve around identifying maxima of the spectrogram. In contrast, zeros of the spectrogram indicate perfect silence, that is, a time at which a particular frequency is absent. Assimilating $\mathbb{R}^2$ to $\mathbb{C}$ through $z = \omega + \mathrm{i}t$, this chapter focuses on time-frequency transforms $\mathcal{V}$ that map signals to analytic functions. The zeros of the spectrogram of a noisy signal are then the zeros of a random analytic function, hence forming a Point Process in $\mathbb{C}$. This chapter is devoted to the study of these Point Processes, to their links with zeros of Gaussian Analytic Functions, and to designing signal detection and denoising algorithms using spatial statistics.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Networks: 4.0 -->
                
            <!-- Math: 3.5 -->
                
            <!-- Reinforcement Learning: 3.2 -->
                
            <!-- Medicine: 2.7 -->
                
            <!-- Robotics: 2.7 -->
                
            <!-- LLMs: 2.2 -->
                
            <!-- Pathfinding: 2.0 -->
                
            <!-- Quantum Computing: 1.4 -->
                
            <!-- GNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8058
            </span>
            <a href="https://arxiv.org/abs/2504.09831" target="_blank" rel="noopener noreferrer">Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent Demand</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Korel Gundem, Zhengling Qi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we study the offline sequential feature-based pricing and inventory control problem where the current demand depends on the past demand levels and any demand exceeding the available inventory is lost. Our goal is to leverage the offline dataset, consisting of past prices, ordering qua</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we study the offline sequential feature-based pricing and inventory control problem where the current demand depends on the past demand levels and any demand exceeding the available inventory is lost. Our goal is to leverage the offline dataset, consisting of past prices, ordering quantities, inventory levels, covariates, and censored sales levels, to estimate the optimal pricing and inventory control policy that maximizes long-term profit. While the underlying dynamic without censoring can be modeled by Markov decision process (MDP), the primary obstacle arises from the observed process where demand censoring is present, resulting in missing profit information, the failure of the Markov property, and a non-stationary optimal policy. To overcome these challenges, we first approximate the optimal policy by solving a high-order MDP characterized by the number of consecutive censoring instances, which ultimately boils down to solving a specialized Bellman equation tailored for this problem. Inspired by offline reinforcement learning and survival analysis, we propose two novel data-driven algorithms to solving these Bellman equations and, thus, estimate the optimal policy. Furthermore, we establish finite sample regret bounds to validate the effectiveness of these algorithms. Finally, we conduct numerical experiments to demonstrate the efficacy of our algorithms in estimating the optimal policy. To the best of our knowledge, this is the first data-driven approach to learning optimal pricing and inventory control policies in a sequential decision-making environment characterized by censored and dependent demand. The implementations of the proposed algorithms are available at https://github.com/gundemkorel/Inventory_Pricing_Control</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 7.0 -->
                
            <!-- LLMs: 4.9 -->
                
            <!-- Medicine: 3.9 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Federated Learning: 2.1 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8167
            </span>
            <a href="https://arxiv.org/abs/2504.10765" target="_blank" rel="noopener noreferrer">Minimal Sensing for Orienting a Solar Panel</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jeremy Klotz, Shree K. Nayar | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A solar panel harvests the most energy when pointing in the direction that maximizes the total illumination (irradiance) falling on it. Given an arbitrary orientation of a panel and an arbitrary environmental illumination, we address the problem of finding the direction of maximum total irradiance. </span>
            
            <span class="abstract-full" style="display: none;">A solar panel harvests the most energy when pointing in the direction that maximizes the total illumination (irradiance) falling on it. Given an arbitrary orientation of a panel and an arbitrary environmental illumination, we address the problem of finding the direction of maximum total irradiance. We develop a minimal sensing approach where measurements from just four photodetectors are used to iteratively vary the tilt of the panel to maximize the irradiance. Many environments produce irradiance functions with multiple local maxima. As a result, simply measuring the gradient of the irradiance function and applying gradient ascent will not work. We show that a larger, optimized tilt between the detectors and the panel is equivalent to blurring the irradiance function. This has the effect of eliminating local maxima and turning the irradiance function into a unimodal one, whose maximum can be found using gradient ascent. We show that there is a close relationship between our approach and scale space theory. We have collected a large dataset of high-dynamic range lighting environments in New York City, called \textit{UrbanSky}. We used this dataset to conduct simulations to verify the robustness of our approach. Finally, we have built a portable solar panel with four compact detectors and an actuator to conduct experiments in various real-world settings: direct sunlight, cloudy sky, urban settings with occlusions and shadows, and complex indoor lighting. In all cases, we show significant improvements in harvested energy compared to standard approaches for controlling the orientation of a solar panel.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 5.3 -->
                
            <!-- Medicine: 4.9 -->
                
            <!-- Reinforcement Learning: 3.6 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.828
            </span>
            <a href="https://arxiv.org/abs/2504.10451" target="_blank" rel="noopener noreferrer">Minimizing Functions of Age of Incorrect Information for Remote Estimation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ismail Cosandal, Sennur Ulukus, Nail Akar | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The age of incorrect information (AoII) process which keeps track of the time since the source and monitor processes are in sync, has been extensively used in remote estimation problems. In this paper, we consider a push-based remote estimation system with a discrete-time Markov chain (DTMC) informa</span>
            
            <span class="abstract-full" style="display: none;">The age of incorrect information (AoII) process which keeps track of the time since the source and monitor processes are in sync, has been extensively used in remote estimation problems. In this paper, we consider a push-based remote estimation system with a discrete-time Markov chain (DTMC) information source transmitting status update packets towards the monitor once the AoII process exceeds a certain estimation-based threshold. In this paper, the time average of an arbitrary function of AoII is taken as the AoII cost, as opposed to using the average AoII as the mismatch metric, whereas this function is also allowed to depend on the estimation value. In this very general setting, our goal is to minimize a weighted sum of AoII and transmission costs. For this purpose, we formulate a discrete-time semi-Markov decision process (SMDP) regarding the multi-threshold status update policy. We propose a novel tool in discrete-time called 'dual-regime absorbing Markov chain' (DR-AMC) and its corresponding absorption time distribution named as 'dual-regime phase-type' (DR-PH) distribution, to obtain the characterizing parameters of the SMDP, which allows us to obtain the distribution of the AoII process for a given policy, and hence the average of any function of AoII. The proposed method is validated with numerical results by which we compare our proposed method against other policies obtained by exhaustive-search, and also various benchmark policies.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.0 -->
                
            <!-- LLMs: 3.8 -->
                
            <!-- Reinforcement Learning: 3.8 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Robotics: 2.4 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Quantum Computing: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8347
            </span>
            <a href="https://arxiv.org/abs/2504.11435" target="_blank" rel="noopener noreferrer">Robust Containment Queries over Collections of Trimmed NURBS Surfaces via Generalized Winding Numbers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jacob Spainhour, Kenneth Weiss | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Efficient and accurate evaluation of containment queries for regions bound by trimmed NURBS surfaces is important in many graphics and engineering applications. However, the algebraic complexity of surface-surface intersections makes gaps and overlaps between surfaces difficult to avoid for in-the-w</span>
            
            <span class="abstract-full" style="display: none;">Efficient and accurate evaluation of containment queries for regions bound by trimmed NURBS surfaces is important in many graphics and engineering applications. However, the algebraic complexity of surface-surface intersections makes gaps and overlaps between surfaces difficult to avoid for in-the-wild surface models. By considering this problem through the lens of the generalized winding number (GWN), a mathematical construction that is indifferent to the arrangement of surfaces in the shape, we can define a containment query that is robust to model watertightness. Applying contemporary techniques for the 3D GWN on arbitrary curved surfaces would require some form of geometric discretization, potentially inducing containment misclassifications near boundary components. In contrast, our proposed method computes an accurate GWN directly on the curved geometry of the input model. We accomplish this using a novel reformulation of the relevant surface integral using Stokes' theorem, which in turn permits an efficient adaptive quadrature calculation on the boundary and trimming curves of the model. While this is sufficient for "far-field" query points that are distant from the surface, we augment this approach for "near-field" query points (i.e., within a bounding box) and even those coincident to the surface patches via a strategy that directly identifies and accounts for the jump discontinuity in the scalar field. We demonstrate that our method of evaluating the GWN field is robust to complex trimming geometry in a CAD model, and is accurate up to arbitrary precision at arbitrary distances from the surface. Furthermore, the derived containment query is robust to non-watertightness while respecting all curved features of the input shape.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.7 -->
                
            <!-- LLMs: 4.2 -->
                
            <!-- Math: 3.3 -->
                
            <!-- Reinforcement Learning: 3.2 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8381
            </span>
            <a href="https://arxiv.org/abs/2405.09408" target="_blank" rel="noopener noreferrer">A velocity-based moving mesh Discontinuous Galerkin method for the advection-diffusion equation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ezra Rozier, J\"orn Behrens | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In convection-dominated flows, robustness of the spatial discretisation is a key property. While Interior Penalty Galerkin (IPG) methods already proved efficient in the situation of large mesh Peclet numbers, Arbitrary Lagrangian-Eulerian (ALE) methods are able to reduce the convection-dominance by </span>
            
            <span class="abstract-full" style="display: none;">In convection-dominated flows, robustness of the spatial discretisation is a key property. While Interior Penalty Galerkin (IPG) methods already proved efficient in the situation of large mesh Peclet numbers, Arbitrary Lagrangian-Eulerian (ALE) methods are able to reduce the convection-dominance by moving the mesh. In this paper, we introduce and analyse a velocity-based moving mesh discontinuous Galerkin (DG) method for the solution of the linear advection-diffusion equation. By introducing a smooth parameterized velocity $\Tilde{V}$ that separates the flow into a mean flow, also called moving mesh velocity, and a remaining advection field $V-\Tilde{V}$, we made a convergence analysis based on the smoothness of the mesh velocity. Furthermore, the reduction of the advection speed improves the stability of an explicit time-stepping. Finally, by adapting the existing robust error criteria to this moving mesh situation, we derived robust \textit{a posteriori} error criteria that describe the potentially small deviation to the mean flow and include the information of a transition towards $V=\Tilde{V}$.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.6 -->
                
            <!-- Reinforcement Learning: 4.2 -->
                
            <!-- LLMs: 3.8 -->
                
            <!-- Math: 3.7 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8464
            </span>
            <a href="https://arxiv.org/abs/2504.08690" target="_blank" rel="noopener noreferrer">Fast-Slow-Thinking: Complex Task Solving with Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yiliu Sun, Yanfang Zhang, Zicheng Zhao, Sheng Wan, Dacheng Tao, Chen Gong | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Nowadays, Large Language Models (LLMs) have been gradually employed to solve complex tasks. To face the challenge, task decomposition has become an effective way, which proposes to divide a complex task into multiple simpler subtasks and then solve them separately so that the difficulty of the origi</span>
            
            <span class="abstract-full" style="display: none;">Nowadays, Large Language Models (LLMs) have been gradually employed to solve complex tasks. To face the challenge, task decomposition has become an effective way, which proposes to divide a complex task into multiple simpler subtasks and then solve them separately so that the difficulty of the original task can be reduced. However, the performance of existing task decomposition methods can be suboptimal when the task contains overly complex logic and constraints. In this situation, the solution generated by LLMs may deviate from the original purpose of the task, or contain redundant or even erroneous content. Therefore, inspired by the fact that humans possess two thinking systems including fast thinking and slow thinking, this paper introduces a new task decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates LLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow Thinking (ST) steps. Here FT focuses more on the general and concise aspect of the task, and ST focuses more on the details of the task. In FT, LLMs are prompted to remove the constraints of the original task, therefore simplifying it to a general and concise one. In ST, we recall the constraints removed in FT, so that LLMs can improve the answer generated in FT to meet the requirements of the original task. Therefore, our FST method enables LLMs to consider a complex problem via a human-like cognition process from coarse to fine, the effectiveness of which has been well demonstrated by the experiments on three types of tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 42.8 -->
                
            <!-- Reinforcement Learning: 2.7 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Quantum Computing: 1.6 -->
                
            <!-- Medicine: 1.6 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8581
            </span>
            <a href="https://arxiv.org/abs/2409.16163" target="_blank" rel="noopener noreferrer">The anonymization problem in social networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rachel G. de Jong, Mark P. J. van der Loo, Frank W. Takes | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper we introduce a general version of the anonymization problem in social networks, in which the goal is to maximize the number of anonymous nodes by altering a given graph. We define three variants of this optimization problem being full, partial and budgeted anonymization. In each, the o</span>
            
            <span class="abstract-full" style="display: none;">In this paper we introduce a general version of the anonymization problem in social networks, in which the goal is to maximize the number of anonymous nodes by altering a given graph. We define three variants of this optimization problem being full, partial and budgeted anonymization. In each, the objective is to maximize the number of k-anonymous nodes, i.e., nodes for which there are at least k-1 equivalent nodes, according to a particular anonymity measure of structural node equivalence. We propose four new heuristic algorithms for solving the anonymization problem which we implement into a reusable computational framework. As a baseline, we use an edge sampling method introduced in previous work. Experiments on both graph models and 23 real-world network datasets result in three empirical findings. First, we demonstrate that edge deletion is the most effective graph alteration operation. Second, we compare four commonly used anonymity measures from the literature and highlight how the choice of anonymity measure has a tremendous effect on both the initial anonymity as well as the difficulty of solving the anonymization problem. Third, we find that the proposed algorithm that preferentially deletes edges with a larger effect on nodes at a structurally unique position consistently outperforms heuristics solely based on network structure. Our best performing algorithm retains on average 14 times more edges in full anonymization, and overall ensures a better trade-off between anonymity and data utility. In the budgeted variant, it achieves 4.8 times more anonymous nodes than the baseline. This work lays foundations for future development of algorithms for anonymizing social networks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.5 -->
                
            <!-- LLMs: 3.9 -->
                
            <!-- Medicine: 3.3 -->
                
            <!-- Math: 2.8 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Pathfinding: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8586
            </span>
            <a href="https://arxiv.org/abs/2503.18236" target="_blank" rel="noopener noreferrer">Research impact evaluation based on effective authorship contribution sensitivity: h-leadership index</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hardik A. Jain, Rohitash Chandra | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The evaluation of a researcher's performance has traditionally relied on various bibliometric measures, with the h-index being one of the most prominent. However, the h-index only accounts for the number of citations received in a publication and does not account for other factors such as the number</span>
            
            <span class="abstract-full" style="display: none;">The evaluation of a researcher's performance has traditionally relied on various bibliometric measures, with the h-index being one of the most prominent. However, the h-index only accounts for the number of citations received in a publication and does not account for other factors such as the number of authors or their specific contributions in collaborative works. Therefore, the h-index has been placed on scrutiny as it has motivated academic integrity issues where non-contributing authors get authorship merely for raising their h-index. In this study, we comprehensively evaluate existing metrics in their ability to account for authorship contribution by their position and introduce a novel variant of the h-index, known as the h-leadership index. The h-leadership index aims to advance the fair evaluation of academic contributions in multi-authored publications by giving importance to authorship position beyond the first and last authors, focused by Stanford's ranking of the top 2 \% of world scientists. We assign weighted citations based on a modified complementary unit Gaussian curve, ensuring that the contributions of middle authors are appropriately recognised. We apply the h-leadership index to analyse the top 50 researchers across the Group of 8 (Go8) universities in Australia, demonstrating its potential to provide a more balanced assessment of research performance. We provide open-source software for extending the work further.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 5.4 -->
                
            <!-- Reinforcement Learning: 3.7 -->
                
            <!-- Medicine: 3.5 -->
                
            <!-- Math: 3.0 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.8596
            </span>
            <a href="https://arxiv.org/abs/2406.18739" target="_blank" rel="noopener noreferrer">RetroGFN: Diverse and Feasible Retrosynthesis using GFlowNets</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Piotr Gai\'nski, Micha{\l} Koziarski, Krzysztof Maziarz, Marwin Segler, Jacek Tabor, Marek \'Smieja | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Single-step retrosynthesis aims to predict a set of reactions that lead to the creation of a target molecule, which is a crucial task in molecular discovery. Although a target molecule can often be synthesized with multiple different reactions, it is not clear how to verify the feasibility of a reac</span>
            
            <span class="abstract-full" style="display: none;">Single-step retrosynthesis aims to predict a set of reactions that lead to the creation of a target molecule, which is a crucial task in molecular discovery. Although a target molecule can often be synthesized with multiple different reactions, it is not clear how to verify the feasibility of a reaction, because the available datasets cover only a tiny fraction of the possible solutions. Consequently, the existing models are not encouraged to explore the space of possible reactions sufficiently. In this paper, we propose a novel single-step retrosynthesis model, RetroGFN, that can explore outside the limited dataset and return a diverse set of feasible reactions by leveraging a feasibility proxy model during the training. We show that RetroGFN achieves competitive results on standard top-k accuracy while outperforming existing methods on round-trip accuracy. Moreover, we provide empirical arguments in favor of using round-trip accuracy, which expands the notion of feasibility with respect to the standard top-k accuracy metric.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 6.1 -->
                
            <!-- Reinforcement Learning: 4.1 -->
                
            <!-- Medicine: 3.6 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Math: 2.1 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Attention: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.9276
            </span>
            <a href="https://arxiv.org/abs/2504.09492" target="_blank" rel="noopener noreferrer">Hybrid Radial Kernels for Solving Weakly Singular Fredholm Integral Equations: Balancing Accuracy and Stability in Meshless Methods</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Davoud Moazami, Mohsen Esmaeilbeigi, Tahereh Akbari | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Over the past few decades, kernel-based approximation methods had achieved astonishing success in solving different problems in the field of science and engineering. However, when employing the direct or standard method of performing computations using infinitely smooth kernels, a conflict arises be</span>
            
            <span class="abstract-full" style="display: none;">Over the past few decades, kernel-based approximation methods had achieved astonishing success in solving different problems in the field of science and engineering. However, when employing the direct or standard method of performing computations using infinitely smooth kernels, a conflict arises between the accuracy that can be theoretically attained and the numerical stability. In other words, when the shape parameter tends to zero, the operational matrix for the standard bases with infinitely smooth kernels become severely ill-conditioned. This conflict can be managed applying hybrid kernels. The hybrid kernels extend the approximation space and provide high flexibility to strike the best possible balance between accuracy and stability. In the current study, an innovative approach using hybrid radial kernels (HRKs) is provided to solve weakly singular Fredholm integral equations (WSFIEs) of the second kind in a meshless scheme. The approach employs hybrid kernels built on dispersed nodes as a basis within the discrete collocation technique. This method transforms the problem being studied into a linear system of algebraic equations. Also, the particle swarm optimization (PSO) algorithm is utilized to calculate the optimal parameters for the hybrid kernels, which is based on minimizing the maximum absolute error (MAE). We also study the error estimate of the suggested scheme. Lastly, we assess the accuracy and validity of the hybrid technique by carrying out various numerical experiments. The numerical findings show that the estimates obtained from hybrid kernels are significantly more accurate in solving WSFIEs compared to pure kernels. Additionally, it was revealed that the hybrid bases remain stable across various values of the shape parameters.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.4 -->
                
            <!-- Math: 4.4 -->
                
            <!-- Medicine: 4.1 -->
                
            <!-- LLMs: 2.6 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Pathfinding: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.9724
            </span>
            <a href="https://arxiv.org/abs/2401.00592" target="_blank" rel="noopener noreferrer">Majority voting is not good for heaven or hell, with mirrored performance</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pavel Chebotarev, Vadim Afonkin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Within the ViSE (Voting in Stochastic Environment) model, we study the effectiveness of majority voting in various environments. By the pit of losses paradox identified in previous work, majority decisions in apparently hostile environments tend to reduce the capital of society. In such cases, the s</span>
            
            <span class="abstract-full" style="display: none;">Within the ViSE (Voting in Stochastic Environment) model, we study the effectiveness of majority voting in various environments. By the pit of losses paradox identified in previous work, majority decisions in apparently hostile environments tend to reduce the capital of society. In such cases, the simple social decision rule of "rejecting all proposals without voting" outperforms majority voting. In this paper, we identify another pit of losses appearing in favorable environments. Here, the simple social decision rule of "accepting all proposals without voting" is superior to majority voting. We prove that under a version of simple majority called symmetrized majority and the antisymmetry of the voting body, the second pit of losses is a mirror image of the pit of losses in hostile environments and explain this phenomenon. Technically, we consider a voting society consisting of individualists whose strategy is supporting all proposals that increase their capital and a group (groups) whose members vote to increase the wealth of their group. According to the main result, the expected capital gain of each agent in the environment whose generator $X$ has mean $\mu>0$ exceeds by $\mu$ their expected capital gain under generator $-X$. This result extends to location families of generators with distributions symmetric about their mean. The mentioned result determines the symmetry of the difference between the expected capital gain under the symmetrized majority and that under the "basic" social decision rule that rejects (resp. accepts) all proposals in unfavorable (resp. favorable) environments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.0 -->
                
            <!-- Math: 3.8 -->
                
            <!-- Medicine: 3.2 -->
                
            <!-- Reinforcement Learning: 2.8 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Pathfinding: 2.2 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Networks: 1.5 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.9806
            </span>
            <a href="https://arxiv.org/abs/2411.16033" target="_blank" rel="noopener noreferrer">Generative AI for Brane Configurations and Coamoeba</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rak-Kyeong Seong | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We introduce a generative AI model to obtain Type IIB brane configurations that realize toric phases of a family of 4d N=1 supersymmetric gauge theories. These 4d N=1 quiver gauge theories are worldvolume theories of a D3-brane probing a toric Calabi-Yau 3-fold. The Type IIB brane configurations are</span>
            
            <span class="abstract-full" style="display: none;">We introduce a generative AI model to obtain Type IIB brane configurations that realize toric phases of a family of 4d N=1 supersymmetric gauge theories. These 4d N=1 quiver gauge theories are worldvolume theories of a D3-brane probing a toric Calabi-Yau 3-fold. The Type IIB brane configurations are given by the coamoeba projection of the mirror curve associated with the toric Calabi-Yau 3-fold. The shape of the mirror curve and its coamoeba projection, as well as the corresponding Type IIB brane configuration and the toric phase of the 4d N=1 theory, all depend on the complex structure moduli parameterizing the mirror curve. We train a generative AI model, a conditional variational autoencoder (CVAE), that takes a choice of complex structure moduli as input and generates the corresponding coamoeba. This enables us not only to obtain a high-resolution representation of the entire phase space for a family of 4d N=1 theories corresponding to the same toric Calabi-Yau 3-fold, but also to continuously track the movements of the mirror curve and the branes wrapping the curve in the corresponding Type IIB brane configurations during phase transitions associated with Seiberg duality.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.9 -->
                
            <!-- Reinforcement Learning: 4.6 -->
                
            <!-- Math: 3.1 -->
                
            <!-- Robotics: 3.0 -->
                
            <!-- LLMs: 2.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Quantum Computing: 1.8 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.9824
            </span>
            <a href="https://arxiv.org/abs/2504.09666" target="_blank" rel="noopener noreferrer">Uncertainty Guided Refinement for Fine-Grained Salient Object Detection</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yao Yuan, Pan Gao, Qun Dai, Jie Qin, Wei Xiang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recently, salient object detection (SOD) methods have achieved impressive performance. However, salient regions predicted by existing methods usually contain unsaturated regions and shadows, which limits the model for reliable fine-grained predictions. To address this, we introduce the uncertainty g</span>
            
            <span class="abstract-full" style="display: none;">Recently, salient object detection (SOD) methods have achieved impressive performance. However, salient regions predicted by existing methods usually contain unsaturated regions and shadows, which limits the model for reliable fine-grained predictions. To address this, we introduce the uncertainty guidance learning approach to SOD, intended to enhance the model's perception of uncertain regions. Specifically, we design a novel Uncertainty Guided Refinement Attention Network (UGRAN), which incorporates three important components, i.e., the Multilevel Interaction Attention (MIA) module, the Scale Spatial-Consistent Attention (SSCA) module, and the Uncertainty Refinement Attention (URA) module. Unlike conventional methods dedicated to enhancing features, the proposed MIA facilitates the interaction and perception of multilevel features, leveraging the complementary characteristics among multilevel features. Then, through the proposed SSCA, the salient information across diverse scales within the aggregated features can be integrated more comprehensively and integrally. In the subsequent steps, we utilize the uncertainty map generated from the saliency prediction map to enhance the model's perception capability of uncertain regions, generating a highly-saturated fine-grained saliency prediction map. Additionally, we devise an adaptive dynamic partition (ADP) mechanism to minimize the computational overhead of the URA module and improve the utilization of uncertainty guidance. Experiments on seven benchmark datasets demonstrate the superiority of the proposed UGRAN over the state-of-the-art methodologies. Codes will be released at https://github.com/I2-Multimedia-Lab/UGRAN.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 6.4 -->
                
            <!-- Medicine: 5.9 -->
                
            <!-- LLMs: 3.3 -->
                
            <!-- Federated Learning: 3.2 -->
                
            <!-- Math: 3.1 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.9845
            </span>
            <a href="https://arxiv.org/abs/2503.16316" target="_blank" rel="noopener noreferrer">On the Cone Effect in the Learning Dynamics</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhanpeng Zhou, Yongyi Yang, Jie Ren, Mahito Sugiyama, Junchi Yan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Understanding the learning dynamics of neural networks is a central topic in the deep learning community. In this paper, we take an empirical perspective to study the learning dynamics of neural networks in real-world settings. Specifically, we investigate the evolution process of the empirical Neur</span>
            
            <span class="abstract-full" style="display: none;">Understanding the learning dynamics of neural networks is a central topic in the deep learning community. In this paper, we take an empirical perspective to study the learning dynamics of neural networks in real-world settings. Specifically, we investigate the evolution process of the empirical Neural Tangent Kernel (eNTK) during training. Our key findings reveal a two-phase learning process: i) in Phase I, the eNTK evolves significantly, signaling the rich regime, and ii) in Phase II, the eNTK keeps evolving but is constrained in a narrow space, a phenomenon we term the cone effect. This two-phase framework builds on the hypothesis proposed by Fort et al. (2020), but we uniquely identify the cone effect in Phase II, demonstrating its significant performance advantages over fully linearized training.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 5.1 -->
                
            <!-- Medicine: 3.2 -->
                
            <!-- Federated Learning: 2.9 -->
                
            <!-- LLMs: 2.6 -->
                
            <!-- Robotics: 2.6 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Pathfinding: 2.0 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.9857
            </span>
            <a href="https://arxiv.org/abs/2504.03162" target="_blank" rel="noopener noreferrer">Beyond Progress Measures: Theoretical Insights into the Mechanism of Grokking</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zihan Gu, Ruoyu Chen, Hua Zhang, Yue Hu, Xiaochun Cao | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Grokking, referring to the abrupt improvement in test accuracy after extended overfitting, offers valuable insights into the mechanisms of model generalization. Existing researches based on progress measures imply that grokking relies on understanding the optimization dynamics when the loss function</span>
            
            <span class="abstract-full" style="display: none;">Grokking, referring to the abrupt improvement in test accuracy after extended overfitting, offers valuable insights into the mechanisms of model generalization. Existing researches based on progress measures imply that grokking relies on understanding the optimization dynamics when the loss function is dominated solely by the weight decay term. However, we find that this optimization merely leads to token uniformity, which is not a sufficient condition for grokking. In this work, we investigate the grokking mechanism underlying the Transformer in the task of prime number operations. Based on theoretical analysis and experimental validation, we present the following insights: (i) The weight decay term encourages uniformity across all tokens in the embedding space when it is minimized. (ii) The occurrence of grokking is jointly determined by the uniformity of the embedding space and the distribution of the training dataset. Building on these insights, we provide a unified perspective for understanding various previously proposed progress measures and introduce a novel, concise, and effective progress measure that could trace the changes in test loss more accurately. Finally, to demonstrate the versatility of our theoretical framework, we design a dedicated dataset to validate our theory on ResNet-18, successfully showcasing the occurrence of grokking. The code is released at https://github.com/Qihuai27/Grokking-Insight.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.3 -->
                
            <!-- Medicine: 4.4 -->
                
            <!-- Reinforcement Learning: 3.9 -->
                
            <!-- Math: 3.2 -->
                
            <!-- Quantum Computing: 2.3 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.992
            </span>
            <a href="https://arxiv.org/abs/2504.11110" target="_blank" rel="noopener noreferrer">Helper-Friendly Latency-Bounded Mitigation Strategies against Reactive Jamming Adversaries</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Soumita Hazra, J. Harshan | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Due to the recent developments in the field of full-duplex radios and cognitive radios, a new class of reactive jamming attacks has gained attention wherein an adversary transmits jamming energy over the victim's frequency band and also monitors various energy statistics in the network so as to dete</span>
            
            <span class="abstract-full" style="display: none;">Due to the recent developments in the field of full-duplex radios and cognitive radios, a new class of reactive jamming attacks has gained attention wherein an adversary transmits jamming energy over the victim's frequency band and also monitors various energy statistics in the network so as to detect countermeasures, thereby trapping the victim. Although cooperative mitigation strategies against such security threats exist, they are known to incur spectral-efficiency loss on the helper node, and are also not robust to variable latency-constraints on victim's messages. Identifying these research gaps in existing countermeasures against reactive jamming attacks, we propose a family of helper-friendly cooperative mitigation strategies that are applicable for a wide-range of latency-requirements on the victim's messages as well as practical radio hardware at the helper nodes. The proposed strategies are designed to facilitate reliable communication for the victim, without compromising the helper's spectral efficiency and also minimally disturbing the various energy statistics in the network. For theoretical guarantees on their efficacy, interesting optimization problems are formulated on the choice of the underlying parameters, followed by extensive mathematical analyses on their error-performance and covertness. Experimental results indicate that the proposed strategies should be preferred over the state-of-the-art methods when the helper node is unwilling to compromise on its error performance for assisting the victim.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.7 -->
                
            <!-- Math: 3.7 -->
                
            <!-- LLMs: 3.1 -->
                
            <!-- Medicine: 3.1 -->
                
            <!-- Quantum Computing: 2.9 -->
                
            <!-- Robotics: 2.3 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.016
            </span>
            <a href="https://arxiv.org/abs/2504.10648" target="_blank" rel="noopener noreferrer">Modeling and solving an integrated periodic vehicle routing and capacitated facility location problem in the context of solid waste collection</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Bego\~na Gonz\'alez, Diego Rossit, Mariano Frutos, M\'aximo M\'endez | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Few activities are as crucial in urban environments as waste management. Mismanagement of waste can cause significant economic, social, and environmental damage. However, waste management is often a complex system to manage and therefore where computational decision-support tools can play a pivotal </span>
            
            <span class="abstract-full" style="display: none;">Few activities are as crucial in urban environments as waste management. Mismanagement of waste can cause significant economic, social, and environmental damage. However, waste management is often a complex system to manage and therefore where computational decision-support tools can play a pivotal role in assisting managers to make faster and better decisions. In this sense, this article proposes, on the one hand, a unified optimization model to address two common waste management system optimization problem: the determination of the capacity of waste bins in the collection network and the design and scheduling of collection routes. The integration of these two problems is not usual in the literature since each of them separately is already a major computational challenge. On the other hand, two improved exact formulations based on mathematical programming and a genetic algorithm (GA) are provided to solve this proposed unified optimization model. It should be noted that the GA considers a mixed chromosome representation of the solutions combining binary and integer alleles, in order to solve realistic instances of this complex problem. Also, different genetic operators have been tested to study which combination of them obtained better results in execution times on the order of that of the exact solvers. The obtained results show that the proposed GA is able to match the results of exact solvers on small instances and, in addition, can obtain feasible solutions on large instances, where exact formulations are not applicable, in reasonable computation times.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.8 -->
                
            <!-- Medicine: 3.9 -->
                
            <!-- Reinforcement Learning: 2.9 -->
                
            <!-- Math: 2.7 -->
                
            <!-- Robotics: 2.6 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.0421
            </span>
            <a href="https://arxiv.org/abs/2504.10807" target="_blank" rel="noopener noreferrer">Power-scaled Bayesian Inference with Score-based Generative mModels</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Huseyin Tuna Erdinc, Yunlin Zeng, Abhinav Prakash Gahlot, Felix J. Herrmann | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose a score-based generative algorithm for sampling from power-scaled priors and likelihoods within the Bayesian inference framework. Our algorithm enables flexible control over prior-likelihood influence without requiring retraining for different power-scaling configurations. Specifically, w</span>
            
            <span class="abstract-full" style="display: none;">We propose a score-based generative algorithm for sampling from power-scaled priors and likelihoods within the Bayesian inference framework. Our algorithm enables flexible control over prior-likelihood influence without requiring retraining for different power-scaling configurations. Specifically, we focus on synthesizing seismic velocity models conditioned on imaged seismic. Our method enables sensitivity analysis by sampling from intermediate power posteriors, allowing us to assess the relative influence of the prior and likelihood on samples of the posterior distribution. Through a comprehensive set of experiments, we evaluate the effects of varying the power parameter in different settings: applying it solely to the prior, to the likelihood of a Bayesian formulation, and to both simultaneously. The results show that increasing the power of the likelihood up to a certain threshold improves the fidelity of posterior samples to the conditioning data (e.g., seismic images), while decreasing the prior power promotes greater structural diversity among samples. Moreover, we find that moderate scaling of the likelihood leads to a reduced shot data residual, confirming its utility in posterior refinement.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 5.4 -->
                
            <!-- LLMs: 5.2 -->
                
            <!-- Medicine: 3.8 -->
                
            <!-- Quantum Computing: 2.9 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.0437
            </span>
            <a href="https://arxiv.org/abs/2402.17097" target="_blank" rel="noopener noreferrer">Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Juyeon Kim, Jeongeun Lee, Yoonho Chang, Chanyeol Choi, Junseong Kim, Jy-yong Sohn | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Mitigating hallucination issues is a key challenge that must be overcome to reliably deploy large language models (LLMs) in real-world scenarios. Recently, various methods have been proposed to detect and revise factual errors in LLM-generated texts, in order to reduce hallucination. In this paper, </span>
            
            <span class="abstract-full" style="display: none;">Mitigating hallucination issues is a key challenge that must be overcome to reliably deploy large language models (LLMs) in real-world scenarios. Recently, various methods have been proposed to detect and revise factual errors in LLM-generated texts, in order to reduce hallucination. In this paper, we propose Re-Ex, a method for post-editing LLM-generated responses. Re-Ex introduces a novel reasoning step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps : first, external tools are used to retrieve the evidences of the factual errors in the initial LLM response; next, LLM is instructed to explain the problematic parts of the response based on the gathered evidence; finally, LLM revises the initial response using the explanations provided in the previous step. In addition to the explanation step, Re-Ex also incorporates new prompting techniques to reduce the token count and inference time required for the response revision process. Compared with existing methods including FacTool, CoVE, and RARR, Re-Ex provides better detection and revision performance with less inference time and fewer tokens in multiple benchmarks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.5 -->
                
            <!-- Medicine: 4.6 -->
                
            <!-- Reinforcement Learning: 3.1 -->
                
            <!-- Robotics: 2.7 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.0565
            </span>
            <a href="https://arxiv.org/abs/2501.07824" target="_blank" rel="noopener noreferrer">Real-time Verification and Refinement of Language Model Text Generation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Joonho Ko, Jinheon Baek, Sung Ju Hwang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their genera</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their generation and further refining them, they are slow in deployment since they are designed to verify the response from LLMs only after their entire generation (from the first to last tokens) is done. Further, we observe that once LLMs generate incorrect tokens early on, there is a higher likelihood that subsequent tokens will also be factually incorrect. To this end, in this work, we propose Streaming-VR (Streaming Verification and Refinement), a novel approach designed to enhance the efficiency of verification and refinement of LLM outputs. Specifically, the proposed Streaming-VR enables on-the-fly verification and correction of tokens as they are being generated, similar to a streaming process, ensuring that each subset of tokens is checked and refined in real-time by another LLM as the LLM constructs its response. Through comprehensive evaluations on multiple datasets, we demonstrate that our approach not only enhances the factual accuracy of LLMs, but also offers a more efficient solution compared to prior refinement methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 57.9%">
                        LLMs
                    </span>
            <!-- Blockchain: 1.9 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- RAG: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Medicine: 1.4 -->
                
            <!-- Quantum Computing: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Reinforcement Learning: 1.1 -->
                
            <!-- Quality Diversity: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.065
            </span>
            <a href="https://arxiv.org/abs/2504.10178" target="_blank" rel="noopener noreferrer">MSCoT: Structured Chain-of-Thought Generation for Multiple Programming Languages</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Naizhu Jin, Zhong Li, Tian Zhang, Qingkai Zeng | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">With the rapid development of code intelligence, the application of multiple programming languages is becoming increasingly widespread. However, most existing code generation models mainly focus on a single or a few programming languages, resulting in unsatisfactory performance in a multilingual env</span>
            
            <span class="abstract-full" style="display: none;">With the rapid development of code intelligence, the application of multiple programming languages is becoming increasingly widespread. However, most existing code generation models mainly focus on a single or a few programming languages, resulting in unsatisfactory performance in a multilingual environment. Chain-of-Thought (CoT) reasoning can significantly improve the performance of the model without the need for retraining or fine-tuning the code generation model by reasonably decomposing complex code generation tasks into multiple subtasks and gradually deriving solutions for each subtask. Nevertheless, the existing CoT generation methods mainly concentrate on Python code, and the performance on other programming languages remains unclear. To fill this gap, we first constructed a CoT generation dataset for 12 programming languages through multi-agent technology. On this basis, we proposed a CoT generation method MSCoT applicable to multiple programming languages. By introducing CoT into the code generation large model, the performance of the code generation large model in a multilingual environment can be improved. Through large-scale empirical research, we compared the generalization abilities of MSCoT and the existing CoT generation methods on multiple programming languages and proved the effectiveness of MSCoT for multiple programming languages. In addition, we also designed a human study to prove the quality of the CoT generated by MSCoT. Finally, we opensourced the model and dataset of MSCoT to promote the research on CoT generation for multiple programming languages.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.7 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Reinforcement Learning: 3.1 -->
                
            <!-- Federated Learning: 2.4 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Quantum Computing: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.0655
            </span>
            <a href="https://arxiv.org/abs/2504.07997" target="_blank" rel="noopener noreferrer">BiasCause: Evaluate Socially Biased Causal Reasoning of Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tian Xie, Tongxin Yin, Vaishakh Keshava, Xueru Zhang, Siddhartha Reddy Jonnalagadda | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">While large language models (LLMs) already play significant roles in society, research has shown that LLMs still generate content including social bias against certain sensitive groups. While existing benchmarks have effectively identified social biases in LLMs, a critical gap remains in our underst</span>
            
            <span class="abstract-full" style="display: none;">While large language models (LLMs) already play significant roles in society, research has shown that LLMs still generate content including social bias against certain sensitive groups. While existing benchmarks have effectively identified social biases in LLMs, a critical gap remains in our understanding of the underlying reasoning that leads to these biased outputs. This paper goes one step further to evaluate the causal reasoning process of LLMs when they answer questions eliciting social biases. We first propose a novel conceptual framework to classify the causal reasoning produced by LLMs. Next, we use LLMs to synthesize $1788$ questions covering $8$ sensitive attributes and manually validate them. The questions can test different kinds of causal reasoning by letting LLMs disclose their reasoning process with causal graphs. We then test 4 state-of-the-art LLMs. All models answer the majority of questions with biased causal reasoning, resulting in a total of $4135$ biased causal graphs. Meanwhile, we discover $3$ strategies for LLMs to avoid biased causal reasoning by analyzing the "bias-free" cases. Finally, we reveal that LLMs are also prone to "mistaken-biased" causal reasoning, where they first confuse correlation with causality to infer specific sensitive group names and then incorporate biased causal reasoning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 67.1%">
                        LLMs
                    </span>
            <!-- RAG: 2.1 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Blockchain: 1.7 -->
                
            <!-- T2I: 1.6 -->
                
            <!-- Quantum Computing: 1.4 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Medicine: 1.2 -->
                
            <!-- Datasets: 1.1 -->
                
            <!-- Quality Diversity: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.073
            </span>
            <a href="https://arxiv.org/abs/2504.08356" target="_blank" rel="noopener noreferrer">An Adaptive Clustering Scheme for Client Selections in Communication-Efficient Federated Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yan-Ann Chen, Guan-Lin Chen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Federated learning is a novel decentralized learning architecture. During the training process, the client and server must continuously upload and receive model parameters, which consumes a lot of network transmission resources. Some methods use clustering to find more representative customers, sele</span>
            
            <span class="abstract-full" style="display: none;">Federated learning is a novel decentralized learning architecture. During the training process, the client and server must continuously upload and receive model parameters, which consumes a lot of network transmission resources. Some methods use clustering to find more representative customers, select only a part of them for training, and at the same time ensure the accuracy of training. However, in federated learning, it is not trivial to know what the number of clusters can bring the best training result. Therefore, we propose to dynamically adjust the number of clusters to find the most ideal grouping results. It may reduce the number of users participating in the training to achieve the effect of reducing communication costs without affecting the model performance. We verify its experimental results on the non-IID handwritten digit recognition dataset and reduce the cost of communication and transmission by almost 50% compared with traditional federated learning without affecting the accuracy of the model.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.1 -->
                
            <!-- Reinforcement Learning: 3.9 -->
                
            <!-- Federated Learning: 3.8 -->
                
            <!-- LLMs: 3.1 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.1036
            </span>
            <a href="https://arxiv.org/abs/2504.10964" target="_blank" rel="noopener noreferrer">Distributed Optimization with Gradient Tracking over Heterogeneous Delay-Prone Directed Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Evagoras Makridis, Gabriele Oliva, Kasagatta Ramesh Narahari, Mohammadreza Doostmohammadian, Usman A. Khan, Themistoklis Charalambous | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we address the distributed optimization problem over unidirectional networks with possibly time-invariant heterogeneous bounded transmission delays. In particular, we propose a modified version of the Accelerated Distributed Directed OPTimization (ADD-OPT) algorithm, herein called Rob</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we address the distributed optimization problem over unidirectional networks with possibly time-invariant heterogeneous bounded transmission delays. In particular, we propose a modified version of the Accelerated Distributed Directed OPTimization (ADD-OPT) algorithm, herein called Robustified ADD-OPT (R-ADD-OPT), which is able to solve the distributed optimization problem, even when the communication links suffer from heterogeneous but bounded transmission delays. We show that if the gradient step-size of the R-ADD-OPT algorithm is within a certain range, which also depends on the maximum time delay in the network, then the nodes are guaranteed to converge to the optimal solution of the distributed optimization problem. The range of the gradient step-size that guarantees convergence can be computed a priori based on the maximum time delay in the network.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 5.6 -->
                
            <!-- Math: 5.5 -->
                
            <!-- LLMs: 3.2 -->
                
            <!-- GNN: 3.1 -->
                
            <!-- Federated Learning: 2.6 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Medicine: 2.1 -->
                
            <!-- Pathfinding: 1.8 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.1202
            </span>
            <a href="https://arxiv.org/abs/2410.10212" target="_blank" rel="noopener noreferrer">Large Language Model-Enhanced Reinforcement Learning for Generic Bus Holding Control Strategies</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiajie Yu, Yuhong Wang, Wei Ma | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Bus holding control is a widely-adopted strategy for maintaining stability and improving the operational efficiency of bus systems. Traditional model-based methods often face challenges with the low accuracy of bus state prediction and passenger demand estimation. In contrast, Reinforcement Learning</span>
            
            <span class="abstract-full" style="display: none;">Bus holding control is a widely-adopted strategy for maintaining stability and improving the operational efficiency of bus systems. Traditional model-based methods often face challenges with the low accuracy of bus state prediction and passenger demand estimation. In contrast, Reinforcement Learning (RL), as a data-driven approach, has demonstrated great potential in formulating bus holding strategies. RL determines the optimal control strategies in order to maximize the cumulative reward, which reflects the overall control goals. However, translating sparse and delayed control goals in real-world tasks into dense and real-time rewards for RL is challenging, normally requiring extensive manual trial-and-error. In view of this, this study introduces an automatic reward generation paradigm by leveraging the in-context learning and reasoning capabilities of Large Language Models (LLMs). This new paradigm, termed the LLM-enhanced RL, comprises several LLM-based modules: reward initializer, reward modifier, performance analyzer, and reward refiner. These modules cooperate to initialize and iteratively improve the reward function according to the feedback from training and test results for the specified RL-based task. Ineffective reward functions generated by the LLM are filtered out to ensure the stable evolution of the RL agents' performance over iterations. To evaluate the feasibility of the proposed LLM-enhanced RL paradigm, it is applied to extensive bus holding control scenarios that vary in the number of bus lines, stops, and passenger demand. The results demonstrate the superiority, generalization capability, and robustness of the proposed paradigm compared to vanilla RL strategies, the LLM-based controller, physics-based feedback controllers, and optimization-based controllers. This study sheds light on the great potential of utilizing LLMs in various smart mobility applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.5 -->
                
            <!-- Medicine: 5.8 -->
                
            <!-- Reinforcement Learning: 5.7 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Quantum Computing: 1.9 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.1341
            </span>
            <a href="https://arxiv.org/abs/2411.16667" target="_blank" rel="noopener noreferrer">OPMOS: Ordered Parallel Algorithm for Multi-Objective Shortest-Paths</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Leo Gold, Adam Bienkowski, David Sidoti, Krishna Pattipati, Omer Khan | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The Multi-Objective Shortest-Path (MOS) problem finds a set of Pareto-optimal solutions from a start node to a destination node in a multi-attribute graph. The literature explores multi-objective A*-style algorithmic approaches to solving the NP-hard MOS problem. These approaches use consistent heur</span>
            
            <span class="abstract-full" style="display: none;">The Multi-Objective Shortest-Path (MOS) problem finds a set of Pareto-optimal solutions from a start node to a destination node in a multi-attribute graph. The literature explores multi-objective A*-style algorithmic approaches to solving the NP-hard MOS problem. These approaches use consistent heuristics to compute an exact set of solutions for the goal node. A generalized MOS algorithm maintains a "frontier" of partial paths at each node and performs ordered processing to ensure that Pareto-optimal paths are generated to reach the goal node. The algorithm becomes computationally intractable at a higher number of objectives due to a rapid increase in the search space for non-dominated paths and the significant increase in Pareto-optimal solutions. While prior works have focused on algorithmic methods to reduce the complexity, we tackle this challenge by exploiting parallelism to accelerate the MOS problem. The key insight is that MOS algorithms rely on the ordered execution of partial paths to maintain high work efficiency. The proposed parallel algorithm (OPMOS) unlocks ordered parallelism and efficiently exploits the concurrent execution of multiple paths in MOS. Experimental evaluation using the NVIDIA GH200 Superchip's 72-core Arm-based CPU shows the performance scaling potential of OPMOS on work efficiency and parallelism using a real-world application to ship routing.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 4.3 -->
                
            <!-- Reinforcement Learning: 4.0 -->
                
            <!-- Medicine: 3.5 -->
                
            <!-- Networks: 3.2 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.1487
            </span>
            <a href="https://arxiv.org/abs/2501.00016" target="_blank" rel="noopener noreferrer">Predicting Crack Nucleation and Propagation in Brittle Materials Using Deep Operator Networks with Diverse Trunk Architectures</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Elham Kiyani (Division of Applied Mathematics, Brown University, Providence, RI, USA), Manav Manav (Department of Mechanical and Process Engineering, ETH Zurich, Zurich, Switzerland), Nikhil Kadivar (School of Engineering, Providence, RI, USA), Laura De Lorenzis (Department of Mechanical and Process Engineering, ETH Zurich, Zurich, Switzerland), George Em Karniadakis (Division of Applied Mathematics, Brown University, Providence, RI, USA) | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Phase-field modeling reformulates fracture problems as energy minimization problems and enables a comprehensive characterization of the fracture process, including crack nucleation, propagation, merging, and branching, without relying on ad-hoc assumptions. However, the numerical solution of phase-f</span>
            
            <span class="abstract-full" style="display: none;">Phase-field modeling reformulates fracture problems as energy minimization problems and enables a comprehensive characterization of the fracture process, including crack nucleation, propagation, merging, and branching, without relying on ad-hoc assumptions. However, the numerical solution of phase-field fracture problems is characterized by a high computational cost. To address this challenge, in this paper, we employ a deep neural operator (DeepONet) consisting of a branch network and a trunk network to solve brittle fracture problems. We explore three distinct approaches that vary in their trunk network configurations. In the first approach, we demonstrate the effectiveness of a two-step DeepONet, which results in a simplification of the learning task. In the second approach, we employ a physics-informed DeepONet, whereby the mathematical expression of the energy is integrated into the trunk network's loss to enforce physical consistency. The integration of physics also results in a substantially smaller data size needed for training. In the third approach, we replace the neural network in the trunk with a Kolmogorov-Arnold Network and train it without the physics loss. Using these methods, we model crack nucleation in a one-dimensional homogeneous bar under prescribed end displacements, as well as crack propagation and branching in single edge-notched specimens with varying notch lengths subjected to tensile and shear loading. We show that the networks predict the solution fields accurately, and the error in the predicted fields is localized near the crack.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.7 -->
                
            <!-- Reinforcement Learning: 3.3 -->
                
            <!-- LLMs: 3.2 -->
                
            <!-- Robotics: 2.8 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Pathfinding: 1.6 -->
                
            <!-- Quantum Computing: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.1572
            </span>
            <a href="https://arxiv.org/abs/2501.11743" target="_blank" rel="noopener noreferrer">Non-Reversible Langevin Algorithms for Constrained Sampling</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hengrong Du, Qi Feng, Changwei Tu, Xiaoyu Wang, Lingjiong Zhu | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We consider the constrained sampling problem where the goal is to sample from a target distribution on a constrained domain. We propose skew-reflected non-reversible Langevin dynamics (SRNLD), a continuous-time stochastic differential equation with skew-reflected boundary. We obtain non-asymptotic c</span>
            
            <span class="abstract-full" style="display: none;">We consider the constrained sampling problem where the goal is to sample from a target distribution on a constrained domain. We propose skew-reflected non-reversible Langevin dynamics (SRNLD), a continuous-time stochastic differential equation with skew-reflected boundary. We obtain non-asymptotic convergence rate of SRNLD to the target distribution in both total variation and 1-Wasserstein distances. By breaking reversibility, we show that the convergence is faster than the special case of the reversible dynamics. Based on the discretization of SRNLD, we propose skew-reflected non-reversible Langevin Monte Carlo (SRNLMC), and obtain non-asymptotic discretization error from SRNLD, and convergence guarantees to the target distribution in 1-Wasserstein distance. We show better performance guarantees than the projected Langevin Monte Carlo in the literature that is based on the reversible dynamics. Numerical experiments are provided for both synthetic and real datasets to show efficiency of the proposed algorithms.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 5.2 -->
                
            <!-- LLMs: 4.2 -->
                
            <!-- Math: 3.7 -->
                
            <!-- Medicine: 3.7 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.1651
            </span>
            <a href="https://arxiv.org/abs/2410.10370" target="_blank" rel="noopener noreferrer">Innovative Thinking, Infinite Humor: Humor Research of Large Language Models through Structured Thought Leaps</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Han Wang, Yilin Zhao, Dian Li, Xiaohan Wang, Gang Liu, Xuguang Lan, Hui Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Humor is previously regarded as a gift exclusive to humans for the following reasons. Humor is a culturally nuanced aspect of human language, presenting challenges for its understanding and generation. Humor generation necessitates a multi-hop reasoning process, with each hop founded on proper ratio</span>
            
            <span class="abstract-full" style="display: none;">Humor is previously regarded as a gift exclusive to humans for the following reasons. Humor is a culturally nuanced aspect of human language, presenting challenges for its understanding and generation. Humor generation necessitates a multi-hop reasoning process, with each hop founded on proper rationales. Although many studies, such as those related to GPT-o1, focus on logical reasoning with reflection and correction, they still fall short in humor generation. Due to the sparsity of the knowledge graph in creative thinking, it is arduous to achieve multi-hop reasoning. Consequently, in this paper, we propose a more robust framework for addressing the humor reasoning task, named LoL. LoL aims to inject external information to mitigate the sparsity of the knowledge graph, thereby enabling multi-hop reasoning. In the first stage of LoL, we put forward an automatic instruction-evolution method to incorporate the deeper and broader thinking processes underlying humor. Judgment-oriented instructions are devised to enhance the model's judgment capability, dynamically supplementing and updating the sparse knowledge graph. Subsequently, through reinforcement learning, the reasoning logic for each online-generated response is extracted using GPT-4o. In this process, external knowledge is re-introduced to aid the model in logical reasoning and the learning of human preferences. Finally, experimental results indicate that the combination of these two processes can enhance both the model's judgment ability and its generative capacity. These findings deepen our comprehension of the creative capabilities of large language models (LLMs) and offer approaches to boost LLMs' creative abilities for cross-domain innovative applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 40.5 -->
                
            <!-- Medicine: 2.6 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Quantum Computing: 1.4 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.188
            </span>
            <a href="https://arxiv.org/abs/2504.10071" target="_blank" rel="noopener noreferrer">Pay Attention to What and Where? Interpretable Feature Extractor in Vision-based Deep Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tien Pham, Angelo Cangelosi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Current approaches in Explainable Deep Reinforcement Learning have limitations in which the attention mask has a displacement with the objects in visual input. This work addresses a spatial problem within traditional Convolutional Neural Networks (CNNs). We propose the Interpretable Feature Extracto</span>
            
            <span class="abstract-full" style="display: none;">Current approaches in Explainable Deep Reinforcement Learning have limitations in which the attention mask has a displacement with the objects in visual input. This work addresses a spatial problem within traditional Convolutional Neural Networks (CNNs). We propose the Interpretable Feature Extractor (IFE) architecture, aimed at generating an accurate attention mask to illustrate both "what" and "where" the agent concentrates on in the spatial domain. Our design incorporates a Human-Understandable Encoding module to generate a fully interpretable attention mask, followed by an Agent-Friendly Encoding module to enhance the agent's learning efficiency. These two components together form the Interpretable Feature Extractor for vision-based deep reinforcement learning to enable the model's interpretability. The resulting attention mask is consistent, highly understandable by humans, accurate in spatial dimension, and effectively highlights important objects or locations in visual input. The Interpretable Feature Extractor is integrated into the Fast and Data-efficient Rainbow framework, and evaluated on 57 ATARI games to show the effectiveness of the proposed approach on Spatial Preservation, Interpretability, and Data-efficiency. Finally, we showcase the versatility of our approach by incorporating the IFE into the Asynchronous Advantage Actor-Critic Model.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.4 -->
                
            <!-- Reinforcement Learning: 4.9 -->
                
            <!-- LLMs: 4.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Robotics: 2.3 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- Pathfinding: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2177
            </span>
            <a href="https://arxiv.org/abs/2504.11066" target="_blank" rel="noopener noreferrer">Improving fingerprint presentation attack detection by an approach integrated into the personal verification stage</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Marco Micheletto, Giulia Orr\`u, Luca Ghiani, Gian Luca Marcialis | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Presentation Attack Detection (PAD) systems are usually designed independently of the fingerprint verification system. While this can be acceptable for use cases where specific user templates are not predetermined, it represents a missed opportunity to enhance security in scenarios where integrating</span>
            
            <span class="abstract-full" style="display: none;">Presentation Attack Detection (PAD) systems are usually designed independently of the fingerprint verification system. While this can be acceptable for use cases where specific user templates are not predetermined, it represents a missed opportunity to enhance security in scenarios where integrating PAD with the fingerprint verification system could significantly leverage users' templates, which are the real target of a potential presentation attack. This does not mean that a PAD should be specifically designed for such users; that would imply the availability of many enrolled users' PAI and, consequently, complexity, time, and cost increase. On the contrary, we propose to equip a basic PAD, designed according to the state of the art, with an innovative add-on module called the Closeness Binary Code (CC) module. The term "closeness" refers to a peculiar property of the bona fide-related features: in an Euclidean feature space, genuine fingerprints tend to cluster in a specific pattern. First, samples from the same finger are close to each other, then samples from other fingers of the same user and finally, samples from fingers of other users. This property is statistically verified in our previous publication, and further confirmed in this paper. It is independent of the user population and the feature set class, which can be handcrafted or deep network-based (embeddings). Therefore, the add-on can be designed without the need for the targeted user samples; moreover, it exploits her/his samples' "closeness" property during the verification stage. Extensive experiments on benchmark datasets and state-of-the-art PAD methods confirm the benefits of the proposed add-on, which can be easily coupled with the main PAD module integrated into the fingerprint verification system.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Reinforcement Learning: 4.9 -->
                
            <!-- Math: 4.5 -->
                
            <!-- Medicine: 4.2 -->
                
            <!-- LLMs: 3.3 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2237
            </span>
            <a href="https://arxiv.org/abs/2501.18563" target="_blank" rel="noopener noreferrer">No Equations Needed: Learning System Dynamics Without Relying on Closed-Form ODEs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Krzysztof Kacprzyk, Mihaela van der Schaar | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Data-driven modeling of dynamical systems is a crucial area of machine learning. In many scenarios, a thorough understanding of the model's behavior becomes essential for practical applications. For instance, understanding the behavior of a pharmacokinetic model, constructed as part of drug developm</span>
            
            <span class="abstract-full" style="display: none;">Data-driven modeling of dynamical systems is a crucial area of machine learning. In many scenarios, a thorough understanding of the model's behavior becomes essential for practical applications. For instance, understanding the behavior of a pharmacokinetic model, constructed as part of drug development, may allow us to both verify its biological plausibility (e.g., the drug concentration curve is non-negative and decays to zero) and to design dosing guidelines. Discovery of closed-form ordinary differential equations (ODEs) can be employed to obtain such insights by finding a compact mathematical equation and then analyzing it (a two-step approach). However, its widespread use is currently hindered because the analysis process may be time-consuming, requiring substantial mathematical expertise, or even impossible if the equation is too complex. Moreover, if the found equation's behavior does not satisfy the requirements, editing it or influencing the discovery algorithms to rectify it is challenging as the link between the symbolic form of an ODE and its behavior can be elusive. This paper proposes a conceptual shift to modeling low-dimensional dynamical systems by departing from the traditional two-step modeling process. Instead of first discovering a closed-form equation and then analyzing it, our approach, direct semantic modeling, predicts the semantic representation of the dynamical system (i.e., description of its behavior) directly from data, bypassing the need for complex post-hoc analysis. This direct approach also allows the incorporation of intuitive inductive biases into the optimization algorithm and editing the model's behavior directly, ensuring that the model meets the desired specifications. Our approach not only simplifies the modeling pipeline but also enhances the transparency and flexibility of the resulting models compared to traditional closed-form ODEs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 4.8 -->
                
            <!-- Reinforcement Learning: 4.7 -->
                
            <!-- Medicine: 4.0 -->
                
            <!-- Math: 3.2 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.3811
            </span>
            <a href="https://arxiv.org/abs/2405.20252" target="_blank" rel="noopener noreferrer">Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt Optimization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yuchi Liu, Jaskirat Singh, Gaowen Liu, Ali Payani, Liang Zheng | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) have shown great progress in responding to user questions, allowing for a multitude of diverse applications. Yet, the quality of LLM outputs heavily depends on the prompt design, where a good prompt might enable the LLM to answer a very challenging question correctly. Th</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) have shown great progress in responding to user questions, allowing for a multitude of diverse applications. Yet, the quality of LLM outputs heavily depends on the prompt design, where a good prompt might enable the LLM to answer a very challenging question correctly. Therefore, recent works have developed many strategies for improving the prompt, including both manual crafting and in-domain optimization. However, their efficacy in unrestricted scenarios remains questionable, as the former depends on human design for specific questions and the latter usually generalizes poorly to unseen scenarios. To address these problems, we give LLMs the freedom to design the best prompts according to themselves. Specifically, we include a hierarchy of LLMs, first constructing a prompt with precise instructions and accurate wording in a hierarchical manner, and then using this prompt to generate the final answer to the user query. We term this pipeline Hierarchical Multi-Agent Workflow, or HMAW. In contrast with prior works, HMAW imposes no human restriction and requires no training, and is completely task-agnostic while capable of adjusting to the nuances of the underlying task. Through both quantitative and qualitative experiments across multiple benchmarks, we verify that despite its simplicity, the proposed approach can create detailed and suitable prompts, further boosting the performance of current LLMs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.0 -->
                
            <!-- Medicine: 5.1 -->
                
            <!-- Reinforcement Learning: 3.4 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.8548
            </span>
            <a href="https://arxiv.org/abs/2501.04486" target="_blank" rel="noopener noreferrer">MB-TaylorFormer V2: Improved Multi-branch Linear Transformer Expanded by Taylor Formula for Image Restoration</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhi Jin, Yuwei Qiu, Kaihao Zhang, Hongdong Li, Wenhan Luo | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recently, Transformer networks have demonstrated outstanding performance in the field of image restoration due to the global receptive field and adaptability to input. However, the quadratic computational complexity of Softmax-attention poses a significant limitation on its extensive application in </span>
            
            <span class="abstract-full" style="display: none;">Recently, Transformer networks have demonstrated outstanding performance in the field of image restoration due to the global receptive field and adaptability to input. However, the quadratic computational complexity of Softmax-attention poses a significant limitation on its extensive application in image restoration tasks, particularly for high-resolution images. To tackle this challenge, we propose a novel variant of the Transformer. This variant leverages the Taylor expansion to approximate the Softmax-attention and utilizes the concept of norm-preserving mapping to approximate the remainder of the first-order Taylor expansion, resulting in a linear computational complexity. Moreover, we introduce a multi-branch architecture featuring multi-scale patch embedding into the proposed Transformer, which has four distinct advantages: 1) various sizes of the receptive field; 2) multi-level semantic information; 3) flexible shapes of the receptive field; 4) accelerated training and inference speed. Hence, the proposed model, named the second version of Taylor formula expansion-based Transformer (for short MB-TaylorFormer V2) has the capability to concurrently process coarse-to-fine features, capture long-distance pixel interactions with limited computational cost, and improve the approximation of the Taylor expansion remainder. Experimental results across diverse image restoration benchmarks demonstrate that MB-TaylorFormer V2 achieves state-of-the-art performance in multiple image restoration tasks, such as image dehazing, deraining, desnowing, motion deblurring, and denoising, with very little computational overhead. The source code is available at https://github.com/FVL2020/MB-TaylorFormerV2.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.5 -->
                
            <!-- LLMs: 5.4 -->
                
            <!-- Reinforcement Learning: 4.1 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Federated Learning: 2.2 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.9669
            </span>
            <a href="https://arxiv.org/abs/2504.10637" target="_blank" rel="noopener noreferrer">Better Estimation of the KL Divergence Between Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Afra Amini, Tim Vieira, Ryan Cotterell | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Estimating the Kullback--Leibler (KL) divergence between language models has many applications, e.g., reinforcement learning from human feedback (RLHF), interpretability, and knowledge distillation. However, computing the exact KL divergence between two arbitrary language models is intractable. Thus</span>
            
            <span class="abstract-full" style="display: none;">Estimating the Kullback--Leibler (KL) divergence between language models has many applications, e.g., reinforcement learning from human feedback (RLHF), interpretability, and knowledge distillation. However, computing the exact KL divergence between two arbitrary language models is intractable. Thus, practitioners often resort to the use of sampling-based estimators. While it is easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased estimate of the KL divergence between language models, this estimator notoriously suffers from high variance, and can even result in a negative estimate of the KL divergence, a non-negative quantity. In this paper, we introduce a Rao--Blackwellized estimator that is also unbiased and provably has variance less than or equal to that of the standard Monte Carlo estimator. In an empirical study on sentiment-controlled fine-tuning, we show that our estimator provides more stable KL estimates and reduces variance substantially in practice. Additionally, we derive an analogous Rao--Blackwellized estimator of the gradient of the KL divergence, which leads to more stable training and produces models that more frequently appear on the Pareto frontier of reward vs. KL compared to the ones trained with the MC estimator of the gradient.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.1 -->
                
            <!-- Reinforcement Learning: 4.0 -->
                
            <!-- Medicine: 4.0 -->
                
            <!-- Math: 2.6 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Networks: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.0716
            </span>
            <a href="https://arxiv.org/abs/2504.08982" target="_blank" rel="noopener noreferrer">Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot Continual Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kyle Stein, Andrew Arash Mahyari, Guillermo Francia III, Eman El-Sheikh | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Integrating new class information without losing previously acquired knowledge remains a central challenge in artificial intelligence, often referred to as catastrophic forgetting. Few-shot class incremental learning (FSCIL) addresses this by first training a model on a robust dataset of base classe</span>
            
            <span class="abstract-full" style="display: none;">Integrating new class information without losing previously acquired knowledge remains a central challenge in artificial intelligence, often referred to as catastrophic forgetting. Few-shot class incremental learning (FSCIL) addresses this by first training a model on a robust dataset of base classes and then incrementally adapting it in successive sessions using only a few labeled examples per novel class. However, this approach is prone to overfitting on the limited new data, which can compromise overall performance and exacerbate forgetting. In this work, we propose a simple yet effective novel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone augmented with parameter-efficient additive updates. Our approach freezes the pre-trained ViT parameters and selectively injects trainable weights into the self-attention modules via an additive update mechanism. This design updates only a small subset of parameters to accommodate new classes without sacrificing the representations learned during the base session. By fine-tuning a limited number of parameters, our method preserves the generalizable features in the frozen ViT while reducing the risk of overfitting. Furthermore, as most parameters remain fixed, the model avoids overwriting previously learned knowledge when small novel data batches are introduced. Extensive experiments on benchmark datasets demonstrate that our approach yields state-of-the-art performance compared to baseline FSCIL methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.5 -->
                
            <!-- Medicine: 5.7 -->
                
            <!-- Reinforcement Learning: 4.2 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Attention: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.1619
            </span>
            <a href="https://arxiv.org/abs/2504.10541" target="_blank" rel="noopener noreferrer">Multi-Modal Hypergraph Enhanced LLM Learning for Recommendation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xu Guo, Tong Zhang, Yuanzhi Wang, Chenxu Wang, Fuyun Wang, Xudong Wang, Xiaoya Zhang, Xin Liu, Zhen Cui | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The burgeoning presence of Large Language Models (LLM) is propelling the development of personalized recommender systems. Most existing LLM-based methods fail to sufficiently explore the multi-view graph structure correlations inherent in recommendation scenarios. To this end, we propose a novel fra</span>
            
            <span class="abstract-full" style="display: none;">The burgeoning presence of Large Language Models (LLM) is propelling the development of personalized recommender systems. Most existing LLM-based methods fail to sufficiently explore the multi-view graph structure correlations inherent in recommendation scenarios. To this end, we propose a novel framework, Hypergraph Enhanced LLM Learning for multimodal Recommendation (HeLLM), designed to equip LLMs with the capability to capture intricate higher-order semantic correlations by fusing graph-level contextual signals with sequence-level behavioral patterns. In the recommender pre-training phase, we design a user hypergraph to uncover shared interest preferences among users and an item hypergraph to capture correlations within multimodal similarities among items. The hypergraph convolution and synergistic contrastive learning mechanism are introduced to enhance the distinguishability of learned representations. In the LLM fine-tuning phase, we inject the learned graph-structured embeddings directly into the LLM's architecture and integrate sequential features capturing each user's chronological behavior. This process enables hypergraphs to leverage graph-structured information as global context, enhancing the LLM's ability to perceive complex relational patterns and integrate multimodal information, while also modeling local temporal dynamics. Extensive experiments demonstrate the superiority of our proposed method over state-of-the-art baselines, confirming the advantages of fusing hypergraph-based context with sequential user behavior in LLMs for recommendation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 17.6 -->
                
            <!-- Medicine: 5.2 -->
                
            <!-- Reinforcement Learning: 2.9 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.3172
            </span>
            <a href="https://arxiv.org/abs/2504.10777" target="_blank" rel="noopener noreferrer">AtlasD: Automatic Local Symmetry Discovery</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Manu Bhat, Jonghyun Park, Jianke Yang, Nima Dehmamy, Robin Walters, Rose Yu | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Existing symmetry discovery methods predominantly focus on global transformations across the entire system or space, but they fail to consider the symmetries in local neighborhoods. This may result in the reported symmetry group being a misrepresentation of the true symmetry. In this paper, we forma</span>
            
            <span class="abstract-full" style="display: none;">Existing symmetry discovery methods predominantly focus on global transformations across the entire system or space, but they fail to consider the symmetries in local neighborhoods. This may result in the reported symmetry group being a misrepresentation of the true symmetry. In this paper, we formalize the notion of local symmetry as atlas equivariance. Our proposed pipeline, automatic local symmetry discovery (AtlasD), recovers the local symmetries of a function by training local predictor networks and then learning a Lie group basis to which the predictors are equivariant. We demonstrate AtlasD is capable of discovering local symmetry groups with multiple connected components in top-quark tagging and partial differential equation experiments. The discovered local symmetry is shown to be a useful inductive bias that improves the performance of downstream tasks in climate segmentation and vision tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 6.1 -->
                
            <!-- Medicine: 4.6 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Reinforcement Learning: 2.9 -->
                
            <!-- Federated Learning: 2.5 -->
                
            <!-- GNN: 2.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.3283
            </span>
            <a href="https://arxiv.org/abs/2504.08019" target="_blank" rel="noopener noreferrer">DGFamba: Learning Flow Factorized State Space for Visual Domain Generalization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Qi Bi, Jingjun Yi, Hao Zheng, Haolan Zhan, Wei Ji, Yawen Huang, Yuexiang Li | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Domain generalization aims to learn a representation from the source domain, which can be generalized to arbitrary unseen target domains. A fundamental challenge for visual domain generalization is the domain gap caused by the dramatic style variation whereas the image content is stable. The realm o</span>
            
            <span class="abstract-full" style="display: none;">Domain generalization aims to learn a representation from the source domain, which can be generalized to arbitrary unseen target domains. A fundamental challenge for visual domain generalization is the domain gap caused by the dramatic style variation whereas the image content is stable. The realm of selective state space, exemplified by VMamba, demonstrates its global receptive field in representing the content. However, the way exploiting the domain-invariant property for selective state space is rarely explored. In this paper, we propose a novel Flow Factorized State Space model, dubbed as DG-Famba, for visual domain generalization. To maintain domain consistency, we innovatively map the style-augmented and the original state embeddings by flow factorization. In this latent flow space, each state embedding from a certain style is specified by a latent probability path. By aligning these probability paths in the latent space, the state embeddings are able to represent the same content distribution regardless of the style differences. Extensive experiments conducted on various visual domain generalization settings show its state-of-the-art performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 4.4 -->
                
            <!-- LLMs: 4.1 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Reinforcement Learning: 3.0 -->
                
            <!-- Math: 2.7 -->
                
            <!-- Federated Learning: 2.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Evolutionary Algorithms: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.4377
            </span>
            <a href="https://arxiv.org/abs/2504.08094" target="_blank" rel="noopener noreferrer">Improving Multiresource Job Scheduling with Markovian Service Rate Policies</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhongrui Chen, Isaac Grosof, Benjamin Berg | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Modern cloud computing workloads are composed of multiresource jobs that require a variety of computational resources in order to run, such as CPU cores, memory, disk space, or hardware accelerators. A single cloud server can typically run many multiresource jobs in parallel, but only if the server </span>
            
            <span class="abstract-full" style="display: none;">Modern cloud computing workloads are composed of multiresource jobs that require a variety of computational resources in order to run, such as CPU cores, memory, disk space, or hardware accelerators. A single cloud server can typically run many multiresource jobs in parallel, but only if the server has sufficient resources to satisfy the demands of every job. A scheduling policy must therefore select sets of multiresource jobs to run in parallel in order to minimize the mean response time across jobs -- the average time from when a job arrives to the system until it is completed. Unfortunately, achieving low response times by selecting sets of jobs that fully utilize the available server resources has proven to be a difficult problem.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Quantum Computing: 5.0 -->
                
            <!-- Reinforcement Learning: 3.5 -->
                
            <!-- Networks: 3.2 -->
                
            <!-- Medicine: 2.6 -->
                
            <!-- GNN: 2.5 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- Attention: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.497
            </span>
            <a href="https://arxiv.org/abs/2504.08413" target="_blank" rel="noopener noreferrer">The Impact of External Sources on the Friedkin-Johnsen Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Charlotte Out, Sijing Tu, Stefan Neumann, Ahad N. Zehmakan | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">To obtain a foundational understanding of timeline algorithms and viral content in shaping public opinions, computer scientists started to study augmented versions of opinion formation models from sociology. In this paper, we generalize the popular Friedkin--Johnsen model to include the effects of e</span>
            
            <span class="abstract-full" style="display: none;">To obtain a foundational understanding of timeline algorithms and viral content in shaping public opinions, computer scientists started to study augmented versions of opinion formation models from sociology. In this paper, we generalize the popular Friedkin--Johnsen model to include the effects of external media sources on opinion formation. Our goal is to mathematically analyze the influence of biased media, arising from factors such as manipulated news reporting or the phenomenon of false balance. Within our framework, we examine the scenario of two opposing media sources, which do not adapt their opinions like ordinary nodes, and analyze the conditions and the number of periods required for radicalizing the opinions in the network. When both media sources possess equal influence, we theoretically characterize the final opinion configuration. In the special case where there is only a single media source present, we prove that media sources which do not adapt their opinions are significantly more powerful than those which do. Lastly, we conduct the experiments on real-world and synthetic datasets, showing that our theoretical guarantees closely align with experimental simulations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 5.3 -->
                
            <!-- Reinforcement Learning: 3.4 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- Math: 2.7 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.5136
            </span>
            <a href="https://arxiv.org/abs/2504.08704" target="_blank" rel="noopener noreferrer">Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vinal Asodia, Zhenhua Feng, Saber Fallah | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Effective leveraging of real-world driving datasets is crucial for enhancing the training of autonomous driving systems. While Offline Reinforcement Learning enables the training of autonomous vehicles using such data, most available datasets lack meaningful reward labels. Reward labeling is essenti</span>
            
            <span class="abstract-full" style="display: none;">Effective leveraging of real-world driving datasets is crucial for enhancing the training of autonomous driving systems. While Offline Reinforcement Learning enables the training of autonomous vehicles using such data, most available datasets lack meaningful reward labels. Reward labeling is essential as it provides feedback for the learning algorithm to distinguish between desirable and undesirable behaviors, thereby improving policy performance. This paper presents a novel pipeline for generating human-aligned reward labels. The proposed approach addresses the challenge of absent reward signals in real-world datasets by generating labels that reflect human judgment and safety considerations. The pipeline incorporates an adaptive safety component, activated by analyzing semantic segmentation maps, allowing the autonomous vehicle to prioritize safety over efficiency in potential collision scenarios. The proposed pipeline is applied to an occluded pedestrian crossing scenario with varying levels of pedestrian traffic, using synthetic and simulation data. The results indicate that the generated reward labels closely match the simulation reward labels. When used to train the driving policy using Behavior Proximal Policy Optimisation, the results are competitive with other baselines. This demonstrates the effectiveness of our method in producing reliable and human-aligned reward signals, facilitating the training of autonomous driving systems through Reinforcement Learning outside of simulation environments and in alignment with human values.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.6 -->
                
            <!-- LLMs: 6.0 -->
                
            <!-- Reinforcement Learning: 5.8 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.6036
            </span>
            <a href="https://arxiv.org/abs/2503.11217" target="_blank" rel="noopener noreferrer">Deep Joint Distribution Optimal Transport for Universal Domain Adaptation on Time Series</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Romain Mussard, Fannia Pacheco, Maxime Berar, Gilles Gasso, Paul Honeine | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Universal Domain Adaptation (UniDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain, even when their classes are not fully shared. Few dedicated UniDA methods exist for Time Series (TS), which remains a challenging case. In general, UniDA approaches align common</span>
            
            <span class="abstract-full" style="display: none;">Universal Domain Adaptation (UniDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain, even when their classes are not fully shared. Few dedicated UniDA methods exist for Time Series (TS), which remains a challenging case. In general, UniDA approaches align common class samples and detect unknown target samples from emerging classes. Such detection often results from thresholding a discriminability metric. The threshold value is typically either a fine-tuned hyperparameter or a fixed value, which limits the ability of the model to adapt to new data. Furthermore, discriminability metrics exhibit overconfidence for unknown samples, leading to misclassifications. This paper introduces UniJDOT, an optimal-transport-based method that accounts for the unknown target samples in the transport cost. Our method also proposes a joint decision space to improve the discriminability of the detection module. In addition, we use an auto-thresholding algorithm to reduce the dependence on fixed or fine-tuned thresholds. Finally, we rely on a Fourier transform-based layer inspired by the Fourier Neural Operator for better TS representation. Experiments on TS benchmarks demonstrate the discriminability, robustness, and state-of-the-art performance of UniJDOT.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.5 -->
                
            <!-- LLMs: 5.1 -->
                
            <!-- Reinforcement Learning: 4.3 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- SpikingNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.6065
            </span>
            <a href="https://arxiv.org/abs/2504.10283" target="_blank" rel="noopener noreferrer">$\alpha$-Flow: A Unified Framework for Continuous-State Discrete Flow Matching Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chaoran Cheng, Jiahan Li, Jiajun Fan, Ge Liu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent efforts have extended the flow-matching framework to discrete generative modeling. One strand of models directly works with the continuous probabilities instead of discrete tokens, which we colloquially refer to as Continuous-State Discrete Flow Matching (CS-DFM). Existing CS-DFM models diffe</span>
            
            <span class="abstract-full" style="display: none;">Recent efforts have extended the flow-matching framework to discrete generative modeling. One strand of models directly works with the continuous probabilities instead of discrete tokens, which we colloquially refer to as Continuous-State Discrete Flow Matching (CS-DFM). Existing CS-DFM models differ significantly in their representations and geometric assumptions. This work presents a unified framework for CS-DFM models, under which the existing variants can be understood as operating on different $\alpha$-representations of probabilities. Building upon the theory of information geometry, we introduce $\alpha$-Flow, a family of CS-DFM models that adheres to the canonical $\alpha$-geometry of the statistical manifold, and demonstrate its optimality in minimizing the generalized kinetic energy. Theoretically, we show that the flow matching loss for $\alpha$-flow establishes a unified variational bound for the discrete negative log-likelihood. We comprehensively evaluate different instantiations of $\alpha$-flow on various discrete generation domains to demonstrate their effectiveness in discrete generative modeling, including intermediate values whose geometries have never been explored before. $\alpha$-flow significantly outperforms its discrete-state counterpart in image and protein sequence generation and better captures the entropy in language modeling.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 5.3 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Reinforcement Learning: 3.1 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.7191
            </span>
            <a href="https://arxiv.org/abs/2503.13983" target="_blank" rel="noopener noreferrer">SpaceVLLM: Endowing Multimodal Large Language Model with Spatio-Temporal Video Grounding Capability</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiankang Wang, Zhihan Zhang, Zhihang Liu, Yang Li, Jiannan Ge, Hongtao Xie, Yongdong Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Multimodal large language models (MLLMs) have made remarkable progress in either temporal or spatial localization. However, they struggle to perform spatio-temporal video grounding. This limitation stems from two major challenges. Firstly, it is difficult to extract accurate spatio-temporal informat</span>
            
            <span class="abstract-full" style="display: none;">Multimodal large language models (MLLMs) have made remarkable progress in either temporal or spatial localization. However, they struggle to perform spatio-temporal video grounding. This limitation stems from two major challenges. Firstly, it is difficult to extract accurate spatio-temporal information of each frame in the video. Secondly, the substantial number of visual tokens makes it challenging to precisely map visual tokens of each frame to their corresponding spatial coordinates. To address these issues, we introduce SpaceVLLM, a MLLM endowed with spatio-temporal video grounding capability. Specifically, we adopt a set of interleaved Spatio-Temporal Aware Queries to capture temporal perception and dynamic spatial information. Moreover, we propose a Query-Guided Space Decoder to establish a corresponding connection between the queries and spatial coordinates. Additionally, due to the lack of spatio-temporal datasets, we construct the Unified Spatio-Temporal Grounding (Uni-STG) dataset, comprising 480K instances across three tasks. This dataset fully exploits the potential of MLLM to simultaneously facilitate localization in both temporal and spatial dimensions. Extensive experiments demonstrate that SpaceVLLM achieves the state-of-the-art performance across 11 benchmarks covering temporal, spatial, spatio-temporal and video understanding tasks, highlighting the effectiveness of our approach. Our code, datasets and model will be released at https://github.com/Jayce1kk/SpaceVLLM.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 19.8 -->
                
            <!-- Medicine: 5.7 -->
                
            <!-- Quantum Computing: 2.9 -->
                
            <!-- Reinforcement Learning: 2.6 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.8211
            </span>
            <a href="https://arxiv.org/abs/2408.11278" target="_blank" rel="noopener noreferrer">The Key of Parameter Skew in Federated Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Junfeng Liao, Sifan Wang, Ye Yuan, Riquan Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Federated Learning (FL) has emerged as an excellent solution for performing deep learning on different data owners without exchanging raw data. However, statistical heterogeneity in FL presents a key challenge, leading to a phenomenon of skewness in local model parameter distributions that researche</span>
            
            <span class="abstract-full" style="display: none;">Federated Learning (FL) has emerged as an excellent solution for performing deep learning on different data owners without exchanging raw data. However, statistical heterogeneity in FL presents a key challenge, leading to a phenomenon of skewness in local model parameter distributions that researchers have largely overlooked. In this work, we propose the concept of parameter skew to describe the phenomenon that can substantially affect the accuracy of global model parameter estimation. Additionally, we introduce FedSA, an aggregation strategy to obtain a high-quality global model, to address the implication from parameter skew. Specifically, we categorize parameters into high-dispersion and low-dispersion groups based on the coefficient of variation. For high-dispersion parameters, Micro-Classes (MIC) and Macro-Classes (MAC) represent the dispersion at the micro and macro levels, respectively, forming the foundation of FedSA. To evaluate the effectiveness of FedSA, we conduct extensive experiments with different FL algorithms on three computer vision datasets. FedSA outperforms eight state-of-the-art baselines by about 4.7% in test accuracy.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #b3ebae" title="Confidence: 75.4%">
                        Federated Learning
                    </span>
            <!-- LLMs: 5.8 -->
                
            <!-- Medicine: 5.6 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Reinforcement Learning: 3.1 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.925
            </span>
            <a href="https://arxiv.org/abs/2503.12045" target="_blank" rel="noopener noreferrer">Auditing Differential Privacy in the Black-Box Setting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kaining Shi, Cong Ma | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper introduces a novel theoretical framework for auditing differential privacy (DP) in a black-box setting. Leveraging the concept of $f$-differential privacy, we explicitly define type I and type II errors and propose an auditing mechanism based on conformal inference. Our approach robustly </span>
            
            <span class="abstract-full" style="display: none;">This paper introduces a novel theoretical framework for auditing differential privacy (DP) in a black-box setting. Leveraging the concept of $f$-differential privacy, we explicitly define type I and type II errors and propose an auditing mechanism based on conformal inference. Our approach robustly controls the type I error rate under minimal assumptions. Furthermore, we establish a fundamental impossibility result, demonstrating the inherent difficulty of simultaneously controlling both type I and type II errors without additional assumptions. Nevertheless, under a monotone likelihood ratio (MLR) assumption, our auditing mechanism effectively controls both errors. We also extend our method to construct valid confidence bands for the trade-off function in the finite-sample regime.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.5 -->
                
            <!-- Medicine: 6.2 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Reinforcement Learning: 2.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.9794
            </span>
            <a href="https://arxiv.org/abs/2501.13341" target="_blank" rel="noopener noreferrer">Multi-aspect Knowledge Distillation with Large Language Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Taegyeong Lee, Jinsik Bang, Soyeong Kwon, Taehwan Kim | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent advancements in deep learning have significantly improved performance on computer vision tasks. Previous image classification methods primarily modify model architectures or add features, and they optimize models using cross-entropy loss on class logits. Since they focus on classifying images</span>
            
            <span class="abstract-full" style="display: none;">Recent advancements in deep learning have significantly improved performance on computer vision tasks. Previous image classification methods primarily modify model architectures or add features, and they optimize models using cross-entropy loss on class logits. Since they focus on classifying images with considering class labels, these methods may struggle to learn various \emph{aspects} of classes (e.g., natural positions and shape changes). Rethinking the previous approach from a novel view, we propose a multi-aspect knowledge distillation method using Multimodal Large Language Models (MLLMs). Our approach involves: 1) querying Large Language Model with multi-aspect questions relevant to the knowledge we want to transfer to the model, 2) extracting corresponding logits from MLLM, and 3) expanding the model's output dimensions to distill these multi-aspect logits. We then apply cross-entropy loss to class logits and binary cross-entropy loss to multi-aspect logits. Through our method, the model can learn not only the knowledge about visual aspects but also the abstract and complex aspects that require a deeper understanding. We primarily apply our method to image classification, and to explore the potential for extending our model, such as object detection. In all experimental results, our method improves the performance of the baselines. Additionally, we analyze the effect of multi-aspect knowledge distillation. These results demonstrate that our method can transfer knowledge about various aspects to the model and the aspect knowledge can enhance model performance in computer vision tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 24.4 -->
                
            <!-- Medicine: 5.3 -->
                
            <!-- Reinforcement Learning: 3.6 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Networks: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0089
            </span>
            <a href="https://arxiv.org/abs/2504.09223" target="_blank" rel="noopener noreferrer">DL-QAT: Weight-Decomposed Low-Rank Quantization-Aware Training for Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wenjin Ke, Zhe Li, Dong Li, Lu Tian, Emad Barsoum | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Improving the efficiency of inference in Large Language Models (LLMs) is a critical area of research. Post-training Quantization (PTQ) is a popular technique, but it often faces challenges at low-bit levels, particularly in downstream tasks. Quantization-aware Training (QAT) can alleviate this probl</span>
            
            <span class="abstract-full" style="display: none;">Improving the efficiency of inference in Large Language Models (LLMs) is a critical area of research. Post-training Quantization (PTQ) is a popular technique, but it often faces challenges at low-bit levels, particularly in downstream tasks. Quantization-aware Training (QAT) can alleviate this problem, but it requires significantly more computational resources. To tackle this, we introduced Weight-Decomposed Low-Rank Quantization-Aware Training (DL-QAT), which merges the advantages of QAT while training only less than 1% of the total parameters. Specifically, we introduce a group-specific quantization magnitude to adjust the overall scale of each quantization group. Within each quantization group, we use LoRA matrices to update the weight size and direction in the quantization space. We validated the effectiveness of our method on the LLaMA and LLaMA2 model families. The results show significant improvements over our baseline method across different quantization granularities. For instance, for LLaMA-7B, our approach outperforms the previous state-of-the-art method by 4.2% in MMLU on 3-bit LLaMA-7B model. Additionally, our quantization results on pre-trained models also surpass previous QAT methods, demonstrating the superior performance and efficiency of our approach.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 22.8 -->
                
            <!-- Medicine: 5.2 -->
                
            <!-- Reinforcement Learning: 2.7 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Networks: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0341
            </span>
            <a href="https://arxiv.org/abs/2305.16524" target="_blank" rel="noopener noreferrer">Classical Distributive Restriction Categories</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Robin Cockett, Jean-Simon Pacaud Lemay | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In the category of sets and partial functions, $\mathsf{PAR}$, while the disjoint union $\sqcup$ is the usual categorical coproduct, the Cartesian product $\times$ becomes a restriction categorical analogue of the categorical product: a restriction product. Nevertheless, $\mathsf{PAR}$ does have a u</span>
            
            <span class="abstract-full" style="display: none;">In the category of sets and partial functions, $\mathsf{PAR}$, while the disjoint union $\sqcup$ is the usual categorical coproduct, the Cartesian product $\times$ becomes a restriction categorical analogue of the categorical product: a restriction product. Nevertheless, $\mathsf{PAR}$ does have a usual categorical product as well in the form $A \& B := A \sqcup B \sqcup (A \times B)$. Surprisingly, asking that a distributive restriction category (a restriction category with restriction products $\times$ and coproducts $\oplus$) has $A \& B$ a categorical product is enough to imply that the category is a classical restriction category. This is a restriction category which has joins and relative complements and, thus, supports classical Boolean reasoning. The first and main observation of the paper is that a distributive restriction category is classical if and only if $A \& B := A \oplus B \oplus (A \times B)$ is a categorical product in which case we call $\&$ the ''classical'' product.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.7 -->
                
            <!-- Networks: 4.0 -->
                
            <!-- LLMs: 3.6 -->
                
            <!-- Quantum Computing: 2.9 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Reinforcement Learning: 2.5 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1607
            </span>
            <a href="https://arxiv.org/abs/2504.10342" target="_blank" rel="noopener noreferrer">VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, Xiang Yue | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing </span>
            
            <span class="abstract-full" style="display: none;">Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with "thinking" modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 30.9 -->
                
            <!-- Medicine: 4.0 -->
                
            <!-- RAG: 3.5 -->
                
            <!-- 3D: 2.7 -->
                
            <!-- Blockchain: 2.5 -->
                
            <!-- T2I: 2.0 -->
                
            <!-- Quantum Computing: 2.0 -->
                
            <!-- HPO and AutoML: 1.5 -->
                
            <!-- Datasets: 1.1 -->
                
            <!-- GNN: 1.0 -->
                
            <!-- Quality Diversity: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1744
            </span>
            <a href="https://arxiv.org/abs/2411.18318" target="_blank" rel="noopener noreferrer">Scaled Relative Graph Analysis of Lur'e Systems and the Generalized Circle Criterion</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Julius P. J. Krebbekx, Roland T\'oth, Amritam Das | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain method for the analysis of nonlinear systems. However, we show that the current SRG analysis suffers from a pitfall that limit its applicability in analyzing practical nonlinear systems. We overcome this pitfall by modifying th</span>
            
            <span class="abstract-full" style="display: none;">Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain method for the analysis of nonlinear systems. However, we show that the current SRG analysis suffers from a pitfall that limit its applicability in analyzing practical nonlinear systems. We overcome this pitfall by modifying the SRG of a linear time invariant operator, combining the SRG with the Nyquist criterion, and apply our result to Lur'e systems. We thereby obtain a generalization of the celebrated circle criterion, which deals with a broader class of nonlinearities, and provides (incremental) $L_2$-gain performance bounds.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 6.9 -->
                
            <!-- Medicine: 6.6 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Reinforcement Learning: 3.2 -->
                
            <!-- Math: 2.6 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2523
            </span>
            <a href="https://arxiv.org/abs/2504.08823" target="_blank" rel="noopener noreferrer">FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xiaobing Yu, Jin Yang, Xiao Wu, Peijie Qiu, Xiaofeng Liu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">How to adapt a pre-trained model continuously for sequential tasks with different prediction class labels and domains and finally learn a generalizable model across diverse tasks is a long-lasting challenge. Continual learning (CL) has emerged as a promising approach to leverage pre-trained models (</span>
            
            <span class="abstract-full" style="display: none;">How to adapt a pre-trained model continuously for sequential tasks with different prediction class labels and domains and finally learn a generalizable model across diverse tasks is a long-lasting challenge. Continual learning (CL) has emerged as a promising approach to leverage pre-trained models (e.g., Transformers) for sequential tasks. While many existing CL methods incrementally store additional learned structures, such as Low-Rank Adaptation (LoRA) adapters or prompts and sometimes even preserve features from previous samples to maintain performance. This leads to unsustainable parameter growth and escalating storage costs as the number of tasks increases. Moreover, current approaches often lack task similarity awareness, which further hinders the models ability to effectively adapt to new tasks without interfering with previously acquired knowledge. To address these challenges, we propose FM-LoRA, a novel and efficient low-rank adaptation method that integrates both a dynamic rank selector (DRS) and dynamic meta-prompting (DMP). This framework allocates model capacity more effectively across tasks by leveraging a shared low-rank subspace critical for preserving knowledge, thereby avoiding continual parameter expansion. Extensive experiments on various CL benchmarks, including ImageNet-R, CIFAR100, and CUB200 for class-incremental learning (CIL), and DomainNet for domain-incremental learning (DIL), with Transformers backbone demonstrate that FM-LoRA effectively mitigates catastrophic forgetting while delivering robust performance across a diverse range of tasks and domains.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 10.9 -->
                
            <!-- LLMs: 8.2 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.5 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- RAG: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.345
            </span>
            <a href="https://arxiv.org/abs/2504.06006" target="_blank" rel="noopener noreferrer">Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Roman Kochnev, Arash Torabi Goodarzi, Zofia Antonina Bentyn, Dmitry Ignatov, Radu Timofte | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Optimal hyperparameter selection is critical for maximizing neural network performance, especially as models grow in complexity. This work investigates the viability of leveraging large language models (LLMs) for hyperparameter optimization by fine-tuning a parameter-efficient version of Code Llama </span>
            
            <span class="abstract-full" style="display: none;">Optimal hyperparameter selection is critical for maximizing neural network performance, especially as models grow in complexity. This work investigates the viability of leveraging large language models (LLMs) for hyperparameter optimization by fine-tuning a parameter-efficient version of Code Llama using LoRA. The adapted LLM is capable of generating accurate and efficient hyperparameter recommendations tailored to diverse neural network architectures. Unlike traditional approaches such as Optuna, which rely on computationally intensive trial-and-error procedures, our method achieves competitive or superior results in terms of Root Mean Square Error (RMSE) while significantly reducing computational overhead. Our findings demonstrate that LLM-based optimization not only matches the performance of state-of-the-art techniques like Tree-structured Parzen Estimators (TPE) but also substantially accelerates the tuning process. This positions LLMs as a promising alternative for rapid experimentation, particularly in resource-constrained environments such as edge devices and mobile platforms, where computational efficiency is essential. In addition to improved efficiency, the method offers time savings and consistent performance across various tasks, highlighting its robustness and generalizability. All generated hyperparameters are included in the LEMUR Neural Network (NN) Dataset, which is publicly available and serves as an open-source benchmark for hyperparameter optimization research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.8 -->
                
            <!-- Medicine: 6.7 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Blockchain: 1.5 -->
                
            <!-- RAG: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4022
            </span>
            <a href="https://arxiv.org/abs/2411.08717" target="_blank" rel="noopener noreferrer">Short note on the mapping of heritage sites impacted by the 2024 floods in Valencia, Spain</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Josep Grau-Bove, Richard Higham, Scott Orr, Pakhee Kumar | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This short note presents preliminary findings on the impact of the October 2024 floods on cultural heritage sites in Valencia, Spain. Using publicly available data, we assess the extent of potential damage by overlaying flood maps with heritage site coordinates. We identify that 3.3% of heritage sit</span>
            
            <span class="abstract-full" style="display: none;">This short note presents preliminary findings on the impact of the October 2024 floods on cultural heritage sites in Valencia, Spain. Using publicly available data, we assess the extent of potential damage by overlaying flood maps with heritage site coordinates. We identify that 3.3% of heritage sites in the region have been potentially impacted, with churches and shrines (81), outdoor religious iconography (78), and historic irrigation features (45) being the most heavily affected. Our analysis utilizes data from OpenStreetMap and listings from the Generalitat Valenciana, suggesting that while OpenStreetMap's crowd-sourced data can provide useful estimates of the proportion of impacted sites, it may not be suitable for a detailed damage assessment. By sharing this data openly, we aim to contribute to international efforts in preserving cultural heritage after the disaster and provide a foundation for future assessments of heritage site vulnerability to climate-related events.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Reinforcement Learning: 2.6 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4105
            </span>
            <a href="https://arxiv.org/abs/2402.04621" target="_blank" rel="noopener noreferrer">Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Soo Yong Lee, Sunwoo Kim, Fanchen Bu, Jaemin Yoo, Jiliang Tang, Kijung Shin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">How would randomly shuffling feature vectors among nodes from the same class affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs the dependence between graph topology and features (A-X dependence) for GNNs to learn from. Surprisingly, we observe a consistent and significa</span>
            
            <span class="abstract-full" style="display: none;">How would randomly shuffling feature vectors among nodes from the same class affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs the dependence between graph topology and features (A-X dependence) for GNNs to learn from. Surprisingly, we observe a consistent and significant improvement in GNN performance following the feature shuffle. Having overlooked the impact of A-X dependence on GNNs, the prior literature does not provide a satisfactory understanding of the phenomenon. Thus, we raise two research questions. First, how should A-X dependence be measured, while controlling for potential confounds? Second, how does A-X dependence affect GNNs? In response, we (i) propose a principled measure for A-X dependence, (ii) design a random graph model that controls A-X dependence, (iii) establish a theory on how A-X dependence relates to graph convolution, and (iv) present empirical analysis on real-world graphs that align with the theory. We conclude that A-X dependence mediates the effect of graph convolution, such that smaller dependence improves GNN-based node classification.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.8 -->
                
            <!-- LLMs: 6.2 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Reinforcement Learning: 3.0 -->
                
            <!-- GNN: 2.5 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4228
            </span>
            <a href="https://arxiv.org/abs/2504.09836" target="_blank" rel="noopener noreferrer">Score Matching Diffusion Based Feedback Control and Planning of Nonlinear Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Karthik Elamvazhuthi, Darshan Gadginmath, Fabio Pasqualetti | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose a novel control-theoretic framework that leverages principles from generative modeling -- specifically, Denoising Diffusion Probabilistic Models (DDPMs) -- to stabilize control-affine systems with nonholonomic constraints. Unlike traditional stochastic approaches, which rely on noise-driv</span>
            
            <span class="abstract-full" style="display: none;">We propose a novel control-theoretic framework that leverages principles from generative modeling -- specifically, Denoising Diffusion Probabilistic Models (DDPMs) -- to stabilize control-affine systems with nonholonomic constraints. Unlike traditional stochastic approaches, which rely on noise-driven dynamics in both forward and reverse processes, our method crucially eliminates the need for noise in the reverse phase, making it particularly relevant for control applications. We introduce two formulations: one where noise perturbs all state dimensions during the forward phase while the control system enforces time reversal deterministically, and another where noise is restricted to the control channels, embedding system constraints directly into the forward process.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Reinforcement Learning: 3.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4786
            </span>
            <a href="https://arxiv.org/abs/2504.05522" target="_blank" rel="noopener noreferrer">User Feedback Alignment for LLM-powered Exploration in Large-scale Recommendation Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jianling Wang, Yifan Liu, Yinghao Sun, Xuejian Ma, Yueqi Wang, He Ma, Zhengyang Su, Minmin Chen, Mingyan Gao, Onkar Dalal, Ed H. Chi, Lichan Hong, Ningren Han, Haokai Lu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Exploration, the act of broadening user experiences beyond their established preferences, is challenging in large-scale recommendation systems due to feedback loops and limited signals on user exploration patterns. Large Language Models (LLMs) offer potential by leveraging their world knowledge to r</span>
            
            <span class="abstract-full" style="display: none;">Exploration, the act of broadening user experiences beyond their established preferences, is challenging in large-scale recommendation systems due to feedback loops and limited signals on user exploration patterns. Large Language Models (LLMs) offer potential by leveraging their world knowledge to recommend novel content outside these loops. A key challenge is aligning LLMs with user preferences while preserving their knowledge and reasoning. While using LLMs to plan for the next novel user interest, this paper introduces a novel approach combining hierarchical planning with LLM inference-time scaling to improve recommendation relevancy without compromising novelty. We decouple novelty and user-alignment, training separate LLMs for each objective. We then scale up the novelty-focused LLM's inference and select the best-of-n predictions using the user-aligned LLM. Live experiments demonstrate efficacy, showing significant gains in both user satisfaction (measured by watch activity and active user counts) and exploration diversity.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 21.4 -->
                
            <!-- Medicine: 6.4 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- RAG: 2.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4928
            </span>
            <a href="https://arxiv.org/abs/2504.08247" target="_blank" rel="noopener noreferrer">Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Liu Xiao, Li Zhiyuan, Lin Yueyu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">State-based sequence models like RWKV-7 offer a compelling alternative to Transformer architectures, achieving linear complexity while demonstrating greater expressive power in short-context scenarios and enabling state tracking beyond the \(\text{TC}^0\) complexity class. However, RWKV-7 lacks mech</span>
            
            <span class="abstract-full" style="display: none;">State-based sequence models like RWKV-7 offer a compelling alternative to Transformer architectures, achieving linear complexity while demonstrating greater expressive power in short-context scenarios and enabling state tracking beyond the \(\text{TC}^0\) complexity class. However, RWKV-7 lacks mechanisms for token-parameter interactions and native scalability, limiting its adaptability and growth without retraining. In this paper, we propose \textbf{Meta-State}, a novel extension to RWKV-7 that replaces attention mechanisms with a fully state-driven approach, integrating token-parameter interactions through a \textbf{Self-State Encoder} (SSE) mechanism. The SSE repurposes a portion of the RWKV-7 Weighted Key-Value (WKV) state as transformation weights to encode token-parameter interactions in a linear, state-driven manner without introducing new trainable matrices or softmax operations, while preserving the autoregressive property of token processing. Meta-State supports progressive model scaling by expanding the WKV state and parameter tokens, reusing existing parameters without retraining. Our approach bridges the gap between state-based modeling, token-parameter interactions, and scalable architectures, offering a flexible framework for efficient and adaptable sequence modeling with linear complexity and constant memory usage.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.8 -->
                
            <!-- LLMs: 8.6 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Reinforcement Learning: 2.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Attention: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.5031
            </span>
            <a href="https://arxiv.org/abs/2504.08404" target="_blank" rel="noopener noreferrer">Statistical Linear Regression Approach to Kalman Filtering and Smoothing under Cyber-Attacks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kundan Kumar, Muhammad Iqbal, Simo S\"arkk\"a | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Remote state estimation in cyber-physical systems is often vulnerable to cyber-attacks due to wireless connections between sensors and computing units. In such scenarios, adversaries compromise the system by injecting false data or blocking measurement transmissions via denial-of-service attacks, di</span>
            
            <span class="abstract-full" style="display: none;">Remote state estimation in cyber-physical systems is often vulnerable to cyber-attacks due to wireless connections between sensors and computing units. In such scenarios, adversaries compromise the system by injecting false data or blocking measurement transmissions via denial-of-service attacks, distorting sensor readings. This paper develops a Kalman filter and Rauch--Tung--Striebel (RTS) smoother for linear stochastic state-space models subject to cyber-attacked measurements. We approximate the faulty measurement model via generalized statistical linear regression (GSLR). The GSLR-based approximated measurement model is then used to develop a Kalman filter and RTS smoother for the problem. The effectiveness of the proposed algorithms under cyber-attacks is demonstrated through a simulated aircraft tracking experiment.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.6 -->
                
            <!-- LLMs: 7.4 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 3.5 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Math: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.5565
            </span>
            <a href="https://arxiv.org/abs/2504.08328" target="_blank" rel="noopener noreferrer">Towards generalizable single-cell perturbation modeling via the Conditional Monge Gap</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alice Driessen, Benedek Harsanyi, Marianna Rapsomaniki, Jannis Born | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Learning the response of single-cells to various treatments offers great potential to enable targeted therapies. In this context, neural optimal transport (OT) has emerged as a principled methodological framework because it inherently accommodates the challenges of unpaired data induced by cell dest</span>
            
            <span class="abstract-full" style="display: none;">Learning the response of single-cells to various treatments offers great potential to enable targeted therapies. In this context, neural optimal transport (OT) has emerged as a principled methodological framework because it inherently accommodates the challenges of unpaired data induced by cell destruction during data acquisition. However, most existing OT approaches are incapable of conditioning on different treatment contexts (e.g., time, drug treatment, drug dosage, or cell type) and we still lack methods that unanimously show promising generalization performance to unseen treatments. Here, we propose the Conditional Monge Gap which learns OT maps conditionally on arbitrary covariates. We demonstrate its value in predicting single-cell perturbation responses conditional to one or multiple drugs, a drug dosage, or combinations thereof. We find that our conditional models achieve results comparable and sometimes even superior to the condition-specific state-of-the-art on scRNA-seq as well as multiplexed protein imaging data. Notably, by aggregating data across conditions we perform cross-task learning which unlocks remarkable generalization abilities to unseen drugs or drug dosages, widely outperforming other conditional models in capturing heterogeneity (i.e., higher moments) in the perturbed population. Finally, by scaling to hundreds of conditions and testing on unseen drugs, we narrow the gap between structure-based and effect-based drug representations, suggesting a promising path to the successful prediction of perturbation effects for unseen treatments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.8 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Medicine: 4.4 -->
                
            <!-- Reinforcement Learning: 3.3 -->
                
            <!-- Federated Learning: 2.1 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Math: 1.4 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.7909
            </span>
            <a href="https://arxiv.org/abs/2504.11042" target="_blank" rel="noopener noreferrer">LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sukannya Purkayastha, Zhuang Li, Anne Lauscher, Lizhen Qu, Iryna Gurevych | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of `quick' heuristics, referred to as lazy thinking, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve t</span>
            
            <span class="abstract-full" style="display: none;">Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of `quick' heuristics, referred to as lazy thinking, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve the peer-reviewing process. However, there is limited NLP research on this issue, and no real-world dataset exists to support the development of detection tools. This work introduces LazyReview, a dataset of peer-review sentences annotated with fine-grained lazy thinking categories. Our analysis reveals that Large Language Models (LLMs) struggle to detect these instances in a zero-shot setting. However, instruction-based fine-tuning on our dataset significantly boosts performance by 10-20 performance points, highlighting the importance of high-quality training data. Furthermore, a controlled experiment demonstrates that reviews revised with lazy thinking feedback are more comprehensive and actionable than those written without such feedback. We will release our dataset and the enhanced guidelines that can be used to train junior reviewers in the community. (Code available here: https://github.com/UKPLab/arxiv2025-lazy-review)</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.6 -->
                
            <!-- Medicine: 4.8 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.8365
            </span>
            <a href="https://arxiv.org/abs/2504.11358" target="_blank" rel="noopener noreferrer">DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yupei Liu, Yuqi Jia, Jinyuan Jia, Dawn Song, Neil Zhenqiang Gong | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">LLM-integrated applications and agents are vulnerable to prompt injection attacks, where an attacker injects prompts into their inputs to induce attacker-desired outputs. A detection method aims to determine whether a given input is contaminated by an injected prompt. However, existing detection met</span>
            
            <span class="abstract-full" style="display: none;">LLM-integrated applications and agents are vulnerable to prompt injection attacks, where an attacker injects prompts into their inputs to induce attacker-desired outputs. A detection method aims to determine whether a given input is contaminated by an injected prompt. However, existing detection methods have limited effectiveness against state-of-the-art attacks, let alone adaptive ones. In this work, we propose DataSentinel, a game-theoretic method to detect prompt injection attacks. Specifically, DataSentinel fine-tunes an LLM to detect inputs contaminated with injected prompts that are strategically adapted to evade detection. We formulate this as a minimax optimization problem, with the objective of fine-tuning the LLM to detect strong adaptive attacks. Furthermore, we propose a gradient-based method to solve the minimax optimization problem by alternating between the inner max and outer min problems. Our evaluation results on multiple benchmark datasets and LLMs show that DataSentinel effectively detects both existing and adaptive prompt injection attacks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 22.4 -->
                
            <!-- Medicine: 3.7 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- RAG: 1.7 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.8842
            </span>
            <a href="https://arxiv.org/abs/2504.09207" target="_blank" rel="noopener noreferrer">Pneuma: Leveraging LLMs for Tabular Data Representation and Retrieval in an End-to-End System</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Muhammad Imam Luthfi Balaka, David Alexander, Qiming Wang, Yue Gong, Adila Krisnadhi, Raul Castro Fernandez | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Finding relevant tables among databases, lakes, and repositories is the first step in extracting value from data. Such a task remains difficult because assessing whether a table is relevant to a problem does not always depend only on its content but also on the context, which is usually tribal knowl</span>
            
            <span class="abstract-full" style="display: none;">Finding relevant tables among databases, lakes, and repositories is the first step in extracting value from data. Such a task remains difficult because assessing whether a table is relevant to a problem does not always depend only on its content but also on the context, which is usually tribal knowledge known to the individual or team. While tools like data catalogs and academic data discovery systems target this problem, they rely on keyword search or more complex interfaces, limiting non-technical users' ability to find relevant data. The advent of large language models (LLMs) offers a unique opportunity for users to ask questions directly in natural language, making dataset discovery more intuitive, accessible, and efficient.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.6 -->
                
            <!-- Medicine: 6.5 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- RAG: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9138
            </span>
            <a href="https://arxiv.org/abs/2503.19887" target="_blank" rel="noopener noreferrer">AI threats to national security can be countered by an incident regime</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alejandro Ortega | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent progress in AI capabilities has heightened concerns that AI systems could pose a threat to national security, for example, by making it easier for malicious actors to perform cyberattacks on critical national infrastructure, or through loss of control of autonomous AI systems. In parallel, fe</span>
            
            <span class="abstract-full" style="display: none;">Recent progress in AI capabilities has heightened concerns that AI systems could pose a threat to national security, for example, by making it easier for malicious actors to perform cyberattacks on critical national infrastructure, or through loss of control of autonomous AI systems. In parallel, federal legislators in the US have proposed nascent 'AI incident regimes' to identify and counter similar threats. In this paper, we consolidate these two trends and present a timely proposal for a legally mandated post-deployment AI incident regime that aims to counter potential national security threats from AI systems. We start the paper by introducing the concept of 'security-critical' to describe doctors that pose extreme risks to national security, before arguing that 'security-critical' describes civilian nuclear power, aviation, life science dual-use research of concern, and frontier AI development. We then present in detail our AI incident regime proposal, justifying each component of the proposal by demonstrating its similarity to US domestic incident regimes in other 'security-critical' sectors. Finally, we sketch a hypothetical scenario where our proposed AI incident regime deals with an AI cyber incident. Our proposed AI incident regime is split into three phases. The first phase revolves around a novel operationalization of what counts as an 'AI incident' and we suggest that AI providers must create a 'national security case' before deploying a frontier AI system. The second and third phases spell out that AI providers should notify a government agency about incidents, and that the government agency should be involved in amending AI providers' security and safety procedures, in order to counter future threats to national security.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.7 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Medicine: 3.1 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- 3D: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9169
            </span>
            <a href="https://arxiv.org/abs/2504.08408" target="_blank" rel="noopener noreferrer">BOISHOMMO: Holistic Approach for Bangla Hate Speech</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Md Abdullah Al Kafi, Sumit Kumar Banshal, Md Sadman Shakib, Showrov Azam, Tamanna Alam Tabashom | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">One of the most alarming issues in digital society is hate speech (HS) on social media. The severity is so high that researchers across the globe are captivated by this domain. A notable amount of work has been conducted to address the identification and alarm system. However, a noticeable gap exist</span>
            
            <span class="abstract-full" style="display: none;">One of the most alarming issues in digital society is hate speech (HS) on social media. The severity is so high that researchers across the globe are captivated by this domain. A notable amount of work has been conducted to address the identification and alarm system. However, a noticeable gap exists, especially for low-resource languages. Comprehensive datasets are the main problem among the constrained resource languages, such as Bangla. Interestingly, hate speech or any particular speech has no single dimensionality. Similarly, the hate component can simultaneously have multiple abusive attributes, which seems to be missed in the existing datasets. Thus, a multi-label Bangla hate speech dataset named BOISHOMMO has been compiled and evaluated in this work. That includes categories of HS across race, gender, religion, politics, and more. With over two thousand annotated examples, BOISHOMMO provides a nuanced understanding of hate speech in Bangla and highlights the complexities of processing non-Latin scripts. Apart from evaluating with multiple algorithmic approaches, it also highlights the complexities of processing Bangla text and assesses model performance. This unique multi-label approach enriches future hate speech detection and analysis studies for low-resource languages by providing a more nuanced, diverse dataset.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.8 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Federated Learning: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9636
            </span>
            <a href="https://arxiv.org/abs/2501.08897" target="_blank" rel="noopener noreferrer">Automated Retrosynthesis Planning of Macromolecules Using Large Language Models and Knowledge Graphs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Qinyu Ma, Yuhao Zhou, Jianfeng Li | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Identifying reliable synthesis pathways in materials chemistry is a complex task, particularly in polymer science, due to the intricate and often non-unique nomenclature of macromolecules. To address this challenge, we propose an agent system that integrates large language models (LLMs) and knowledg</span>
            
            <span class="abstract-full" style="display: none;">Identifying reliable synthesis pathways in materials chemistry is a complex task, particularly in polymer science, due to the intricate and often non-unique nomenclature of macromolecules. To address this challenge, we propose an agent system that integrates large language models (LLMs) and knowledge graphs. By leveraging LLMs' powerful capabilities for extracting and recognizing chemical substance names, and storing the extracted data in a structured knowledge graph, our system fully automates the retrieval of relevant literatures, extraction of reaction data, database querying, construction of retrosynthetic pathway trees, further expansion through the retrieval of additional literature and recommendation of optimal reaction pathways. By considering the complex interdependencies among chemical reactants, a novel Multi-branched Reaction Pathway Search Algorithm (MBRPS) is proposed to help identify all valid multi-branched reaction pathways, which arise when a single product decomposes into multiple reaction intermediates. In contrast, previous studies were limited to cases where a product decomposes into at most one reaction intermediate. This work represents the first attempt to develop a fully automated retrosynthesis planning agent tailored specially for macromolecules powered by LLMs. Applied to polyimide synthesis, our new approach constructs a retrosynthetic pathway tree with hundreds of pathways and recommends optimized routes, including both known and novel pathways. This demonstrates utilizing LLMs for literature consultation to accomplish specific tasks is possible and crucial for future materials research, given the vast amount of materials-related literature.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 18.2 -->
                
            <!-- Medicine: 9.2 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- 3D: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.965
            </span>
            <a href="https://arxiv.org/abs/2504.08972" target="_blank" rel="noopener noreferrer">Improving municipal responsiveness through AI-powered image analysis in E-Government</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Catalin Vrabie | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Integration of Machine Learning (ML) techniques into public administration marks a new and transformative era for e-government systems. While traditionally e-government studies were focusing on text-based interactions, this one explores the innovative application of ML for image analysis, an approac</span>
            
            <span class="abstract-full" style="display: none;">Integration of Machine Learning (ML) techniques into public administration marks a new and transformative era for e-government systems. While traditionally e-government studies were focusing on text-based interactions, this one explores the innovative application of ML for image analysis, an approach that enables governments to address citizen petitions more efficiently. By using image classification and object detection algorithms, the model proposed in this article supports public institutions in identifying and fast responding to evidence submitted by citizens in picture format, such as infrastructure issues, environmental concerns or other urban issues that citizens might face. The research also highlights the Jevons Paradox as a critical factor, wherein increased efficiency from the citizen side (especially using mobile platforms and apps) may generate higher demand which should lead to scalable and robust solutions. Using as a case study a Romanian municipality who provided datasets of citizen-submitted images, the author analysed and proved that ML can improve accuracy and responsiveness of public institutions. The findings suggest that adopting ML for e-petition systems can not only enhance citizen participation but also speeding up administrative processes, paving the way for more transparent and effective governance. This study contributes to the discourse on e-government 3.0 by showing the potential of Artificial Intelligence (AI) to transform public service delivery, ensuring sustainable (and scalable) solutions for the growing demands of modern urban governance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.5 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.986
            </span>
            <a href="https://arxiv.org/abs/2503.18865" target="_blank" rel="noopener noreferrer">Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Junlan Chen, Kexin Zhang, Daifeng Li, Yangyang Feng, Yuxuan Zhang, Bowen Deng | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The emergence of large language models offers new possibilities for structured exploration of scientific knowledge. Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights. </span>
            
            <span class="abstract-full" style="display: none;">The emergence of large language models offers new possibilities for structured exploration of scientific knowledge. Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights. Specifically, we investigate how knowledge unit--especially those tied to methodological design--can be modeled and recombined to yield research breakthroughs. Our proposed framework addresses two key challenges. First, we introduce a contrastive learning-based mechanism to identify distinguishing features of historically disruptive method combinations within problem-driven contexts. Second, we propose a reasoning-guided Monte Carlo search algorithm that leverages the chain-of-thought capability of LLMs to identify promising knowledge recombinations for new problem statements.Empirical studies across multiple domains show that the framework is capable of modeling the structural dynamics of innovation and successfully highlights combinations with high disruptive potential. This research provides a new path for computationally guided scientific ideation grounded in structured reasoning and historical data modeling.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 22.0 -->
                
            <!-- Medicine: 5.2 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- RAG: 1.5 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9935
            </span>
            <a href="https://arxiv.org/abs/2504.10025" target="_blank" rel="noopener noreferrer">Progressive Transfer Learning for Multi-Pass Fundus Image Restoration</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Uyen Phan, Ozer Can Devecioglu, Serkan Kiranyaz, Moncef Gabbouj | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Diabetic retinopathy is a leading cause of vision impairment, making its early diagnosis through fundus imaging critical for effective treatment planning. However, the presence of poor quality fundus images caused by factors such as inadequate illumination, noise, blurring and other motion artifacts</span>
            
            <span class="abstract-full" style="display: none;">Diabetic retinopathy is a leading cause of vision impairment, making its early diagnosis through fundus imaging critical for effective treatment planning. However, the presence of poor quality fundus images caused by factors such as inadequate illumination, noise, blurring and other motion artifacts yields a significant challenge for accurate DR screening. In this study, we propose progressive transfer learning for multi pass restoration to iteratively enhance the quality of degraded fundus images, ensuring more reliable DR screening. Unlike previous methods that often focus on a single pass restoration, multi pass restoration via PTL can achieve a superior blind restoration performance that can even improve most of the good quality fundus images in the dataset. Initially, a Cycle GAN model is trained to restore low quality images, followed by PTL induced restoration passes over the latest restored outputs to improve overall quality in each pass. The proposed method can learn blind restoration without requiring any paired data while surpassing its limitations by leveraging progressive learning and fine tuning strategies to minimize distortions and preserve critical retinal features. To evaluate PTL's effectiveness on multi pass restoration, we conducted experiments on DeepDRiD, a large scale fundus imaging dataset specifically curated for diabetic retinopathy detection. Our result demonstrates state of the art performance, showcasing PTL's potential as a superior approach to iterative image quality restoration.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.1 -->
                
            <!-- LLMs: 7.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- Reinforcement Learning: 2.4 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Evolutionary Algorithms: 1.7 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9981
            </span>
            <a href="https://arxiv.org/abs/2504.09152" target="_blank" rel="noopener noreferrer">MatWheel: Addressing Data Scarcity in Materials Science Through Synthetic Data</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wentao Li, Yizhe Chen, Jiangjie Qiu, Xiaonan Wang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Data scarcity and the high cost of annotation have long been persistent challenges in the field of materials science. Inspired by its potential in other fields like computer vision, we propose the MatWheel framework, which train the material property prediction model using the synthetic data generat</span>
            
            <span class="abstract-full" style="display: none;">Data scarcity and the high cost of annotation have long been persistent challenges in the field of materials science. Inspired by its potential in other fields like computer vision, we propose the MatWheel framework, which train the material property prediction model using the synthetic data generated by the conditional generative model. We explore two scenarios: fully-supervised and semi-supervised learning. Using CGCNN for property prediction and Con-CDVAE as the conditional generative model, experiments on two data-scarce material property datasets from Matminer database are conducted. Results show that synthetic data has potential in extreme data-scarce scenarios, achieving performance close to or exceeding that of real samples in all two tasks. We also find that pseudo-labels have little impact on generated data quality. Future work will integrate advanced models and optimize generation conditions to boost the effectiveness of the materials data flywheel.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.8 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0405
            </span>
            <a href="https://arxiv.org/abs/2503.00399" target="_blank" rel="noopener noreferrer">Extremely low-bitrate Image Compression Semantically Disentangled by LMMs from a Human Perception Perspective</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Juan Song, Lijie Yang, Mingtao Feng | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">It remains a significant challenge to compress images at extremely low bitrate while achieving both semantic consistency and high perceptual quality. Inspired by human progressive perception mechanism, we propose a Semantically Disentangled Image Compression framework (SEDIC) in this paper. Initiall</span>
            
            <span class="abstract-full" style="display: none;">It remains a significant challenge to compress images at extremely low bitrate while achieving both semantic consistency and high perceptual quality. Inspired by human progressive perception mechanism, we propose a Semantically Disentangled Image Compression framework (SEDIC) in this paper. Initially, an extremely compressed reference image is obtained through a learned image encoder. Then we leverage LMMs to extract essential semantic components, including overall descriptions, object detailed description, and semantic segmentation masks. We propose a training-free Object Restoration model with Attention Guidance (ORAG) built on pre-trained ControlNet to restore object details conditioned by object-level text descriptions and semantic masks. Based on the proposed ORAG, we design a multistage semantic image decoder to progressively restore the details object by object, starting from the extremely compressed reference image, ultimately generating high-quality and high-fidelity reconstructions. Experimental results demonstrate that SEDIC significantly outperforms state-of-the-art approaches, achieving superior perceptual quality and semantic consistency at extremely low-bitrates ($\le$ 0.05 bpp).</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- 3D: 2.5 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- RAG: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0603
            </span>
            <a href="https://arxiv.org/abs/2504.03976" target="_blank" rel="noopener noreferrer">OLAF: An Open Life Science Analysis Framework for Conversational Bioinformatics Powered by Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dylan Riffle, Nima Shirooni, Cody He, Manush Murali, Sovit Nayak, Rishikumar Gopalan, Diego Gonzalez Lopez | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">OLAF (Open Life Science Analysis Framework) is an open-source platform that enables researchers to perform bioinformatics analyses using natural language. By combining large language models (LLMs) with a modular agent-pipe-router architecture, OLAF generates and executes bioinformatics code on real </span>
            
            <span class="abstract-full" style="display: none;">OLAF (Open Life Science Analysis Framework) is an open-source platform that enables researchers to perform bioinformatics analyses using natural language. By combining large language models (LLMs) with a modular agent-pipe-router architecture, OLAF generates and executes bioinformatics code on real scientific data, including formats like .h5ad. The system includes an Angular front end and a Python/Firebase backend, allowing users to run analyses such as single-cell RNA-seq workflows, gene annotation, and data visualization through a simple web interface. Unlike general-purpose AI tools, OLAF integrates code execution, data handling, and scientific libraries in a reproducible, user-friendly environment. It is designed to lower the barrier to computational biology for non-programmers and support transparent, AI-powered life science research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 18.4 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- RAG: 3.1 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- 3D: 2.2 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1703
            </span>
            <a href="https://arxiv.org/abs/2504.10031" target="_blank" rel="noopener noreferrer">Using Reinforcement Learning to Integrate Subjective Wellbeing into Climate Adaptation Decision Making</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Arthur Vandervoort, Miguel Costa, Morten W. Petersen, Martin Drews, Sonja Haustein, Karyn Morrissey, Francisco C. Pereira | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Subjective wellbeing is a fundamental aspect of human life, influencing life expectancy and economic productivity, among others. Mobility plays a critical role in maintaining wellbeing, yet the increasing frequency and intensity of both nuisance and high-impact floods due to climate change are expec</span>
            
            <span class="abstract-full" style="display: none;">Subjective wellbeing is a fundamental aspect of human life, influencing life expectancy and economic productivity, among others. Mobility plays a critical role in maintaining wellbeing, yet the increasing frequency and intensity of both nuisance and high-impact floods due to climate change are expected to significantly disrupt access to activities and destinations, thereby affecting overall wellbeing. Addressing climate adaptation presents a complex challenge for policymakers, who must select and implement policies from a broad set of options with varying effects while managing resource constraints and uncertain climate projections. In this work, we propose a multi-modular framework that uses reinforcement learning as a decision-support tool for climate adaptation in Copenhagen, Denmark. Our framework integrates four interconnected components: long-term rainfall projections, flood modeling, transport accessibility, and wellbeing modeling. This approach enables decision-makers to identify spatial and temporal policy interventions that help sustain or enhance subjective wellbeing over time. By modeling climate adaptation as an open-ended system, our framework provides a structured framework for exploring and evaluating adaptation policy pathways. In doing so, it supports policymakers to make informed decisions that maximize wellbeing in the long run.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.0 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- RAG: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1741
            </span>
            <a href="https://arxiv.org/abs/2504.11081" target="_blank" rel="noopener noreferrer">DPS: Design Pattern Summarisation Using Code Features</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Najam Nazar, Sameer Sikka, Christoph Treude | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Automatic summarisation has been used efficiently in recent years to condense texts, conversations, audio, code, and various other artefacts. A range of methods, from simple template-based summaries to complex machine learning techniques -- and more recently, large language models -- have been emplo</span>
            
            <span class="abstract-full" style="display: none;">Automatic summarisation has been used efficiently in recent years to condense texts, conversations, audio, code, and various other artefacts. A range of methods, from simple template-based summaries to complex machine learning techniques -- and more recently, large language models -- have been employed to generate these summaries. Summarising software design patterns is important because it helps developers quickly understand and reuse complex design concepts, thereby improving software maintainability and development efficiency. However, the generation of summaries for software design patterns has not yet been explored. Our approach utilises code features and JavaParser to parse the code and create a JSON representation. Using an NLG library on this JSON representation, we convert it into natural language text that acts as a summary of the code, capturing the contextual information of the design pattern. Our empirical results indicate that the summaries generated by our approach capture the context in which patterns are applied in the codebase. Statistical evaluations demonstrate that our summaries closely align with human-written summaries, as evident from high values in the ROUGE-L, BLEU-4, NIST, and FrugalScore metrics. A follow-up survey further shows that DPS summaries were rated as capturing context better than human-generated summaries.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.5 -->
                
            <!-- Medicine: 5.4 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Blockchain: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.209
            </span>
            <a href="https://arxiv.org/abs/2504.08862" target="_blank" rel="noopener noreferrer">RTLRepoCoder: Repository-Level RTL Code Completion through the Combination of Fine-Tuning and Retrieval Augmentation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Peiyang Wu, Nan Guo, Junliang Lv, Xiao Xiao, Xiaochun Ye | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As an essential part of modern hardware design, manually writing Register Transfer Level (RTL) code such as Verilog is often labor-intensive. Following the tremendous success of large language models (LLMs), researchers have begun to explore utilizing LLMs for generating RTL code. However, current s</span>
            
            <span class="abstract-full" style="display: none;">As an essential part of modern hardware design, manually writing Register Transfer Level (RTL) code such as Verilog is often labor-intensive. Following the tremendous success of large language models (LLMs), researchers have begun to explore utilizing LLMs for generating RTL code. However, current studies primarily focus on generating simple single modules, which can not meet the demands in real world. In fact, due to challenges in managing long-context RTL code and complex cross-file dependencies, existing solutions cannot handle large-scale Verilog repositories in practical hardware development. As the first endeavor to exclusively adapt LLMs for large-scale RTL development, we propose RTLRepoCoder, a groundbreaking solution that incorporates specific fine-tuning and Retrieval-Augmented Generation (RAG) for repository-level Verilog code completion. Open-source Verilog repositories from the real world, along with an extended context size, are used for domain-specific fine-tuning. The optimized RAG system improves the information density of the input context by retrieving relevant code snippets. Tailored optimizations for RAG are carried out, including the embedding model, the cross-file context splitting strategy, and the chunk size. Our solution achieves state-of-the-art performance on public benchmark, significantly surpassing GPT-4 and advanced domain-specific LLMs on Edit Similarity and Exact Match rate. Comprehensive experiments demonstrate the remarkable effectiveness of our approach and offer insights for future work.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.9 -->
                
            <!-- Medicine: 9.7 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2295
            </span>
            <a href="https://arxiv.org/abs/2504.09291" target="_blank" rel="noopener noreferrer">Towards Explainable Partial-AIGC Image Quality Assessment</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiaying Qian, Ziheng Jia, Zicheng Zhang, Zeyu Zhang, Guangtao Zhai, Xiongkuo Min | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The rapid advancement of AI-driven visual generation technologies has catalyzed significant breakthroughs in image manipulation, particularly in achieving photorealistic localized editing effects on natural scene images (NSIs). Despite extensive research on image quality assessment (IQA) for AI-gene</span>
            
            <span class="abstract-full" style="display: none;">The rapid advancement of AI-driven visual generation technologies has catalyzed significant breakthroughs in image manipulation, particularly in achieving photorealistic localized editing effects on natural scene images (NSIs). Despite extensive research on image quality assessment (IQA) for AI-generated images (AGIs), most studies focus on fully AI-generated outputs (e.g., text-to-image generation), leaving the quality assessment of partial-AIGC images (PAIs)-images with localized AI-driven edits an almost unprecedented field. Motivated by this gap, we construct the first large-scale PAI dataset towards explainable partial-AIGC image quality assessment (EPAIQA), the EPAIQA-15K, which includes 15K images with localized AI manipulation in different regions and over 300K multi-dimensional human ratings. Based on this, we leverage large multi-modal models (LMMs) and propose a three-stage model training paradigm. This paradigm progressively trains the LMM for editing region grounding, quantitative quality scoring, and quality explanation. Finally, we develop the EPAIQA series models, which possess explainable quality feedback capabilities. Our work represents a pioneering effort in the perceptual IQA field for comprehensive PAI quality assessment.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 11.5 -->
                
            <!-- LLMs: 9.8 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.284
            </span>
            <a href="https://arxiv.org/abs/2504.10010" target="_blank" rel="noopener noreferrer">Investigating Environments' and Avatars' Effects on Thermal Perception in Virtual Reality to Reduce Energy Consumption</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Martin Kocur, Niels Henze | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Understanding thermal regulation and subjective perception of temperature is crucial for improving thermal comfort and human energy consumption in times of global warming. Previous work shows that an environment's color temperature affects the experienced temperature. As virtual reality (VR) enables</span>
            
            <span class="abstract-full" style="display: none;">Understanding thermal regulation and subjective perception of temperature is crucial for improving thermal comfort and human energy consumption in times of global warming. Previous work shows that an environment's color temperature affects the experienced temperature. As virtual reality (VR) enables visual immersion, recent work suggests that a VR scene's color temperature also affects experienced temperature. In addition, virtual avatars representing thermal cues influence users' thermal perception and even the body temperature. As immersive technology becomes increasingly prevalent in daily life, leveraging thermal cues to enhance thermal comfort - without relying on actual thermal energy - presents a promising opportunity. Understanding these effects is crucial for optimizing virtual experiences and promoting sustainable energy practices. Therefore, we propose three controlled experiments to learn more about thermal effects caused by virtual worlds and avatars.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.5 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- RAG: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Blockchain: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4425
            </span>
            <a href="https://arxiv.org/abs/2503.19821" target="_blank" rel="noopener noreferrer">IgCraft: A versatile sequence generation framework for antibody discovery and engineering</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Matthew Greenig, Haowen Zhao, Vladimir Radenkovic, Aubin Ramon, Pietro Sormanni | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Designing antibody sequences to better resemble those observed in natural human repertoires is a key challenge in biologics development. We introduce IgCraft: a multi-purpose model for paired human antibody sequence generation, built on Bayesian Flow Networks. IgCraft presents one of the first unifi</span>
            
            <span class="abstract-full" style="display: none;">Designing antibody sequences to better resemble those observed in natural human repertoires is a key challenge in biologics development. We introduce IgCraft: a multi-purpose model for paired human antibody sequence generation, built on Bayesian Flow Networks. IgCraft presents one of the first unified generative modeling frameworks capable of addressing multiple antibody sequence design tasks with a single model, including unconditional sampling, sequence inpainting, inverse folding, and CDR motif scaffolding. Our approach achieves competitive results across the full spectrum of these tasks while constraining generation to the space of human antibody sequences, exhibiting particular strengths in CDR motif scaffolding (grafting) where we achieve state-of-the-art performance in terms of humanness and preservation of structural properties. By integrating previously separate tasks into a single scalable generative model, IgCraft provides a versatile platform for sampling human antibody sequences under a variety of contexts relevant to antibody discovery and engineering. Model code and weights are publicly available at https://github.com/mgreenig/IgCraft.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.7 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4557
            </span>
            <a href="https://arxiv.org/abs/2504.09205" target="_blank" rel="noopener noreferrer">Query-based Knowledge Transfer for Heterogeneous Learning Environments</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Norah Alballa, Wenxuan Zhang, Ziquan Liu, Ahmed M. Abdelmoniem, Mohamed Elhoseiny, Marco Canini | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Decentralized collaborative learning under data heterogeneity and privacy constraints has rapidly advanced. However, existing solutions like federated learning, ensembles, and transfer learning, often fail to adequately serve the unique needs of clients, especially when local data representation is </span>
            
            <span class="abstract-full" style="display: none;">Decentralized collaborative learning under data heterogeneity and privacy constraints has rapidly advanced. However, existing solutions like federated learning, ensembles, and transfer learning, often fail to adequately serve the unique needs of clients, especially when local data representation is limited. To address this issue, we propose a novel framework called Query-based Knowledge Transfer (QKT) that enables tailored knowledge acquisition to fulfill specific client needs without direct data exchange. QKT employs a data-free masking strategy to facilitate communication-efficient query-focused knowledge transfer while refining task-specific parameters to mitigate knowledge interference and forgetting. Our experiments, conducted on both standard and clinical benchmarks, show that QKT significantly outperforms existing collaborative learning methods by an average of 20.91\% points in single-class query settings and an average of 14.32\% points in multi-class query scenarios. Further analysis and ablation studies reveal that QKT effectively balances the learning of new and existing knowledge, showing strong potential for its application in decentralized learning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.8 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- RAG: 2.0 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4898
            </span>
            <a href="https://arxiv.org/abs/2504.11259" target="_blank" rel="noopener noreferrer">The Cambridge Report on Database Research</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anastasia Ailamaki, Samuel Madden, Daniel Abadi, Gustavo Alonso, Sihem Amer-Yahia, Magdalena Balazinska, Philip A. Bernstein, Peter Boncz, Michael Cafarella, Surajit Chaudhuri, Susan Davidson, David DeWitt, Yanlei Diao, Xin Luna Dong, Michael Franklin, Juliana Freire, Johannes Gehrke, Alon Halevy, Joseph M. Hellerstein, Mark D. Hill, Stratos Idreos, Yannis Ioannidis, Christoph Koch, Donald Kossmann, Tim Kraska, Arun Kumar, Guoliang Li, Volker Markl, Ren\'ee Miller, C. Mohan, Thomas Neumann, Beng Chin Ooi, Fatma Ozcan, Aditya Parameswaran, Ippokratis Pandis, Jignesh M. Patel, Andrew Pavlo, Danica Porobic, Viktor Sanca, Michael Stonebraker, Julia Stoyanovich, Dan Suciu, Wang-Chiew Tan, Shiv Venkataraman, Matei Zaharia, Stanley B. Zdonik | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">On October 19 and 20, 2023, the authors of this report convened in Cambridge, MA, to discuss the state of the database research field, its recent accomplishments and ongoing challenges, and future directions for research and community engagement. This gathering continues a long standing tradition in</span>
            
            <span class="abstract-full" style="display: none;">On October 19 and 20, 2023, the authors of this report convened in Cambridge, MA, to discuss the state of the database research field, its recent accomplishments and ongoing challenges, and future directions for research and community engagement. This gathering continues a long standing tradition in the database community, dating back to the late 1980s, in which researchers meet roughly every five years to produce a forward looking report.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.2 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- RAG: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5404
            </span>
            <a href="https://arxiv.org/abs/2502.12486" target="_blank" rel="noopener noreferrer">EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 35.3 -->
                
            <!-- Medicine: 4.9 -->
                
            <!-- RAG: 4.1 -->
                
            <!-- T2I: 2.4 -->
                
            <!-- 3D: 2.4 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Blockchain: 2.0 -->
                
            <!-- HPO and AutoML: 1.5 -->
                
            <!-- Datasets: 1.4 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5931
            </span>
            <a href="https://arxiv.org/abs/2502.03897" target="_blank" rel="noopener noreferrer">UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video Generation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lei Zhao, Linfeng Feng, Dongxu Ge, Rujin Chen, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, Xuelong Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">With the rise of diffusion models, audio-video generation has been revolutionized. However, most existing methods rely on separate modules for each modality, with limited exploration of unified generative architectures. In addition, many are confined to a single task and small-scale datasets. To add</span>
            
            <span class="abstract-full" style="display: none;">With the rise of diffusion models, audio-video generation has been revolutionized. However, most existing methods rely on separate modules for each modality, with limited exploration of unified generative architectures. In addition, many are confined to a single task and small-scale datasets. To address these limitations, we first propose UniForm, a unified multi-task diffusion transformer that jointly generates audio and visual modalities in a shared latent space. A single diffusion process models both audio and video, capturing the inherent correlations between sound and vision. Second, we introduce task-specific noise schemes and task tokens, enabling a single model to support multiple tasks, including text-to-audio-video, audio-to-video, and video-to-audio generation. Furthermore, by leveraging large language models and a large-scale text-audio-video combined dataset, UniForm achieves greater generative diversity than prior approaches. Extensive experiments show that UniForm achieves the state-of-the-art performance across audio-video generation tasks, producing content that is both well-aligned and close to real-world data distributions. Our demos are available at https://uniform-t2av.github.io/.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.8 -->
                
            <!-- Medicine: 6.6 -->
                
            <!-- RAG: 3.1 -->
                
            <!-- 3D: 2.6 -->
                
            <!-- T2I: 2.3 -->
                
            <!-- Quantum Computing: 2.1 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6921
            </span>
            <a href="https://arxiv.org/abs/2504.09812" target="_blank" rel="noopener noreferrer">Efficient Multi-Task Modeling through Automated Fusion of Trained Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jingxuan Zhou, Weidong Bao, Ji Wang, Zhengyi Zhong, Dayu Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Although multi-task learning is widely applied in intelligent services, traditional multi-task modeling methods often require customized designs based on specific task combinations, resulting in a cumbersome modeling process. Inspired by the rapid development and excellent performance of single-task</span>
            
            <span class="abstract-full" style="display: none;">Although multi-task learning is widely applied in intelligent services, traditional multi-task modeling methods often require customized designs based on specific task combinations, resulting in a cumbersome modeling process. Inspired by the rapid development and excellent performance of single-task models, this paper proposes an efficient multi-task modeling method that can automatically fuse trained single-task models with different structures and tasks to form a multi-task model. As a general framework, this method allows modelers to simply prepare trained models for the required tasks, simplifying the modeling process while fully utilizing the knowledge contained in the trained models. This eliminates the need for excessive focus on task relationships and model structure design. To achieve this goal, we consider the structural differences among various trained models and employ model decomposition techniques to hierarchically decompose them into multiple operable model components. Furthermore, we have designed an Adaptive Knowledge Fusion (AKF) module based on Transformer, which adaptively integrates intra-task and inter-task knowledge based on model components. Through the proposed method, we achieve efficient and automated construction of multi-task models, and its effectiveness is verified through extensive experiments on three datasets.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.5 -->
                
            <!-- Medicine: 10.1 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Federated Learning: 2.3 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Math: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6928
            </span>
            <a href="https://arxiv.org/abs/2411.18822" target="_blank" rel="noopener noreferrer">RelCon: Relative Contrastive Learning for a Motion Foundation Model for Wearable Data</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Maxwell A. Xu, Jaya Narain, Gregory Darnell, Haraldur Hallgrimsson, Hyewon Jeong, Darren Forde, Richard Fineman, Karthik J. Raghuram, James M. Rehg, Shirley Ren | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present RelCon, a novel self-supervised Relative Contrastive learning approach for training a motion foundation model from wearable accelerometry sensors. First, a learnable distance measure is trained to capture motif similarity and domain-specific semantic information such as rotation invarianc</span>
            
            <span class="abstract-full" style="display: none;">We present RelCon, a novel self-supervised Relative Contrastive learning approach for training a motion foundation model from wearable accelerometry sensors. First, a learnable distance measure is trained to capture motif similarity and domain-specific semantic information such as rotation invariance. Then, the learned distance provides a measurement of semantic similarity between a pair of accelerometry time-series, which we use to train our foundation model to model relative relationships across time and across subjects. The foundation model is trained on 1 billion segments from 87,376 participants, and achieves state-of-the-art performance across multiple downstream tasks, including human activity recognition and gait metric regression. To our knowledge, we are the first to show the generalizability of a foundation model with motion data from wearables across distinct evaluation tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 13.3 -->
                
            <!-- LLMs: 6.0 -->
                
            <!-- Networks: 3.4 -->
                
            <!-- Reinforcement Learning: 3.1 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7757
            </span>
            <a href="https://arxiv.org/abs/2411.17459" target="_blank" rel="noopener noreferrer">WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, Li Yuan | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes</span>
            
            <span class="abstract-full" style="display: none;">Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.1 -->
                
            <!-- Medicine: 6.1 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Math: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7928
            </span>
            <a href="https://arxiv.org/abs/2310.14629" target="_blank" rel="noopener noreferrer">Making informed decisions in cutting tool maintenance in milling: A KNN-based model agnostic approach</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Revati M. Wahul, Aditya M. Rahalkar, Om M. Khare, Abhishek D. Patange, Rohan N. Soman | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Tool Condition Monitoring (TCM) is vital for maintaining productivity and product quality in machining. This study leverages machine learning to analyze real-time force signals collected from experiments under various tool wear conditions. Statistical analysis and feature selection using decision tr</span>
            
            <span class="abstract-full" style="display: none;">Tool Condition Monitoring (TCM) is vital for maintaining productivity and product quality in machining. This study leverages machine learning to analyze real-time force signals collected from experiments under various tool wear conditions. Statistical analysis and feature selection using decision trees were followed by classification using a K-Nearest Neighbors (KNN) algorithm, with hyperparameter tuning to enhance performance. While machine learning has been widely applied in TCM, interpretability remains limited. This work introduces a KNN-based white-box model that enhances transparency in decision-making by revealing how features influence classification. The model not only detects tool wear but also provides insights into the reasoning behind each decision, enabling manufacturers to make informed maintenance choices.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.7 -->
                
            <!-- Medicine: 9.4 -->
                
            <!-- Quantum Computing: 5.1 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- GNN: 2.7 -->
                
            <!-- RAG: 2.2 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8026
            </span>
            <a href="https://arxiv.org/abs/2504.09623" target="_blank" rel="noopener noreferrer">Ges3ViG: Incorporating Pointing Gestures into Language-Based 3D Visual Grounding for Embodied Reference Understanding</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Atharv Mahesh Mane, Dulanga Weerakoon, Vigneshwaran Subbaraju, Sougata Sen, Sanjay E. Sarma, Archan Misra | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">3-Dimensional Embodied Reference Understanding (3D-ERU) combines a language description and an accompanying pointing gesture to identify the most relevant target object in a 3D scene. Although prior work has explored pure language-based 3D grounding, there has been limited exploration of 3D-ERU, whi</span>
            
            <span class="abstract-full" style="display: none;">3-Dimensional Embodied Reference Understanding (3D-ERU) combines a language description and an accompanying pointing gesture to identify the most relevant target object in a 3D scene. Although prior work has explored pure language-based 3D grounding, there has been limited exploration of 3D-ERU, which also incorporates human pointing gestures. To address this gap, we introduce a data augmentation framework-Imputer, and use it to curate a new benchmark dataset-ImputeRefer for 3D-ERU, by incorporating human pointing gestures into existing 3D scene datasets that only contain language instructions. We also propose Ges3ViG, a novel model for 3D-ERU that achieves ~30% improvement in accuracy as compared to other 3D-ERU models and ~9% compared to other purely language-based 3D grounding models. Our code and dataset are available at https://github.com/AtharvMane/Ges3ViG.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.4 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- 3D: 3.1 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- RAG: 2.2 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8136
            </span>
            <a href="https://arxiv.org/abs/2504.11134" target="_blank" rel="noopener noreferrer">Visual Re-Ranking with Non-Visual Side Information</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Gustav Hanning, Gabrielle Flood, Viktor Larsson | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The standard approach for visual place recognition is to use global image descriptors to retrieve the most similar database images for a given query image. The results can then be further improved with re-ranking methods that re-order the top scoring images. However, existing methods focus on re-ran</span>
            
            <span class="abstract-full" style="display: none;">The standard approach for visual place recognition is to use global image descriptors to retrieve the most similar database images for a given query image. The results can then be further improved with re-ranking methods that re-order the top scoring images. However, existing methods focus on re-ranking based on the same image descriptors that were used for the initial retrieval, which we argue provides limited additional signal.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 6.1 -->
                
            <!-- LLMs: 5.9 -->
                
            <!-- GNN: 2.7 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.6 -->
                
            <!-- Federated Learning: 2.4 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- SpikingNN: 1.4 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- 3D: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8728
            </span>
            <a href="https://arxiv.org/abs/2504.11168" target="_blank" rel="noopener noreferrer">Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: William Hackett, Lewis Birch, Stefan Trawicki, Neeraj Suri, Peter Garraghan | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character inj</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 34.8 -->
                
            <!-- RAG: 4.5 -->
                
            <!-- Medicine: 3.5 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- 3D: 2.3 -->
                
            <!-- Blockchain: 2.2 -->
                
            <!-- HPO and AutoML: 1.8 -->
                
            <!-- T2I: 1.8 -->
                
            <!-- Finance: 1.1 -->
                
            <!-- Datasets: 1.0 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8888
            </span>
            <a href="https://arxiv.org/abs/2504.10507" target="_blank" rel="noopener noreferrer">PinRec: Outcome-Conditioned, Multi-Token Generative Retrieval for Industry-Scale Recommendation Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anirudhan Badrinath, Prabhat Agarwal, Laksh Bhasin, Jaewon Yang, Jiajing Xu, Charles Rosenberg | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative retrieval methods utilize generative sequential modeling techniques, such as transformers, to generate candidate items for recommender systems. These methods have demonstrated promising results in academic benchmarks, surpassing traditional retrieval models like two-tower architectures. H</span>
            
            <span class="abstract-full" style="display: none;">Generative retrieval methods utilize generative sequential modeling techniques, such as transformers, to generate candidate items for recommender systems. These methods have demonstrated promising results in academic benchmarks, surpassing traditional retrieval models like two-tower architectures. However, current generative retrieval methods lack the scalability required for industrial recommender systems, and they are insufficiently flexible to satisfy the multiple metric requirements of modern systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.6 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 5.9 -->
                
            <!-- RAG: 2.7 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Blockchain: 1.7 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Math: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Hardware: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9513
            </span>
            <a href="https://arxiv.org/abs/2411.01894" target="_blank" rel="noopener noreferrer">Efficient Active Imitation Learning with Random Network Distillation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Emilien Bir\'e, Anthony Kobanda, Ludovic Denoyer, R\'emy Portelas | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Developing agents for complex and underspecified tasks, where no clear objective exists, remains challenging but offers many opportunities. This is especially true in video games, where simulated players (bots) need to play realistically, and there is no clear reward to evaluate them. While imitatio</span>
            
            <span class="abstract-full" style="display: none;">Developing agents for complex and underspecified tasks, where no clear objective exists, remains challenging but offers many opportunities. This is especially true in video games, where simulated players (bots) need to play realistically, and there is no clear reward to evaluate them. While imitation learning has shown promise in such domains, these methods often fail when agents encounter out-of-distribution scenarios during deployment. Expanding the training dataset is a common solution, but it becomes impractical or costly when relying on human demonstrations. This article addresses active imitation learning, aiming to trigger expert intervention only when necessary, reducing the need for constant expert input along training. We introduce Random Network Distillation DAgger (RND-DAgger), a new active imitation learning method that limits expert querying by using a learned state-based out-of-distribution measure to trigger interventions. This approach avoids frequent expert-agent action comparisons, thus making the expert intervene only when it is useful. We evaluate RND-DAgger against traditional imitation learning and other active approaches in 3D video games (racing and third-person navigation) and in a robotic locomotion task and show that RND-DAgger surpasses previous methods by reducing expert queries. https://sites.google.com/view/rnd-dagger</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.7 -->
                
            <!-- Medicine: 6.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- GNN: 3.1 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- RAG: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0567
            </span>
            <a href="https://arxiv.org/abs/2405.17049" target="_blank" rel="noopener noreferrer">Verifying Properties of Binary Neural Networks Using Sparse Polynomial Optimization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jianting Yang, Sre\'cko {\DH}ura\v{s}inovi\'c, Jean-Bernard Lasserre, Victor Magron, Jun Zhao | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper explores methods for verifying the properties of Binary Neural Networks (BNNs), focusing on robustness against adversarial attacks. Despite their lower computational and memory needs, BNNs, like their full-precision counterparts, are also sensitive to input perturbations. Established meth</span>
            
            <span class="abstract-full" style="display: none;">This paper explores methods for verifying the properties of Binary Neural Networks (BNNs), focusing on robustness against adversarial attacks. Despite their lower computational and memory needs, BNNs, like their full-precision counterparts, are also sensitive to input perturbations. Established methods for solving this problem are predominantly based on Satisfiability Modulo Theories and Mixed-Integer Linear Programming techniques, which are characterized by NP complexity and often face scalability issues.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.7 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 5.2 -->
                
            <!-- RAG: 2.8 -->
                
            <!-- Blockchain: 2.6 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- SpikingNN: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- HPO and AutoML: 1.3 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0751
            </span>
            <a href="https://arxiv.org/abs/2412.18416" target="_blank" rel="noopener noreferrer">Muse: A Multimodal Conversational Recommendation Dataset with Scenario-Grounded User Profiles</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zihan Wang, Xiaocui Yang, Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Current conversational recommendation systems focus predominantly on text. However, real-world recommendation settings are generally multimodal, causing a significant gap between existing research and practical applications. To address this issue, we propose Muse, the first multimodal conversational</span>
            
            <span class="abstract-full" style="display: none;">Current conversational recommendation systems focus predominantly on text. However, real-world recommendation settings are generally multimodal, causing a significant gap between existing research and practical applications. To address this issue, we propose Muse, the first multimodal conversational recommendation dataset. Muse comprises 83,148 utterances from 7,000 conversations centered around the Clothing domain. Each conversation contains comprehensive multimodal interactions, rich elements, and natural dialogues. Data in Muse are automatically synthesized by a multi-agent framework powered by multimodal large language models (MLLMs). It innovatively derives user profiles from real-world scenarios rather than depending on manual design and history data for better scalability, and then it fulfills conversation simulation and optimization. Both human and LLM evaluations demonstrate the high quality of conversations in Muse. Additionally, fine-tuning experiments on three MLLMs demonstrate Muse's learnable patterns for recommendations and responses, confirming its value for multimodal conversational recommendation. Our dataset and codes are available at https://anonymous.4open.science/r/Muse-0086.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.5 -->
                
            <!-- Medicine: 10.2 -->
                
            <!-- RAG: 3.5 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- 3D: 2.1 -->
                
            <!-- Blockchain: 2.1 -->
                
            <!-- T2I: 1.6 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Reinforcement Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0766
            </span>
            <a href="https://arxiv.org/abs/2503.20093" target="_blank" rel="noopener noreferrer">SoK: Decoding the Enigma of Encrypted Network Traffic Classifiers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Nimesha Wickramasinghe, Arash Shaghaghi, Gene Tsudik, Sanjay Jha | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The adoption of modern encryption protocols such as TLS 1.3 has significantly challenged traditional network traffic classification (NTC) methods. As a consequence, researchers are increasingly turning to machine learning (ML) approaches to overcome these obstacles. In this paper, we comprehensively</span>
            
            <span class="abstract-full" style="display: none;">The adoption of modern encryption protocols such as TLS 1.3 has significantly challenged traditional network traffic classification (NTC) methods. As a consequence, researchers are increasingly turning to machine learning (ML) approaches to overcome these obstacles. In this paper, we comprehensively analyze ML-based NTC studies, developing a taxonomy of their design choices, benchmarking suites, and prevalent assumptions impacting classifier performance. Through this systematization, we demonstrate widespread reliance on outdated datasets, oversights in design choices, and the consequences of unsubstantiated assumptions. Our evaluation reveals that the majority of proposed encrypted traffic classifiers have mistakenly utilized unencrypted traffic due to the use of legacy datasets. Furthermore, by conducting 348 feature occlusion experiments on state-of-the-art classifiers, we show how oversights in NTC design choices lead to overfitting, and validate or refute prevailing assumptions with empirical evidence. By highlighting lessons learned, we offer strategic insights, identify emerging research directions, and recommend best practices to support the development of real-world applicable NTC methodologies.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.5 -->
                
            <!-- Medicine: 5.7 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0838
            </span>
            <a href="https://arxiv.org/abs/2504.10112" target="_blank" rel="noopener noreferrer">Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Andreas Happe, J\"urgen Cito | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have emerged as a powerful approach for driving offensive penetration-testing tooling. This paper analyzes the methodology and benchmarking practices used for evaluating Large Language Model (LLM)-driven attacks, focusing on offensive uses of LLMs in cybersecurity. We re</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have emerged as a powerful approach for driving offensive penetration-testing tooling. This paper analyzes the methodology and benchmarking practices used for evaluating Large Language Model (LLM)-driven attacks, focusing on offensive uses of LLMs in cybersecurity. We review 16 research papers detailing 15 prototypes and their respective testbeds.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 34.9 -->
                
            <!-- Medicine: 5.7 -->
                
            <!-- RAG: 3.3 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Blockchain: 2.6 -->
                
            <!-- 3D: 2.2 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- HPO and AutoML: 1.4 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- Datasets: 1.2 -->
                
            <!-- Fuzzy Logic: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1191
            </span>
            <a href="https://arxiv.org/abs/2412.15726" target="_blank" rel="noopener noreferrer">Fine-tuning Whisper on Low-Resource Languages for Real-World Applications</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vincenzo Timmel, Claudio Paonessa, Reza Kakooee, Manfred Vogel, Daniel Perruchoud | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper presents a new approach to fine-tuning OpenAI's Whisper model for low-resource languages by introducing a novel data generation method that converts sentence-level data into a long-form corpus, using Swiss German as a case study. Non-sentence-level data, which could improve the performanc</span>
            
            <span class="abstract-full" style="display: none;">This paper presents a new approach to fine-tuning OpenAI's Whisper model for low-resource languages by introducing a novel data generation method that converts sentence-level data into a long-form corpus, using Swiss German as a case study. Non-sentence-level data, which could improve the performance of long-form audio, is difficult to obtain and often restricted by copyright laws. Our method bridges this gap by transforming more accessible sentence-level data into a format that preserves the model's ability to handle long-form audio and perform segmentation without requiring non-sentence-level data. Our data generation process improves performance in several real-world applications and leads to the development of a new state-of-the-art speech-to-text (STT) model for Swiss German. We compare our model with a non-fine-tuned Whisper and our previous state-of-the-art Swiss German STT models, where our new model achieves higher BLEU scores. Our results also indicate that the proposed method is adaptable to other low-resource languages, supported by written guidance and code that allows the creation of fine-tuned Whisper models, which keep segmentation capabilities and allow the transcription of longer audio files using only sentence-level data with high quality.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 10.1 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1356
            </span>
            <a href="https://arxiv.org/abs/2303.14557" target="_blank" rel="noopener noreferrer">Clo(o)k: Human-Time Interactions Through a Clock That "Looks"</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhuoyue Lyu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">What if a clock could do more than tell time - what if it could look around? This project explores the conceptualization, design, and construction of a timepiece with visual perception capabilities, featuring three types of human-time interactions. Informal observations during a demonstration highli</span>
            
            <span class="abstract-full" style="display: none;">What if a clock could do more than tell time - what if it could look around? This project explores the conceptualization, design, and construction of a timepiece with visual perception capabilities, featuring three types of human-time interactions. Informal observations during a demonstration highlight its unique user experiences. https://www.zhuoyuelyu.com/clook</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 3.3 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- RAG: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Math: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1973
            </span>
            <a href="https://arxiv.org/abs/2504.10961" target="_blank" rel="noopener noreferrer">Evaluating Trust in AI, Human, and Co-produced Feedback Among Undergraduate Students</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Audrey Zhang, Yifei Gao, Wannapon Suraworachet, Tanya Nazaretsky, Mutlu Cukurova | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As generative AI transforms educational feedback practices, understanding students' perceptions of different feedback providers becomes crucial for effective implementation. This study addresses a critical gap by comparing undergraduate students' trust in AI-generated, human-created, and human-AI co</span>
            
            <span class="abstract-full" style="display: none;">As generative AI transforms educational feedback practices, understanding students' perceptions of different feedback providers becomes crucial for effective implementation. This study addresses a critical gap by comparing undergraduate students' trust in AI-generated, human-created, and human-AI co-produced feedback, informing how institutions can adapt feedback practices in this new era. Through a within-subject experiment with 91 participants, we investigated factors predicting students' ability to distinguish between feedback types, perception of feedback quality, and potential biases to AI involvement. Findings revealed that students generally preferred AI and co-produced feedback over human feedback in terms of perceived usefulness and objectivity. Only AI feedback suffered a decline in perceived genuineness when feedback sources were revealed, while co-produced feedback maintained its positive perception. Educational AI experience improved students' ability to identify AI feedback and increased their trust in all feedback types, while general AI experience decreased perceived usefulness and credibility. Male students consistently rated all feedback types as less valuable than their female and non-binary counterparts. These insights inform evidence-based guidelines for integrating AI into higher education feedback systems while addressing trust concerns and fostering AI literacy among students.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 22.2 -->
                
            <!-- Medicine: 6.5 -->
                
            <!-- RAG: 3.4 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- 3D: 2.0 -->
                
            <!-- Blockchain: 2.0 -->
                
            <!-- T2I: 1.6 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- HPO and AutoML: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2523
            </span>
            <a href="https://arxiv.org/abs/2409.06857" target="_blank" rel="noopener noreferrer">What is the Role of Small Models in the LLM Era: A Survey</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lihu Chen, Ga\"el Varoquaux | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consump</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources. At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated. This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research. In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition. We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources. The code is available at https://github.com/tigerchen52/role_of_small_models</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 32.0 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 2.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- Reinforcement Learning: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2611
            </span>
            <a href="https://arxiv.org/abs/2504.08734" target="_blank" rel="noopener noreferrer">Towards an Understanding of Context Utilization in Code Intelligence</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yanlin Wang, Kefeng Duan, Dewu Zheng, Ensheng Shi, Fengji Zhang, Yanli Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Hongyu Zhang, Qianxiang Wang, Zibin Zheng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Code intelligence is an emerging domain in software engineering, aiming to improve the effectiveness and efficiency of various code-related tasks. Recent research suggests that incorporating contextual information beyond the basic original task inputs (i.e., source code) can substantially enhance mo</span>
            
            <span class="abstract-full" style="display: none;">Code intelligence is an emerging domain in software engineering, aiming to improve the effectiveness and efficiency of various code-related tasks. Recent research suggests that incorporating contextual information beyond the basic original task inputs (i.e., source code) can substantially enhance model performance. Such contextual signals may be obtained directly or indirectly from sources such as API documentation or intermediate representations like abstract syntax trees can significantly improve the effectiveness of code intelligence. Despite growing academic interest, there is a lack of systematic analysis of context in code intelligence. To address this gap, we conduct an extensive literature review of 146 relevant studies published between September 2007 and August 2024. Our investigation yields four main contributions. (1) A quantitative analysis of the research landscape, including publication trends, venues, and the explored domains; (2) A novel taxonomy of context types used in code intelligence; (3) A task-oriented analysis investigating context integration strategies across diverse code intelligence tasks; (4) A critical evaluation of evaluation methodologies for context-aware methods. Based on these findings, we identify fundamental challenges in context utilization in current code intelligence systems and propose a research roadmap that outlines key opportunities for future research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 19.3 -->
                
            <!-- Medicine: 6.8 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- RAG: 1.7 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3762
            </span>
            <a href="https://arxiv.org/abs/2403.11116" target="_blank" rel="noopener noreferrer">PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Xirong Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Multimodal Large Language Models (MLLMs) hallucinate, resulting in an emerging topic of visual hallucination evaluation (VHE). This paper contributes a ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective VHE at a large scale. The essence of VHE is to ask an MLLM questions ab</span>
            
            <span class="abstract-full" style="display: none;">Multimodal Large Language Models (MLLMs) hallucinate, resulting in an emerging topic of visual hallucination evaluation (VHE). This paper contributes a ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective VHE at a large scale. The essence of VHE is to ask an MLLM questions about specific images to assess its susceptibility to hallucination. Depending on what to ask (objects, attributes, sentiment, etc.) and how the questions are asked, we structure PhD along two dimensions, i.e. task and mode. Five visual recognition tasks, ranging from low-level (object / attribute recognition) to middle-level (sentiment / position recognition and counting), are considered. Besides a normal visual QA mode, which we term PhD-base, PhD also asks questions with specious context (PhD-sec) or with incorrect context ({PhD-icc), or with AI-generated counter common sense images (PhD-ccs). We construct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules: task-specific hallucinatory item (hitem) selection, hitem-embedded question generation, specious / incorrect context generation, and counter-common-sense (CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA triplets in total, PhD reveals considerable variability in MLLMs' performance across various modes and tasks, offering valuable insights into the nature of hallucination. As such, PhD stands as a potent tool not only for VHE but may also play a significant role in the refinement of MLLMs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.4 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- RAG: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.473
            </span>
            <a href="https://arxiv.org/abs/2504.03193" target="_blank" rel="noopener noreferrer">Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xin Zhang, Robby T. Tan | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary str</span>
            
            <span class="abstract-full" style="display: none;">Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.5 -->
                
            <!-- Medicine: 9.0 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- 3D: 2.2 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- RAG: 1.9 -->
                
            <!-- Blockchain: 1.9 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Reinforcement Learning: 1.4 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5945
            </span>
            <a href="https://arxiv.org/abs/2411.04677" target="_blank" rel="noopener noreferrer">Lightning IR: Straightforward Fine-tuning and Inference of Transformer-based Language Models for Information Retrieval</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ferdinand Schlatt, Maik Fr\"obe, Matthias Hagen | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A wide range of transformer-based language models have been proposed for information retrieval tasks. However, including transformer-based models in retrieval pipelines is often complex and requires substantial engineering effort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch Ligh</span>
            
            <span class="abstract-full" style="display: none;">A wide range of transformer-based language models have been proposed for information retrieval tasks. However, including transformer-based models in retrieval pipelines is often complex and requires substantial engineering effort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch Lightning-based framework for applying transformer-based language models in retrieval scenarios. Lightning IR provides a modular and extensible architecture that supports all stages of a retrieval pipeline: from fine-tuning and indexing to searching and re-ranking. Designed to be scalable and reproducible, Lightning IR is available as open-source: https://github.com/webis-de/lightning-ir.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 25.0 -->
                
            <!-- Medicine: 9.5 -->
                
            <!-- RAG: 3.5 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- Blockchain: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- HPO and AutoML: 1.3 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5974
            </span>
            <a href="https://arxiv.org/abs/2504.06106" target="_blank" rel="noopener noreferrer">A ROS2-based software library for inverse dynamics computation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vincenzo Petrone, Enrico Ferrentino, Pasquale Chiacchio | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Inverse dynamics computation is a critical component in robot control, planning and simulation, enabling the calculation of joint torques required to achieve a desired motion. This paper presents a ROS2-based software library designed to solve the inverse dynamics problem for robotic systems. The li</span>
            
            <span class="abstract-full" style="display: none;">Inverse dynamics computation is a critical component in robot control, planning and simulation, enabling the calculation of joint torques required to achieve a desired motion. This paper presents a ROS2-based software library designed to solve the inverse dynamics problem for robotic systems. The library is built around an abstract class with three concrete implementations: one for simulated robots and two for real UR10 and Franka robots. This contribution aims to provide a flexible, extensible, robot-agnostic solution to inverse dynamics, suitable for both simulation and real-world scenarios involving planning and control applications. The related software is available at https://github.com/unisa-acg/inverse-dynamics-solver/tree/rap.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 11.4 -->
                
            <!-- LLMs: 6.2 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 3.7 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- RAG: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- Reinforcement Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6146
            </span>
            <a href="https://arxiv.org/abs/2504.08044" target="_blank" rel="noopener noreferrer">Large-Scale Analysis of Online Questions Related to Opioid Use Disorder on Reddit</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tanmay Laud, Akadia Kacha-Ochana, Steven A. Sumner, Vikram Krishnasamy, Royal Law, Lyna Schieber, Munmun De Choudhury, Mai ElSherief | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Opioid use disorder (OUD) is a leading health problem that affects individual well-being as well as general public health. Due to a variety of reasons, including the stigma faced by people using opioids, online communities for recovery and support were formed on different social media platforms. In </span>
            
            <span class="abstract-full" style="display: none;">Opioid use disorder (OUD) is a leading health problem that affects individual well-being as well as general public health. Due to a variety of reasons, including the stigma faced by people using opioids, online communities for recovery and support were formed on different social media platforms. In these communities, people share their experiences and solicit information by asking questions to learn about opioid use and recovery. However, these communities do not always contain clinically verified information. In this paper, we study natural language questions asked in the context of OUD-related discourse on Reddit. We adopt transformer-based question detection along with hierarchical clustering across 19 subreddits to identify six coarse-grained categories and 69 fine-grained categories of OUD-related questions. Our analysis uncovers ten areas of information seeking from Reddit users in the context of OUD: drug sales, specific drug-related questions, OUD treatment, drug uses, side effects, withdrawal, lifestyle, drug testing, pain management and others, during the study period of 2018-2021. Our work provides a major step in improving the understanding of OUD-related questions people ask unobtrusively on Reddit. We finally discuss technological interventions and public health harm reduction techniques based on the topics of these questions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.8 -->
                
            <!-- Medicine: 6.4 -->
                
            <!-- Quantum Computing: 4.7 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6772
            </span>
            <a href="https://arxiv.org/abs/2504.08418" target="_blank" rel="noopener noreferrer">seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yilin Ning, Yian Ma, Mingxuan Liu, Xin Li, Nan Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Fairness in artificial intelligence (AI) prediction models is increasingly emphasized to support responsible adoption in high-stakes domains such as health care and criminal justice. Guidelines and implementation frameworks highlight the importance of both predictive accuracy and equitable outcomes.</span>
            
            <span class="abstract-full" style="display: none;">Fairness in artificial intelligence (AI) prediction models is increasingly emphasized to support responsible adoption in high-stakes domains such as health care and criminal justice. Guidelines and implementation frameworks highlight the importance of both predictive accuracy and equitable outcomes. However, current fairness toolkits often evaluate classification performance disparities in isolation, with limited attention to other critical aspects such as calibration. To address these gaps, we present seeBias, an R package for comprehensive evaluation of model fairness and predictive performance. seeBias offers an integrated evaluation across classification, calibration, and other performance domains, providing a more complete view of model behavior. It includes customizable visualizations to support transparent reporting and responsible AI implementation. Using public datasets from criminal justice and healthcare, we demonstrate how seeBias supports fairness evaluations, and uncovers disparities that conventional fairness metrics may overlook. The R package is available on GitHub, and a Python version is under development.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.9 -->
                
            <!-- Medicine: 11.3 -->
                
            <!-- RAG: 2.9 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Blockchain: 2.0 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- T2I: 1.6 -->
                
            <!-- Networks: 1.4 -->
                
            <!-- HPO and AutoML: 1.4 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6796
            </span>
            <a href="https://arxiv.org/abs/2504.10147" target="_blank" rel="noopener noreferrer">A Survey of Personalization: From RAG to Agent</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xiaopeng Li, Pengyue Jia, Derong Xu, Yi Wen, Yingyi Zhang, Wenlin Zhang, Wanyu Wang, Yichao Wang, Zhaocheng Du, Xiangyang Li, Yong Liu, Huifeng Guo, Ruiming Tang, Xiangyu Zhao | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Personalization has become an essential capability in modern AI systems, enabling customized interactions that align with individual user preferences, contexts, and goals. Recent research has increasingly concentrated on Retrieval-Augmented Generation (RAG) frameworks and their evolution into more a</span>
            
            <span class="abstract-full" style="display: none;">Personalization has become an essential capability in modern AI systems, enabling customized interactions that align with individual user preferences, contexts, and goals. Recent research has increasingly concentrated on Retrieval-Augmented Generation (RAG) frameworks and their evolution into more advanced agent-based architectures within personalized settings to enhance user satisfaction. Building on this foundation, this survey systematically examines personalization across the three core stages of RAG: pre-retrieval, retrieval, and generation. Beyond RAG, we further extend its capabilities into the realm of Personalized LLM-based Agents, which enhance traditional RAG systems with agentic functionalities, including user understanding, personalized planning and execution, and dynamic generation. For both personalization in RAG and agent-based personalization, we provide formal definitions, conduct a comprehensive review of recent literature, and summarize key datasets and evaluation metrics. Additionally, we discuss fundamental challenges, limitations, and promising research directions in this evolving field. Relevant papers and resources are continuously updated at https://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #fb6a72" title="Confidence: 64.9%">
                        RAG
                    </span>
            <!-- LLMs: 17.6 -->
                
            <!-- Medicine: 5.1 -->
                
            <!-- Blockchain: 2.7 -->
                
            <!-- T2I: 2.1 -->
                
            <!-- 3D: 2.1 -->
                
            <!-- Quantum Computing: 1.7 -->
                
            <!-- HPO and AutoML: 1.7 -->
                
            <!-- Datasets: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6917
            </span>
            <a href="https://arxiv.org/abs/2310.07971" target="_blank" rel="noopener noreferrer">Interval Decomposition of Persistence Modules over a Principal Ideal Domain</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiajie Luo, Gregory Henselman-Petrusek | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The study of persistent homology has contributed new insights and perspectives into a variety of interesting problems in science and engineering. Work in this domain relies on the result that any finitely-indexed persistence module of finite-dimensional vector spaces admits an interval decomposition</span>
            
            <span class="abstract-full" style="display: none;">The study of persistent homology has contributed new insights and perspectives into a variety of interesting problems in science and engineering. Work in this domain relies on the result that any finitely-indexed persistence module of finite-dimensional vector spaces admits an interval decomposition -- that is, a decomposition as a direct sum of simpler components called interval modules. This result fails if we replace vector spaces with modules over more general coefficient rings.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.5 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Blockchain: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.8609
            </span>
            <a href="https://arxiv.org/abs/2501.05708" target="_blank" rel="noopener noreferrer">Differential Properties of Information in Jump-diffusion Channels</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Luyao Fan, Jiayang Zou, Jiayang Gao, Jia Wang | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose a channel modeling using jump-diffusion processes, and study the differential properties of entropy and mutual information. By utilizing the Kramers-Moyal and Kolmogorov-Feller equations, we express the mutual information between the input and the output in series and integral forms, pres</span>
            
            <span class="abstract-full" style="display: none;">We propose a channel modeling using jump-diffusion processes, and study the differential properties of entropy and mutual information. By utilizing the Kramers-Moyal and Kolmogorov-Feller equations, we express the mutual information between the input and the output in series and integral forms, presented by Fisher-type information and mismatched KL divergence. We extend de Bruijn's identity and the I-MMSE relation to encompass general Markov processes.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 10.9 -->
                
            <!-- LLMs: 9.5 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Robotics: 2.2 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.9203
            </span>
            <a href="https://arxiv.org/abs/2504.08475" target="_blank" rel="noopener noreferrer">TickIt: Leveraging Large Language Models for Automated Ticket Escalation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Fengrui Liu, Xiao He, Tieying Zhang, Jianjun Chen, Yi Li, Lihua Yi, Haipeng Zhang, Gang Wu, Rui Shi | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In large-scale cloud service systems, support tickets serve as a critical mechanism for resolving customer issues and maintaining service quality. However, traditional manual ticket escalation processes encounter significant challenges, including inefficiency, inaccuracy, and difficulty in handling </span>
            
            <span class="abstract-full" style="display: none;">In large-scale cloud service systems, support tickets serve as a critical mechanism for resolving customer issues and maintaining service quality. However, traditional manual ticket escalation processes encounter significant challenges, including inefficiency, inaccuracy, and difficulty in handling the high volume and complexity of tickets. While previous research has proposed various machine learning models for ticket classification, these approaches often overlook the practical demands of real-world escalations, such as dynamic ticket updates, topic-specific routing, and the analysis of ticket relationships. To bridge this gap, this paper introduces TickIt, an innovative online ticket escalation framework powered by Large Language Models. TickIt enables topic-aware, dynamic, and relationship-driven ticket escalations by continuously updating ticket states, assigning tickets to the most appropriate support teams, exploring ticket correlations, and leveraging category-guided supervised fine-tuning to continuously improve its performance. By deploying TickIt in ByteDance's cloud service platform Volcano Engine, we validate its efficacy and practicality, marking a significant advancement in the field of automated ticket escalation for large-scale cloud service systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 17.2 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- RAG: 1.8 -->
                
            <!-- Blockchain: 1.7 -->
                
            <!-- Networks: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.9259
            </span>
            <a href="https://arxiv.org/abs/2504.08841" target="_blank" rel="noopener noreferrer">ES-HPC-MPC: Exponentially Stable Hybrid Perception Constrained MPC for Quadrotor with Suspended Payloads</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Luis F. Recalde, Mrunal Sarvaiya, Giuseppe Loianno, Guanrui Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Aerial transportation using quadrotors with cable-suspended payloads holds great potential for applications in disaster response, logistics, and infrastructure maintenance. However, their hybrid and underactuated dynamics pose significant control and perception challenges. Traditional approaches oft</span>
            
            <span class="abstract-full" style="display: none;">Aerial transportation using quadrotors with cable-suspended payloads holds great potential for applications in disaster response, logistics, and infrastructure maintenance. However, their hybrid and underactuated dynamics pose significant control and perception challenges. Traditional approaches often assume a taut cable condition, limiting their effectiveness in real-world applications where slack-to-taut transitions occur due to disturbances. We introduce ES-HPC-MPC, a model predictive control framework that enforces exponential stability and perception-constrained control under hybrid dynamics.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.6 -->
                
            <!-- Medicine: 11.6 -->
                
            <!-- Quantum Computing: 4.7 -->
                
            <!-- RAG: 3.1 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- 3D: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- T2I: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.9497
            </span>
            <a href="https://arxiv.org/abs/2504.10556" target="_blank" rel="noopener noreferrer">VAE-based Feature Disentanglement for Data Augmentation and Compression in Generalized GNSS Interference Classification</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lucas Heublein, Simon Kocher, Tobias Feigl, Alexander R\"ugamer, Christopher Mutschler, Felix Ott | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Distributed learning and Edge AI necessitate efficient data processing, low-latency communication, decentralized model training, and stringent data privacy to facilitate real-time intelligence on edge devices while reducing dependency on centralized infrastructure and ensuring high model performance</span>
            
            <span class="abstract-full" style="display: none;">Distributed learning and Edge AI necessitate efficient data processing, low-latency communication, decentralized model training, and stringent data privacy to facilitate real-time intelligence on edge devices while reducing dependency on centralized infrastructure and ensuring high model performance. In the context of global navigation satellite system (GNSS) applications, the primary objective is to accurately monitor and classify interferences that degrade system performance in distributed environments, thereby enhancing situational awareness. To achieve this, machine learning (ML) models can be deployed on low-resource devices, ensuring minimal communication latency and preserving data privacy. The key challenge is to compress ML models while maintaining high classification accuracy. In this paper, we propose variational autoencoders (VAEs) for disentanglement to extract essential latent features that enable accurate classification of interferences. We demonstrate that the disentanglement approach can be leveraged for both data compression and data augmentation by interpolating the lower-dimensional latent representations of signal power. To validate our approach, we evaluate three VAE variants - vanilla, factorized, and conditional generative - on four distinct datasets, including two collected in controlled indoor environments and two real-world highway datasets. Additionally, we conduct extensive hyperparameter searches to optimize performance. Our proposed VAE achieves a data compression rate ranging from 512 to 8,192 and achieves an accuracy up to 99.92%.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 13.3 -->
                
            <!-- LLMs: 13.1 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.02
            </span>
            <a href="https://arxiv.org/abs/2504.09113" target="_blank" rel="noopener noreferrer">Adaptive and Efficient Log Parsing as a Cloud Service</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zeyan Li, Jie Song, Tieying Zhang, Tao Yang, Xiongjun Ou, Yingjie Ye, Pengfei Duan, Muchen Lin, Jianjun Chen | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Logs are a critical data source for cloud systems, enabling advanced features like monitoring, alerting, and root cause analysis. However, the massive scale and diverse formats of unstructured logs pose challenges for adaptable, efficient, and accurate parsing methods. This paper introduces ByteBrai</span>
            
            <span class="abstract-full" style="display: none;">Logs are a critical data source for cloud systems, enabling advanced features like monitoring, alerting, and root cause analysis. However, the massive scale and diverse formats of unstructured logs pose challenges for adaptable, efficient, and accurate parsing methods. This paper introduces ByteBrain-LogParser, an innovative log parsing framework designed specifically for cloud environments. ByteBrain-LogParser employs a hierarchical clustering algorithm to allow real-time precision adjustments, coupled with optimizations such as positional similarity distance, deduplication, and hash encoding to enhance performance. Experiments on large-scale datasets show that it processes 229,000 logs per second on average, achieving an 840% speedup over the fastest baseline while maintaining accuracy comparable to state-of-the-art methods. Real-world evaluations further validate its efficiency and adaptability, demonstrating its potential as a robust cloud-based log parsing solution.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.8 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- RAG: 2.1 -->
                
            <!-- 3D: 2.0 -->
                
            <!-- Blockchain: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.1337
            </span>
            <a href="https://arxiv.org/abs/2504.11305" target="_blank" rel="noopener noreferrer">CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable Wood Defect Detection</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jincheng Kang, Yi Cen, Yigang Cen, Ke Wang, Yuhan Liu | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Wood defect detection is critical for ensuring quality control in the wood processing industry. However, current industrial applications face two major challenges: traditional methods are costly, subjective, and labor-intensive, while mainstream deep learning models often struggle to balance detecti</span>
            
            <span class="abstract-full" style="display: none;">Wood defect detection is critical for ensuring quality control in the wood processing industry. However, current industrial applications face two major challenges: traditional methods are costly, subjective, and labor-intensive, while mainstream deep learning models often struggle to balance detection accuracy and computational efficiency for edge deployment. To address these issues, this study proposes CFIS-YOLO, a lightweight object detection model optimized for edge devices. The model introduces an enhanced C2f structure, a dynamic feature recombination module, and a novel loss function that incorporates auxiliary bounding boxes and angular constraints. These innovations improve multi-scale feature fusion and small object localization while significantly reducing computational overhead. Evaluated on a public wood defect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of 77.5\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON BM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to 17.3\% of the original implementation, and incurs only a 0.5 percentage point drop in mAP. These results demonstrate that CFIS-YOLO is a practical and effective solution for real-world wood defect detection in resource-constrained environments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 14.7 -->
                
            <!-- LLMs: 6.3 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.1754
            </span>
            <a href="https://arxiv.org/abs/2504.08694" target="_blank" rel="noopener noreferrer">TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hang Ni, Fan Liu, Xinyu Ma, Lixin Su, Shuaiqiang Wang, Dawei Yin, Hui Xiong, Hao Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptab</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMs' intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 31.5 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- RAG: 3.2 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- 3D: 2.3 -->
                
            <!-- Blockchain: 2.3 -->
                
            <!-- T2I: 2.0 -->
                
            <!-- HPO and AutoML: 1.5 -->
                
            <!-- Datasets: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.2237
            </span>
            <a href="https://arxiv.org/abs/2504.09891" target="_blank" rel="noopener noreferrer">NR-SSOR right preconditioned RRGMRES for arbitrary singular systems and least squares problems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kouta Sugihara, Ken Hayami | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">GMRES is known to determine a least squares solution of $ A x = b $ where $ A \in R^{n \times n} $ without breakdown for arbitrary $ b \in R^n $, and initial iterate $ x_0 \in R^n $ if and only if $ A $ is range-symmetric, i.e. $ R(A^T) = R(A) $, where $ A $ may be singular and $ b $ may not be in t</span>
            
            <span class="abstract-full" style="display: none;">GMRES is known to determine a least squares solution of $ A x = b $ where $ A \in R^{n \times n} $ without breakdown for arbitrary $ b \in R^n $, and initial iterate $ x_0 \in R^n $ if and only if $ A $ is range-symmetric, i.e. $ R(A^T) = R(A) $, where $ A $ may be singular and $ b $ may not be in the range space $ R(A) $ of $ A $.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 11.0 -->
                
            <!-- LLMs: 10.5 -->
                
            <!-- Quantum Computing: 5.0 -->
                
            <!-- RAG: 2.4 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Blockchain: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.3464
            </span>
            <a href="https://arxiv.org/abs/2311.09245" target="_blank" rel="noopener noreferrer">Affine Invariance in Continuous-Domain Convolutional Neural Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ali Mohaddes, Johannes Lederer | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The notion of group invariance helps neural networks in recognizing patterns and features under geometric transformations. Group convolutional neural networks enhance traditional convolutional neural networks by incorporating group-based geometric structures into their design. This research studies </span>
            
            <span class="abstract-full" style="display: none;">The notion of group invariance helps neural networks in recognizing patterns and features under geometric transformations. Group convolutional neural networks enhance traditional convolutional neural networks by incorporating group-based geometric structures into their design. This research studies affine invariance on continuous-domain convolutional neural networks. Despite other research considering isometric invariance or similarity invariance, we focus on the full structure of affine transforms generated by the group of all invertible $2 \times 2$ real matrices (generalized linear group $\mathrm{GL}_2(\mathbb{R})$). We introduce a new criterion to assess the invariance of two signals under affine transformations. The input image is embedded into the affine Lie group $G_2 = \mathbb{R}^2 \ltimes \mathrm{GL}_2(\mathbb{R})$ to facilitate group convolution operations that respect affine invariance. Then, we analyze the convolution of embedded signals over $G_2$. In sum, our research could eventually extend the scope of geometrical transformations that usual deep-learning pipelines can handle.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 6.2 -->
                
            <!-- Quantum Computing: 5.6 -->
                
            <!-- GNN: 2.5 -->
                
            <!-- Reinforcement Learning: 2.4 -->
                
            <!-- Math: 2.3 -->
                
            <!-- SpikingNN: 2.0 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.3787
            </span>
            <a href="https://arxiv.org/abs/2504.09904" target="_blank" rel="noopener noreferrer">LiteTracker: Leveraging Temporal Causality for Accurate Low-latency Tissue Tracking</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mert Asim Karaoglu, Wenbo Ji, Ahmed Abbas, Nassir Navab, Benjamin Busam, Alexander Ladikos | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Tissue tracking plays a critical role in various surgical navigation and extended reality (XR) applications. While current methods trained on large synthetic datasets achieve high tracking accuracy and generalize well to endoscopic scenes, their runtime performances fail to meet the low-latency requ</span>
            
            <span class="abstract-full" style="display: none;">Tissue tracking plays a critical role in various surgical navigation and extended reality (XR) applications. While current methods trained on large synthetic datasets achieve high tracking accuracy and generalize well to endoscopic scenes, their runtime performances fail to meet the low-latency requirements necessary for real-time surgical applications. To address this limitation, we propose LiteTracker, a low-latency method for tissue tracking in endoscopic video streams. LiteTracker builds on a state-of-the-art long-term point tracking method, and introduces a set of training-free runtime optimizations. These optimizations enable online, frame-by-frame tracking by leveraging a temporal memory buffer for efficient feature reuse and utilizing prior motion for accurate track initialization. LiteTracker demonstrates significant runtime improvements being around 7x faster than its predecessor and 2x than the state-of-the-art. Beyond its primary focus on efficiency, LiteTracker delivers high-accuracy tracking and occlusion prediction, performing competitively on both the STIR and SuPer datasets. We believe LiteTracker is an important step toward low-latency tissue tracking for real-time surgical applications in the operating room.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 13.1 -->
                
            <!-- LLMs: 6.8 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- RAG: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Reinforcement Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.6616
            </span>
            <a href="https://arxiv.org/abs/2504.10916" target="_blank" rel="noopener noreferrer">Embedding Radiomics into Vision Transformers for Multimodal Medical Image Classification</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhenyu Yang, Haiming Zhu, Rihui Zhang, Haipeng Zhang, Jianliang Wang, Chunhao Wang, Minbin Chen, Fang-Fang Yin | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Background: Deep learning has significantly advanced medical image analysis, with Vision Transformers (ViTs) offering a powerful alternative to convolutional models by modeling long-range dependencies through self-attention. However, ViTs are inherently data-intensive and lack domain-specific induct</span>
            
            <span class="abstract-full" style="display: none;">Background: Deep learning has significantly advanced medical image analysis, with Vision Transformers (ViTs) offering a powerful alternative to convolutional models by modeling long-range dependencies through self-attention. However, ViTs are inherently data-intensive and lack domain-specific inductive biases, limiting their applicability in medical imaging. In contrast, radiomics provides interpretable, handcrafted descriptors of tissue heterogeneity but suffers from limited scalability and integration into end-to-end learning frameworks. In this work, we propose the Radiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features with data-driven visual embeddings within a ViT backbone.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 14.2 -->
                
            <!-- LLMs: 12.1 -->
                
            <!-- Quantum Computing: 4.9 -->
                
            <!-- RAG: 2.7 -->
                
            <!-- GNN: 2.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Blockchain: 1.5 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.68
            </span>
            <a href="https://arxiv.org/abs/2409.17091" target="_blank" rel="noopener noreferrer">Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence Classification</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xinrui Zhou, Yuhao Huang, Haoran Dou, Shijing Chen, Ao Chang, Jia Liu, Weiran Long, Jian Zheng, Erjiao Xu, Jie Ren, Ruobing Huang, Jun Cheng, Wufeng Xue, Dong Ni | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In the medical field, the limited availability of large-scale datasets and labor-intensive annotation processes hinder the performance of deep models. Diffusion-based generative augmentation approaches present a promising solution to this issue, having been proven effective in advancing downstream m</span>
            
            <span class="abstract-full" style="display: none;">In the medical field, the limited availability of large-scale datasets and labor-intensive annotation processes hinder the performance of deep models. Diffusion-based generative augmentation approaches present a promising solution to this issue, having been proven effective in advancing downstream medical recognition tasks. Nevertheless, existing works lack sufficient semantic and sequential steerability for challenging video/3D sequence generation, and neglect quality control of noisy synthesized samples, resulting in unreliable synthetic databases and severely limiting the performance of downstream tasks. In this work, we present Ctrl-GenAug, a novel and general generative augmentation framework that enables highly semantic- and sequential-customized sequence synthesis and suppresses incorrectly synthesized samples, to aid medical sequence classification. Specifically, we first design a multimodal conditions-guided sequence generator for controllably synthesizing diagnosis-promotive samples. A sequential augmentation module is integrated to enhance the temporal/stereoscopic coherence of generated samples. Then, we propose a noisy synthetic data filter to suppress unreliable cases at semantic and sequential levels. Extensive experiments on 3 medical datasets, using 11 networks trained on 3 paradigms, comprehensively analyze the effectiveness and generality of Ctrl-GenAug, particularly in underrepresented high-risk populations and out-domain conditions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 15.8 -->
                
            <!-- LLMs: 9.8 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.9642
            </span>
            <a href="https://arxiv.org/abs/2501.09267" target="_blank" rel="noopener noreferrer">Are Open-Vocabulary Models Ready for Detection of MEP Elements on Construction Sites</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Abdalwhab Abdalwhab, Ali Imran, Sina Heydarian, Ivanka Iordanova, David St-Onge | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The construction industry has long explored robotics and computer vision, yet their deployment on construction sites remains very limited. These technologies have the potential to revolutionize traditional workflows by enhancing accuracy, efficiency, and safety in construction management. Ground rob</span>
            
            <span class="abstract-full" style="display: none;">The construction industry has long explored robotics and computer vision, yet their deployment on construction sites remains very limited. These technologies have the potential to revolutionize traditional workflows by enhancing accuracy, efficiency, and safety in construction management. Ground robots equipped with advanced vision systems could automate tasks such as monitoring mechanical, electrical, and plumbing (MEP) systems. The present research evaluates the applicability of open-vocabulary vision-language models compared to fine-tuned, lightweight, closed-set object detectors for detecting MEP components using a mobile ground robotic platform. A dataset collected with cameras mounted on a ground robot was manually annotated and analyzed to compare model performance. The results demonstrate that, despite the versatility of vision-language models, fine-tuned lightweight models still largely outperform them in specialized environments and for domain-specific tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 14.1 -->
                
            <!-- LLMs: 10.9 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Blockchain: 1.7 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- RAG: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -9.016
            </span>
            <a href="https://arxiv.org/abs/2504.08440" target="_blank" rel="noopener noreferrer">Speech Command + Speech Emotion: Exploring Emotional Speech Commands as a Compound and Playful Modality</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ilhan Aslan, Timothy Merritt, Stine S. Johansen, Niels van Berkel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In an era of human-computer interaction with increasingly agentic AI systems capable of connecting with users conversationally, speech is an important modality for commanding agents. By recognizing and using speech emotions (i.e., how a command is spoken), we can provide agents with the ability to e</span>
            
            <span class="abstract-full" style="display: none;">In an era of human-computer interaction with increasingly agentic AI systems capable of connecting with users conversationally, speech is an important modality for commanding agents. By recognizing and using speech emotions (i.e., how a command is spoken), we can provide agents with the ability to emotionally accentuate their responses and socially enrich users' perceptions and experiences. To explore the concept and impact of speech emotion commands on user perceptions, we realized a prototype and conducted a user study (N = 14) where speech commands are used to steer two vehicles in a minimalist and retro game style implementation. While both agents execute user commands, only one of the agents uses speech emotion information to adapt its execution behavior. We report on differences in how users perceived each agent, including significant differences in stimulation and dependability, outline implications for designing interactions with agents using emotional speech commands, and provide insights on how users consciously emote, which we describe as "voice acting".</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 11.1 -->
                
            <!-- LLMs: 8.9 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- 3D: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -9.2177
            </span>
            <a href="https://arxiv.org/abs/2504.10265" target="_blank" rel="noopener noreferrer">When Technologies Are Not Enough: Understanding How Domestic Workers Employ (and Avoid) Online Technologies in Their Work Practices</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mariana Fernandez-Espinosa, Mariana Gonzalez-Bejar, Jacobo Wiesner, Diego Gomez-Zara | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Although domestic work is often viewed as manual labor, it involves significant interaction with online technologies. However, the detailed exploration of how domestic workers use these technologies remains limited. This study examines the impact of online technologies on domestic workers' work prac</span>
            
            <span class="abstract-full" style="display: none;">Although domestic work is often viewed as manual labor, it involves significant interaction with online technologies. However, the detailed exploration of how domestic workers use these technologies remains limited. This study examines the impact of online technologies on domestic workers' work practices, perceptions, and relationships with customers and employers. We interviewed 30 domestic workers residing in the United States, who provided examples that highlight the insufficient transformative role of current online technologies in their work. By conducting a thematic analysis, we characterize how they approach and avoid these digital tools at different stages of their work. Through these findings, we investigate the limitations of technology and identify challenges and opportunities that could inform the design of more suitable tools to improve the conditions of this marginalized group.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.7 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 5.4 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -9.4498
            </span>
            <a href="https://arxiv.org/abs/2504.10294" target="_blank" rel="noopener noreferrer">Ankle Exoskeletons in Walking and Load-Carrying Tasks: Insights into Biomechanics and Human-Robot Interaction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: J. F. Almeida, J. Andr\'e, C. P. Santos | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Background: Lower limb exoskeletons can enhance quality of life, but widespread adoption is limited by the lack of frameworks to assess their biomechanical and human-robot interaction effects, which are essential for developing adaptive and personalized control strategies. Understanding impacts on k</span>
            
            <span class="abstract-full" style="display: none;">Background: Lower limb exoskeletons can enhance quality of life, but widespread adoption is limited by the lack of frameworks to assess their biomechanical and human-robot interaction effects, which are essential for developing adaptive and personalized control strategies. Understanding impacts on kinematics, muscle activity, and HRI dynamics is key to achieve improved usability of wearable robots. Objectives: We propose a systematic methodology evaluate an ankle exoskeleton's effects on human movement during walking and load-carrying (10 kg front pack), focusing on joint kinematics, muscle activity, and HRI torque signals. Materials and Methods: Using Xsens MVN (inertial motion capture), Delsys EMG, and a unilateral exoskeleton, three experiments were conducted: (1) isolated dorsiflexion/plantarflexion; (2) gait analysis (two subjects, passive/active modes); and (3) load-carrying under assistance. Results and Conclusions: The first experiment confirmed that the HRI sensor captured both voluntary and involuntary torques, providing directional torque insights. The second experiment showed that the device slightly restricted ankle range of motion (RoM) but supported normal gait patterns across all assistance modes. The exoskeleton reduced muscle activity, particularly in active mode. HRI torque varied according to gait phases and highlighted reduced synchronization, suggesting a need for improved support. The third experiment revealed that load-carrying increased GM and TA muscle activity, but the device partially mitigated user effort by reducing muscle activity compared to unassisted walking. HRI increased during load-carrying, providing insights into user-device dynamics. These results demonstrate the importance of tailoring exoskeleton evaluation methods to specific devices and users, while offering a framework for future studies on exoskeleton biomechanics and HRI.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.9 -->
                
            <!-- Medicine: 12.1 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Blockchain: 1.7 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- RAG: 1.4 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -9.8812
            </span>
            <a href="https://arxiv.org/abs/2504.09634" target="_blank" rel="noopener noreferrer">Evaluating Machine Learning-Driven Intrusion Detection Systems in IoT: Performance and Energy Consumption</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Saeid Jamshidi, Kawser Wazed Nafi, Amin Nikanjam, Foutse Khomh | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In the evolving landscape of the Internet of Things (IoT), Machine Learning (ML)-based Intrusion Detection Systems (IDS) represent a significant advancement, especially when integrated with Software-Defined Networking (SDN). These systems play a critical role in enhancing security infrastructure wit</span>
            
            <span class="abstract-full" style="display: none;">In the evolving landscape of the Internet of Things (IoT), Machine Learning (ML)-based Intrusion Detection Systems (IDS) represent a significant advancement, especially when integrated with Software-Defined Networking (SDN). These systems play a critical role in enhancing security infrastructure within resource-constrained IoT systems. Despite their growing adoption, limited research has explored the impact of ML-based IDS on key performance metrics, such as CPU load, CPU usage, and energy consumption, particularly under real-time cyber threats. This study bridges that gap through an empirical evaluation of cutting-edge ML-based IDSs deployed at the edge of IoT networks under both benign and attack scenarios. Additionally, we investigate how SDN's centralized control and dynamic resource management influence IDS performance. Our experimental framework compares traditional ML-based IDS with deep learning (DL)-based counterparts, both with and without SDN integration. Results reveal that edge-deployed ML-based IDSs significantly impact system performance during cyber threats, with marked increases in resource consumption. SDN integration further influences these outcomes, emphasizing the need for optimized architectural design. Statistical analysis using ANOVA confirms the significance of our findings. This research provides critical insights into the performance and trade-offs of deploying ML-based IDSs in edge-based IoT systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.8 -->
                
            <!-- Medicine: 11.1 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -10.2117
            </span>
            <a href="https://arxiv.org/abs/2408.02479" target="_blank" rel="noopener noreferrer">From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numero</span>
            
            <span class="abstract-full" style="display: none;">With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 28.8 -->
                
            <!-- Medicine: 10.2 -->
                
            <!-- Quantum Computing: 1.7 -->
                
            <!-- Blockchain: 1.6 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -10.2649
            </span>
            <a href="https://arxiv.org/abs/2504.09496" target="_blank" rel="noopener noreferrer">Extending Behavioral Software Engineering: Decision-Making and Collaboration in Human-AI Teams for Responsible Software Engineering</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lekshmi Murali Rani | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The study of behavioral and social dimensions of software engineering (SE) tasks characterizes behavioral software engineering (BSE);however, the increasing significance of human-AI collaboration (HAIC) brings new directions in BSE by presenting new challenges and opportunities.This PhD research foc</span>
            
            <span class="abstract-full" style="display: none;">The study of behavioral and social dimensions of software engineering (SE) tasks characterizes behavioral software engineering (BSE);however, the increasing significance of human-AI collaboration (HAIC) brings new directions in BSE by presenting new challenges and opportunities.This PhD research focuses on decision-making (DM) for SE tasks and collaboration within human-AI teams, aiming to promote responsible software engineering through a cognitive partnership between humans and AI.The goal of the research is to identify the challenges and nuances in HAIC from a cognitive perspective, design and optimize collaboration/partnership (human-AI team) that enhance collective intelligence and promote better, responsible DM in SE through human-centered approaches. The research addresses HAIC and its impact on individual, team, and organizational level aspects of BSE.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.7 -->
                
            <!-- Medicine: 13.5 -->
                
            <!-- Quantum Computing: 2.6 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- RAG: 1.7 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Reinforcement Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -11.1094
            </span>
            <a href="https://arxiv.org/abs/2504.08875" target="_blank" rel="noopener noreferrer">DataMap: A Portable Application for Visualizing High-Dimensional Data</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xijin Ge | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Motivation: The visualization and analysis of high-dimensional data are essential in biomedical research. There is a need for secure, scalable, and reproducible tools to facilitate data exploration and interpretation. Results: We introduce DataMap, a browser-based application for visualization of hi</span>
            
            <span class="abstract-full" style="display: none;">Motivation: The visualization and analysis of high-dimensional data are essential in biomedical research. There is a need for secure, scalable, and reproducible tools to facilitate data exploration and interpretation. Results: We introduce DataMap, a browser-based application for visualization of high-dimensional data using heatmaps, principal component analysis (PCA), and t-distributed stochastic neighbor embedding (t-SNE). DataMap runs in the web browser, ensuring data privacy while eliminating the need for installation or a server. The application has an intuitive user interface for data transformation, annotation, and generation of reproducible R code. Availability and Implementation: Freely available as a GitHub page https://gexijin.github.io/datamap/. The source code can be found at https://github.com/gexijin/datamap, and can also be installed as an R package. Contact: Xijin.Ge@sdstate.ed</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 18.9 -->
                
            <!-- LLMs: 6.9 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- RAG: 1.4 -->
                
            <!-- Reinforcement Learning: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -11.4404
            </span>
            <a href="https://arxiv.org/abs/2504.10049" target="_blank" rel="noopener noreferrer">Summarization of Multimodal Presentations with Vision-Language Models: Study of the Effect of Modalities and Structure</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Th\'eo Gigant, Camille Guinaudeau, Fr\'ed\'eric Dufaux | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Vision-Language Models (VLMs) can process visual and textual information in multiple formats: texts, images, interleaved texts and images, or even hour-long videos. In this work, we conduct fine-grained quantitative and qualitative analyses of automatic summarization of multimodal presentations usin</span>
            
            <span class="abstract-full" style="display: none;">Vision-Language Models (VLMs) can process visual and textual information in multiple formats: texts, images, interleaved texts and images, or even hour-long videos. In this work, we conduct fine-grained quantitative and qualitative analyses of automatic summarization of multimodal presentations using VLMs with various representations as input. From these experiments, we suggest cost-effective strategies for generating summaries from text-heavy multimodal documents under different input-length budgets using VLMs. We show that slides extracted from the video stream can be beneficially used as input against the raw video, and that a structured representation from interleaved slides and transcript provides the best performance. Finally, we reflect and comment on the nature of cross-modal interactions in multimodal presentations and share suggestions to improve the capabilities of VLMs to understand documents of this nature.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.5 -->
                
            <!-- Medicine: 12.3 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Networks: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -11.4612
            </span>
            <a href="https://arxiv.org/abs/2504.09004" target="_blank" rel="noopener noreferrer">Exploring Families' Use and Mediation of Generative AI: A Multi-User Perspective</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shirley Zhang, Bengisu Cagiltay, Jennica Li, Dakota Sullivan, Bilge Mutlu, Heather Kirkorian, Kassem Fawaz | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Applications of Generative AI (GenAI), such as ChatGPT, have gained popularity among the public due to their ease of access, use, and support of educational and creative activities. Despite these benefits, GenAI poses unique risks for families, such as lacking sufficient safeguards tailored to prote</span>
            
            <span class="abstract-full" style="display: none;">Applications of Generative AI (GenAI), such as ChatGPT, have gained popularity among the public due to their ease of access, use, and support of educational and creative activities. Despite these benefits, GenAI poses unique risks for families, such as lacking sufficient safeguards tailored to protect children under 16 years of age and not offering parental control features. This study explores families' use and co-use of GenAI, the perceived risks and opportunities of ChatGPT, and how parents mediate their children's use of GenAI. Through semi-structured interviews with 12 families, we identified ways families used and mediated GenAI and factors that influenced parents' GenAI mediation strategies. We contextualize our findings with a modified model of family mediation strategies, drawing from previous family media and mediation frameworks. We provide insights for future research on family-GenAI interactions and highlight the need for more robust protective measures on GenAI platforms for families.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 15.4 -->
                
            <!-- LLMs: 14.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Blockchain: 2.3 -->
                
            <!-- RAG: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -14.0609
            </span>
            <a href="https://arxiv.org/abs/2504.10692" target="_blank" rel="noopener noreferrer">PlantD: Performance, Latency ANalysis, and Testing for Data Pipelines -- An Open Source Measurement, Testing, and Simulation Framework</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Christopher Bogart, Rajeev Chhajer, Baljit Singh, Tony Fontana, Majd Sakr | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As the volume of data available from sensor-enabled devices such as vehicles expands, it is increasingly hard for companies to make informed decisions about the cost of capturing, processing, and storing the data from every device. Business teams may forecast costs associated with deployments and us</span>
            
            <span class="abstract-full" style="display: none;">As the volume of data available from sensor-enabled devices such as vehicles expands, it is increasingly hard for companies to make informed decisions about the cost of capturing, processing, and storing the data from every device. Business teams may forecast costs associated with deployments and use patterns of devices that they sell, yet lack ways of forecasting the cost and performance of the data pipelines needed to support their devices. Without such forecasting, a company's safest choice is to make worst-case capacity estimates, and pay for overprovisioned infrastructure. Existing data pipeline benchmarking tools can measure latency, cost, and throughput as needed for development, but cannot easily close the gap in communicating the implications with business teams to inform cost forecasting. In this paper, we introduce an open-source tool, PlantD, a harness for measuring data pipelines as they are being developed, and for interpreting that data in a business context. PlantD collects a complete suite of metrics and visualizations, when developing or evaluating data pipeline architectures, configurations, and business use cases. It acts as a metaphorical data pipeline wind tunnel, enabling experiments with synthetic data to characterize and compare the performance of pipelines. It then uses those results to allow modeling of expected annual cost and performance under projected real-world loads. We describe the architecture of PlantD, walk through an example of using it to measure and compare three variants of a pipeline for processing automotive telemetry, and demonstrate how business and engineering teams can simulate scenarios together and answer "what-if" questions about the pipeline's performance under different business assumptions, allowing them to intelligently predict performance and cost measures of their critical, high-data generation infrastructure.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 19.3 -->
                
            <!-- LLMs: 8.3 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -19.4587
            </span>
            <a href="https://arxiv.org/abs/2504.08177" target="_blank" rel="noopener noreferrer">SynthFM: Training Modality-agnostic Foundation Models for Medical Image Segmentation without Real Medical Data</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sourya Sengupta, Satrajit Chakrabarty, Keerthi Sravan Ravi, Gopal Avinash, Ravi Soni | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Foundation models like the Segment Anything Model (SAM) excel in zero-shot segmentation for natural images but struggle with medical image segmentation due to differences in texture, contrast, and noise. Annotating medical images is costly and requires domain expertise, limiting large-scale annotate</span>
            
            <span class="abstract-full" style="display: none;">Foundation models like the Segment Anything Model (SAM) excel in zero-shot segmentation for natural images but struggle with medical image segmentation due to differences in texture, contrast, and noise. Annotating medical images is costly and requires domain expertise, limiting large-scale annotated data availability. To address this, we propose SynthFM, a synthetic data generation framework that mimics the complexities of medical images, enabling foundation models to adapt without real medical data. Using SAM's pretrained encoder and training the decoder from scratch on SynthFM's dataset, we evaluated our method on 11 anatomical structures across 9 datasets (CT, MRI, and Ultrasound). SynthFM outperformed zero-shot baselines like SAM and MedSAM, achieving superior results under different prompt settings and on out-of-distribution datasets.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 70.3%">
                        Medicine
                    </span>
            <!-- LLMs: 8.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -19.8354
            </span>
            <a href="https://arxiv.org/abs/2503.14049" target="_blank" rel="noopener noreferrer">A Modular Edge Device Network for Surgery Digitalization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vincent Schorp, Fr\'ed\'eric Giraud, Gianluca Parg\"atzi, Michael W\"aspe, Lorenzo von Ritter-Zahony, Marcel Wegmann, Nicola A. Cavalcanti, John Garcia Henao, Nicholas B\"unger, Dominique Cachin, Sebastiano Caprara, Philipp F\"urnstahl, Fabio Carrillo | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Future surgical care demands real-time, integrated data to drive informed decision-making and improve patient outcomes. The pressing need for seamless and efficient data capture in the OR motivates our development of a modular solution that bridges the gap between emerging machine learning technique</span>
            
            <span class="abstract-full" style="display: none;">Future surgical care demands real-time, integrated data to drive informed decision-making and improve patient outcomes. The pressing need for seamless and efficient data capture in the OR motivates our development of a modular solution that bridges the gap between emerging machine learning techniques and interventional medicine. We introduce a network of edge devices, called Data Hubs (DHs), that interconnect diverse medical sensors, imaging systems, and robotic tools via optical fiber and a centralized network switch. Built on the NVIDIA Jetson Orin NX, each DH supports multiple interfaces (HDMI, USB-C, Ethernet) and encapsulates device-specific drivers within Docker containers using the Isaac ROS framework and ROS2. A centralized user interface enables straightforward configuration and real-time monitoring, while an Nvidia DGX computer provides state-of-the-art data processing and storage. We validate our approach through an ultrasound-based 3D anatomical reconstruction experiment that combines medical imaging, pose tracking, and RGB-D data acquisition.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 66.1%">
                        Medicine
                    </span>
            <!-- LLMs: 7.1 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -20.7186
            </span>
            <a href="https://arxiv.org/abs/2410.08010" target="_blank" rel="noopener noreferrer">Securing HHL Quantum Algorithm against Quantum Computer Attacks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yizhuo Tan, Hrvoje Kukina, Jakub Szefer | Date: 2025-04-16
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As the quantum research community expands and new quantum algorithms are created and implemented, it is essential to consider the security implications and potential threats that could lead to the compromise the information processed by them. This work focuses on securing the HHL quantum algorithm a</span>
            
            <span class="abstract-full" style="display: none;">As the quantum research community expands and new quantum algorithms are created and implemented, it is essential to consider the security implications and potential threats that could lead to the compromise the information processed by them. This work focuses on securing the HHL quantum algorithm against attacks while it executes on a quantum computer. Specifically, two types of potential attacks could be deployed on a cloud-based quantum computer by an attacker circuit attempting to interfere with the victim HHL circuit: the Improper Initialization Attack (IIA) and the Higher Energy Attack (HEA). To protect the HHL algorithm from IIA and HEA, this work proposes first-of-a-kind defense strategies against these attacks on the HHL quantum algorithm. Next, this work demonstrates an implementation of a new quantum circuit for the HHL quantum algorithm that incorporates these defenses. The redesigned quantum circuit is necessary to successfully apply and realize all proposed defense strategies. Finally, this work illustrates how these defense strategies function in practice in the redesigned circuit, specifically how they can protect the HHL quantum algorithm from both IIA and HEA across multiple qubits involving all three types of qubits used in the HHL algorithms: ancilla, clock, and b. The defense requires minimal modification to the circuit, and has only a very small effect on the fidelity of the circuits. The circuits have been tested and validated in both simulation, and also on real IBM quantum computer hardware. The work further analyzes how the modified HHL circuit with the defenses is affected by noise during quantum computation. This work in the end demonstrates that it is practical to add protections to quantum circuits so that they not only perform correct computation, but also self-detect if an attack has occured during the execution.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Quantum Computing: 24.6 -->
                
            <!-- Medicine: 2.9 -->
                
            <!-- Reinforcement Learning: 2.8 -->
                
            <!-- Math: 2.7 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- LLMs: 1.9 -->
                
            <!-- Evolutionary Algorithms: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -22.4681
            </span>
            <a href="https://arxiv.org/abs/2504.08141" target="_blank" rel="noopener noreferrer">Variational quantum and neural quantum states algorithms for the linear complementarity problem</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Saibal De, Oliver Knitter, Rohan Kodati, Paramsothy Jayakumar, James Stokes, Shravan Veerapaneni | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of </span>
            
            <span class="abstract-full" style="display: none;">Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of concept, their practical utility in solving real-world problems -- and whether quantum-inspired classical algorithms can match their performance -- remains an open question. We present a novel application of the variational quantum linear solver (VQLS) and its classical neural quantum states-based counterpart, the variational neural linear solver (VNLS), as key components within a minimum map Newton solver for a complementarity-based rigid body contact model. We demonstrate using the VNLS that our solver accurately simulates the dynamics of rigid spherical bodies during collision events. These results suggest that quantum and quantum-inspired linear algebra algorithms can serve as viable alternatives to standard linear algebra solvers for modeling certain physical systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Quantum Computing: 31.2 -->
                
            <!-- Medicine: 4.2 -->
                
            <!-- LLMs: 4.1 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Evolutionary Algorithms: 1.7 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Federated Learning: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -23.2165
            </span>
            <a href="https://arxiv.org/abs/2411.03976" target="_blank" rel="noopener noreferrer">HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion Segmentation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ziyuan Ding, Yixiong Liang, Shichao Kan, Qing Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">High resolution is crucial for precise segmentation in fundus images, yet handling high-resolution inputs incurs considerable GPU memory costs, with diminishing performance gains as overhead increases. To address this issue while tackling the challenge of segmenting tiny objects, recent studies have</span>
            
            <span class="abstract-full" style="display: none;">High resolution is crucial for precise segmentation in fundus images, yet handling high-resolution inputs incurs considerable GPU memory costs, with diminishing performance gains as overhead increases. To address this issue while tackling the challenge of segmenting tiny objects, recent studies have explored local-global fusion methods. These methods preserve fine details using local regions and capture long-range context information from downscaled global images. However, the necessity of multiple forward passes inevitably incurs significant computational overhead, adversely affecting inference speed. In this paper, we propose HRDecoder, a simple High-Resolution Decoder network for fundus lesion segmentation. It integrates a high-resolution representation learning module to capture fine-grained local features and a high-resolution fusion module to fuse multi-scale predictions. Our method effectively improves the overall segmentation accuracy of fundus lesions while consuming reasonable memory and computational overhead, and maintaining satisfying inference speed. Experimental results on the IDRiD and DDR datasets demonstrate the effectiveness of our method. Code is available at https://github.com/CVIU-CSU/HRDecoder.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 71.3%">
                        Medicine
                    </span>
            <!-- LLMs: 6.0 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Math: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -24.5141
            </span>
            <a href="https://arxiv.org/abs/2504.08329" target="_blank" rel="noopener noreferrer">MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Junmo Kim, Namkyeong Lee, Jiwon Kim, Kwangsoo Kim | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generali</span>
            
            <span class="abstract-full" style="display: none;">Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generality of EHR foundation models and the integration of models trained with different vocabularies. To deal with this problem, we propose MedRep for EHR foundation models based on the observational medical outcome partnership (OMOP) common data model (CDM), providing the integrated medical concept representations and the basic data augmentation strategy for patient trajectories. For concept representation learning, we enrich the information of each concept with a minimal definition through large language model (LLM) prompts and enhance the text-based representations through graph ontology of OMOP vocabulary. Trajectory augmentation randomly replaces selected concepts with other similar concepts that have closely related representations to let the model practice with the concepts out-of-vocabulary. Finally, we demonstrate that EHR foundation models trained with MedRep better maintain the prediction performance in external datasets. Our code implementation is publicly available at https://github.com/kicarussays/MedRep.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #4ff278" title="Confidence: 79.7%">
                        Medicine
                    </span>
            <!-- LLMs: 5.5 -->
                
            <!-- Federated Learning: 2.9 -->
                
            <!-- Quantum Computing: 2.8 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -26.2138
            </span>
            <a href="https://arxiv.org/abs/2504.09213" target="_blank" rel="noopener noreferrer">Spiking Neural Network for Intra-cortical Brain Signal Decoding</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Song Yang, Haotian Fu, Herui Zhang, Peng Zhang, Wei Li, Dongrui Wu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Decoding brain signals accurately and efficiently is crucial for intra-cortical brain-computer interfaces. Traditional decoding approaches based on neural activity vector features suffer from low accuracy, whereas deep learning based approaches have high computational cost. To improve both the decod</span>
            
            <span class="abstract-full" style="display: none;">Decoding brain signals accurately and efficiently is crucial for intra-cortical brain-computer interfaces. Traditional decoding approaches based on neural activity vector features suffer from low accuracy, whereas deep learning based approaches have high computational cost. To improve both the decoding accuracy and efficiency, this paper proposes a spiking neural network (SNN) for effective and energy-efficient intra-cortical brain signal decoding. We also propose a feature fusion approach, which integrates the manually extracted neural activity vector features with those extracted by a deep neural network, to further improve the decoding accuracy. Experiments in decoding motor-related intra-cortical brain signals of two rhesus macaques demonstrated that our SNN model achieved higher accuracy than traditional artificial neural networks; more importantly, it was tens or hundreds of times more efficient. The SNN model is very suitable for high precision and low power applications like intra-cortical brain-computer interfaces.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #e2e4bd" title="Confidence: 70.0%">
                        SpikingNN
                    </span>
            <!-- Medicine: 10.9 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- LLMs: 3.6 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- GNN: 2.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -26.5188
            </span>
            <a href="https://arxiv.org/abs/2504.08659" target="_blank" rel="noopener noreferrer">BowelRCNN: Region-based Convolutional Neural Network System for Bowel Sound Auscultation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Igor Matynia, Robert Nowak | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Sound events representing intestinal activity detection is a diagnostic tool with potential to identify gastrointestinal conditions. This article introduces BowelRCNN, a novel bowel sound detection system that uses audio recording, spectrogram analysys and region-based convolutional neural network (</span>
            
            <span class="abstract-full" style="display: none;">Sound events representing intestinal activity detection is a diagnostic tool with potential to identify gastrointestinal conditions. This article introduces BowelRCNN, a novel bowel sound detection system that uses audio recording, spectrogram analysys and region-based convolutional neural network (RCNN) architecture. The system was trained and validated on a real recording dataset gathered from 19 patients, comprising 60 minutes of prepared and annotated audio data. BowelRCNN achieved a classification accuracy of 96% and an F1 score of 71%. This research highlights the feasibility of using CNN architectures for bowel sound auscultation, achieving results comparable to those of recurrent-convolutional methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 71.2%">
                        Medicine
                    </span>
            <!-- LLMs: 4.6 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- SpikingNN: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -28.2707
            </span>
            <a href="https://arxiv.org/abs/2106.00571" target="_blank" rel="noopener noreferrer">A reduced 3D-0D FSI model of the aortic valve including leaflet curvature</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ivan Fumagalli, Luca Dede', Alfio Quarteroni | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We introduce an innovative lumped-parameter model of the aortic valve, designed to efficiently simulate the impact of valve dynamics on blood flow. Our reduced model includes the elastic effects associated with the leaflets' curvature and the stress exchanged with the blood flow. The introduction of</span>
            
            <span class="abstract-full" style="display: none;">We introduce an innovative lumped-parameter model of the aortic valve, designed to efficiently simulate the impact of valve dynamics on blood flow. Our reduced model includes the elastic effects associated with the leaflets' curvature and the stress exchanged with the blood flow. The introduction of a lumped-parameter model based on momentum balance entails an easier calibration of the model parameters: phenomenological-based models, on the other hand, typically have numerous parameters. This model is coupled to 3D Navier-Stokes equations describing the blood flow, where the moving valve leaflets are immersed in the fluid domain by a resistive method. A stabilized finite element method with a BDF time scheme is adopted for the discretization of the coupled problem, and the computational results show the suitability of the system in representing the leaflet motion, the blood flow in the ascending aorta, and the pressure jump across the leaflets. Both physiological and stenotic configurations are investigated, and we analyze the effects of different treatments for the leaflet velocity on the blood flow.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #4ff278" title="Confidence: 82.6%">
                        Medicine
                    </span>
            <!-- Math: 4.3 -->
                
            <!-- Reinforcement Learning: 3.9 -->
                
            <!-- Federated Learning: 2.7 -->
                
            <!-- Robotics: 2.4 -->
                
            <!-- Pathfinding: 1.6 -->
                
            <!-- LLMs: 1.3 -->
                
            <!-- Networks: 1.2 -->
                
            <!-- Quantum Computing: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -28.2912
            </span>
            <a href="https://arxiv.org/abs/2504.08876" target="_blank" rel="noopener noreferrer">Is Productivity in Quantum Programming Equivalent to Expressiveness?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Francini Corrales-Garro, Danny Valerio-Ram\'irez, Santiago N\'u\~ez-Corrales | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The expressiveness of quantum programming languages plays a crucial role in the efficient and comprehensible representation of quantum algorithms. Unlike classical programming languages, which offer mature and well-defined abstraction mechanisms, quantum languages must integrate cognitively challeng</span>
            
            <span class="abstract-full" style="display: none;">The expressiveness of quantum programming languages plays a crucial role in the efficient and comprehensible representation of quantum algorithms. Unlike classical programming languages, which offer mature and well-defined abstraction mechanisms, quantum languages must integrate cognitively challenging concepts such as superposition, interference and entanglement while maintaining clarity and usability. However, identifying and characterizing differences in expressiveness between quantum programming paradigms remains an open area of study. Our work investigates the landscape of expressiveness through a comparative analysis of hosted quantum programming languages such as Qiskit, Cirq, Qrisp, and quAPL, and standalone languages including Q# and Qmod. We focused on evaluating how different quantum programming languages support the implementation of core quantum algorithms -- Deutsch-Jozsa, Simon, Bernstein-Vazirani, and Grover -- using expressiveness metrics: Lines of Code (LOC), Cyclomatic Complexity (CC), and Halstead Complexity (HC) metrics as proxies for developer productivity. Our findings suggest that different quantum programming paradigms offer distinct trade-offs between expressiveness and productivity, highlighting the importance of language design in quantum software development.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Quantum Computing: 25.2 -->
                
            <!-- LLMs: 10.7 -->
                
            <!-- Medicine: 8.8 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Blockchain: 1.8 -->
                
            <!-- Networks: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -30.0108
            </span>
            <a href="https://arxiv.org/abs/2504.08469" target="_blank" rel="noopener noreferrer">Artifact detection and localization in single-channel mobile EEG for sleep research using deep learning and attention mechanisms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Khrystyna Semkiv, Jia Zhang, Maria Laura Ferster, Walter Karlen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Artifacts in the electroencephalogram (EEG) degrade signal quality and impact the analysis of brain activity. Current methods for detecting artifacts in sleep EEG rely on simple threshold-based algorithms that require manual intervention, which is time-consuming and impractical due to the vast volum</span>
            
            <span class="abstract-full" style="display: none;">Artifacts in the electroencephalogram (EEG) degrade signal quality and impact the analysis of brain activity. Current methods for detecting artifacts in sleep EEG rely on simple threshold-based algorithms that require manual intervention, which is time-consuming and impractical due to the vast volume of data that novel mobile recording systems generate. We propose a convolutional neural network (CNN) model incorporating a convolutional block attention module (CNN-CBAM) to detect and identify the location of artifacts in the sleep EEG with attention maps. We benchmarked this model against six other machine learning and signal processing approaches. We trained/tuned all models on 72 manually annotated EEG recordings obtained during home-based monitoring from 18 healthy participants with a mean (SD) age of 68.05 y ($\pm$5.02). We tested them on 26 separate recordings from 6 healthy participants with a mean (SD) age of 68.33 y ($\pm$4.08), with contained artifacts in 4\% of epochs. CNN-CBAM achieved the highest area under the receiver operating characteristic curve (0.88), sensitivity (0.81), and specificity (0.86) when compared to the other approaches. The attention maps from CNN-CBAM localized artifacts within the epoch with a sensitivity of 0.71 and specificity of 0.67. This work demonstrates the feasibility of automating the detection and localization of artifacts in wearable sleep EEG.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #4ff278" title="Confidence: 81.7%">
                        Medicine
                    </span>
            <!-- Quantum Computing: 3.1 -->
                
            <!-- LLMs: 3.0 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -32.6393
            </span>
            <a href="https://arxiv.org/abs/2504.07822" target="_blank" rel="noopener noreferrer">DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wanna Cui, Peizheng Wang, Faliang Yin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with stati</span>
            
            <span class="abstract-full" style="display: none;">Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 68.2%">
                        GNN
                    </span>
            <!-- LLMs: 7.7 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Medicine: 3.9 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- Reinforcement Learning: 2.7 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Attention: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -32.6514
            </span>
            <a href="https://arxiv.org/abs/2504.09909" target="_blank" rel="noopener noreferrer">Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Farha Nausheen, Khandakar Ahmed, M Imad Khan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent developments, deep learning methodologies applied to Natural Language Processing (NLP) have revealed a paradox: They improve performance but demand considerable data and resources for their training. Alternatively, quantum computing exploits the principles of quantum mechanics to overcome </span>
            
            <span class="abstract-full" style="display: none;">In recent developments, deep learning methodologies applied to Natural Language Processing (NLP) have revealed a paradox: They improve performance but demand considerable data and resources for their training. Alternatively, quantum computing exploits the principles of quantum mechanics to overcome the computational limitations of current methodologies, thereby establishing an emerging field known as quantum natural language processing (QNLP). This domain holds the potential to attain a quantum advantage in the processing of linguistic structures, surpassing classical models in both efficiency and accuracy. In this paper, it is proposed to categorise QNLP models based on quantum computing principles, architecture, and computational approaches. This paper attempts to provide a survey on how quantum meets language by mapping state-of-the-art in this area, embracing quantum encoding techniques for classical data, QNLP models for prevalent NLP tasks, and quantum optimisation techniques for hyper parameter tuning. The landscape of quantum computing approaches applied to various NLP tasks is summarised by showcasing the specific QNLP methods used, and the popularity of these methods is indicated by their count. From the findings, it is observed that QNLP approaches are still limited to small data sets, with only a few models explored extensively, and there is increasing interest in the application of quantum computing to natural language processing tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Quantum Computing: 38.7 -->
                
            <!-- LLMs: 4.4 -->
                
            <!-- Medicine: 4.2 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Evolutionary Algorithms: 2.0 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Math: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -33.4147
            </span>
            <a href="https://arxiv.org/abs/2504.08310" target="_blank" rel="noopener noreferrer">DeQompile: quantum circuit decompilation using genetic programming for explainable quantum architecture search</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shubing Xie, Aritra Sarkar, Sebastian Feld | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Demonstrating quantum advantage using conventional quantum algorithms remains challenging on current noisy gate-based quantum computers. Automated quantum circuit synthesis via quantum machine learning has emerged as a promising solution, employing trainable parametric quantum circuits to alleviate </span>
            
            <span class="abstract-full" style="display: none;">Demonstrating quantum advantage using conventional quantum algorithms remains challenging on current noisy gate-based quantum computers. Automated quantum circuit synthesis via quantum machine learning has emerged as a promising solution, employing trainable parametric quantum circuits to alleviate this. The circuit ansatz in these solutions is often designed through reinforcement learning-based quantum architecture search when the domain knowledge of the problem and hardware are not effective. However, the interpretability of these synthesized circuits remains a significant bottleneck, limiting their scalability and applicability across diverse problem domains.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Quantum Computing: 47.1 -->
                
            <!-- Medicine: 4.9 -->
                
            <!-- LLMs: 3.6 -->
                
            <!-- Evolutionary Algorithms: 3.5 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -33.7916
            </span>
            <a href="https://arxiv.org/abs/2504.03175" target="_blank" rel="noopener noreferrer">Mathematical Modeling of Option Pricing with an Extended Black-Scholes Framework</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Nikhil Shivakumar Nayak | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This study investigates enhancing option pricing by extending the Black-Scholes model to include stochastic volatility and interest rate variability within the Partial Differential Equation (PDE). The PDE is solved using the finite difference method. The extended Black-Scholes model and a machine le</span>
            
            <span class="abstract-full" style="display: none;">This study investigates enhancing option pricing by extending the Black-Scholes model to include stochastic volatility and interest rate variability within the Partial Differential Equation (PDE). The PDE is solved using the finite difference method. The extended Black-Scholes model and a machine learning-based LSTM model are developed and evaluated for pricing Google stock options. Both models were backtested using historical market data. While the LSTM model exhibited higher predictive accuracy, the finite difference method demonstrated superior computational efficiency. This work provides insights into model performance under varying market conditions and emphasizes the potential of hybrid approaches for robust financial modeling.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #c48298" title="Confidence: 72.8%">
                        Finance
                    </span>
            <!-- LLMs: 16.4 -->
                
            <!-- Medicine: 3.6 -->
                
            <!-- Blockchain: 2.8 -->
                
            <!-- 3D: 2.4 -->
                
            <!-- RAG: 2.4 -->
                
            <!-- T2I: 2.2 -->
                
            <!-- HPO and AutoML: 1.8 -->
                
            <!-- Fuzzy Logic: 1.5 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Bayesian Optimization: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -35.1392
            </span>
            <a href="https://arxiv.org/abs/2412.09404" target="_blank" rel="noopener noreferrer">Opinion de-polarization of social networks with GNNs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Konstantinos Mylonas, Thrasyvoulos Spyropoulos | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Nowadays, social media is the ground for political debate and exchange of opinions. There is a significant amount of research that suggests that social media are highly polarized. A phenomenon that is commonly observed is the echo chamber structure, where users are organized in polarized communities</span>
            
            <span class="abstract-full" style="display: none;">Nowadays, social media is the ground for political debate and exchange of opinions. There is a significant amount of research that suggests that social media are highly polarized. A phenomenon that is commonly observed is the echo chamber structure, where users are organized in polarized communities and form connections only with similar-minded individuals, limiting themselves to consume specific content. In this paper we explore a way to decrease the polarization of networks with two echo chambers. Particularly, we observe that if some users adopt a moderate opinion about a topic, the polarization of the network decreases. Based on this observation, we propose an efficient algorithm to identify a good set of K users, such that if they adopt a moderate stance around a topic, the polarization is minimized. Our algorithm employs a Graph Neural Network and thus it can handle large graphs more effectively than other approaches</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b243cd" title="Confidence: 69.9%">
                        GNN
                    </span>
            <!-- LLMs: 4.9 -->
                
            <!-- Medicine: 3.9 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.7 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -40.3774
            </span>
            <a href="https://arxiv.org/abs/2504.07048" target="_blank" rel="noopener noreferrer">Context Switching for Secure Multi-programming of Near-Term Quantum Computers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Avinash Kumar, Meng Wang, Chenxu Liu, Ang Li, Prashant J. Nair, Poulami Das | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Multi-programming quantum computers improve device utilization and throughput. However, crosstalk from concurrent two-qubit CNOT gates poses security risks, compromising the fidelity and output of co-running victim programs. We design Zero Knowledge Tampering Attacks (ZKTAs), using which attackers c</span>
            
            <span class="abstract-full" style="display: none;">Multi-programming quantum computers improve device utilization and throughput. However, crosstalk from concurrent two-qubit CNOT gates poses security risks, compromising the fidelity and output of co-running victim programs. We design Zero Knowledge Tampering Attacks (ZKTAs), using which attackers can exploit crosstalk without knowledge of the hardware error profile. ZKTAs can alter victim program outputs in 40% of cases on commercial systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 68.7%">
                        Quantum Computing
                    </span>
            <!-- LLMs: 8.3 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- SpikingNN: 1.6 -->
                
            <!-- Hardware: 1.3 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -47.7128
            </span>
            <a href="https://arxiv.org/abs/2405.08190" target="_blank" rel="noopener noreferrer">Barren plateaus are amplified by the dimension of qudits</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lucas Friedrich, Tiago de Souza Farias, Jonas Maziero | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for attaining quantum advantage in diverse scientific and technological domains, notably within Quantum Neural Networks. However, despite their potential, VQAs encounter significant obstacles, chief among them being the vanishi</span>
            
            <span class="abstract-full" style="display: none;">Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for attaining quantum advantage in diverse scientific and technological domains, notably within Quantum Neural Networks. However, despite their potential, VQAs encounter significant obstacles, chief among them being the vanishing gradient problem, commonly referred to as barren plateaus. In this article, through meticulous analysis, we demonstrate that existing literature implicitly suggests the intrinsic influence of qudit dimensionality on barren plateaus. To instantiate these findings, we present numerical results that exemplify the impact of qudit dimensionality on barren plateaus. Therefore, despite the proposition of various error mitigation techniques, our results call for further scrutiny about their efficacy in the context of VQAs with qudits.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 73.8%">
                        Quantum Computing
                    </span>
            <!-- LLMs: 7.0 -->
                
            <!-- Medicine: 5.9 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- SpikingNN: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -49.7271
            </span>
            <a href="https://arxiv.org/abs/2410.03358" target="_blank" rel="noopener noreferrer">Oracle Separation Between Quantum Commitments and Quantum One-wayness</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: John Bostanci, Boyang Chen, Barak Nehoran | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We show that there exists an oracle relative to which quantum commitments exist but no (efficiently verifiable) one-way state generators exist. Both have been widely considered candidates for replacing one-way functions as the minimal assumption for cryptography: the weakest cryptographic assumption</span>
            
            <span class="abstract-full" style="display: none;">We show that there exists an oracle relative to which quantum commitments exist but no (efficiently verifiable) one-way state generators exist. Both have been widely considered candidates for replacing one-way functions as the minimal assumption for cryptography: the weakest cryptographic assumption implied by all of computational cryptography. Recent work has shown that commitments can be constructed from one-way state generators, but the other direction has remained open. Our results rule out any black-box construction, and thus settles this crucial open problem, suggesting that quantum commitments (as well as its equivalency class of EFI pairs, quantum oblivious transfer, and secure quantum multiparty computation) appear to be strictly weakest among all known cryptographic primitives.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 70.9%">
                        Quantum Computing
                    </span>
            <!-- LLMs: 7.4 -->
                
            <!-- Medicine: 5.3 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Hardware: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -53.0414
            </span>
            <a href="https://arxiv.org/abs/2504.06951" target="_blank" rel="noopener noreferrer">GLT hidden structures in mean-field quantum spin systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Christiaan J. F. van de Ven, Muhammad Faisal Khan, S. Serra-Capizzano | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work explores structured matrix sequences arising in mean-field quantum spin systems. We express these sequences within the framework of generalized locally Toeplitz (GLT) $*$-algebras, leveraging the fact that each GLT matrix sequence has a unique GLT symbol. This symbol characterizes both the</span>
            
            <span class="abstract-full" style="display: none;">This work explores structured matrix sequences arising in mean-field quantum spin systems. We express these sequences within the framework of generalized locally Toeplitz (GLT) $*$-algebras, leveraging the fact that each GLT matrix sequence has a unique GLT symbol. This symbol characterizes both the asymptotic singular value distribution and, for Hermitian or quasi-Hermitian sequences, the asymptotic spectral distribution. Specifically, we analyze two cases of real symmetric matrix sequences stemming from mean-field quantum spin systems and determine their associated distributions using GLT theory. Our study concludes with visualizations and numerical tests that validate the theoretical findings, followed by a discussion of open problems and future directions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #d37d97" title="Confidence: 75.7%">
                        Quantum Computing
                    </span>
            <!-- Medicine: 6.2 -->
                
            <!-- LLMs: 4.8 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -57.2052
            </span>
            <a href="https://arxiv.org/abs/2504.08170" target="_blank" rel="noopener noreferrer">Efficient measurement of neutral-atom qubits with matched filters</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Robert M. Kent, Linipun Phuttitarn, Chaithanya Naik Mude, Swamit Tannu, Mark Saffman, Gregory Lafyatis, Daniel J. Gauthier | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Quantum computers require high-fidelity measurement of many qubits to achieve a quantum advantage. Traditional approaches suffer from readout crosstalk for a neutral-atom quantum processor with a tightly spaced array. Although classical machine learning algorithms based on convolutional neural netwo</span>
            
            <span class="abstract-full" style="display: none;">Quantum computers require high-fidelity measurement of many qubits to achieve a quantum advantage. Traditional approaches suffer from readout crosstalk for a neutral-atom quantum processor with a tightly spaced array. Although classical machine learning algorithms based on convolutional neural networks can improve fidelity, they are computationally expensive, making it difficult to scale them to large qubit counts. We present two simpler and scalable machine learning algorithms that realize matched filters for the readout problem. One is a local model that focuses on a single qubit, and the other uses information from neighboring qubits in the array to prevent crosstalk among the qubits. We demonstrate error reductions of up to 32% and 43% for the site and array models, respectively, compared to a conventional Gaussian threshold approach. Additionally, our array model uses two orders of magnitude fewer trainable parameters and four orders of magnitude fewer multiplications and nonlinear function evaluations than a recent convolutional neural network approach, with only a minor (3.5%) increase in error across different readout times. Another strength of our approach is its physical interpretability: the learned filter can be visualized to provide insights into experimental imperfections. We also show that a convolutional neural network model for improved can be pruned to have 70x and 4000x fewer parameters, respectively, while maintaining similar errors. Our work shows that simple machine learning approaches can achieve high-fidelity qubit measurements while remaining scalable to systems with larger qubit counts.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 74.9%">
                        Quantum Computing
                    </span>
            <!-- Medicine: 6.2 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- LLMs: 3.1 -->
                
            <!-- Reinforcement Learning: 2.9 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Evolutionary Algorithms: 1.4 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Math: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -58.1925
            </span>
            <a href="https://arxiv.org/abs/2402.14696" target="_blank" rel="noopener noreferrer">On Schr\"odingerization based quantum algorithms for linear dynamical systems with inhomogeneous terms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shi Jin, Nana Liu, Chuwen Ma | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We analyze the Schr\"odingerization method for quantum simulation of a general class of non-unitary dynamics with inhomogeneous source terms. The Schr\"odingerization technique, introduced in [31], transforms any linear ordinary and partial differential equations with non-unitary dynamics into a sys</span>
            
            <span class="abstract-full" style="display: none;">We analyze the Schr\"odingerization method for quantum simulation of a general class of non-unitary dynamics with inhomogeneous source terms. The Schr\"odingerization technique, introduced in [31], transforms any linear ordinary and partial differential equations with non-unitary dynamics into a system under unitary dynamics via a warped phase transition that maps the equations into a higher dimension, making them suitable for quantum simulation. This technique can also be applied to these equations with inhomogeneous terms modeling source or forcing terms, or boundary and interface conditions, and discrete dynamical systems such as iterative methods in numerical linear algebra, through extra equations in the system. Difficulty arises with the presence of inhomogeneous terms since they can change the stability of the original system. In this paper, we systematically study-both theoretically and numerically-the important issue of recovering the original variables from the Schr\"odingerized equations, even when the evolution operator contains unstable modes. We show that, even with unstable modes, one can still construct a stable scheme; however, to recover the original variable, one needs to use suitable data in the extended space. We analyze and compare both the discrete and continuous Fourier transforms used in the extended dimension and derive corresponding error estimates, which allow one to use the more appropriate transform for specific equations. We also provide a smoother initialization for the Schr\"odingerized system to gain higher-order accuracy in the extended space. We homogenize the inhomogeneous terms with a stretch transformation, making it easier to recover the original variable. Our recovery technique also provides a simple and generic framework to solve general ill-posed problems in a computationally stable way.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #d37d97" title="Confidence: 78.7%">
                        Quantum Computing
                    </span>
            <!-- Reinforcement Learning: 3.7 -->
                
            <!-- Medicine: 3.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Robotics: 1.9 -->
                
            <!-- LLMs: 1.9 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -73.8499
            </span>
            <a href="https://arxiv.org/abs/2503.10790" target="_blank" rel="noopener noreferrer">Quantum Error Detection For Early Term Fault-Tolerant Quantum Algorithms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tom Ginsberg, Vyom Patel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Quantum error detection (QED) offers a promising pathway to fault tolerance in near-term quantum devices by balancing error suppression with minimal resource overhead. However, its practical utility hinges on optimizing design parameters-such as syndrome measurement frequency-to avoid diminishing re</span>
            
            <span class="abstract-full" style="display: none;">Quantum error detection (QED) offers a promising pathway to fault tolerance in near-term quantum devices by balancing error suppression with minimal resource overhead. However, its practical utility hinges on optimizing design parameters-such as syndrome measurement frequency-to avoid diminishing returns from detection overhead. In this work, we present a comprehensive framework for fault-tolerant compilation and simulation of quantum algorithms using [[n, n-2, 2]] codes, which enable low-qubit-overhead error detection and a simple nearly fault-tolerant universal set of operations. We demonstrate and analyze our pipeline with a purely statistical interpretation and through the implementation of Grover's search algorithm. Our results are used to answer the question is quantum error detection a worthwhile avenue for early-term fault tolerance, and if so how can we get the most out of it? Simulations under the circuit-level noise model reveal that finding optimal syndrome schedules improves algorithm success probabilities by an average of 6.7x but eventual statistical limits from post-selection in noisy/resource-limited regimes constrain scalability. Furthermore, we propose a simple data-driven approach to predict fault tolerant compilation parameters, such as optimal syndrome schedules, and expected fault tolerant performance gains based on circuit and noise features. These results provide actionable guidelines for implementing QED in early-term quantum experiments and underscore its role as a pragmatic, constant-overhead error mitigation layer for shallow algorithms. To aid in further research, we release all simulation data computed for this work and provide an experimental QED compiler at https://codeqraft.xyz/qed.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 70.2%">
                        Quantum Computing
                    </span>
            <!-- Medicine: 8.4 -->
                
            <!-- LLMs: 4.5 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Evolutionary Algorithms: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -83.4762
            </span>
            <a href="https://arxiv.org/abs/2504.07589" target="_blank" rel="noopener noreferrer">Copy-and-Paste? Identifying EVM-Inequivalent Code Smells in Multi-chain Reuse Contracts</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zexu Wang, Jiachi Chen, Tao Zhang, Yu Zhang, Weizhe Zhang, Yuming Feng, Zibin Zheng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As the development of Solidity contracts on Ethereum, more developers are reusing them on other compatible blockchains. However, developers may overlook the differences between the designs of the blockchain system, such as the Gas Mechanism and Consensus Protocol, leading to the same contracts on di</span>
            
            <span class="abstract-full" style="display: none;">As the development of Solidity contracts on Ethereum, more developers are reusing them on other compatible blockchains. However, developers may overlook the differences between the designs of the blockchain system, such as the Gas Mechanism and Consensus Protocol, leading to the same contracts on different blockchains not being able to achieve consistent execution as on Ethereum. This inconsistency reveals design flaws in reused contracts, exposing code smells that hinder code reusability, and we define this inconsistency as EVM-Inequivalent Code Smells. In this paper, we conducted the first empirical study to reveal the causes and characteristics of EVM-Inequivalent Code Smells. To ensure the identified smells reflect real developer concerns, we collected and analyzed 1,379 security audit reports and 326 Stack Overflow posts related to reused contracts on EVM-compatible blockchains, such as Binance Smart Chain (BSC) and Polygon. Using the open card sorting method, we defined six types of EVM-Inequivalent Code Smells. For automated detection, we developed a tool named EquivGuard. It employs static taint analysis to identify key paths from different patterns and uses symbolic execution to verify path reachability. Our analysis of 905,948 contracts across six major blockchains shows that EVM-Inequivalent Code Smells are widespread, with an average prevalence of 17.70%. While contracts with code smells do not necessarily lead to financial loss and attacks, their high prevalence and significant asset management underscore the potential threats of reusing these smelly Ethereum contracts. Thus, developers are advised to abandon Copy-and-Paste programming practices and detect EVM-Inequivalent Code Smells before reusing Ethereum contracts.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #3cc377" title="Confidence: 54.6%">
                        Blockchain
                    </span>
            <!-- LLMs: 14.3 -->
                
            <!-- Medicine: 3.1 -->
                
            <!-- Quantum Computing: 1.7 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -93.0951
            </span>
            <a href="https://arxiv.org/abs/2305.14046" target="_blank" rel="noopener noreferrer">Enhancing Smart Contract Security Analysis with Execution Property Graphs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kaihua Qin, Zhe Ye, Zhun Wang, Weilin Li, Liyi Zhou, Chao Zhang, Dawn Song, Arthur Gervais | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Smart contract vulnerabilities have led to significant financial losses, with their increasing complexity rendering outright prevention of hacks increasingly challenging. This trend highlights the crucial need for advanced forensic analysis and real-time intrusion detection, where dynamic analysis p</span>
            
            <span class="abstract-full" style="display: none;">Smart contract vulnerabilities have led to significant financial losses, with their increasing complexity rendering outright prevention of hacks increasingly challenging. This trend highlights the crucial need for advanced forensic analysis and real-time intrusion detection, where dynamic analysis plays a key role in dissecting smart contract executions. Therefore, there is a pressing need for a unified and generic representation of smart contract executions, complemented by an efficient methodology that enables the modeling and identification of a broad spectrum of emerging attacks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #3cc377" title="Confidence: 75.0%">
                        Blockchain
                    </span>
            <!-- LLMs: 16.1 -->
                
            <!-- RAG: 3.3 -->
                
            <!-- 3D: 2.9 -->
                
            <!-- T2I: 2.1 -->
                
            <!-- HPO and AutoML: 1.7 -->
                
            <!-- Medicine: 1.6 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Finance: 1.3 -->
                
            <!-- Bayesian Optimization: 1.2 -->
                
            <!-- Quality Diversity: 1.1 -->
                
            <!-- Datasets: 1.1 -->
                
            
        </div>
    </div>
    
    
    <div id="jsonPopup" class="json-popup">
        <pre id="jsonContent"></pre>
        <button onclick="copyJson()">Copy to Clipboard</button>
        <button onclick="closePopup()">Close</button>
    </div>

    <script>
        function extractPaperData(paperElement) {
            const titleElement = paperElement.querySelector('.paper-title a');
            const metaElement = paperElement.querySelector('.paper-meta');
            const abstractElement = paperElement.querySelector('.paper-abstract');
            const tagsElement = paperElement.querySelector('.paper-tags');
            
            const authorsText = metaElement.textContent.split('|')[0].replace('Authors:', '').trim();
            const dateText = metaElement.textContent.split('|')[1].replace('Date:', '').trim();
            
            const paperData = {
                title: titleElement.textContent,
                url: titleElement.href,
                authors: authorsText.split(',').map(author => author.trim()),
                created: dateText,
                abstract: abstractElement.querySelector('.abstract-full').textContent
            };
            
            return paperData;
        }

        function showJson(paperElement) {
            const popup = document.getElementById('jsonPopup');
            const content = document.getElementById('jsonContent');
            const paperData = extractPaperData(paperElement);
            content.textContent = JSON.stringify(paperData, null, null);
            popup.style.display = 'block';
            document.addEventListener('click', function closePopupOnClick(event) {
                if (!popup.contains(event.target)) {
                    popup.style.display = 'none';
                    document.removeEventListener('click', closePopupOnClick);
                }
            });
        }
        function toggleAbstract(element) {
            const abstract = element.parentElement;
            const short = abstract.querySelector('.abstract-short');
            const full = abstract.querySelector('.abstract-full');
            const lowConfidenceTags = abstract.parentElement.querySelectorAll('.tag-badge.low-confidence');
            
            if (element.textContent === '... more') {
                short.style.display = 'none';
                full.style.display = 'inline';
                element.textContent = ' less';
                lowConfidenceTags.forEach(tag => tag.style.display = 'inline-block');
            } else {
                short.style.display = 'inline';
                full.style.display = 'none';
                element.textContent = '... more';
                lowConfidenceTags.forEach(tag => tag.style.display = 'none');
            }
        }

        function closePopup() {
            document.getElementById('jsonPopup').style.display = 'none';
        }

        function copyJson() {
            const content = document.getElementById('jsonContent').textContent;
            navigator.clipboard.writeText(content).catch(() => {
                // If clipboard API is not available, just show the popup
                alert('Could not copy to clipboard. JSON is displayed in the popup.');
            });
        }

        // Close popup when clicking outside
        window.onclick = function(event) {
            const popup = document.getElementById('jsonPopup');
            if (event.target === popup) {
                popup.style.display = 'none';
            }
        }
    </script>
</body>
</html> 