<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Frontpage</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .paper {
            margin-bottom: 30px;
            margin-top: 30px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .paper-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .paper-meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        .paper-abstract {
            margin-bottom: 10px;
        }
        .abstract-short {
            display: inline;
        }
        .abstract-full {
            display: none;
        }
        .more-link {
            color: blue;
            cursor: pointer;
            text-decoration: underline;
        }
        .tag-badge {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 5px;
            margin-bottom: 5px;
            border-radius: 3px;
            font-size: 0.8em;
            color: white;
        }
        .tag-badge.high-confidence {
            opacity: 1;
        }
        .tag-badge.low-confidence {
            opacity: 0.6;
            display: none;
        }
        .interestingness-score {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 10px;
            color: white;
            border-radius: 3px;
            font-weight: bold;
        }
        .interestingness-positive {
            background-color: #4CAF50;
        }
        .interestingness-negative {
            background-color: #f44336;
        }
        .interestingness-neutral {
            background-color: #9e9e9e;
        }
        .last-updated {
            text-align: right;
            color: #666;
            font-size: 0.9em;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .intro {
            text-align: center;
            max-width: 60em;
            margin: 0 auto;
            color: #888;
        }
        .copy-icon {
            display: inline-block;
            width: 16px;
            height: 16px;
            cursor: pointer;
            margin-left: 5px;
            opacity: 0.5;
        }
        .copy-icon:hover {
            opacity: 1;
        }
        .json-popup {
            display: none;
            position: fixed;
            top: 20px;
            right: 20px;
            background: white;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            max-width: 500px;
            max-height: 300px;
            overflow: auto;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        a {
            color: inherit;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h1 {
            text-align: center;
        }
        h1 a {
            text-decoration: underline;
        }
        .date-section {
            margin-bottom: 40px;
        }
        .date-header {
            color: #666;
            font-size: 1.5em;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #eee;
        }
    </style>
</head>
<body>
    <h1>
        <a href="https://github.com/DataWraith/arxiv-frontpage">DataWraith's</a> ArXiv Frontpage
    </h1>

    <div class="last-updated">
        Last updated: 2025-04-21
    </div>

    <p class="intro">
        This frontpage is made by scraping ArXiv's computer science RSS feed and tagging papers with a classifier.
    </p>

    <p class="intro">
        Each tag is weighted according to my preferences to compute a paper's <i>interestingness</i> score.
    </p>
    
    
    <div class="date-section">
        <h2 class="date-header">2025-04-21</h2>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    2.2035
                </span>
                <a href="https://arxiv.org/abs/2412.04153" target="_blank" rel="noopener noreferrer">A Dynamic Safety Shield for Safe and Efficient Reinforcement Learning of Navigation Tasks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Murad Dawood, Ahmed Shokry, Maren Bennewitz
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reinforcement learning (RL) has been successfully applied to a variety of robotics applications, where it outperforms classical methods. However, the safety aspect of RL and the transfer to the real world remain an open challenge. A prominent field for tackling this challenge and ensuring the safety</span>
                
                <span class="abstract-full" style="display: none;">Reinforcement learning (RL) has been successfully applied to a variety of robotics applications, where it outperforms classical methods. However, the safety aspect of RL and the transfer to the real world remain an open challenge. A prominent field for tackling this challenge and ensuring the safety of the agents during training and execution is safe reinforcement learning. Safe RL can be achieved through constrained RL and safe exploration approaches. The former learns the safety constraints over the course of training to achieve a safe behavior by the end of training, at the cost of high number of collisions at earlier stages of the training. The latter offers robust safety by enforcing the safety constraints as hard constraints, which prevents collisions but hinders the exploration of the RL agent, resulting in lower rewards and poor performance. To overcome those drawbacks, we propose a novel safety shield, that combines the robustness of the optimization-based controllers with the long prediction capabilities of the RL agents, allowing the RL agent to adaptively tune the parameters of the controller. Our approach is able to improve the exploration of the RL agents for navigation tasks, while minimizing the number of collisions. Experiments in simulation show that our approach outperforms state-of-the-art baselines in the reached goals-to-collisions ratio in different challenging environments. The goals-to-collisions ratio metrics emphasizes the importance of minimizing the number of collisions, while learning to accomplish the task. Our approach achieves a higher number of reached goals compared to the classic safety shields and fewer collisions compared to constrained RL approaches. Finally, we demonstrate the performance of the proposed method in a real-world experiment.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 13.2 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Math: 3.3 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- Pathfinding: 2.3 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13191" target="_blank" rel="noopener noreferrer">Universal Representations for Classification-enhanced Lossy Compression</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nam Nguyen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In lossy compression, the classical tradeoff between compression rate and reconstruction distortion has traditionally guided algorithm design. However, Blau and Michaeli [5] introduced a generalized framework, known as the rate-distortion-perception (RDP) function, incorporating perceptual quality a</span>
                
                <span class="abstract-full" style="display: none;">In lossy compression, the classical tradeoff between compression rate and reconstruction distortion has traditionally guided algorithm design. However, Blau and Michaeli [5] introduced a generalized framework, known as the rate-distortion-perception (RDP) function, incorporating perceptual quality as an additional dimension of evaluation. More recently, the rate-distortion-classification (RDC) function was investigated in [19], evaluating compression performance by considering classification accuracy alongside distortion. In this paper, we explore universal representations, where a single encoder is developed to achieve multiple decoding objectives across various distortion and classification (or perception) constraints. This universality avoids retraining encoders for each specific operating point within these tradeoffs. Our experimental validation on the MNIST dataset indicates that a universal encoder incurs only minimal performance degradation compared to individually optimized encoders for perceptual image compression tasks, aligning with prior results from [23]. Nonetheless, we also identify that in the RDC setting, reusing an encoder optimized for one specific classification-distortion tradeoff leads to a significant distortion penalty when applied to alternative points.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.8 -->
                    
                <!-- LLMs: 5.6 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13198" target="_blank" rel="noopener noreferrer">Overcoming Bottlenecks in Homomorphic Encryption for the 2024 Mexican Federal Election</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Eric Landquist, Nimit Sawhney, Simer Sawhney
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">On June 2, 2024, Mexico held its federal elections. The majority of Mexican citizens voted in person at the polls in this historic election. For the first time though, Mexican citizens living outside their country were able to vote online via a web app, either on a personal device or using an electr</span>
                
                <span class="abstract-full" style="display: none;">On June 2, 2024, Mexico held its federal elections. The majority of Mexican citizens voted in person at the polls in this historic election. For the first time though, Mexican citizens living outside their country were able to vote online via a web app, either on a personal device or using an electronic voting kiosk at one of 23 embassies and consulates in the U.S., Canada, and Europe. In total, 144,734 people voted outside of Mexico: 122,496 on a personal device and 22,238 in-person at a kiosk. Voting was open for remote voting from 8PM, May 18, 2024 to 6PM, June 2, 2024 and was open for in-person voting from 8AM-6PM on June 2, 2024. This article describes the technical and cryptographic tools applied to secure the ex-patriate component of the election and to enable INE (Mexico's National Electoral Institute) to generate provable election results within minutes of the close of the election. This article will also describe how the solutions we present scale to elections on a national level.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.0 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13203" target="_blank" rel="noopener noreferrer">X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Salman Rahman, Liwei Jiang, James Shiffer, Genglin Liu, Sheriff Issaka, Md Rizwan Parvez, Hamid Palangi, Kai-Wei Chang, Yejin Choi, Saadia Gabriel
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-te</span>
                
                <span class="abstract-full" style="display: none;">Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically explores how seemingly harmless interactions escalate into harmful outcomes and generates corresponding attack scenarios. X-Teaming employs collaborative agents for planning, attack optimization, and verification, achieving state-of-the-art multi-turn jailbreak effectiveness and diversity with success rates up to 98.1% across representative leading open-weight and closed-source models. In particular, X-Teaming achieves a 96.2% attack success rate against the latest Claude 3.7 Sonnet model, which has been considered nearly immune to single-turn attacks. Building on X-Teaming, we introduce XGuard-Train, an open-source multi-turn safety training dataset that is 20x larger than the previous best resource, comprising 30K interactive jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our work offers essential tools and insights for mitigating sophisticated conversational attacks, advancing the multi-turn safety of LMs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.6 -->
                    
                <!-- LLMs: 9.4 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13204" target="_blank" rel="noopener noreferrer">EDGS: Eliminating Densification for Efficient Convergence of 3DGS</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dmytro Kotovenko, Olga Grebenkova, Bj\"orn Ommer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">3D Gaussian Splatting reconstructs scenes by starting from a sparse Structure-from-Motion initialization and iteratively refining under-reconstructed regions. This process is inherently slow, as it requires multiple densification steps where Gaussians are repeatedly split and adjusted, following a l</span>
                
                <span class="abstract-full" style="display: none;">3D Gaussian Splatting reconstructs scenes by starting from a sparse Structure-from-Motion initialization and iteratively refining under-reconstructed regions. This process is inherently slow, as it requires multiple densification steps where Gaussians are repeatedly split and adjusted, following a lengthy optimization path. Moreover, this incremental approach often leads to suboptimal renderings, particularly in high-frequency regions where detail is critical.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.8 -->
                    
                <!-- Medicine: 6.7 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- 3D: 3.7 -->
                    
                <!-- Networks: 3.0 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13206" target="_blank" rel="noopener noreferrer">DuoLoRA : Cycle-consistent and Rank-disentangled Content-Style Personalization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aniket Roy, Shubhankar Borse, Shreya Kadambi, Debasmit Das, Shweta Mahajan, Risheek Garrepalli, Hyojin Park, Ankita Nayak, Rama Chellappa, Munawar Hayat, Fatih Porikli
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We tackle the challenge of jointly personalizing content and style from a few examples. A promising approach is to train separate Low-Rank Adapters (LoRA) and merge them effectively, preserving both content and style. Existing methods, such as ZipLoRA, treat content and style as independent entities</span>
                
                <span class="abstract-full" style="display: none;">We tackle the challenge of jointly personalizing content and style from a few examples. A promising approach is to train separate Low-Rank Adapters (LoRA) and merge them effectively, preserving both content and style. Existing methods, such as ZipLoRA, treat content and style as independent entities, merging them by learning masks in LoRA's output dimensions. However, content and style are intertwined, not independent. To address this, we propose DuoLoRA, a content-style personalization framework featuring three key components: (i) rank-dimension mask learning, (ii) effective merging via layer priors, and (iii) Constyle loss, which leverages cycle-consistency in the merging process. First, we introduce ZipRank, which performs content-style merging within the rank dimension, offering adaptive rank flexibility and significantly reducing the number of learnable parameters. Additionally, we incorporate SDXL layer priors to apply implicit rank constraints informed by each layer's content-style bias and adaptive merger initialization, enhancing the integration of content and style. To further refine the merging process, we introduce Constyle loss, which leverages the cycle-consistency between content and style. Our experimental results demonstrate that DuoLoRA outperforms state-of-the-art content-style merging methods across multiple benchmarks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.6 -->
                    
                <!-- LLMs: 8.0 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13214" target="_blank" rel="noopener noreferrer">Wavelet-based Variational Autoencoders for High-Resolution Image Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andrew Kiruluta
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Variational Autoencoders (VAEs) are powerful generative models capable of learning compact latent representations. However, conventional VAEs often generate relatively blurry images due to their assumption of an isotropic Gaussian latent space and constraints in capturing high-frequency details. In </span>
                
                <span class="abstract-full" style="display: none;">Variational Autoencoders (VAEs) are powerful generative models capable of learning compact latent representations. However, conventional VAEs often generate relatively blurry images due to their assumption of an isotropic Gaussian latent space and constraints in capturing high-frequency details. In this paper, we explore a novel wavelet-based approach (Wavelet-VAE) in which the latent space is constructed using multi-scale Haar wavelet coefficients. We propose a comprehensive method to encode the image features into multi-scale detail and approximation coefficients and introduce a learnable noise parameter to maintain stochasticity. We thoroughly discuss how to reformulate the reparameterization trick, address the KL divergence term, and integrate wavelet sparsity principles into the training objective. Our experimental evaluation on CIFAR-10 and other high-resolution datasets demonstrates that the Wavelet-VAE improves visual fidelity and recovers higher-resolution details compared to conventional VAEs. We conclude with a discussion of advantages, potential limitations, and future research directions for wavelet-based generative modeling.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.5 -->
                    
                <!-- LLMs: 6.4 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Attention: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13226" target="_blank" rel="noopener noreferrer">Image Editing with Diffusion Models: A Survey</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jia Wang, Jie Hu, Xiaoqi Ma, Hanghang Ma, Xiaoming Wei, Enhua Wu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">With deeper exploration of diffusion model, developments in the field of image generation have triggered a boom in image creation. As the quality of base-model generated images continues to improve, so does the demand for further application like image editing. In recent years, many remarkable works</span>
                
                <span class="abstract-full" style="display: none;">With deeper exploration of diffusion model, developments in the field of image generation have triggered a boom in image creation. As the quality of base-model generated images continues to improve, so does the demand for further application like image editing. In recent years, many remarkable works are realizing a wide variety of editing effects. However, the wide variety of editing types and diverse editing approaches have made it difficult for researchers to establish a comprehensive view of the development of this field. In this survey, we summarize the image editing field from four aspects: tasks definition, methods classification, results evaluation and editing datasets. First, we provide a definition of image editing, which in turn leads to a variety of editing task forms from the perspective of operation parts and manipulation actions. Subsequently, we categorize and summary methods for implementing editing into three categories: inversion-based, fine-tuning-based and adapter-based. In addition, we organize the currently used metrics, available datasets and corresponding construction methods. At the end, we present some visions for the future development of the image editing field based on the previous summaries.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.1 -->
                    
                <!-- LLMs: 6.7 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13228" target="_blank" rel="noopener noreferrer">Modelling Mean-Field Games with Neural Ordinary Differential Equations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Anna C. M. Th\"oni, Yoram Bachrach, Tal Kachman
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Mean-field game theory relies on approximating games that would otherwise have been intractable to model. While the games can be solved analytically via the associated system of partial derivatives, this approach is not model-free, can lead to the loss of the existence or uniqueness of solutions and</span>
                
                <span class="abstract-full" style="display: none;">Mean-field game theory relies on approximating games that would otherwise have been intractable to model. While the games can be solved analytically via the associated system of partial derivatives, this approach is not model-free, can lead to the loss of the existence or uniqueness of solutions and may suffer from modelling bias. To reduce the dependency between the model and the game, we combine mean-field game theory with deep learning in the form of neural ordinary differential equations. The resulting model is data-driven, lightweight and can learn extensive strategic interactions that are hard to capture using mean-field theory alone. In addition, the model is based on automatic differentiation, making it more robust and objective than approaches based on finite differences. We highlight the efficiency and flexibility of our approach by solving three mean-field games that vary in their complexity, observability and the presence of noise. Using these results, we show that the model is flexible, lightweight and requires few observations to learn the distribution underlying the data.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.7 -->
                    
                <!-- Medicine: 5.5 -->
                    
                <!-- Math: 3.0 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Robotics: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13233" target="_blank" rel="noopener noreferrer">Auto-FEDUS: Autoregressive Generative Modeling of Doppler Ultrasound Signals from Fetal Electrocardiograms</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alireza Rafiei, Gari D. Clifford, Nasim Katebi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Fetal health monitoring through one-dimensional Doppler ultrasound (DUS) signals offers a cost-effective and accessible approach that is increasingly gaining interest. Despite its potential, the development of machine learning based techniques to assess the health condition of mothers and fetuses us</span>
                
                <span class="abstract-full" style="display: none;">Fetal health monitoring through one-dimensional Doppler ultrasound (DUS) signals offers a cost-effective and accessible approach that is increasingly gaining interest. Despite its potential, the development of machine learning based techniques to assess the health condition of mothers and fetuses using DUS signals remains limited. This scarcity is primarily due to the lack of extensive DUS datasets with a reliable reference for interpretation and data imbalance across different gestational ages. In response, we introduce a novel autoregressive generative model designed to map fetal electrocardiogram (FECG) signals to corresponding DUS waveforms (Auto-FEDUS). By leveraging a neural temporal network based on dilated causal convolutions that operate directly on the waveform level, the model effectively captures both short and long-range dependencies within the signals, preserving the integrity of generated data. Cross-subject experiments demonstrate that Auto-FEDUS outperforms conventional generative architectures across both time and frequency domain evaluations, producing DUS signals that closely resemble the morphology of their real counterparts. The realism of these synthesized signals was further gauged using a quality assessment model, which classified all as good quality, and a heart rate estimation model, which produced comparable results for generated and real data, with a Bland-Altman limit of 4.5 beats per minute. This advancement offers a promising solution for mitigating limited data availability and enhancing the training of DUS-based fetal models, making them more effective and generalizable.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.7 -->
                    
                <!-- LLMs: 5.0 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13236" target="_blank" rel="noopener noreferrer">NNTile: a machine learning framework capable of training extremely large GPT language models on a single node</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aleksandr Mikhalev, Aleksandr Katrutsa, Konstantin Sozykin, Ivan Oseledets
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study presents an NNTile framework for training large deep neural networks in heterogeneous clusters. The NNTile is based on a StarPU library, which implements task-based parallelism and schedules all provided tasks onto all available processing units (CPUs and GPUs). It means that a particular</span>
                
                <span class="abstract-full" style="display: none;">This study presents an NNTile framework for training large deep neural networks in heterogeneous clusters. The NNTile is based on a StarPU library, which implements task-based parallelism and schedules all provided tasks onto all available processing units (CPUs and GPUs). It means that a particular operation, necessary to train a large neural network, can be performed on any of the CPU cores or GPU devices, depending on automatic scheduling decisions. Such an approach shifts the burden of deciding where to compute and when to communicate from a human being to an automatic decision maker, whether a simple greedy heuristic or a complex AI-based software. The performance of the presented tool for training large language models is demonstrated in extensive numerical experiments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.9 -->
                    
                <!-- Medicine: 5.8 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13241" target="_blank" rel="noopener noreferrer">Recursive Deep Inverse Reinforcement Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Paul Ghanem, Michael Potter, Owen Howell, Pau Closas, Alireza Ramezani, Deniz Erdogmus, Robert Platt, Tales Imbiriba
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Inferring an adversary's goals from exhibited behavior is crucial for counterplanning and non-cooperative multi-agent systems in domains like cybersecurity, military, and strategy games. Deep Inverse Reinforcement Learning (IRL) methods based on maximum entropy principles show promise in recovering </span>
                
                <span class="abstract-full" style="display: none;">Inferring an adversary's goals from exhibited behavior is crucial for counterplanning and non-cooperative multi-agent systems in domains like cybersecurity, military, and strategy games. Deep Inverse Reinforcement Learning (IRL) methods based on maximum entropy principles show promise in recovering adversaries' goals but are typically offline, require large batch sizes with gradient descent, and rely on first-order updates, limiting their applicability in real-time scenarios. We propose an online Recursive Deep Inverse Reinforcement Learning (RDIRL) approach to recover the cost function governing the adversary actions and goals. Specifically, we minimize an upper bound on the standard Guided Cost Learning (GCL) objective using sequential second-order Newton updates, akin to the Extended Kalman Filter (EKF), leading to a fast (in terms of convergence) learning algorithm. We demonstrate that RDIRL is able to recover cost and reward functions of expert agents in standard and adversarial benchmark tasks. Experiments on benchmark tasks show that our proposed approach outperforms several leading IRL algorithms.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.5 -->
                    
                <!-- LLMs: 6.1 -->
                    
                <!-- Reinforcement Learning: 3.3 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13242" target="_blank" rel="noopener noreferrer">Dynamic Memory-enhanced Transformer for Hyperspectral Image Classification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Muhammad Ahmad, Manuel Mazzara, Salvatore Distefano, Adil Mehmood Khan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Hyperspectral image (HSI) classification remains a challenging task due to the intricate spatial-spectral correlations. Existing transformer models excel in capturing long-range dependencies but often suffer from information redundancy and attention inefficiencies, limiting their ability to model fi</span>
                
                <span class="abstract-full" style="display: none;">Hyperspectral image (HSI) classification remains a challenging task due to the intricate spatial-spectral correlations. Existing transformer models excel in capturing long-range dependencies but often suffer from information redundancy and attention inefficiencies, limiting their ability to model fine-grained relationships crucial for HSI classification. To overcome these limitations, this work proposes MemFormer, a lightweight and memory-enhanced transformer. MemFormer introduces a memory-enhanced multi-head attention mechanism that iteratively refines a dynamic memory module, enhancing feature extraction while reducing redundancy across layers. Additionally, a dynamic memory enrichment strategy progressively captures complex spatial and spectral dependencies, leading to more expressive feature representations. To further improve structural consistency, we incorporate a spatial-spectral positional encoding (SSPE) tailored for HSI data, ensuring continuity without the computational burden of convolution-based approaches. Extensive experiments on benchmark datasets demonstrate that MemFormer achieves superior classification accuracy, outperforming state-of-the-art methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.5 -->
                    
                <!-- LLMs: 6.0 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- 3D: 2.9 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Attention: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13266" target="_blank" rel="noopener noreferrer">Graph Learning at Scale: Characterizing and Optimizing Pre-Propagation GNNs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zichao Yue, Chenhui Deng, Zhiru Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph neural networks (GNNs) are widely used for learning node embeddings in graphs, typically adopting a message-passing scheme. This approach, however, leads to the neighbor explosion problem, with exponentially growing computational and memory demands as layers increase. Graph sampling has become</span>
                
                <span class="abstract-full" style="display: none;">Graph neural networks (GNNs) are widely used for learning node embeddings in graphs, typically adopting a message-passing scheme. This approach, however, leads to the neighbor explosion problem, with exponentially growing computational and memory demands as layers increase. Graph sampling has become the predominant method for scaling GNNs to large graphs, mitigating but not fully solving the issue. Pre-propagation GNNs (PP-GNNs) represent a new class of models that decouple feature propagation from training through pre-processing, addressing neighbor explosion in theory. Yet, their practical advantages and system-level optimizations remain underexplored. This paper provides a comprehensive characterization of PP-GNNs, comparing them with graph-sampling-based methods in training efficiency, scalability, and accuracy. While PP-GNNs achieve comparable accuracy, we identify data loading as the key bottleneck for training efficiency and input expansion as a major scalability challenge. To address these issues, we propose optimized data loading schemes and tailored training methods that improve PP-GNN training throughput by an average of 15$\times$ over the PP-GNN baselines, with speedup of up to 2 orders of magnitude compared to sampling-based GNNs on large graph benchmarks. Our implementation is publicly available at https://github.com/cornell-zhang/preprop-gnn.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.5 -->
                    
                <!-- LLMs: 7.3 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13268" target="_blank" rel="noopener noreferrer">Dichotomy for orderings?</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: G\'abor Kun, Jaroslav Ne\v{s}et\v{r}il
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The class $NP$ can be defined by the means of Monadic Second-Order logic going back to Fagin and Feder-Vardi, and also by forbidden expanded substructures (cf. lifts and shadows of Kun and Ne\v{s}et\v{r}il). Consequently, for such problems there is no dichotomy, unlike for $CSP$'s. We prove that ord</span>
                
                <span class="abstract-full" style="display: none;">The class $NP$ can be defined by the means of Monadic Second-Order logic going back to Fagin and Feder-Vardi, and also by forbidden expanded substructures (cf. lifts and shadows of Kun and Ne\v{s}et\v{r}il). Consequently, for such problems there is no dichotomy, unlike for $CSP$'s. We prove that ordering problems for graphs defined by finitely many forbidden ordered subgraphs still capture the class $NP$. In particular, we refute a conjecture of Hell, Mohar and Rafiey that dichotomy holds for this class. On the positive side, we confirm the conjecture of Duffus, Ginn and R\"odl that ordering problems defined by one single biconnected ordered graph are $NP$-complete but for the ordered complete graph. An interesting feature appeared and was noticed several times. For finite sets of biconnected patterns (which may be colored structures or ordered structures) complexity dichotomy holds. A principal tool for obtaining this result is known as the Sparse Incomparability Lemma, a classical result in the theory of homomorphisms of graphs and structures. We prove it here in the setting of ordered graphs as a Temporal Sparse Incomparability Lemma for orderings. Interestingly, our proof involves the Lov\'asz Local Lemma.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.0 -->
                    
                <!-- LLMs: 5.5 -->
                    
                <!-- Quantum Computing: 4.2 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13276" target="_blank" rel="noopener noreferrer">Strategic Planning of Stealthy Backdoor Attacks in Markov Decision Processes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xinyi Wei, Shuo Han, Ahmed H. Hemida, Charles A. Kamhoua, Jie Fu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper investigates backdoor attack planning in stochastic control systems modeled as Markov Decision Processes (MDPs). In a backdoor attack, the adversary provides a control policy that behaves well in the original MDP to pass the testing phase. However, when such a policy is deployed with a tr</span>
                
                <span class="abstract-full" style="display: none;">This paper investigates backdoor attack planning in stochastic control systems modeled as Markov Decision Processes (MDPs). In a backdoor attack, the adversary provides a control policy that behaves well in the original MDP to pass the testing phase. However, when such a policy is deployed with a trigger policy, which perturbs the system dynamics at runtime, it optimizes the attacker's objective instead. To solve jointly the control policy and its trigger, we formulate the attack planning problem as a constrained optimal planning problem in an MDP with augmented state space, with the objective to maximize the attacker's total rewards in the system with an activated trigger, subject to the constraint that the control policy is near optimal in the original MDP. We then introduce a gradient-based optimization method to solve the optimal backdoor attack policy as a pair of coordinated control and trigger policies. Experimental results from a case study validate the effectiveness of our approach in achieving stealthy backdoor attacks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 7.7 -->
                    
                <!-- Networks: 3.6 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Pathfinding: 2.3 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Medicine: 1.8 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13278" target="_blank" rel="noopener noreferrer">A Stochastic Nonlinear Dynamical System for Smoothing Noisy Eye Gaze Data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Thoa Thieu, Roderick Melnik
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this study, we address the challenges associated with accurately determining gaze location on a screen, which is often compromised by noise from factors such as eye tracker limitations, calibration drift, ambient lighting changes, and eye blinks. We propose the use of an extended Kalman filter (E</span>
                
                <span class="abstract-full" style="display: none;">In this study, we address the challenges associated with accurately determining gaze location on a screen, which is often compromised by noise from factors such as eye tracker limitations, calibration drift, ambient lighting changes, and eye blinks. We propose the use of an extended Kalman filter (EKF) to smooth the gaze data collected during eye-tracking experiments, and systematically explore the interaction of different system parameters. Our results demonstrate that the EKF significantly reduces noise, leading to a marked improvement in tracking accuracy. Furthermore, we show that our proposed stochastic nonlinear dynamical model aligns well with real experimental data and holds promise for applications in related fields.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.6 -->
                    
                <!-- LLMs: 6.6 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13279" target="_blank" rel="noopener noreferrer">Just Another Hour on TikTok: Reverse-engineering unique identifiers to obtain a complete slice of TikTok</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Benjamin Steel, Miriam Schirmer, Derek Ruths, Juergen Pfeffer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">TikTok is now a massive platform, and has a deep impact on global events. But for all the preliminary studies done on it, there are still issues with determining fundamental characteristics of the platform. We develop a method to extract a representative sample from a specific time range on TikTok, </span>
                
                <span class="abstract-full" style="display: none;">TikTok is now a massive platform, and has a deep impact on global events. But for all the preliminary studies done on it, there are still issues with determining fundamental characteristics of the platform. We develop a method to extract a representative sample from a specific time range on TikTok, and use it to collect >99\% of posts from a full hour on the platform, alongside a dataset of >99\% of posts from a single minute from each hour of a day. Through this, we obtain post metadata, video media data, and comments from a close to complete slice of TikTok. Using this dataset, we report the critical statistics of the platform, notably estimating a total of 117 million posts produced on the day we looked at on TikTok.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.7 -->
                    
                <!-- Medicine: 4.3 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13284" target="_blank" rel="noopener noreferrer">Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Derguene Mbaye, Madoune Robert Seye, Moussa Diallo, Mamadou Lamine Ndiaye, Djiby Sow, Dimitri Samuel Adjanohoun, Tatiana Mbengue, Cheikh Samba Wade, De Roulet Pablo, Jean-Claude Baraka Munyaka, Jerome Chenal
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Internet penetration rates in Africa are rising steadily, and mobile Internet is getting an even bigger boost with the availability of smartphones. Young people are increasingly using the Internet, especially social networks, and Senegal is no exception to this revolution. Social networks have becom</span>
                
                <span class="abstract-full" style="display: none;">Internet penetration rates in Africa are rising steadily, and mobile Internet is getting an even bigger boost with the availability of smartphones. Young people are increasingly using the Internet, especially social networks, and Senegal is no exception to this revolution. Social networks have become the main means of expression for young people. Despite this evolution in Internet access, there are few operators on the market, which limits the alternatives available in terms of value for money. In this paper, we will look at how young people feel about the price of mobile Internet in Senegal, in relation to the perceived quality of the service, through their comments on social networks. We scanned a set of Twitter and Facebook comments related to the subject and applied a sentiment analysis model to gather their general feelings.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.5 -->
                    
                <!-- Medicine: 4.5 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Math: 2.4 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13286" target="_blank" rel="noopener noreferrer">A Model Predictive Control Approach for Quadrotor Cruise Control</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zekai Chen, Leon Kehler
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper investigates the application of a Model Predictive Controller (MPC) for the cruise control system of a quadrotor, focusing on hovering point stabilization and reference tracking. Initially, a full-state-feedback MPC is designed for the ideal scenario. To account for real-world conditions,</span>
                
                <span class="abstract-full" style="display: none;">This paper investigates the application of a Model Predictive Controller (MPC) for the cruise control system of a quadrotor, focusing on hovering point stabilization and reference tracking. Initially, a full-state-feedback MPC is designed for the ideal scenario. To account for real-world conditions, a constant disturbance is introduced to the quadrotor, simulating a gust of wind in a specific direction. In response, an output-feedback offset-free MPC is developed to stabilize the quadrotor while rejecting the disturbance. We validate the design of the controller by conducting stability analysis, as well as numerical simulations under different circumstances. It is shown that the designed controller can achieve all the expected goals for the cruise control, including reference tracking and disturbance rejection. This project was implemented using Python and the CVXPY library for convex optimization.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.7 -->
                    
                <!-- Networks: 3.6 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- LLMs: 1.8 -->
                    
                <!-- Pathfinding: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13288" target="_blank" rel="noopener noreferrer">Integrated Control and Active Perception in POMDPs for Temporal Logic Tasks and Information Acquisition</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chongyang Shi, Michael R. Dorothy, Jie Fu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper studies the synthesis of a joint control and active perception policy for a stochastic system modeled as a partially observable Markov decision process (POMDP), subject to temporal logic specifications. The POMDP actions influence both system dynamics (control) and the emission function (</span>
                
                <span class="abstract-full" style="display: none;">This paper studies the synthesis of a joint control and active perception policy for a stochastic system modeled as a partially observable Markov decision process (POMDP), subject to temporal logic specifications. The POMDP actions influence both system dynamics (control) and the emission function (perception). Beyond task completion, the planner seeks to maximize information gain about certain temporal events (the secret) through coordinated perception and control. To enable active information acquisition, we introduce minimizing the Shannon conditional entropy of the secret as a planning objective, alongside maximizing the probability of satisfying the temporal logic formula within a finite horizon. Using a variant of observable operators in hidden Markov models (HMMs) and POMDPs, we establish key properties of the conditional entropy gradient with respect to policy parameters. These properties facilitate efficient policy gradient computation. We validate our approach through graph-based examples, inspired by common security applications with UAV surveillance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.3 -->
                    
                <!-- LLMs: 5.1 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13292" target="_blank" rel="noopener noreferrer">Let Me Grok for You: Accelerating Grokking via Embedding Transfer from a Weaker Model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhiwei Xu, Zhiyu Ni, Yixin Wang, Wei Hu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">''Grokking'' is a phenomenon where a neural network first memorizes training data and generalizes poorly, but then suddenly transitions to near-perfect generalization after prolonged training. While intriguing, this delayed generalization phenomenon compromises predictability and efficiency. Ideally</span>
                
                <span class="abstract-full" style="display: none;">''Grokking'' is a phenomenon where a neural network first memorizes training data and generalizes poorly, but then suddenly transitions to near-perfect generalization after prolonged training. While intriguing, this delayed generalization phenomenon compromises predictability and efficiency. Ideally, models should generalize directly without delay. To this end, this paper proposes GrokTransfer, a simple and principled method for accelerating grokking in training neural networks, based on the key observation that data embedding plays a crucial role in determining whether generalization is delayed. GrokTransfer first trains a smaller, weaker model to reach a nontrivial (but far from optimal) test performance. Then, the learned input embedding from this weaker model is extracted and used to initialize the embedding in the target, stronger model. We rigorously prove that, on a synthetic XOR task where delayed generalization always occurs in normal training, GrokTransfer enables the target model to generalize directly without delay. Moreover, we demonstrate that, across empirical studies of different tasks, GrokTransfer effectively reshapes the training dynamics and eliminates delayed generalization, for both fully-connected neural networks and Transformers.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.3 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13294" target="_blank" rel="noopener noreferrer">TAXI: Traveling Salesman Problem Accelerator with X-bar-based Ising Macros Powered by SOT-MRAMs and Hierarchical Clustering</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sangmin Yoo, Amod Holla, Sourav Sanyal, Dong Eun Kim, Francesca Iacopi, Dwaipayan Biswas, James Myers, Kaushik Roy
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Ising solvers with hierarchical clustering have shown promise for large-scale Traveling Salesman Problems (TSPs), in terms of latency and energy. However, most of these methods still face unacceptable quality degradation as the problem size increases beyond a certain extent. Additionally, their hard</span>
                
                <span class="abstract-full" style="display: none;">Ising solvers with hierarchical clustering have shown promise for large-scale Traveling Salesman Problems (TSPs), in terms of latency and energy. However, most of these methods still face unacceptable quality degradation as the problem size increases beyond a certain extent. Additionally, their hardware-agnostic adoptions limit their ability to fully exploit available hardware resources. In this work, we introduce TAXI -- an in-memory computing-based TSP accelerator with crossbar(Xbar)-based Ising macros. Each macro independently solves a TSP sub-problem, obtained by hierarchical clustering, without the need for any off-macro data movement, leading to massive parallelism. Within the macro, Spin-Orbit-Torque (SOT) devices serve as compact energy-efficient random number generators enabling rapid "natural annealing". By leveraging hardware-algorithm co-design, TAXI offers improvements in solution quality, speed, and energy-efficiency on TSPs up to 85,900 cities (the largest TSPLIB instance). TAXI produces solutions that are only 22% and 20% longer than the Concorde solver's exact solution on 33,810 and 85,900 city TSPs, respectively. TAXI outperforms a current state-of-the-art clustering-based Ising solver, being 8x faster on average across 20 benchmark problems from TSPLib.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.4 -->
                    
                <!-- Medicine: 6.9 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13296" target="_blank" rel="noopener noreferrer">Enhanced Pruning Strategy for Multi-Component Neural Architectures Using Component-Aware Graph Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ganesh Sundaram, Jonas Ulmen, Daniel G\"orges
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Deep neural networks (DNNs) deliver outstanding performance, but their complexity often prohibits deployment in resource-constrained settings. Comprehensive structured pruning frameworks based on parameter dependency analysis reduce model size with specific regard to computational performance. When </span>
                
                <span class="abstract-full" style="display: none;">Deep neural networks (DNNs) deliver outstanding performance, but their complexity often prohibits deployment in resource-constrained settings. Comprehensive structured pruning frameworks based on parameter dependency analysis reduce model size with specific regard to computational performance. When applying them to Multi-Component Neural Architectures (MCNAs), they risk network integrity by removing large parameter groups. We introduce a component-aware pruning strategy, extending dependency graphs to isolate individual components and inter-component flows. This creates smaller, targeted pruning groups that conserve functional integrity. Demonstrated effectively on a control task, our approach achieves greater sparsity and reduced performance degradation, opening a path for optimizing complex, multi-component DNNs efficiently.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.8 -->
                    
                <!-- Medicine: 7.5 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- GNN: 3.0 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Attention: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13297" target="_blank" rel="noopener noreferrer">Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andreas Lau Hansen, Lukas Wanzeck, Dim P. Papadopoulos
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Monocular 3D object detection is an essential task in computer vision, and it has several applications in robotics and virtual reality. However, 3D object detectors are typically trained in a fully supervised way, relying extensively on 3D labeled data, which is labor-intensive and costly to annotat</span>
                
                <span class="abstract-full" style="display: none;">Monocular 3D object detection is an essential task in computer vision, and it has several applications in robotics and virtual reality. However, 3D object detectors are typically trained in a fully supervised way, relying extensively on 3D labeled data, which is labor-intensive and costly to annotate. This work focuses on weakly-supervised 3D detection to reduce data needs using a monocular method that leverages a singlecamera system over expensive LiDAR sensors or multi-camera setups. We propose a general model Weak Cube R-CNN, which can predict objects in 3D at inference time, requiring only 2D box annotations for training by exploiting the relationship between 2D projections of 3D cubes. Our proposed method utilizes pre-trained frozen foundation 2D models to estimate depth and orientation information on a training set. We use these estimated values as pseudo-ground truths during training. We design loss functions that avoid 3D labels by incorporating information from the external models into the loss. In this way, we aim to implicitly transfer knowledge from these large foundation 2D models without having access to 3D bounding box annotations. Experimental results on the SUN RGB-D dataset show increased performance in accuracy compared to an annotation time equalized Cube R-CNN baseline. While not precise for centimetre-level measurements, this method provides a strong foundation for further research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.3 -->
                    
                <!-- LLMs: 5.6 -->
                    
                <!-- 3D: 5.2 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13301" target="_blank" rel="noopener noreferrer">DYNAMITE: Dynamic Defense Selection for Enhancing Machine Learning-based Intrusion Detection Against Adversarial Attacks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jing Chen, Onat Gungor, Zhengli Shang, Elvin Li, Tajana Rosing
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rapid proliferation of the Internet of Things (IoT) has introduced substantial security vulnerabilities, highlighting the need for robust Intrusion Detection Systems (IDS). Machine learning-based intrusion detection systems (ML-IDS) have significantly improved threat detection capabilities; howe</span>
                
                <span class="abstract-full" style="display: none;">The rapid proliferation of the Internet of Things (IoT) has introduced substantial security vulnerabilities, highlighting the need for robust Intrusion Detection Systems (IDS). Machine learning-based intrusion detection systems (ML-IDS) have significantly improved threat detection capabilities; however, they remain highly susceptible to adversarial attacks. While numerous defense mechanisms have been proposed to enhance ML-IDS resilience, a systematic approach for selecting the most effective defense against a specific adversarial attack remains absent. To address this challenge, we propose Dynamite, a dynamic defense selection framework that enhances ML-IDS by intelligently identifying and deploying the most suitable defense using a machine learning-driven selection mechanism. Our results demonstrate that Dynamite achieves a 96.2% reduction in computational time compared to the Oracle, significantly decreasing computational overhead while preserving strong prediction performance. Dynamite also demonstrates an average F1-score improvement of 76.7% over random defense and 65.8% over the best static state-of-the-art defense.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.4 -->
                    
                <!-- Medicine: 6.1 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13302" target="_blank" rel="noopener noreferrer">Training Autoencoders Using Stochastic Hessian-Free Optimization with LSMR</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ibrahim Emirahmetoglu, David E. Stewart
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Hessian-free (HF) optimization has been shown to effectively train deep autoencoders (Martens, 2010). In this paper, we aim to accelerate HF training of autoencoders by reducing the amount of data used in training. HF utilizes the conjugate gradient algorithm to estimate update directions. Instead, </span>
                
                <span class="abstract-full" style="display: none;">Hessian-free (HF) optimization has been shown to effectively train deep autoencoders (Martens, 2010). In this paper, we aim to accelerate HF training of autoencoders by reducing the amount of data used in training. HF utilizes the conjugate gradient algorithm to estimate update directions. Instead, we propose using the LSMR method, which is known for effectively solving large sparse linear systems. We also incorporate Chapelle & Erhan (2011)'s improved preconditioner for HF optimization. In addition, we introduce a new mini-batch selection algorithm to mitigate overfitting. Our algorithm starts with a small subset of the training data and gradually increases the mini-batch size based on (i) variance estimates obtained during the computation of a mini-batch gradient (Byrd et al., 2012) and (ii) the relative decrease in objective value for the validation data. Our experimental results demonstrate that our stochastic Hessian-free optimization, using the LSMR method and the new sample selection algorithm, leads to rapid training of deep autoencoders with improved generalization error.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.4 -->
                    
                <!-- Medicine: 6.0 -->
                    
                <!-- Reinforcement Learning: 3.9 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13305" target="_blank" rel="noopener noreferrer">Implementation of Field Programmable Gate Arrays (FPGAs) in Extremely Cold Environments for Space and Cryogenic Computing Applications</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christopher Lewis, Drew Sellers, Michael Hamilton
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The operation of CMOS Field Programmable Gate Arrays (FPGAs) at extremely cold environments as low as 4 K is demonstrated. Various FPGA and periphery hardware design techniques spanning from HDL design to improvements of peripheral circuitry such as discrete voltage regulators are displayed, and the</span>
                
                <span class="abstract-full" style="display: none;">The operation of CMOS Field Programmable Gate Arrays (FPGAs) at extremely cold environments as low as 4 K is demonstrated. Various FPGA and periphery hardware design techniques spanning from HDL design to improvements of peripheral circuitry such as discrete voltage regulators are displayed, and their respective performances are reported. While general operating conditions for voltage regulators are widened, FPGAs see a broader temperature range with improved jitter performance, reduced LUT delays, and enhanced transceiver performance at extremely low temperatures.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.4 -->
                    
                <!-- LLMs: 8.1 -->
                    
                <!-- Quantum Computing: 4.1 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- Blockchain: 2.5 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13310" target="_blank" rel="noopener noreferrer">SAR Object Detection with Self-Supervised Pretraining and Curriculum-Aware Sampling</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yasin Almalioglu, Andrzej Kucik, Geoffrey French, Dafni Antotsiou, Alexander Adam, Cedric Archambeau
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Object detection in satellite-borne Synthetic Aperture Radar (SAR) imagery holds immense potential in tasks such as urban monitoring and disaster response. However, the inherent complexities of SAR data and the scarcity of annotations present significant challenges in the advancement of object detec</span>
                
                <span class="abstract-full" style="display: none;">Object detection in satellite-borne Synthetic Aperture Radar (SAR) imagery holds immense potential in tasks such as urban monitoring and disaster response. However, the inherent complexities of SAR data and the scarcity of annotations present significant challenges in the advancement of object detection in this domain. Notably, the detection of small objects in satellite-borne SAR images poses a particularly intricate problem, because of the technology's relatively low spatial resolution and inherent noise. Furthermore, the lack of large labelled SAR datasets hinders the development of supervised deep learning-based object detection models. In this paper, we introduce TRANSAR, a novel self-supervised end-to-end vision transformer-based SAR object detection model that incorporates masked image pre-training on an unlabeled SAR image dataset that spans more than $25,700$ km\textsuperscript{2} ground area. Unlike traditional object detection formulation, our approach capitalises on auxiliary binary semantic segmentation, designed to segregate objects of interest during the post-tuning, especially the smaller ones, from the background. In addition, to address the innate class imbalance due to the disproportion of the object to the image size, we introduce an adaptive sampling scheduler that dynamically adjusts the target class distribution during training based on curriculum learning and model feedback. This approach allows us to outperform conventional supervised architecture such as DeepLabv3 or UNet, and state-of-the-art self-supervised learning-based arhitectures such as DPT, SegFormer or UperNet, as shown by extensive evaluations on benchmark SAR datasets.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.3 -->
                    
                <!-- Medicine: 4.7 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Networks: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13312" target="_blank" rel="noopener noreferrer">The role of boundary constraints in simulating a nonlocal Gray-Scott model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Loic Cappanera, Gabriela Jaramillo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present second-order algorithms to approximate the solution of a nonlocal Gray-Scott model that is known to generate interesting spatio-temporal structures such as pulse and stripes solutions. Our algorithms rely on a quadrature method for the spatial discretization and the method of lines using </span>
                
                <span class="abstract-full" style="display: none;">We present second-order algorithms to approximate the solution of a nonlocal Gray-Scott model that is known to generate interesting spatio-temporal structures such as pulse and stripes solutions. Our algorithms rely on a quadrature method for the spatial discretization and the method of lines using a second-order Adams-Bashforth for the time marching. We focus on studying the impact of the type of boundary constraints, e.g. nonlocal Dirichlet/Neumann or local periodic, and the type of nonlocal diffusion, i.e. integral operator with thin- or fat-tailed kernels, on the generation of pulse solutions. Our numerical investigations show that when the spread of the kernel is large, i.e. when the model is nonlocal, both the type of kernels and type of boundary constraints have a strong impact on the solutions profiles.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 5.1 -->
                    
                <!-- Medicine: 5.0 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Math: 2.9 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13324" target="_blank" rel="noopener noreferrer">Robust Estimation of Battery State of Health Using Reference Voltage Trajectory</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rui Huang, Jackson Fogelquist, Xinfan Lin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate estimation of state of health (SOH) is critical for battery applications. Current model-based SOH estimation methods typically rely on low C-rate constant current tests to extract health parameters like solid phase volume fraction and lithium-ion stoichiometry, which are often impractical i</span>
                
                <span class="abstract-full" style="display: none;">Accurate estimation of state of health (SOH) is critical for battery applications. Current model-based SOH estimation methods typically rely on low C-rate constant current tests to extract health parameters like solid phase volume fraction and lithium-ion stoichiometry, which are often impractical in real-world scenarios due to time and operational constraints. Additionally, these methods are susceptible to modeling uncertainties that can significantly degrade the estimation accuracy, especially when jointly estimating multiple parameters. In this paper, we present a novel reference voltage-based method for robust battery SOH estimation. This method utilizes the voltage response of a battery under a predefined current excitation at the beginning of life (BOL) as a reference to compensate for modeling uncertainty. As the battery degrades, the same excitation is applied to generate the voltage response, which is compared with the BOL trajectory to estimate the key health parameters accurately. The current excitation is optimally designed using the Particle Swarm Optimization algorithm to maximize the information content of the target parameters. Simulation results demonstrate that our proposed method significantly improves parameter estimation accuracy under different degradation levels, compared to conventional methods relying only on direct voltage measurements. Furthermore, our method jointly estimates four key SOH parameters in only 10 minutes, making it practical for real-world battery health diagnostics, e.g., fast testing to enable battery repurposing.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.1 -->
                    
                <!-- Reinforcement Learning: 3.5 -->
                    
                <!-- Networks: 3.4 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13325" target="_blank" rel="noopener noreferrer">From Bayesian Asymptotics to General Large-Scale MIMO Capacity</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sheng Yang, Richard Combes
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a unifying framework that bridges Bayesian asymptotics and information theory to analyze the asymptotic Shannon capacity of general large-scale MIMO channels including ones with non-linearities or imperfect hardware. We derive both an analytic capacity formula and an asymptotically optima</span>
                
                <span class="abstract-full" style="display: none;">We present a unifying framework that bridges Bayesian asymptotics and information theory to analyze the asymptotic Shannon capacity of general large-scale MIMO channels including ones with non-linearities or imperfect hardware. We derive both an analytic capacity formula and an asymptotically optimal input distribution in the large-antenna regime, each of which depends solely on the single-output channel's Fisher information through a term we call the (tilted) Jeffreys' factor. We demonstrate how our method applies broadly to scenarios with clipping, coarse quantization (including 1-bit ADCs), phase noise, fading with imperfect CSI, and even optical Poisson channels. Our asymptotic analysis motivates a practical approach to constellation design via a compander-like transformation. Furthermore, we introduce a low-complexity receiver structure that approximates the log-likelihood by quantizing the channel outputs into finitely many bins, enabling near-capacity performance with computational complexity independent of the output dimension. Numerical results confirm that the proposed method unifies and simplifies many previously intractable MIMO capacity problems and reveals how the Fisher information alone governs the channel's asymptotic behavior.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.2 -->
                    
                <!-- Medicine: 4.3 -->
                    
                <!-- Reinforcement Learning: 4.0 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13334" target="_blank" rel="noopener noreferrer">Utilizing Virtual Reality for Wildfire Evacuation Training</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alison Crosby, MJ Johns, Katherine Isbister, Sri Kurniawan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The risk of loss of lives and property damage has increased all around the world in recent years as wildfire seasons have become longer and fires have become larger. Knowing how to prepare and evacuate safely is critical, yet it may be daunting for those who have never experienced a wildfire threat </span>
                
                <span class="abstract-full" style="display: none;">The risk of loss of lives and property damage has increased all around the world in recent years as wildfire seasons have become longer and fires have become larger. Knowing how to prepare and evacuate safely is critical, yet it may be daunting for those who have never experienced a wildfire threat before. This paper considers the potential for utilizing virtual reality (VR) technology to prepare people for an evacuation scenario. We discuss the unique affordances of VR for this type of work, as well as the initial steps in creating a training simulation. We also explore the next steps for what a tool like this may mean for the future of evacuation preparedness training.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.2 -->
                    
                <!-- LLMs: 4.6 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13335" target="_blank" rel="noopener noreferrer">Multiharmonic algorithms for contrast-enhanced ultrasound</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vanja Nikoli\'c, Teresa Rauscher
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Harmonic generation plays a crucial role in contrast-enhanced ultrasound, both for imaging and therapeutic applications. However, accurately capturing these nonlinear effects is computationally very demanding when using traditional time-domain approaches. To address this issue, in this work, we deve</span>
                
                <span class="abstract-full" style="display: none;">Harmonic generation plays a crucial role in contrast-enhanced ultrasound, both for imaging and therapeutic applications. However, accurately capturing these nonlinear effects is computationally very demanding when using traditional time-domain approaches. To address this issue, in this work, we develop algorithms based on a time discretization that uses a multiharmonic Ansatz applied to a model that couples the Westervelt equation for acoustic pressure with a volume-based approximation of the Rayleigh--Plesset equation for the dynamics of microbubble contrast agents. We first rigorously establish the existence of time-periodic solutions for this Westervelt-ODE system. We then derive a multiharmonic representation of the system under time-periodic excitation and develop iterative algorithms that rely on the successive computation of higher harmonics under the assumption of real-valued or complex solution fields. In the real-valued setting, we characterize the approximation error in terms of the number of harmonics and a contribution owing to the fixed-point iteration. Finally, we investigate these algorithms numerically and illustrate how the number of harmonics and presence of microbubbles influence the propagation of acoustic waves.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.0 -->
                    
                <!-- Reinforcement Learning: 4.6 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Math: 2.8 -->
                    
                <!-- Pathfinding: 1.5 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13342" target="_blank" rel="noopener noreferrer">Levenshtein's Sequence Reconstruction Problem and Results for Larger Alphabet Sizes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ville Junnila, Tero Laihonen, Tuomo Lehtil\"a
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The problem of storing large amounts of information safely for a long period of time has become essential. One of the most promising new data storage mediums are the polymer-based data storage systems, like the DNA-storage system. These storage systems are highly durable and they consume very little</span>
                
                <span class="abstract-full" style="display: none;">The problem of storing large amounts of information safely for a long period of time has become essential. One of the most promising new data storage mediums are the polymer-based data storage systems, like the DNA-storage system. These storage systems are highly durable and they consume very little energy to store the data. When information is retrieved from a storage, however, several different types of errors may occur in the process. It is known that the Levenshtein's sequence reconstruction framework is well-suited to overcome such errors and to retrieve the original information. Many of the previous results regarding Levenshtein's sequence reconstruction method are so far given only for the binary alphabet. However, larger alphabets are natural for the polymer-based data storage. For example, the quaternary alphabet is suitable for DNA-storage due to the four amino-acids in DNA. The results for larger alphabets often require, as we will see in this work, different and more complicated techniques compared to the binary case. Moreover, we show that an increase in the alphabet size makes some error types behave rather surprisingly.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.7 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Math: 2.6 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Pathfinding: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13351" target="_blank" rel="noopener noreferrer">Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chen Wang, Fei Xia, Wenhao Yu, Tingnan Zhang, Ruohan Zhang, C. Karen Liu, Li Fei-Fei, Jie Tan, Jacky Liang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Learning to perform manipulation tasks from human videos is a promising approach for teaching robots. However, many manipulation tasks require changing control parameters during task execution, such as force, which visual data alone cannot capture. In this work, we leverage sensing devices such as a</span>
                
                <span class="abstract-full" style="display: none;">Learning to perform manipulation tasks from human videos is a promising approach for teaching robots. However, many manipulation tasks require changing control parameters during task execution, such as force, which visual data alone cannot capture. In this work, we leverage sensing devices such as armbands that measure human muscle activities and microphones that record sound, to capture the details in the human manipulation process, and enable robots to extract task plans and control parameters to perform the same task. To achieve this, we introduce Chain-of-Modality (CoM), a prompting strategy that enables Vision Language Models to reason about multimodal human demonstration data -- videos coupled with muscle or audio signals. By progressively integrating information from each modality, CoM refines a task plan and generates detailed control parameters, enabling robots to perform manipulation tasks based on a single multimodal human video prompt. Our experiments show that CoM delivers a threefold improvement in accuracy for extracting task plans and control parameters compared to baselines, with strong generalization to new task setups and objects in real-world robot experiments. Videos and code are available at https://chain-of-modality.github.io</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.3 -->
                    
                <!-- Medicine: 4.4 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Robotics: 2.3 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13354" target="_blank" rel="noopener noreferrer">A Formalization of Co-Transcriptional Splicing as an Operation on Formal Languages</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Da-Jung Cho, Szil\'ard Zsolt Fazekas, Shinnosuke Seki, Max Wiedenh\"oft
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">RNA co-transcriptionality is the process where RNA sequences are spliced while being transcribed from DNA templates. This process holds potential as a key tool for molecular programming. Co-transcriptional folding has been shown to be programmable for assembling nano-scale RNA structures, and recent</span>
                
                <span class="abstract-full" style="display: none;">RNA co-transcriptionality is the process where RNA sequences are spliced while being transcribed from DNA templates. This process holds potential as a key tool for molecular programming. Co-transcriptional folding has been shown to be programmable for assembling nano-scale RNA structures, and recent advances have proven its Turing universality. While post-transcriptional splicing has been extensively studied, co-transcriptional splicing is gaining attention for its potential to save resources and space in molecular systems. However, its unpredictability has limited its practical applications. In this paper, we focus on engineering co-transcriptional splicing, moving beyond natural occurrences to program RNA sequences that produce specific target sequences through DNA templates. We introduce a formal model of co-transcriptional splicing, defined by constant-, linear-, and logarithmic-bounded hairpin deletion operations, as well as an unbounded hairpin deletion operation. We examine the complexity of the template constructability problem associated with these operations and study the closure properties of the languages they generate, providing insights for RNA template design in molecular programming systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.3 -->
                    
                <!-- Medicine: 5.8 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13355" target="_blank" rel="noopener noreferrer">Denoising and Reconstruction of Nonlinear Dynamics using Truncated Reservoir Computing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Omid Sedehi, Manish Yadav, Merten Stender, Sebastian Oberst
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Measurements acquired from distributed physical systems are often sparse and noisy. Therefore, signal processing and system identification tools are required to mitigate noise effects and reconstruct unobserved dynamics from limited sensor data. However, this process is particularly challenging beca</span>
                
                <span class="abstract-full" style="display: none;">Measurements acquired from distributed physical systems are often sparse and noisy. Therefore, signal processing and system identification tools are required to mitigate noise effects and reconstruct unobserved dynamics from limited sensor data. However, this process is particularly challenging because the fundamental equations governing the dynamics are largely unavailable in practice. Reservoir Computing (RC) techniques have shown promise in efficiently simulating dynamical systems through an unstructured and efficient computation graph comprising a set of neurons with random connectivity. However, the potential of RC to operate in noisy regimes and distinguish noise from the primary dynamics of the system has not been fully explored. This paper presents a novel RC method for noise filtering and reconstructing nonlinear dynamics, offering a novel learning protocol associated with hyperparameter optimization. The performance of the RC in terms of noise intensity, noise frequency content, and drastic shifts in dynamical parameters are studied in two illustrative examples involving the nonlinear dynamics of the Lorenz attractor and adaptive exponential integrate-and-fire system (AdEx). It is shown that the denoising performance improves via truncating redundant nodes and edges of the computing reservoir, as well as properly optimizing the hyperparameters, e.g., the leakage rate, the spectral radius, the input connectivity, and the ridge regression parameter. Furthermore, the presented framework shows good generalization behavior when tested for reconstructing unseen attractors from the bifurcation diagram. Compared to the Extended Kalman Filter (EKF), the presented RC framework yields competitive accuracy at low signal-to-noise ratios (SNRs) and high-frequency ranges.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.5 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Robotics: 2.6 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Pathfinding: 1.6 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13358" target="_blank" rel="noopener noreferrer">GraphQLer: Enhancing GraphQL Security with Context-Aware API Testing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Omar Tsai, Jianing Li, Tsz Tung Cheung, Lejing Huang, Hao Zhu, Jianrui Xiao, Iman Sharafaldin, Mohammad A. Tayebi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">GraphQL is an open-source data query and manipulation language for web applications, offering a flexible alternative to RESTful APIs. However, its dynamic execution model and lack of built-in security mechanisms expose it to vulnerabilities such as unauthorized data access, denial-of-service (DoS) a</span>
                
                <span class="abstract-full" style="display: none;">GraphQL is an open-source data query and manipulation language for web applications, offering a flexible alternative to RESTful APIs. However, its dynamic execution model and lack of built-in security mechanisms expose it to vulnerabilities such as unauthorized data access, denial-of-service (DoS) attacks, and injections. Existing testing tools focus on functional correctness, often overlooking security risks stemming from query interdependencies and execution context. This paper presents GraphQLer, the first context-aware security testing framework for GraphQL APIs. GraphQLer constructs a dependency graph to analyze relationships among mutations, queries, and objects, capturing critical interdependencies. It chains related queries and mutations to reveal authentication and authorization flaws, access control bypasses, and resource misuse. Additionally, GraphQLer tracks internal resource usage to uncover data leakage, privilege escalation, and replay attack vectors. We assess GraphQLer on various GraphQL APIs, demonstrating improved testing coverage - averaging a 35% increase, with up to 84% in some cases - compared to top-performing baselines. Remarkably, this is achieved in less time, making GraphQLer suitable for time-sensitive contexts. GraphQLer also successfully detects a known CVE and potential vulnerabilities in large-scale production APIs. These results underline GraphQLer's utility in proactively securing GraphQL APIs through automated, context-aware vulnerability detection.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.9 -->
                    
                <!-- LLMs: 8.3 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- RAG: 2.5 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13361" target="_blank" rel="noopener noreferrer">Automated Taxi Booking Operations for Autonomous Vehicles</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Linh Van Ma, Shoaib Azam, Farzeen Munir, Moongu Jeon
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In a conventional taxi booking system, all taxi operations are mostly done by a decision made by drivers which is hard to implement in unmanned vehicles. To address this challenge, we introduce a taxi booking system which assists autonomous vehicles to pick up customers. The system can allocate an a</span>
                
                <span class="abstract-full" style="display: none;">In a conventional taxi booking system, all taxi operations are mostly done by a decision made by drivers which is hard to implement in unmanned vehicles. To address this challenge, we introduce a taxi booking system which assists autonomous vehicles to pick up customers. The system can allocate an autonomous vehicle (AV) as well as plan service trips for a customer request. We use our own AV to serve a customer who uses a mobile application to make his taxi request. Apart from customer and AV, we build a server to monitor customers and AVs. It also supports inter-communication between a customer and an AV once AV decided to pick up a customer.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.8 -->
                    
                <!-- LLMs: 4.7 -->
                    
                <!-- Networks: 3.9 -->
                    
                <!-- 3D: 3.0 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Attention: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13365" target="_blank" rel="noopener noreferrer">VLLFL: A Vision-Language Model Based Lightweight Federated Learning Framework for Smart Agriculture</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Long Li, Jiajia Li, Dong Chen, Lina Pu, Haibo Yao, Yanbo Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In modern smart agriculture, object detection plays a crucial role by enabling automation, precision farming, and monitoring of resources. From identifying crop health and pest infestations to optimizing harvesting processes, accurate object detection enhances both productivity and sustainability. H</span>
                
                <span class="abstract-full" style="display: none;">In modern smart agriculture, object detection plays a crucial role by enabling automation, precision farming, and monitoring of resources. From identifying crop health and pest infestations to optimizing harvesting processes, accurate object detection enhances both productivity and sustainability. However, training object detection models often requires large-scale data collection and raises privacy concerns, particularly when sensitive agricultural data is distributed across farms. To address these challenges, we propose VLLFL, a vision-language model-based lightweight federated learning framework (VLLFL). It harnesses the generalization and context-aware detection capabilities of the vision-language model (VLM) and leverages the privacy-preserving nature of federated learning. By training a compact prompt generator to boost the performance of the VLM deployed across different farms, VLLFL preserves privacy while reducing communication overhead. Experimental results demonstrate that VLLFL achieves 14.53% improvement in the performance of VLM while reducing 99.3% communication overhead. Spanning tasks from identifying a wide variety of fruits to detecting harmful animals in agriculture, the proposed framework offers an efficient, scalable, and privacy-preserving solution specifically tailored to agricultural applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.4 -->
                    
                <!-- Medicine: 7.2 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13368" target="_blank" rel="noopener noreferrer">An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Haoran Xu, Shuozhe Li, Harshit Sikchi, Scott Niekum, Amy Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce Iterative Dual Reinforcement Learning (IDRL), a new method that takes an optimal discriminator-weighted imitation view of solving RL. Our method is motivated by a simple experiment in which we find training a discriminator using the offline dataset plus an additional expert dataset and </span>
                
                <span class="abstract-full" style="display: none;">We introduce Iterative Dual Reinforcement Learning (IDRL), a new method that takes an optimal discriminator-weighted imitation view of solving RL. Our method is motivated by a simple experiment in which we find training a discriminator using the offline dataset plus an additional expert dataset and then performing discriminator-weighted behavior cloning gives strong results on various types of datasets. That optimal discriminator weight is quite similar to the learned visitation distribution ratio in Dual-RL, however, we find that current Dual-RL methods do not correctly estimate that ratio. In IDRL, we propose a correction method to iteratively approach the optimal visitation distribution ratio in the offline dataset given no addtional expert dataset. During each iteration, IDRL removes zero-weight suboptimal transitions using the learned ratio from the previous iteration and runs Dual-RL on the remaining subdataset. This can be seen as replacing the behavior visitation distribution with the optimized visitation distribution from the previous iteration, which theoretically gives a curriculum of improved visitation distribution ratios that are closer to the optimal discriminator weight. We verify the effectiveness of IDRL on various kinds of offline datasets, including D4RL datasets and more realistic corrupted demonstrations. IDRL beats strong Primal-RL and Dual-RL baselines in terms of both performance and stability, on all datasets.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.6 -->
                    
                <!-- Medicine: 4.5 -->
                    
                <!-- Reinforcement Learning: 4.4 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13371" target="_blank" rel="noopener noreferrer">The Impact of AI on the Cyber Offense-Defense Balance and the Character of Cyber Conflict</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andrew J. Lohn
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Unlike other domains of conflict, and unlike other fields with high anticipated risk from AI, the cyber domain is intrinsically digital with a tight feedback loop between AI training and cyber application. Cyber may have some of the largest and earliest impacts from AI, so it is important to underst</span>
                
                <span class="abstract-full" style="display: none;">Unlike other domains of conflict, and unlike other fields with high anticipated risk from AI, the cyber domain is intrinsically digital with a tight feedback loop between AI training and cyber application. Cyber may have some of the largest and earliest impacts from AI, so it is important to understand how the cyber domain may change as AI continues to advance. Our approach reviewed the literature, collecting nine arguments that have been proposed for offensive advantage in cyber conflict and nine proposed arguments for defensive advantage. We include an additional forty-eight arguments that have been proposed to give cyber conflict and competition its character as collected separately by Healey, Jervis, and Nandrajog. We then consider how each of those arguments and propositions might change with varying degrees of AI advancement. We find that the cyber domain is too multifaceted for a single answer to whether AI will enhance offense or defense broadly. AI will improve some aspects, hinder others, and leave some aspects unchanged. We collect and present forty-four ways that we expect AI to impact the cyber offense-defense balance and the character of cyber conflict and competition.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.6 -->
                    
                <!-- LLMs: 7.8 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13372" target="_blank" rel="noopener noreferrer">Integration of a Graph-Based Path Planner and Mixed-Integer MPC for Robot Navigation in Cluttered Environments</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Joshua A. Robbins, Stephen J. Harnett, Andrew F. Thompson, Sean Brennan, Herschel C. Pangborn
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The ability to update a path plan is a required capability for autonomous mobile robots navigating through uncertain environments. This paper proposes a re-planning strategy using a multilayer planning and control framework for cases where the robot's environment is partially known. A medial axis gr</span>
                
                <span class="abstract-full" style="display: none;">The ability to update a path plan is a required capability for autonomous mobile robots navigating through uncertain environments. This paper proposes a re-planning strategy using a multilayer planning and control framework for cases where the robot's environment is partially known. A medial axis graph-based planner defines a global path plan based on known obstacles where each edge in the graph corresponds to a unique corridor. A mixed-integer model predictive control (MPC) method detects if a terminal constraint derived from the global plan is infeasible, subject to a non-convex description of the local environment. Infeasibility detection is used to trigger efficient global re-planning via medial axis graph edge deletion. The proposed re-planning strategy is demonstrated experimentally.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 5.0 -->
                    
                <!-- Medicine: 4.9 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Robotics: 2.3 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Pathfinding: 1.6 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13374" target="_blank" rel="noopener noreferrer">The generalized scalar auxiliary variable applied to the incompressible Boussinesq Equation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andreas Wagner, Barbara Wohlmuth, Jan Zawallich
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces a second-order time discretization for solving the incompressible Boussinesq equation. It uses the generalized scalar auxiliary variable (GSAV) and a backward differentiation formula (BDF), based on a Taylor expansion around $t^{n+k}$ for $k\geq3$. An exponential time integrato</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces a second-order time discretization for solving the incompressible Boussinesq equation. It uses the generalized scalar auxiliary variable (GSAV) and a backward differentiation formula (BDF), based on a Taylor expansion around $t^{n+k}$ for $k\geq3$. An exponential time integrator is used for the auxiliary variable to ensure stability independent of the time step size. We give rigorous asymptotic error estimates of the time-stepping scheme, thereby justifying its accuracy and stability. The scheme is reformulated into one amenable to a $H^1$-conforming finite element discretization. Finally, we validate our theoretical results with numerical experiments using a Taylor--Hood-based finite element discretization and show its applicability to large-scale 3-dimensional problems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.8 -->
                    
                <!-- Networks: 3.7 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13378" target="_blank" rel="noopener noreferrer">SMPL-GPTexture: Dual-View 3D Human Texture Estimation using Text-to-Image Generation Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mingxiao Tu, Shuchang Ye, Hoijoon Jung, Jinman Kim
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generating high-quality, photorealistic textures for 3D human avatars remains a fundamental yet challenging task in computer vision and multimedia field. However, real paired front and back images of human subjects are rarely available with privacy, ethical and cost of acquisition, which restricts s</span>
                
                <span class="abstract-full" style="display: none;">Generating high-quality, photorealistic textures for 3D human avatars remains a fundamental yet challenging task in computer vision and multimedia field. However, real paired front and back images of human subjects are rarely available with privacy, ethical and cost of acquisition, which restricts scalability of the data. Additionally, learning priors from image inputs using deep generative models, such as GANs or diffusion models, to infer unseen regions such as the human back often leads to artifacts, structural inconsistencies, or loss of fine-grained detail. To address these issues, we present SMPL-GPTexture (skinned multi-person linear model - general purpose Texture), a novel pipeline that takes natural language prompts as input and leverages a state-of-the-art text-to-image generation model to produce paired high-resolution front and back images of a human subject as the starting point for texture estimation. Using the generated paired dual-view images, we first employ a human mesh recovery model to obtain a robust 2D-to-3D SMPL alignment between image pixels and the 3D model's UV coordinates for each views. Second, we use an inverted rasterization technique that explicitly projects the observed colour from the input images into the UV space, thereby producing accurate, complete texture maps. Finally, we apply a diffusion-based inpainting module to fill in the missing regions, and the fusion mechanism then combines these results into a unified full texture map. Extensive experiments shows that our SMPL-GPTexture can generate high resolution texture aligned with user's prompts.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.9 -->
                    
                <!-- LLMs: 5.0 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13379" target="_blank" rel="noopener noreferrer">Radial Basis Function Techniques for Neural Field Models on Surfaces</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sage B Shaw, Zachary P Kilpatrick, Daniele Avitabile
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a numerical framework for solving neural field equations on surfaces using Radial Basis Function (RBF) interpolation and quadrature. Neural field models describe the evolution of macroscopic brain activity, but modeling studies often overlook the complex geometry of curved cortical domain</span>
                
                <span class="abstract-full" style="display: none;">We present a numerical framework for solving neural field equations on surfaces using Radial Basis Function (RBF) interpolation and quadrature. Neural field models describe the evolution of macroscopic brain activity, but modeling studies often overlook the complex geometry of curved cortical domains. Traditional numerical methods, such as finite element or spectral methods, can be computationally expensive and challenging to implement on irregular domains. In contrast, RBF-based methods provide a flexible alternative by offering interpolation and quadrature schemes that efficiently handle arbitrary geometries with high-order accuracy. We first develop an RBF-based interpolatory projection framework for neural field models on general surfaces. Quadrature for both flat and curved domains are derived in detail, ensuring high-order accuracy and stability as they depend on RBF hyperparameters (basis functions, augmenting polynomials, and stencil size). Through numerical experiments, we demonstrate the convergence of our method, highlighting its advantages over traditional approaches in terms of flexibility and accuracy. We conclude with an exposition of numerical simulations of spatiotemporal activity on complex surfaces, illustrating the method's ability to capture complex wave propagation patterns.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.8 -->
                    
                <!-- LLMs: 7.1 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13381" target="_blank" rel="noopener noreferrer">Improved Decoding Algorithm of BD-LRPC Codes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hermann Tchatchiem Kamche
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A Bounded-Degree Low-Rank Parity-Check (BD-LRPC) code is a rank-metric code that admits a parity-check matrix whose support is generated by a set of powers of an element. This specific structure of the parity-check matrix was employed to enhance the first phase of the decoding algorithm through the </span>
                
                <span class="abstract-full" style="display: none;">A Bounded-Degree Low-Rank Parity-Check (BD-LRPC) code is a rank-metric code that admits a parity-check matrix whose support is generated by a set of powers of an element. This specific structure of the parity-check matrix was employed to enhance the first phase of the decoding algorithm through the expansion of the syndrome support. However, this expansion decreases the probability of recovering the error support in the second phase of the decoding algorithm. This paper introduces a novel method based on successive intersections to recover the error support. This method offers two key advantages: it increases the probability of successful decoding and enables the decoding of a greater number of errors.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.9 -->
                    
                <!-- Reinforcement Learning: 4.6 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- Math: 3.2 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Pathfinding: 1.7 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13385" target="_blank" rel="noopener noreferrer">EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for Enhanced Cache Occupancy Attacks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tianhong Xu, Aidong Adam Ding, Yunsi Fei
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Cache occupancy attacks exploit the shared nature of cache hierarchies to infer a victim's activities by monitoring overall cache usage, unlike access-driven cache attacks that focus on specific cache lines or sets. There exists some prior work that target the last-level cache (LLC) of Intel process</span>
                
                <span class="abstract-full" style="display: none;">Cache occupancy attacks exploit the shared nature of cache hierarchies to infer a victim's activities by monitoring overall cache usage, unlike access-driven cache attacks that focus on specific cache lines or sets. There exists some prior work that target the last-level cache (LLC) of Intel processors, which is inclusive of higher-level caches, and L2 caches of ARM systems. In this paper, we target the System-Level Cache (SLC) of Apple M-series SoCs, which is exclusive to higher-level CPU caches. We address the challenges of the exclusiveness and propose a suite of SLC-cache occupancy attacks, the first of its kind, where an adversary can monitor GPU and other CPU cluster activities from their own CPU cluster. We first discover the structure of SLC in Apple M1 SOC and various policies pertaining to access and sharing through reverse engineering. We propose two attacks against websites. One is a coarse-grained fingerprinting attack, recognizing which website is accessed based on their different GPU memory access patterns monitored through the SLC occupancy channel. The other attack is a fine-grained pixel stealing attack, which precisely monitors the GPU memory usage for rendering different pixels, through the SLC occupancy channel. Third, we introduce a novel screen capturing attack which works beyond webpages, with the monitoring granularity of 57 rows of pixels (there are 1600 rows for the screen). This significantly expands the attack surface, allowing the adversary to retrieve any screen display, posing a substantial new threat to system security. Our findings reveal critical vulnerabilities in Apple's M-series SoCs and emphasize the urgent need for effective countermeasures against cache occupancy attacks in heterogeneous computing environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.5 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13386" target="_blank" rel="noopener noreferrer">Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Radek Dan\v{e}\v{c}ek, Carolin Schmitt, Senya Polikovsky, Michael J. Black
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In order to be widely applicable, speech-driven 3D head avatars must articulate their lips in accordance with speech, while also conveying the appropriate emotions with dynamically changing facial expressions. The key problem is that deterministic models produce high-quality lip-sync but without ric</span>
                
                <span class="abstract-full" style="display: none;">In order to be widely applicable, speech-driven 3D head avatars must articulate their lips in accordance with speech, while also conveying the appropriate emotions with dynamically changing facial expressions. The key problem is that deterministic models produce high-quality lip-sync but without rich expressions, whereas stochastic models generate diverse expressions but with lower lip-sync quality. To get the best of both, we seek a stochastic model with accurate lip-sync. To that end, we develop a new approach based on the following observation: if a method generates realistic 3D lip motions, it should be possible to infer the spoken audio from the lip motion. The inferred speech should match the original input audio, and erroneous predictions create a novel supervision signal for training 3D talking head avatars with accurate lip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under Neural Differentiable Elocution Reconstruction), a 3D talking head avatar framework that introduces a novel supervision mechanism via differentiable sound production. First, we train a novel mesh-to-speech model that regresses audio from facial animation. Then, we incorporate this model into a diffusion-based talking avatar framework. During training, the mesh-to-speech model takes the generated animation and produces a sound that is compared to the input speech, creating a differentiable analysis-by-audio-synthesis supervision loop. Our extensive qualitative and quantitative experiments demonstrate that THUNDER significantly improves the quality of the lip-sync of talking head avatars while still allowing for generation of diverse, high-quality, expressive facial animations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.2 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- 3D: 3.0 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13387" target="_blank" rel="noopener noreferrer">Bibliometric Analysis of Scientific Publications on Blockchain Research and Applications</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lingfeng Bao, Jiameng Yang, Xiaohu Yang, Chunming Rong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Since the introduction of Bitcoin in 2008, blockchain technology has garnered widespread attention. Scholars from various research fields, countries, and institutions have published a significant number of papers on this subject. However, there is currently a lack of comprehensive analysis specifica</span>
                
                <span class="abstract-full" style="display: none;">Since the introduction of Bitcoin in 2008, blockchain technology has garnered widespread attention. Scholars from various research fields, countries, and institutions have published a significant number of papers on this subject. However, there is currently a lack of comprehensive analysis specifically focusing on the scientific publications in the field of blockchain.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.4 -->
                    
                <!-- Medicine: 7.8 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13396" target="_blank" rel="noopener noreferrer">A global structure-preserving kernel method for the learning of Poisson systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jianyu Hu, Juan-Pablo Ortega, Daiying Yin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A structure-preserving kernel ridge regression method is presented that allows the recovery of globally defined, potentially high-dimensional, and nonlinear Hamiltonian functions on Poisson manifolds out of datasets made of noisy observations of Hamiltonian vector fields. The proposed method is base</span>
                
                <span class="abstract-full" style="display: none;">A structure-preserving kernel ridge regression method is presented that allows the recovery of globally defined, potentially high-dimensional, and nonlinear Hamiltonian functions on Poisson manifolds out of datasets made of noisy observations of Hamiltonian vector fields. The proposed method is based on finding the solution of a non-standard kernel ridge regression where the observed data is generated as the noisy image by a vector bundle map of the differential of the function that one is trying to estimate. Additionally, it is shown how a suitable regularization solves the intrinsic non-identifiability of the learning problem due to the degeneracy of the Poisson tensor and the presence of Casimir functions. A full error analysis is conducted that provides convergence rates using fixed and adaptive regularization parameters. The good performance of the proposed estimator is illustrated with several numerical experiments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.0 -->
                    
                <!-- Reinforcement Learning: 4.4 -->
                    
                <!-- Math: 4.3 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13398" target="_blank" rel="noopener noreferrer">Insecurity Through Obscurity: Veiled Vulnerabilities in Closed-Source Contracts</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sen Yang, Kaihua Qin, Aviv Yaish, Fan Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Most blockchains cannot hide the binary code of programs (i.e., smart contracts) running on them. To conceal proprietary business logic and to potentially deter attacks, many smart contracts are closed-source and employ layers of obfuscation. However, we demonstrate that such obfuscation can obscure</span>
                
                <span class="abstract-full" style="display: none;">Most blockchains cannot hide the binary code of programs (i.e., smart contracts) running on them. To conceal proprietary business logic and to potentially deter attacks, many smart contracts are closed-source and employ layers of obfuscation. However, we demonstrate that such obfuscation can obscure critical vulnerabilities rather than enhance security, a phenomenon we term insecurity through obscurity. To systematically analyze these risks on a large scale, we present SKANF, a novel EVM bytecode analysis tool tailored for closed-source and obfuscated contracts. SKANF combines control-flow deobfuscation, symbolic execution, and concolic execution based on historical transactions to identify and exploit asset management vulnerabilities. Our evaluation on real-world Maximal Extractable Value (MEV) bots reveals that SKANF detects vulnerabilities in 1,028 contracts and successfully generates exploits for 373 of them, with potential losses exceeding \$9.0M. Additionally, we uncover 40 real-world MEV bot attacks that collectively resulted in \$900K in losses.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.6 -->
                    
                <!-- Medicine: 6.4 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Blockchain: 2.6 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13399" target="_blank" rel="noopener noreferrer">Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous Object Detection for Autonomous Driving Safety</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shashank Shriram, Srinivasa Perisetla, Aryan Keskar, Harsha Krishnaswamy, Tonko Emil Westerhof Bossen, Andreas M{\o}gelmose, Ross Greer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Detecting anomalous hazards in visual data, particularly in video streams, is a critical challenge in autonomous driving. Existing models often struggle with unpredictable, out-of-label hazards due to their reliance on predefined object categories. In this paper, we propose a multimodal approach tha</span>
                
                <span class="abstract-full" style="display: none;">Detecting anomalous hazards in visual data, particularly in video streams, is a critical challenge in autonomous driving. Existing models often struggle with unpredictable, out-of-label hazards due to their reliance on predefined object categories. In this paper, we propose a multimodal approach that integrates vision-language reasoning with zero-shot object detection to improve hazard identification and explanation. Our pipeline consists of a Vision-Language Model (VLM), a Large Language Model (LLM), in order to detect hazardous objects within a traffic scene. We refine object detection by incorporating OpenAI's CLIP model to match predicted hazards with bounding box annotations, improving localization accuracy. To assess model performance, we create a ground truth dataset by denoising and extending the foundational COOOL (Challenge-of-Out-of-Label) anomaly detection benchmark dataset with complete natural language descriptions for hazard annotations. We define a means of hazard detection and labeling evaluation on the extended dataset using cosine similarity. This evaluation considers the semantic similarity between the predicted hazard description and the annotated ground truth for each video. Additionally, we release a set of tools for structuring and managing large-scale hazard detection datasets. Our findings highlight the strengths and limitations of current vision-language-based approaches, offering insights into future improvements in autonomous hazard detection systems. Our models, scripts, and data can be found at https://github.com/mi3labucm/COOOLER.git</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.2 -->
                    
                <!-- LLMs: 6.6 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13403" target="_blank" rel="noopener noreferrer">Documentation on Encrypted Dynamic Control Simulation Code using Ring-LWE based Cryptosystems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yeongjun Jang, Joowon Lee, Junsoo Kim
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Encrypted controllers offer secure computation by employing modern cryptosystems to execute control operations directly over encrypted data without decryption. However, incorporating cryptosystems into dynamic controllers significantly increases the computational load. This paper aims to provide an </span>
                
                <span class="abstract-full" style="display: none;">Encrypted controllers offer secure computation by employing modern cryptosystems to execute control operations directly over encrypted data without decryption. However, incorporating cryptosystems into dynamic controllers significantly increases the computational load. This paper aims to provide an accessible guideline for running encrypted controllers using an open-source library Lattigo, which supports an efficient implementation of Ring-Learing With Errors (LWE) based encrypted controllers, and our explanations are assisted with example codes that are fully available at https://github.com/CDSL-EncryptedControl/CDSL.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.7 -->
                    
                <!-- LLMs: 8.4 -->
                    
                <!-- Quantum Computing: 4.5 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13405" target="_blank" rel="noopener noreferrer">ProgRoCC: A Progressive Approach to Rough Crowd Counting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shengqin Jiang, Linfei Li, Haokui Zhang, Qingshan Liu, Amin Beheshti, Jian Yang, Anton van den Hengel, Quan Z. Sheng, Yuankai Qi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As the number of individuals in a crowd grows, enumeration-based techniques become increasingly infeasible and their estimates increasingly unreliable. We propose instead an estimation-based version of the problem: we label Rough Crowd Counting that delivers better accuracy on the basis of training </span>
                
                <span class="abstract-full" style="display: none;">As the number of individuals in a crowd grows, enumeration-based techniques become increasingly infeasible and their estimates increasingly unreliable. We propose instead an estimation-based version of the problem: we label Rough Crowd Counting that delivers better accuracy on the basis of training data that is easier to acquire. Rough crowd counting requires only rough annotations of the number of targets in an image, instead of the more traditional, and far more expensive, per-target annotations. We propose an approach to the rough crowd counting problem based on CLIP, termed ProgRoCC. Specifically, we introduce a progressive estimation learning strategy that determines the object count through a coarse-to-fine approach. This approach delivers answers quickly, outperforms the state-of-the-art in semi- and weakly-supervised crowd counting. In addition, we design a vision-language matching adapter that optimizes key-value pairs by mining effective matches of two modalities to refine the visual features, thereby improving the final performance. Extensive experimental results on three widely adopted crowd counting datasets demonstrate the effectiveness of our method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.8 -->
                    
                <!-- Reinforcement Learning: 4.3 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13406" target="_blank" rel="noopener noreferrer">LangCoop: Collaborative Driving with Language</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiangbo Gao, Yuheng Wu, Rujia Wang, Chenxi Liu, Yang Zhou, Zhengzhong Tu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communicatio</span>
                
                <span class="abstract-full" style="display: none;">Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communication media, including high bandwidth demands, agent heterogeneity, and information loss. To address these challenges, we introduce LangCoop, a new paradigm for collaborative autonomous driving that leverages natural language as a compact yet expressive medium for inter-agent communication. LangCoop features two key innovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured zero-shot vision-language reasoning and Natural Language Information Packaging (LangPack) for efficiently packaging information into concise, language-based messages. Through extensive experiments conducted in the CARLA simulations, we demonstrate that LangCoop achieves a remarkable 96\% reduction in communication bandwidth (< 2KB per message) compared to image-based communication, while maintaining competitive driving performance in the closed-loop evaluation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.5 -->
                    
                <!-- Medicine: 6.7 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13407" target="_blank" rel="noopener noreferrer">LoRA-Based Continual Learning with Constraints on Critical Parameter Changes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shimou Ling, Liang Zhang, Jiangwei Zhao, Lili Pan, Hongliang Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">LoRA-based continual learning represents a promising avenue for leveraging pre-trained models in downstream continual learning tasks. Recent studies have shown that orthogonal LoRA tuning effectively mitigates forgetting. However, this work unveils that under orthogonal LoRA tuning, the critical par</span>
                
                <span class="abstract-full" style="display: none;">LoRA-based continual learning represents a promising avenue for leveraging pre-trained models in downstream continual learning tasks. Recent studies have shown that orthogonal LoRA tuning effectively mitigates forgetting. However, this work unveils that under orthogonal LoRA tuning, the critical parameters for pre-tasks still change notably after learning post-tasks. To address this problem, we directly propose freezing the most critical parameter matrices in the Vision Transformer (ViT) for pre-tasks before learning post-tasks. In addition, building on orthogonal LoRA tuning, we propose orthogonal LoRA composition (LoRAC) based on QR decomposition, which may further enhance the plasticity of our method. Elaborate ablation studies and extensive comparisons demonstrate the effectiveness of our proposed method. Our results indicate that our method achieves state-of-the-art (SOTA) performance on several well-known continual learning benchmarks. For instance, on the Split CIFAR-100 dataset, our method shows a 6.35\% improvement in accuracy and a 3.24\% reduction in forgetting compared to previous methods. Our code is available at https://github.com/learninginvision/LoRAC-IPC.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.1 -->
                    
                <!-- LLMs: 5.4 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13412" target="_blank" rel="noopener noreferrer">How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent Kernel Analysis of Multigrid Parametric Encodings</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Samuel Audia, Soheil Feizi, Matthias Zwicker, Dinesh Manocha
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Neural networks that map between low dimensional spaces are ubiquitous in computer graphics and scientific computing; however, in their naive implementation, they are unable to learn high frequency information. We present a comprehensive analysis comparing the two most common techniques for mitigati</span>
                
                <span class="abstract-full" style="display: none;">Neural networks that map between low dimensional spaces are ubiquitous in computer graphics and scientific computing; however, in their naive implementation, they are unable to learn high frequency information. We present a comprehensive analysis comparing the two most common techniques for mitigating this spectral bias: Fourier feature encodings (FFE) and multigrid parametric encodings (MPE). FFEs are seen as the standard for low dimensional mappings, but MPEs often outperform them and learn representations with higher resolution and finer detail. FFE's roots in the Fourier transform, make it susceptible to aliasing if pushed too far, while MPEs, which use a learned grid structure, have no such limitation. To understand the difference in performance, we use the neural tangent kernel (NTK) to evaluate these encodings through the lens of an analogous kernel regression. By finding a lower bound on the smallest eigenvalue of the NTK, we prove that MPEs improve a network's performance through the structure of their grid and not their learnable embedding. This mechanism is fundamentally different from FFEs, which rely solely on their embedding space to improve performance. Results are empirically validated on a 2D image regression task using images taken from 100 synonym sets of ImageNet and 3D implicit surface regression on objects from the Stanford graphics dataset. Using peak signal-to-noise ratio (PSNR) and multiscale structural similarity (MS-SSIM) to evaluate how well fine details are learned, we show that the MPE increases the minimum eigenvalue by 8 orders of magnitude over the baseline and 2 orders of magnitude over the FFE. The increase in spectrum corresponds to a 15 dB (PSNR) / 0.65 (MS-SSIM) increase over baseline and a 12 dB (PSNR) / 0.33 (MS-SSIM) increase over the FFE.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.3 -->
                    
                <!-- Medicine: 4.0 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13413" target="_blank" rel="noopener noreferrer">A Model-Based Approach to Imitation Learning through Multi-Step Predictions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Haldun Balim, Yang Hu, Yuyang Zhang, Na Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Imitation learning is a widely used approach for training agents to replicate expert behavior in complex decision-making tasks. However, existing methods often struggle with compounding errors and limited generalization, due to the inherent challenge of error correction and the distribution shift be</span>
                
                <span class="abstract-full" style="display: none;">Imitation learning is a widely used approach for training agents to replicate expert behavior in complex decision-making tasks. However, existing methods often struggle with compounding errors and limited generalization, due to the inherent challenge of error correction and the distribution shift between training and deployment. In this paper, we present a novel model-based imitation learning framework inspired by model predictive control, which addresses these limitations by integrating predictive modeling through multi-step state predictions. Our method outperforms traditional behavior cloning numerical benchmarks, demonstrating superior robustness to distribution shift and measurement noise both in available data and during execution. Furthermore, we provide theoretical guarantees on the sample complexity and error bounds of our method, offering insights into its convergence properties.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.3 -->
                    
                <!-- Medicine: 7.4 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13420" target="_blank" rel="noopener noreferrer">Testing the Fault-Tolerance of Multi-Sensor Fusion Perception in Autonomous Driving Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Haoxiang Tian, Wenqiang Ding, Xingshuo Han, Guoquan Wu, An Guo, Junqi Zhang. Wei Chen, Jun Wei, Tianwei Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">High-level Autonomous Driving Systems (ADSs), such as Google Waymo and Baidu Apollo, typically rely on multi-sensor fusion (MSF) based approaches to perceive their surroundings. This strategy increases perception robustness by combining the respective strengths of the camera and LiDAR and directly a</span>
                
                <span class="abstract-full" style="display: none;">High-level Autonomous Driving Systems (ADSs), such as Google Waymo and Baidu Apollo, typically rely on multi-sensor fusion (MSF) based approaches to perceive their surroundings. This strategy increases perception robustness by combining the respective strengths of the camera and LiDAR and directly affects the safety-critical driving decisions of autonomous vehicles (AVs). However, in real-world autonomous driving scenarios, cameras and LiDAR are subject to various faults, which can probably significantly impact the decision-making and behaviors of ADSs. Existing MSF testing approaches only discovered corner cases that the MSF-based perception cannot accurately detected by MSF-based perception, while lacking research on how sensor faults affect the system-level behaviors of ADSs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.0 -->
                    
                <!-- Medicine: 5.4 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13421" target="_blank" rel="noopener noreferrer">"Can't believe I'm crying over an anime girl": Public Parasocial Grieving and Coping Towards VTuber Graduation and Termination</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ken Jen Lee, PiaoHong Wang, Zhicong Lu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Despite the significant increase in popularity of Virtual YouTubers (VTubers), research on the unique dynamics of viewer-VTuber parasocial relationships is nascent. This work investigates how English-speaking viewers grieved VTubers whose identities are no longer used, an interesting context as the </span>
                
                <span class="abstract-full" style="display: none;">Despite the significant increase in popularity of Virtual YouTubers (VTubers), research on the unique dynamics of viewer-VTuber parasocial relationships is nascent. This work investigates how English-speaking viewers grieved VTubers whose identities are no longer used, an interesting context as the nakanohito (i.e., the person behind the VTuber identity) is usually alive post-retirement and might "reincarnate" as another VTuber. We propose a typology for VTuber retirements and analyzed 13,655 Reddit posts and comments spanning nearly three years using mixed-methods. Findings include how viewers coped using methods similar to when losing loved ones, alongside novel coping methods reflecting different attachment styles. Although emotions like sadness, shock, concern, disapproval, confusion, and love decreased with time, regret and loyalty showed opposite trends. Furthermore, viewers' reactions situated a VTuber identity within a community of content creators and viewers. We also discuss design implications alongside implications on the VTuber ecosystem and future research directions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.7 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13422" target="_blank" rel="noopener noreferrer">Equilibrium Conserving Neural Operators for Super-Resolution Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vivek Oommen, Andreas E. Robertson, Daniel Diaz, Coleman Alleman, Zhen Zhang, Anthony D. Rollett, George E. Karniadakis, R\'emi Dingreville
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Neural surrogate solvers can estimate solutions to partial differential equations in physical problems more efficiently than standard numerical methods, but require extensive high-resolution training data. In this paper, we break this limitation; we introduce a framework for super-resolution learnin</span>
                
                <span class="abstract-full" style="display: none;">Neural surrogate solvers can estimate solutions to partial differential equations in physical problems more efficiently than standard numerical methods, but require extensive high-resolution training data. In this paper, we break this limitation; we introduce a framework for super-resolution learning in solid mechanics problems. Our approach allows one to train a high-resolution neural network using only low-resolution data. Our Equilibrium Conserving Operator (ECO) architecture embeds known physics directly into the network to make up for missing high-resolution information during training. We evaluate this ECO-based super-resolution framework that strongly enforces conservation-laws in the predicted solutions on two working examples: embedded pores in a homogenized matrix and randomly textured polycrystalline materials. ECO eliminates the reliance on high-fidelity data and reduces the upfront cost of data collection by two orders of magnitude, offering a robust pathway for resource-efficient surrogate modeling in materials modeling. ECO is readily generalizable to other physics-based problems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.9 -->
                    
                <!-- Medicine: 5.2 -->
                    
                <!-- Quantum Computing: 4.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13424" target="_blank" rel="noopener noreferrer">Decentralized Handover Parameter Optimization with MARL for Load Balancing in 5G Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yang Shen, Shuqi Chai, Bing Li, Xiaodong Luo, Qingjiang Shi, Rongqing Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In cellular networks, cell handover refers to the process where a device switches from one base station to another, and this mechanism is crucial for balancing the load among different cells. Traditionally, engineers would manually adjust parameters based on experience. However, the explosive growth</span>
                
                <span class="abstract-full" style="display: none;">In cellular networks, cell handover refers to the process where a device switches from one base station to another, and this mechanism is crucial for balancing the load among different cells. Traditionally, engineers would manually adjust parameters based on experience. However, the explosive growth in the number of cells has rendered manual tuning impractical. Existing research tends to overlook critical engineering details in order to simplify handover problems. In this paper, we classify cell handover into three types, and jointly model their mutual influence. To achieve load balancing, we propose a multi-agent-reinforcement-learning (MARL)-based scheme to automatically optimize the parameters. To reduce the agent interaction costs, a distributed training is implemented based on consensus approximation of global average load, and it is shown that the approximation error is bounded. Experimental results show that our proposed scheme outperforms existing benchmarks in balancing load and improving network performance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.7 -->
                    
                <!-- Medicine: 5.3 -->
                    
                <!-- Networks: 3.3 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13426" target="_blank" rel="noopener noreferrer">Simplifying Graph Convolutional Networks with Redundancy-Free Neighbors</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jielong LuZhihao Wu, Zhiling Cai, Yueyang Pi, Shiping Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In recent years, Graph Convolutional Networks (GCNs) have gained popularity for their exceptional ability to process graph-structured data. Existing GCN-based approaches typically employ a shallow model architecture due to the over-smoothing phenomenon. Current approaches to mitigating over-smoothin</span>
                
                <span class="abstract-full" style="display: none;">In recent years, Graph Convolutional Networks (GCNs) have gained popularity for their exceptional ability to process graph-structured data. Existing GCN-based approaches typically employ a shallow model architecture due to the over-smoothing phenomenon. Current approaches to mitigating over-smoothing primarily involve adding supplementary components to GCN architectures, such as residual connections and random edge-dropping strategies. However, these improvements toward deep GCNs have achieved only limited success. In this work, we analyze the intrinsic message passing mechanism of GCNs and identify a critical issue: messages originating from high-order neighbors must traverse through low-order neighbors to reach the target node. This repeated reliance on low-order neighbors leads to redundant information aggregation, a phenomenon we term over-aggregation. Our analysis demonstrates that over-aggregation not only introduces significant redundancy but also serves as the fundamental cause of over-smoothing in GCNs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.3 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- GNN: 2.9 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13429" target="_blank" rel="noopener noreferrer">Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shenzhi Yang, Bin Liang, An Liu, Lin Gui, Xingkai Yao, Xiaofang Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Given the critical role of graphs in real-world applications and their high-security requirements, improving the ability of graph neural networks (GNNs) to detect out-of-distribution (OOD) data is an urgent research problem. The recent work GNNSAFE proposes a framework based on the aggregation of ne</span>
                
                <span class="abstract-full" style="display: none;">Given the critical role of graphs in real-world applications and their high-security requirements, improving the ability of graph neural networks (GNNs) to detect out-of-distribution (OOD) data is an urgent research problem. The recent work GNNSAFE proposes a framework based on the aggregation of negative energy scores that significantly improves the performance of GNNs to detect node-level OOD data. However, our study finds that score aggregation among nodes is susceptible to extreme values due to the unboundedness of the negative energy scores and logit shifts, which severely limits the accuracy of GNNs in detecting node-level OOD data. In this paper, we propose NODESAFE: reducing the generation of extreme scores of nodes by adding two optimization terms that make the negative energy scores bounded and mitigate the logit shift. Experimental results show that our approach dramatically improves the ability of GNNs to detect OOD data at the node level, e.g., in detecting OOD data induced by Structure Manipulation, the metric of FPR95 (lower is better) in scenarios without (with) OOD data exposure are reduced from the current SOTA by 28.4% (22.7%).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.3 -->
                    
                <!-- Medicine: 4.2 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Reinforcement Learning: 3.3 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13430" target="_blank" rel="noopener noreferrer">The Long Arm of Nashian Allocation in Online $p$-Mean Welfare Maximization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhiyi Huang, Chui Shan Lee, Xinkai Shu, Zhaozi Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the online allocation of divisible items to $n$ agents with additive valuations for $p$-mean welfare maximization, a problem introduced by Barman, Khan, and Maiti~(2022). Our algorithmic and hardness results characterize the optimal competitive ratios for the entire spectrum of $-\infty \le</span>
                
                <span class="abstract-full" style="display: none;">We study the online allocation of divisible items to $n$ agents with additive valuations for $p$-mean welfare maximization, a problem introduced by Barman, Khan, and Maiti~(2022). Our algorithmic and hardness results characterize the optimal competitive ratios for the entire spectrum of $-\infty \le p \le 1$. Surprisingly, our improved algorithms for all $p \le \frac{1}{\log n}$ are simply the greedy algorithm for the Nash welfare, supplemented with two auxiliary components to ensure all agents have non-zero utilities and to help a small number of agents with low utilities. In this sense, the long arm of Nashian allocation achieves near-optimal competitive ratios not only for Nash welfare but also all the way to egalitarian welfare.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.3 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13432" target="_blank" rel="noopener noreferrer">Circular Image Deturbulence using Quasi-conformal Geometry</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chu Chen, Han Zhang, Lok Ming Lui
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The presence of inhomogeneous media between optical sensors and objects leads to distorted imaging outputs, significantly complicating downstream image-processing tasks. A key challenge in image restoration is the lack of high-quality, paired-label images required for training supervised models. In </span>
                
                <span class="abstract-full" style="display: none;">The presence of inhomogeneous media between optical sensors and objects leads to distorted imaging outputs, significantly complicating downstream image-processing tasks. A key challenge in image restoration is the lack of high-quality, paired-label images required for training supervised models. In this paper, we introduce the Circular Quasi-Conformal Deturbulence (CQCD) framework, an unsupervised approach for removing image distortions through a circular architecture. This design ensures that the restored image remains both geometrically accurate and visually faithful while preventing the accumulation of incorrect estimations.The circular restoration process involves both forward and inverse mapping. To ensure the bijectivity of the estimated non-rigid deformations, computational quasi-conformal geometry theories are leveraged to regularize the mapping, enforcing its homeomorphic properties. This guarantees a well-defined transformation that preserves structural integrity and prevents unwanted artifacts. Furthermore, tight-frame blocks are integrated to encode distortion-sensitive features for precise recovery. To validate the performance of our approach, we conduct evaluations on various synthetic and real-world captured images. Experimental results demonstrate that CQCD not only outperforms existing state-of-the-art deturbulence methods in terms of image restoration quality but also provides highly accurate deformation field estimations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.6 -->
                    
                <!-- Medicine: 6.8 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13436" target="_blank" rel="noopener noreferrer">RT-HDIST: Ray-Tracing Core-based Hausdorff Distance Computation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: YoungWoo Kim, Jaehong Lee, Duksu Kim
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The Hausdorff distance is a fundamental metric with widespread applications across various fields. However, its computation remains computationally expensive, especially for large-scale datasets. In this work, we present RT-HDIST, the first Hausdorff distance algorithm accelerated by ray-tracing cor</span>
                
                <span class="abstract-full" style="display: none;">The Hausdorff distance is a fundamental metric with widespread applications across various fields. However, its computation remains computationally expensive, especially for large-scale datasets. In this work, we present RT-HDIST, the first Hausdorff distance algorithm accelerated by ray-tracing cores (RT-cores). By reformulating the Hausdorff distance problem as a series of nearest-neighbor searches and introducing a novel quantized index space, RT-HDIST achieves significant reductions in computational overhead while maintaining exact results. Extensive benchmarks demonstrate up to a two-order-of-magnitude speedup over prior state-of-the-art methods, underscoring RT-HDIST's potential for real-time and large-scale applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.3 -->
                    
                <!-- LLMs: 6.7 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13445" target="_blank" rel="noopener noreferrer">How to Mine Potentially Popular Items? A Reverse MIPS-based Approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Daichi Amagata, Kazuyoshi Aoayama, Keito Kido, Sumio Fujita
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The $k$-MIPS ($k$ Maximum Inner Product Search) problem has been employed in many fields. Recently, its reverse version, the reverse $k$-MIPS problem, has been proposed. Given an item vector (i.e., query), it retrieves all user vectors such that their $k$-MIPS results contain the item vector. Consid</span>
                
                <span class="abstract-full" style="display: none;">The $k$-MIPS ($k$ Maximum Inner Product Search) problem has been employed in many fields. Recently, its reverse version, the reverse $k$-MIPS problem, has been proposed. Given an item vector (i.e., query), it retrieves all user vectors such that their $k$-MIPS results contain the item vector. Consider the cardinality of a reverse $k$-MIPS result. A large cardinality means that the item is potentially popular, because it is included in the $k$-MIPS results of many users. This mining is important in recommender systems, market analysis, and new item development. Motivated by this, we formulate a new problem. In this problem, the score of each item is defined as the cardinality of its reverse $k$-MIPS result, and the $N$ items with the highest score are retrieved. A straightforward approach is to compute the scores of all items, but this is clearly prohibitive for large numbers of users and items. We remove this inefficiency issue and propose a fast algorithm for this problem. Because the main bottleneck of the problem is to compute the score of each item, we devise a new upper-bounding technique that is specific to our problem and filters unnecessary score computations. We conduct extensive experiments on real datasets and show the superiority of our algorithm over competitors.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.7 -->
                    
                <!-- Medicine: 4.2 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Math: 2.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Pathfinding: 1.4 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13446" target="_blank" rel="noopener noreferrer">Approximate Reverse $k$-Ranks Queries in High Dimensions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Daichi Amagata, Kazuyoshi Aoyama, Keito Kido, Sumio Fujita
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Many objects are represented as high-dimensional vectors nowadays. In this setting, the relevance between two objects (vectors) is usually evaluated by their inner product. Recently, item-centric searches, which search for users relevant to query items, have received attention and find important app</span>
                
                <span class="abstract-full" style="display: none;">Many objects are represented as high-dimensional vectors nowadays. In this setting, the relevance between two objects (vectors) is usually evaluated by their inner product. Recently, item-centric searches, which search for users relevant to query items, have received attention and find important applications, such as product promotion and market analysis. To support these applications, this paper considers reverse $k$-ranks queries. Given a query vector $\mathbf{q}$, $k$, a set $\mathbf{U}$ of user vectors, and a set $\mathbf{P}$ of item vectors, this query retrieves the $k$ user vectors $\mathbf{u} \in \mathbf{U}$ with the highest $r(\mathbf{q},\mathbf{u},\mathbf{P})$, where $r(\mathbf{q},\mathbf{u},\mathbf{P})$ shows the rank of $\mathbf{q}$ for $\mathbf{u}$ among $\mathbf{P}$. Because efficiently computing the exact answer for this query is difficult in high dimensions, we address the problem of approximate reverse $k$-ranks queries. Informally, given an approximation factor $c$, this problem allows, as an output, a user $\mathbf{u}'$ such that $r(\mathbf{q},\mathbf{u}',\mathbf{P}) > \tau$ but $r(\mathbf{q},\mathbf{u}',\mathbf{P}) \leq c \times \tau$, where $\tau$ is the rank threshold for the exact answer. We propose a new algorithm for solving this problem efficiently. Through theoretical and empirical analyses, we confirm the efficiency and effectiveness of our algorithm.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.9 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13452" target="_blank" rel="noopener noreferrer">MicroFlow: Domain-Specific Optical Flow for Ground Deformation Estimation in Seismic Events</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Juliette Bertrand, Sophie Giffard-Roisin, James Hollingsworth, Julien Mairal
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Dense ground displacement measurements are crucial for geological studies but are impractical to collect directly. Traditionally, displacement fields are estimated using patch matching on optical satellite images from different acquisition times. While deep learning-based optical flow models are pro</span>
                
                <span class="abstract-full" style="display: none;">Dense ground displacement measurements are crucial for geological studies but are impractical to collect directly. Traditionally, displacement fields are estimated using patch matching on optical satellite images from different acquisition times. While deep learning-based optical flow models are promising, their adoption in ground deformation analysis is hindered by challenges such as the absence of real ground truth, the need for sub-pixel precision, and temporal variations due to geological or anthropogenic changes. In particular, we identify that deep learning models relying on explicit correlation layers struggle at estimating small displacements in real-world conditions. Instead, we propose a model that employs iterative refinements with explicit warping layers and a correlation-independent backbone, enabling sub-pixel precision. Additionally, a non-convex variant of Total Variation regularization preserves fault-line sharpness while maintaining smoothness elsewhere. Our model significantly outperforms widely used geophysics methods on semi-synthetic benchmarks and generalizes well to challenging real-world scenarios captured by both medium- and high-resolution sensors. Project page: https://jbertrand89.github.io/microflow/.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.4 -->
                    
                <!-- LLMs: 7.1 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13453" target="_blank" rel="noopener noreferrer">Using Machine Learning and Neural Networks to Analyze and Predict Chaos in Multi-Pendulum and Chaotic Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vasista Ramachandruni, Sai Hruday Reddy Nara, Geo Lalu, Sabrina Yang, Mohit Ramesh Kumar, Aarjav Jain, Pratham Mehta, Hankyu Koo, Jason Damonte, Marx Akl
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A chaotic system is a highly volatile system characterized by its sensitive dependence on initial conditions and outside factors. Chaotic systems are prevalent throughout the world today: in weather patterns, disease outbreaks, and even financial markets. Chaotic systems are seen in every field of s</span>
                
                <span class="abstract-full" style="display: none;">A chaotic system is a highly volatile system characterized by its sensitive dependence on initial conditions and outside factors. Chaotic systems are prevalent throughout the world today: in weather patterns, disease outbreaks, and even financial markets. Chaotic systems are seen in every field of science and humanities, so being able to predict these systems is greatly beneficial to society. In this study, we evaluate 10 different machine learning models and neural networks [1] based on Root Mean Squared Error (RMSE) and R^2 values for their ability to predict one of these systems, the multi-pendulum. We begin by generating synthetic data representing the angles of the pendulum over time using the Runge Kutta Method for solving 4th Order Differential Equations (ODE-RK4) [2]. At first, we used the single-step sliding window approach, predicting the 50st step after training for steps 0-49 and so forth. However, to more accurately cover chaotic motion and behavior in these systems, we transitioned to a time-step based approach. Here, we trained the model/network on many initial angles and tested it on a completely new set of initial angles, or 'in-between' to capture chaotic motion to its fullest extent. We also evaluated the stability of the system using Lyapunov exponents. We concluded that for a double pendulum, the best model was the Long Short Term Memory Network (LSTM)[3] for the sliding window and time step approaches in both friction and frictionless scenarios. For triple pendulum, the Vanilla Recurrent Neural Network (VRNN)[4] was the best for the sliding window and Gated Recurrent Network (GRU) [5] was the best for the time step approach, but for friction, LSTM was the best.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.4 -->
                    
                <!-- LLMs: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13457" target="_blank" rel="noopener noreferrer">Neural Ganglion Sensors: Learning Task-specific Event Cameras Inspired by the Neural Circuit of the Human Retina</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Haley M. So, Gordon Wetzstein
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Inspired by the data-efficient spiking mechanism of neurons in the human eye, event cameras were created to achieve high temporal resolution with minimal power and bandwidth requirements by emitting asynchronous, per-pixel intensity changes rather than conventional fixed-frame rate images. Unlike re</span>
                
                <span class="abstract-full" style="display: none;">Inspired by the data-efficient spiking mechanism of neurons in the human eye, event cameras were created to achieve high temporal resolution with minimal power and bandwidth requirements by emitting asynchronous, per-pixel intensity changes rather than conventional fixed-frame rate images. Unlike retinal ganglion cells (RGCs) in the human eye, however, which integrate signals from multiple photoreceptors within a receptive field to extract spatio-temporal features, conventional event cameras do not leverage local spatial context when deciding which events to fire. Moreover, the eye contains around 20 different kinds of RGCs operating in parallel, each attuned to different features or conditions. Inspired by this biological design, we introduce Neural Ganglion Sensors, an extension of traditional event cameras that learns task-specific spatio-temporal retinal kernels (i.e., RGC "events"). We evaluate our design on two challenging tasks: video interpolation and optical flow. Our results demonstrate that our biologically inspired sensing improves performance relative to conventional event cameras while reducing overall event bandwidth. These findings highlight the promise of RGC-inspired event sensors for edge devices and other low-power, real-time applications requiring efficient, high-resolution visual streams.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.3 -->
                    
                <!-- Medicine: 5.4 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2504.13458" target="_blank" rel="noopener noreferrer">Learning from Noisy Pseudo-labels for All-Weather Land Cover Mapping</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wang Liu, Zhiyu Wang, Xin Guo, Puhong Duan, Xudong Kang, Shutao Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Semantic segmentation of SAR images has garnered significant attention in remote sensing due to the immunity of SAR sensors to cloudy weather and light conditions. Nevertheless, SAR imagery lacks detailed information and is plagued by significant speckle noise, rendering the annotation or segmentati</span>
                
                <span class="abstract-full" style="display: none;">Semantic segmentation of SAR images has garnered significant attention in remote sensing due to the immunity of SAR sensors to cloudy weather and light conditions. Nevertheless, SAR imagery lacks detailed information and is plagued by significant speckle noise, rendering the annotation or segmentation of SAR images a formidable task. Recent efforts have resorted to annotating paired optical-SAR images to generate pseudo-labels through the utilization of an optical image segmentation network. However, these pseudo-labels are laden with noise, leading to suboptimal performance in SAR image segmentation. In this study, we introduce a more precise method for generating pseudo-labels by incorporating semi-supervised learning alongside a novel image resolution alignment augmentation. Furthermore, we introduce a symmetric cross-entropy loss to mitigate the impact of noisy pseudo-labels. Additionally, a bag of training and testing tricks is utilized to generate better land-cover mapping results. Our experiments on the GRSS data fusion contest indicate the effectiveness of the proposed method, which achieves first place. The code is available at https://github.com/StuLiu/DFC2025Track1.git.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.2 -->
                    
                <!-- Medicine: 6.4 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.0956
                </span>
                <a href="https://arxiv.org/abs/2504.13192" target="_blank" rel="noopener noreferrer">CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Liang-bo Ning, Shijie Wang, Wenqi Fan, Qing Li, Xin Xu, Hao Chen, Feiran Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys s</span>
                
                <span class="abstract-full" style="display: none;">Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.5 -->
                    
                <!-- Medicine: 3.8 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1029
                </span>
                <a href="https://arxiv.org/abs/2504.13776" target="_blank" rel="noopener noreferrer">Fighting Fires from Space: Leveraging Vision Transformers for Enhanced Wildfire Detection and Characterization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aman Agarwal, James Gearon, Raksha Rank, Etienne Chenevert
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Wildfires are increasing in intensity, frequency, and duration across large parts of the world as a result of anthropogenic climate change. Modern hazard detection and response systems that deal with wildfires are under-equipped for sustained wildfire seasons. Recent work has proved automated wildfi</span>
                
                <span class="abstract-full" style="display: none;">Wildfires are increasing in intensity, frequency, and duration across large parts of the world as a result of anthropogenic climate change. Modern hazard detection and response systems that deal with wildfires are under-equipped for sustained wildfire seasons. Recent work has proved automated wildfire detection using Convolutional Neural Networks (CNNs) trained on satellite imagery are capable of high-accuracy results. However, CNNs are computationally expensive to train and only incorporate local image context. Recently, Vision Transformers (ViTs) have gained popularity for their efficient training and their ability to include both local and global contextual information. In this work, we show that ViT can outperform well-trained and specialized CNNs to detect wildfires on a previously published dataset of LandSat-8 imagery. One of our ViTs outperforms the baseline CNN comparison by 0.92%. However, we find our own implementation of CNN-based UNet to perform best in every category, showing their sustained utility in image tasks. Overall, ViTs are comparably capable in detecting wildfires as CNNs, though well-tuned CNNs are still the best technique for detecting wildfire with our UNet providing an IoU of 93.58%, better than the baseline UNet by some 4.58%.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.3 -->
                    
                <!-- Medicine: 9.0 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1168
                </span>
                <a href="https://arxiv.org/abs/2410.21465" target="_blank" rel="noopener noreferrer">ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each t</span>
                
                <span class="abstract-full" style="display: none;">With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch sizes and boost throughput by up to 3.04$\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 11.7 -->
                    
                <!-- Medicine: 5.7 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1413
                </span>
                <a href="https://arxiv.org/abs/2504.13755" target="_blank" rel="noopener noreferrer">Predictors of Childhood Vaccination Uptake in England: An Explainable Machine Learning Analysis of Longitudinal Regional Data (2021-2024)</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Amin Noroozi, Sidratul Muntaha Esha, Mansoureh Ghari
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Childhood vaccination is a cornerstone of public health, yet disparities in vaccination coverage persist across England. These disparities are shaped by complex interactions among various factors, including geographic, demographic, socioeconomic, and cultural (GDSC) factors. Previous studies mostly </span>
                
                <span class="abstract-full" style="display: none;">Childhood vaccination is a cornerstone of public health, yet disparities in vaccination coverage persist across England. These disparities are shaped by complex interactions among various factors, including geographic, demographic, socioeconomic, and cultural (GDSC) factors. Previous studies mostly rely on cross-sectional data and traditional statistical approaches that assess individual or limited sets of variables in isolation. Such methods may fall short in capturing the dynamic and multivariate nature of vaccine uptake. In this paper, we conducted a longitudinal machine learning analysis of childhood vaccination coverage across 150 districts in England from 2021 to 2024. Using vaccination data from NHS records, we applied hierarchical clustering to group districts by vaccination coverage into low- and high-coverage clusters. A CatBoost classifier was then trained to predict districts' vaccination clusters using their GDSC data. Finally, the SHapley Additive exPlanations (SHAP) method was used to interpret the predictors' importance. The classifier achieved high accuracies of 92.1, 90.6, and 86.3 in predicting districts' vaccination clusters for the years 2021-2022, 2022-2023, and 2023-2024, respectively. SHAP revealed that geographic, cultural, and demographic variables, particularly rurality, English language proficiency, the percentage of foreign-born residents, and ethnic composition, were the most influential predictors of vaccination coverage, whereas socioeconomic variables, such as deprivation and employment, consistently showed lower importance, especially in 2023-2024. Surprisingly, rural districts were significantly more likely to have higher vaccination rates. Additionally, districts with lower vaccination coverage had higher populations whose first language was not English, who were born outside the UK, or who were from ethnic minority groups.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 14.1 -->
                    
                <!-- Medicine: 7.2 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1509
                </span>
                <a href="https://arxiv.org/abs/2504.13834" target="_blank" rel="noopener noreferrer">Science Hierarchography: Hierarchical Organization of Science Literature</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Muhan Gao, Jash Shah, Weiqi Wang, Daniel Khashabi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Scientific knowledge is growing rapidly, making it challenging to track progress and high-level conceptual links across broad disciplines. While existing tools like citation networks and search engines make it easy to access a few related papers, they fundamentally lack the flexible abstraction need</span>
                
                <span class="abstract-full" style="display: none;">Scientific knowledge is growing rapidly, making it challenging to track progress and high-level conceptual links across broad disciplines. While existing tools like citation networks and search engines make it easy to access a few related papers, they fundamentally lack the flexible abstraction needed to represent the density of activity in various scientific subfields. We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature into a high-quality hierarchical structure that allows for the categorization of scientific work across varying levels of abstraction, from very broad fields to very specific studies. Such a representation can provide insights into which fields are well-explored and which are under-explored. To achieve the goals of SCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach combines fast embedding-based clustering with LLM-based prompting to balance the computational efficiency of embedding methods with the semantic precision offered by LLM prompting. We demonstrate that this approach offers the best trade-off between quality and speed compared to methods that heavily rely on LLM prompting, such as iterative tree construction with LLMs. To better reflect the interdisciplinary and multifaceted nature of research papers, our hierarchy captures multiple dimensions of categorization beyond simple topic labels. We evaluate the utility of our framework by assessing how effectively an LLM-based agent can locate target papers using the hierarchy. Results show that this structured approach enhances interpretability, supports trend discovery, and offers an alternative pathway for exploring scientific literature beyond traditional search methods. Code, data and demo: $\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 12.1 -->
                    
                <!-- Medicine: 4.4 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1528
                </span>
                <a href="https://arxiv.org/abs/2504.13201" target="_blank" rel="noopener noreferrer">Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jirui Yang, Zheyu Lin, Shuhan Yang, Zhihui Lu, Xin Du
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Embodied Intelligence (EI) systems integrated with large language models (LLMs) face significant security risks, particularly from jailbreak attacks that manipulate models into generating harmful outputs or executing unsafe physical actions. Traditional defense strategies, such as input filtering an</span>
                
                <span class="abstract-full" style="display: none;">Embodied Intelligence (EI) systems integrated with large language models (LLMs) face significant security risks, particularly from jailbreak attacks that manipulate models into generating harmful outputs or executing unsafe physical actions. Traditional defense strategies, such as input filtering and output monitoring, often introduce high computational overhead or interfere with task performance in real-time embodied scenarios. To address these challenges, we propose Concept Enhancement Engineering (CEE), a novel defense framework that leverages representation engineering to enhance the safety of embodied LLMs by dynamically steering their internal activations. CEE operates by (1) extracting multilingual safety patterns from model activations, (2) constructing control directions based on safety-aligned concept subspaces, and (3) applying subspace concept rotation to reinforce safe behavior during inference. Our experiments demonstrate that CEE effectively mitigates jailbreak attacks while maintaining task performance, outperforming existing defense methods in both robustness and efficiency. This work contributes a scalable and interpretable safety mechanism for embodied AI, bridging the gap between theoretical representation engineering and practical security applications. Our findings highlight the potential of latent-space interventions as a viable defense paradigm against emerging adversarial threats in physically grounded AI systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 18.3 -->
                    
                <!-- Medicine: 5.9 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1558
                </span>
                <a href="https://arxiv.org/abs/2504.13416" target="_blank" rel="noopener noreferrer">STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Saksham Rastogi, Pratyush Maini, Danish Pruthi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Given how large parts of publicly available text are crawled to pretrain large language models (LLMs), data creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-s</span>
                
                <span class="abstract-full" style="display: none;">Given how large parts of publicly available text are crawled to pretrain large language models (LLMs), data creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership-i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves first generating multiple rephrases, each embedding a watermark with a unique secret key. One version is to be released publicly, while others are to be kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that STAMP preserves both the semantic meaning and the utility of the original data in comparing different models. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 12.5 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1801
                </span>
                <a href="https://arxiv.org/abs/2504.13816" target="_blank" rel="noopener noreferrer">Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chenghao Xiao, Hou Pong Chan, Hao Zhang, Mahani Aljunied, Lidong Bing, Noura Al Moubayed, Yu Rong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing t</span>
                
                <span class="abstract-full" style="display: none;">While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMs' perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMs' recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 21.6 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Medicine: 3.3 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.2007
                </span>
                <a href="https://arxiv.org/abs/2504.13277" target="_blank" rel="noopener noreferrer">Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Soorya Ram Shimgekar, Violeta J. Rodriguez, Paul A. Bloom, Dong Whi Yoo, Koustuv Saha
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Suicide is a critical global public health issue, with millions experiencing suicidal ideation (SI) each year. Online spaces enable individuals to express SI and seek peer support. While prior research has revealed the potential of detecting SI using machine learning and natural language analysis, a</span>
                
                <span class="abstract-full" style="display: none;">Suicide is a critical global public health issue, with millions experiencing suicidal ideation (SI) each year. Online spaces enable individuals to express SI and seek peer support. While prior research has revealed the potential of detecting SI using machine learning and natural language analysis, a key limitation is the lack of a theoretical framework to understand the underlying factors affecting high-risk suicidal intent. To bridge this gap, we adopted the Interpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607 posts from Reddit's r/SuicideWatch, categorizing them into SI dimensions (Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk factors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired Capability of Suicide). We found that high-risk SI posts express planning and attempts, methods and tools, and weaknesses and pain. In addition, we also examined the language of supportive responses through psycholinguistic and content analyses to find that individuals respond differently to different stages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI chatbots in providing effective supportive responses to suicidal ideation posts. We found that although AI improved structural coherence, expert evaluations highlight persistent shortcomings in providing dynamic, personalized, and deeply empathetic support. These findings underscore the need for careful reflection and deeper understanding in both the development and consideration of AI-driven interventions for effective mental health support.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 12.0 -->
                    
                <!-- Medicine: 7.9 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Networks: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.2319
                </span>
                <a href="https://arxiv.org/abs/2504.13474" target="_blank" rel="noopener noreferrer">Everything You Wanted to Know About LLM-based Vulnerability Detection But Were Afraid to Ask</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yue Li, Xiao Li, Hao Wu, Minghui Xu, Yue Zhang, Xiuzhen Cheng, Fengyuan Xu, Sheng Zhong
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large Language Models are a promising tool for automated vulnerability detection, thanks to their success in code generation and repair. However, despite widespread adoption, a critical question remains: Are LLMs truly effective at detecting real-world vulnerabilities? Current evaluations, which oft</span>
                
                <span class="abstract-full" style="display: none;">Large Language Models are a promising tool for automated vulnerability detection, thanks to their success in code generation and repair. However, despite widespread adoption, a critical question remains: Are LLMs truly effective at detecting real-world vulnerabilities? Current evaluations, which often assess models on isolated functions or files, ignore the broader execution and data-flow context essential for understanding vulnerabilities. This oversight leads to two types of misleading outcomes: incorrect conclusions and flawed rationales, collectively undermining the reliability of prior assessments. Therefore, in this paper, we challenge three widely held community beliefs: that LLMs are (i) unreliable, (ii) insensitive to code patches, and (iii) performance-plateaued across model scales. We argue that these beliefs are artifacts of context-deprived evaluations. To address this, we propose CORRECT (Context-Rich Reasoning Evaluation of Code with Trust), a new evaluation framework that systematically incorporates contextual information into LLM-based vulnerability detection. We construct a context-rich dataset of 2,000 vulnerable-patched program pairs spanning 99 CWEs and evaluate 13 LLMs across four model families. Our framework elicits both binary predictions and natural-language rationales, which are further validated using LLM-as-a-judge techniques. Our findings overturn existing misconceptions. When provided with sufficient context, SOTA LLMs achieve significantly improved performance (e.g., 0.7 F1-score on key CWEs), with 0.8 precision. We show that most false positives stem from reasoning errors rather than misclassification, and that while model and test-time scaling improve performance, they introduce diminishing returns and trade-offs in recall. Finally, we uncover new flaws in current LLM-based detection systems, such as limited generalization and overthinking biases.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 25.5 -->
                    
                <!-- Medicine: 6.1 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.2478
                </span>
                <a href="https://arxiv.org/abs/2504.13629" target="_blank" rel="noopener noreferrer">Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Cong William Lin, Wu Zhu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large Language Models (LLMs), such as ChatGPT, are reshaping content creation and academic writing. This study investigates the impact of AI-assisted generative revisions on research manuscripts, focusing on heterogeneous adoption patterns and their influence on writing convergence. Leveraging a dat</span>
                
                <span class="abstract-full" style="display: none;">Large Language Models (LLMs), such as ChatGPT, are reshaping content creation and academic writing. This study investigates the impact of AI-assisted generative revisions on research manuscripts, focusing on heterogeneous adoption patterns and their influence on writing convergence. Leveraging a dataset of over 627,000 academic papers from arXiv, we develop a novel classification framework by fine-tuning prompt- and discipline-specific large language models to detect the style of ChatGPT-revised texts. Our findings reveal substantial disparities in LLM adoption across academic disciplines, gender, native language status, and career stage, alongside a rapid evolution in scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness, and adherence to formal writing conventions, with improvements varying by revision type. Finally, a difference-in-differences analysis shows that while LLMs drive convergence in academic writing, early adopters, male researchers, non-native speakers, and junior scholars exhibit the most pronounced stylistic shifts, aligning their writing more closely with that of established researchers.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 27.3 -->
                    
                <!-- Medicine: 5.3 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.2609
                </span>
                <a href="https://arxiv.org/abs/2504.13837" target="_blank" rel="noopener noreferrer">Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasonin</span>
                
                <span class="abstract-full" style="display: none;">Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@\textit{k} metric with large values of \textit{k} to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of $k$ (\eg, $k$=1), base models can achieve a comparable or even higher pass@$k$ score compared to their RL counterparts at large $k$ values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: https://limit-of-RLVR.github.io</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 28.7 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Medicine: 1.9 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.3355
                </span>
                <a href="https://arxiv.org/abs/2402.11005" target="_blank" rel="noopener noreferrer">A Theory of LLM Sampling: Part Descriptive and Part Prescriptive</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sarath Sivaprasad, Pramod Kaushik, Sahar Abdelnabi, Mario Fritz
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large Language Models (LLMs) are increasingly utilized in autonomous decision-making, where they sample options from vast action spaces. However, the heuristics that guide this sampling process remain under-explored. We study this sampling behavior and show that this underlying heuristics resembles </span>
                
                <span class="abstract-full" style="display: none;">Large Language Models (LLMs) are increasingly utilized in autonomous decision-making, where they sample options from vast action spaces. However, the heuristics that guide this sampling process remain under-explored. We study this sampling behavior and show that this underlying heuristics resembles that of human decision-making: comprising a descriptive component (reflecting statistical norm) and a prescriptive component (implicit ideal encoded in the LLM) of a concept. We show that this deviation of a sample from the statistical norm towards a prescriptive component consistently appears in concepts across diverse real-world domains like public health, and economic trends. To further illustrate the theory, we demonstrate that concept prototypes in LLMs are affected by prescriptive norms, similar to the concept of normality in humans. Through case studies and comparison with human studies, we illustrate that in real-world applications, the shift of samples toward an ideal value in LLMs' outputs can result in significantly biased decision-making, raising ethical concerns.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 30.2 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.3668
                </span>
                <a href="https://arxiv.org/abs/2504.13475" target="_blank" rel="noopener noreferrer">LLM Sensitivity Evaluation Framework for Clinical Diagnosis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chenwei Yan, Xiangling Fu, Yuxuan Xiong, Tianyi Wang, Siu Cheung Hui, Ji Wu, Xien Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language models (LLMs) have demonstrated impressive performance across various domains. However, for clinical diagnosis, higher expectations are required for LLM's reliability and sensitivity: thinking like physicians and remaining sensitive to key medical information that affects diagnostic r</span>
                
                <span class="abstract-full" style="display: none;">Large language models (LLMs) have demonstrated impressive performance across various domains. However, for clinical diagnosis, higher expectations are required for LLM's reliability and sensitivity: thinking like physicians and remaining sensitive to key medical information that affects diagnostic reasoning, as subtle variations can lead to different diagnosis results. Yet, existing works focus mainly on investigating the sensitivity of LLMs to irrelevant context and overlook the importance of key information. In this paper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini, Claude3 and LLaMA2-7b, to key medical information by introducing different perturbation strategies. The evaluation results highlight the limitations of current LLMs in remaining sensitive to key medical information for diagnostic decision-making. The evolution of LLMs must focus on improving their reliability, enhancing their ability to be sensitive to key information, and effectively utilizing this information. These improvements will enhance human trust in LLMs and facilitate their practical application in real-world scenarios. Our code and dataset are available at https://github.com/chenwei23333/DiagnosisQA.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 29.3 -->
                    
                <!-- Medicine: 8.0 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Networks: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.2342
                </span>
                <a href="https://arxiv.org/abs/2504.13540" target="_blank" rel="noopener noreferrer">EG-Gaussian: Epipolar Geometry and Graph Network Enhanced 3D Gaussian Splatting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Beizhen Zhao, Yifan Zhou, Zijian Wang, Hao Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we explore an open research problem concerning the reconstruction of 3D scenes from images. Recent methods have adopt 3D Gaussian Splatting (3DGS) to produce 3D scenes due to its efficient training process. However, these methodologies may generate incomplete 3D scenes or blurred mult</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we explore an open research problem concerning the reconstruction of 3D scenes from images. Recent methods have adopt 3D Gaussian Splatting (3DGS) to produce 3D scenes due to its efficient training process. However, these methodologies may generate incomplete 3D scenes or blurred multiviews. This is because of (1) inaccurate 3DGS point initialization and (2) the tendency of 3DGS to flatten 3D Gaussians with the sparse-view input. To address these issues, we propose a novel framework EG-Gaussian, which utilizes epipolar geometry and graph networks for 3D scene reconstruction. Initially, we integrate epipolar geometry into the 3DGS initialization phase to enhance initial 3DGS point construction. Then, we specifically design a graph learning module to refine 3DGS spatial features, in which we incorporate both spatial coordinates and angular relationships among neighboring points. Experiments on indoor and outdoor benchmark datasets demonstrate that our approach significantly improves reconstruction accuracy compared to 3DGS-based methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- 3D: 11.3 -->
                    
                <!-- Medicine: 8.9 -->
                    
                <!-- LLMs: 5.9 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1488
                </span>
                <a href="https://arxiv.org/abs/2504.13428" target="_blank" rel="noopener noreferrer">HSACNet: Hierarchical Scale-Aware Consistency Regularized Semi-Supervised Change Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Qi'ao Xu, Pengfei Wang, Yanjun Li, Tianwen Qian, Xiaoling Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Semi-supervised change detection (SSCD) aims to detect changes between bi-temporal remote sensing images by utilizing limited labeled data and abundant unlabeled data. Existing methods struggle in complex scenarios, exhibiting poor performance when confronted with noisy data. They typically neglect </span>
                
                <span class="abstract-full" style="display: none;">Semi-supervised change detection (SSCD) aims to detect changes between bi-temporal remote sensing images by utilizing limited labeled data and abundant unlabeled data. Existing methods struggle in complex scenarios, exhibiting poor performance when confronted with noisy data. They typically neglect intra-layer multi-scale features while emphasizing inter-layer fusion, harming the integrity of change objects with different scales. In this paper, we propose HSACNet, a Hierarchical Scale-Aware Consistency regularized Network for SSCD. Specifically, we integrate Segment Anything Model 2 (SAM2), using its Hiera backbone as the encoder to extract inter-layer multi-scale features and applying adapters for parameter-efficient fine-tuning. Moreover, we design a Scale-Aware Differential Attention Module (SADAM) that can precisely capture intra-layer multi-scale change features and suppress noise. Additionally, a dual-augmentation consistency regularization strategy is adopted to effectively utilize the unlabeled data. Extensive experiments across four CD benchmarks demonstrate that our HSACNet achieves state-of-the-art performance, with reduced parameters and computational cost.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.0 -->
                    
                <!-- LLMs: 8.0 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- 3D: 2.6 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1622
                </span>
                <a href="https://arxiv.org/abs/2412.10338" target="_blank" rel="noopener noreferrer">XYScanNet: A State Space Model for Single Image Deblurring</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hanzhou Liu, Chengkai Liu, Jiacong Xu, Peng Jiang, Mi Lu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Deep state-space models (SSMs), like recent Mamba architectures, are emerging as a promising alternative to CNN and Transformer networks. Existing Mamba-based restoration methods process visual data by leveraging a flatten-and-scan strategy that converts image patches into a 1D sequence before scann</span>
                
                <span class="abstract-full" style="display: none;">Deep state-space models (SSMs), like recent Mamba architectures, are emerging as a promising alternative to CNN and Transformer networks. Existing Mamba-based restoration methods process visual data by leveraging a flatten-and-scan strategy that converts image patches into a 1D sequence before scanning. However, this scanning paradigm ignores local pixel dependencies and introduces spatial misalignment by positioning distant pixels incorrectly adjacent, which reduces local noise-awareness and degrades image sharpness in low-level vision tasks. To overcome these issues, we propose a novel slice-and-scan strategy that alternates scanning along intra- and inter-slices. We further design a new Vision State Space Module (VSSM) for image deblurring, and tackle the inefficiency challenges of the current Mamba-based vision module. Building upon this, we develop XYScanNet, an SSM architecture integrated with a lightweight feature fusion module for enhanced image deblurring. XYScanNet, maintains competitive distortion metrics and significantly improves perceptual performance. Experimental results show that XYScanNet enhances KID by $17\%$ compared to the nearest competitor.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.1 -->
                    
                <!-- LLMs: 5.0 -->
                    
                <!-- 3D: 3.2 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Attention: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3806
                </span>
                <a href="https://arxiv.org/abs/2504.13224" target="_blank" rel="noopener noreferrer">ICAS: IP Adapter and ControlNet-based Attention Structure for Multi-Subject Style Transfer Optimization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fuwei Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generating multi-subject stylized images remains a significant challenge due to the ambiguity in defining style attributes (e.g., color, texture, atmosphere, and structure) and the difficulty in consistently applying them across multiple subjects. Although recent diffusion-based text-to-image models</span>
                
                <span class="abstract-full" style="display: none;">Generating multi-subject stylized images remains a significant challenge due to the ambiguity in defining style attributes (e.g., color, texture, atmosphere, and structure) and the difficulty in consistently applying them across multiple subjects. Although recent diffusion-based text-to-image models have achieved remarkable progress, existing methods typically rely on computationally expensive inversion procedures or large-scale stylized datasets. Moreover, these methods often struggle with maintaining multi-subject semantic fidelity and are limited by high inference costs. To address these limitations, we propose ICAS (IP-Adapter and ControlNet-based Attention Structure), a novel framework for efficient and controllable multi-subject style transfer. Instead of full-model tuning, ICAS adaptively fine-tunes only the content injection branch of a pre-trained diffusion model, thereby preserving identity-specific semantics while enhancing style controllability. By combining IP-Adapter for adaptive style injection with ControlNet for structural conditioning, our framework ensures faithful global layout preservation alongside accurate local style synthesis. Furthermore, ICAS introduces a cyclic multi-subject content embedding mechanism, which enables effective style transfer under limited-data settings without the need for extensive stylized corpora. Extensive experiments show that ICAS achieves superior performance in structure preservation, style consistency, and inference efficiency, establishing a new paradigm for multi-subject style transfer in real-world applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.1 -->
                    
                <!-- LLMs: 4.8 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- 3D: 2.7 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4187
                </span>
                <a href="https://arxiv.org/abs/2504.13182" target="_blank" rel="noopener noreferrer">Challenging the Eye-Mind Link Hypothesis: Visualizing Gazes For Each Programming Problem</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Michael T. Lopez II
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This investigates the relationship between eye fixation patterns and performance in Java programming exercises using eye-tracking technology. Thirty-one students from a university in Metro Manila participated, and their eye movements were recorded while solving five Java programming exercises (three</span>
                
                <span class="abstract-full" style="display: none;">This investigates the relationship between eye fixation patterns and performance in Java programming exercises using eye-tracking technology. Thirty-one students from a university in Metro Manila participated, and their eye movements were recorded while solving five Java programming exercises (three of the five exercises were picked). The fixation data were preprocessed and visualized using heatmap bin graphs, dividing the participants into correct and wrong answer groups. The Mann-Whitney U Test was employed to determine if there were significant differences in the fixation patterns between the two groups.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.5 -->
                    
                <!-- LLMs: 6.3 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4484
                </span>
                <a href="https://arxiv.org/abs/2504.13393" target="_blank" rel="noopener noreferrer">BeetleVerse: A study on taxonomic classification of ground beetles</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: S M Rayeed, Alyson East, Samuel Stevens, Sydne Record, Charles V Stewart
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Ground beetles are a highly sensitive and speciose biological indicator, making them vital for monitoring biodiversity. However, they are currently an underutilized resource due to the manual effort required by taxonomic experts to perform challenging species differentiations based on subtle morphol</span>
                
                <span class="abstract-full" style="display: none;">Ground beetles are a highly sensitive and speciose biological indicator, making them vital for monitoring biodiversity. However, they are currently an underutilized resource due to the manual effort required by taxonomic experts to perform challenging species differentiations based on subtle morphological differences, precluding widespread applications. In this paper, we evaluate 12 vision models on taxonomic classification across four diverse, long-tailed datasets spanning over 230 genera and 1769 species, with images ranging from controlled laboratory settings to challenging field-collected (in-situ) photographs. We further explore taxonomic classification in two important real-world contexts: sample efficiency and domain adaptation. Our results show that the Vision and Language Transformer combined with an MLP head is the best performing model, with 97\% accuracy at genus and 94\% at species level. Sample efficiency analysis shows that we can reduce train data requirements by up to 50\% with minimal compromise in performance. The domain adaptation experiments reveal significant challenges when transferring models from lab to in-situ images, highlighting a critical domain gap. Overall, our study lays a foundation for large-scale automated taxonomic classification of beetles, and beyond that, advances sample-efficient learning and cross-domain adaptation for diverse long-tailed ecological datasets.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.6 -->
                    
                <!-- LLMs: 8.4 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.5097
                </span>
                <a href="https://arxiv.org/abs/2410.11838" target="_blank" rel="noopener noreferrer">High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Junhwa Hur, Charles Herrmann, Saurabh Saxena, Janne Kontkanen, Wei-Sheng Lai, Yichang Shih, Michael Rubinstein, David J. Fleet, Deqing Sun
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Despite the recent progress, existing frame interpolation methods still struggle with processing extremely high resolution input and handling challenging cases such as repetitive textures, thin objects, and large motion. To address these issues, we introduce a patch-based cascaded pixel diffusion mo</span>
                
                <span class="abstract-full" style="display: none;">Despite the recent progress, existing frame interpolation methods still struggle with processing extremely high resolution input and handling challenging cases such as repetitive textures, thin objects, and large motion. To address these issues, we introduce a patch-based cascaded pixel diffusion model for high resolution frame interpolation, HIFI, that excels in these scenarios while achieving competitive performance on standard benchmarks. Cascades, which generate a series of images from low to high resolution, can help significantly with large or complex motion that require both global context for a coarse solution and detailed context for high resolution output. However, contrary to prior work on cascaded diffusion models which perform diffusion on increasingly large resolutions, we use a single model that always performs diffusion at the same resolution and upsamples by processing patches of the inputs and the prior solution. At inference time, this drastically reduces memory usage and allows a single model, solving both frame interpolation (base model's task) and spatial up-sampling, saving training cost as well. HIFI excels at high-resolution images and complex repeated textures that require global context, achieving comparable or state-of-the-art performance on various benchmarks (Vimeo, Xiph, X-Test, and SEPE-8K). We further introduce a new dataset, LaMoR, that focuses on particularly challenging cases, and HIFI significantly outperforms other baselines. Please visit our project page for video results: https://hifi-diffusion.github.io</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.0 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.6033
                </span>
                <a href="https://arxiv.org/abs/2504.13190" target="_blank" rel="noopener noreferrer">Cellular-X: An LLM-empowered Cellular Agent for Efficient Base Station Operations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Liujianfu Wang, Xinyi Long, Yuyang Du, Xiaoyan Liu, Kexin Chen, Soung Chang Liew
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper introduces Cellular-X, an LLM-powered agent designed to automate cellular base station (BS) maintenance. Leveraging multimodal LLM and retrieval-augmented generation (RAG) techniques, Cellular-X significantly enhances field engineer efficiency by quickly interpreting user intents, retriev</span>
                
                <span class="abstract-full" style="display: none;">This paper introduces Cellular-X, an LLM-powered agent designed to automate cellular base station (BS) maintenance. Leveraging multimodal LLM and retrieval-augmented generation (RAG) techniques, Cellular-X significantly enhances field engineer efficiency by quickly interpreting user intents, retrieving relevant technical information, and configuring a BS through iterative self-correction. Key features of the demo include automatic customized BS setup, document-based query answering, and voice-controlled configuration reporting and revision. We implemented Cellular-X on a USRP X310 testbed for demonstration. Demo videos and implementation details are available at https://github.com/SeaBreezing/Cellular-X.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.4 -->
                    
                <!-- LLMs: 8.7 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- 3D: 2.6 -->
                    
                <!-- RAG: 2.3 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.6645
                </span>
                <a href="https://arxiv.org/abs/2504.09958" target="_blank" rel="noopener noreferrer">C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fuqiang Niu, Yi Yang, Xianghua Fu, Genan Dai, Bowen Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Stance detection has become an essential tool for analyzing public discussions on social media. Current methods face significant challenges, particularly in Chinese language processing and multi-turn conversational analysis. To address these limitations, we introduce C-MTCSD, the largest Chinese mul</span>
                
                <span class="abstract-full" style="display: none;">Stance detection has become an essential tool for analyzing public discussions on social media. Current methods face significant challenges, particularly in Chinese language processing and multi-turn conversational analysis. To address these limitations, we introduce C-MTCSD, the largest Chinese multi-turn conversational stance detection dataset, comprising 24,264 carefully annotated instances from Sina Weibo, which is 4.2 times larger than the only prior Chinese conversational stance detection dataset. Our comprehensive evaluation using both traditional approaches and large language models reveals the complexity of C-MTCSD: even state-of-the-art models achieve only 64.07% F1 score in the challenging zero-shot setting, while performance consistently degrades with increasing conversation depth. Traditional models particularly struggle with implicit stance detection, achieving below 50% F1 score. This work establishes a challenging new benchmark for Chinese stance detection research, highlighting significant opportunities for future improvements.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.0 -->
                    
                <!-- LLMs: 10.1 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.6807
                </span>
                <a href="https://arxiv.org/abs/2504.13733" target="_blank" rel="noopener noreferrer">Dynamic Regularized CBDT: Variance-Calibrated Causal Boosting for Interpretable Heterogeneous Treatment Effects</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yichen Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Heterogeneous treatment effect estimation in high-stakes applications demands models that simultaneously optimize precision, interpretability, and calibration. Many existing tree-based causal inference techniques, however, exhibit high estimation errors when applied to observational data because the</span>
                
                <span class="abstract-full" style="display: none;">Heterogeneous treatment effect estimation in high-stakes applications demands models that simultaneously optimize precision, interpretability, and calibration. Many existing tree-based causal inference techniques, however, exhibit high estimation errors when applied to observational data because they struggle to capture complex interactions among factors and rely on static regularization schemes. In this work, we propose Dynamic Regularized Causal Boosted Decision Trees (CBDT), a novel framework that integrates variance regularization and average treatment effect calibration into the loss function of gradient boosted decision trees. Our approach dynamically updates the regularization parameters using gradient statistics to better balance the bias-variance tradeoff. Extensive experiments on standard benchmark datasets and real-world clinical data demonstrate that the proposed method significantly improves estimation accuracy while maintaining reliable coverage of true treatment effects. In an intensive care unit patient triage study, the method successfully identified clinically actionable rules and achieved high accuracy in treatment effect estimation. The results validate that dynamic regularization can effectively tighten error bounds and enhance both predictive performance and model interpretability.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 11.3 -->
                    
                <!-- Medicine: 10.3 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7085
                </span>
                <a href="https://arxiv.org/abs/2504.13480" target="_blank" rel="noopener noreferrer">Integrating Locality-Aware Attention with Transformers for General Geometry PDEs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Minsu Koh, Beom-Chul Park, Heejo Kong, Seong-Whan Lee
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Neural operators have emerged as promising frameworks for learning mappings governed by partial differential equations (PDEs), serving as data-driven alternatives to traditional numerical methods. While methods such as the Fourier neural operator (FNO) have demonstrated notable performance, their re</span>
                
                <span class="abstract-full" style="display: none;">Neural operators have emerged as promising frameworks for learning mappings governed by partial differential equations (PDEs), serving as data-driven alternatives to traditional numerical methods. While methods such as the Fourier neural operator (FNO) have demonstrated notable performance, their reliance on uniform grids restricts their applicability to complex geometries and irregular meshes. Recently, Transformer-based neural operators with linear attention mechanisms have shown potential in overcoming these limitations for large-scale PDE simulations. However, these approaches predominantly emphasize global feature aggregation, often overlooking fine-scale dynamics and localized PDE behaviors essential for accurate solutions. To address these challenges, we propose the Locality-Aware Attention Transformer (LA2Former), which leverages K-nearest neighbors for dynamic patchifying and integrates global-local attention for enhanced PDE modeling. By combining linear attention for efficient global context encoding with pairwise attention for capturing intricate local interactions, LA2Former achieves an optimal balance between computational efficiency and predictive accuracy. Extensive evaluations across six benchmark datasets demonstrate that LA2Former improves predictive accuracy by over 50% relative to existing linear attention methods, while also outperforming full pairwise attention under optimal conditions. This work underscores the critical importance of localized feature learning in advancing Transformer-based neural operators for solving PDEs on complex and irregular domains.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.6 -->
                    
                <!-- LLMs: 4.7 -->
                    
                <!-- Quantum Computing: 4.4 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Attention: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.726
                </span>
                <a href="https://arxiv.org/abs/2504.13519" target="_blank" rel="noopener noreferrer">Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for Low-Dose CT with Attention-Guided Bilateral Filtering</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yipeng Sun, Linda-Sophie Schneider, Mingxuan Gu, Siyuan Mei, Chengze Ye, Fabian Wagner, Siming Bayer, Andreas Maier
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Effective denoising is crucial in low-dose CT to enhance subtle structures and low-contrast lesions while preventing diagnostic errors. Supervised methods struggle with limited paired datasets, and self-supervised approaches often require multiple noisy images and rely on deep networks like U-Net, o</span>
                
                <span class="abstract-full" style="display: none;">Effective denoising is crucial in low-dose CT to enhance subtle structures and low-contrast lesions while preventing diagnostic errors. Supervised methods struggle with limited paired datasets, and self-supervised approaches often require multiple noisy images and rely on deep networks like U-Net, offering little insight into the denoising mechanism. To address these challenges, we propose an interpretable self-supervised single-image denoising framework -- Filter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral Filter that adapted to each noisy input through a lightweight module that predicts spatially varying filter parameters, which can be visualized and adjusted post-training for user-controlled denoising in specific regions of interest. To enable single-image training, we introduce a novel downsampling shuffle strategy with a new self-supervised loss function that extends the concept of Noise2Noise to a single image and addresses spatially correlated noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving transparency, user control, and parametric efficiency. These features provide key advantages for medical applications that require precise and interpretable noise reduction. Our code is demonstrated at https://github.com/sypsyp97/Filter2Noise.git .</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.7 -->
                    
                <!-- LLMs: 6.2 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- 3D: 2.5 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7432
                </span>
                <a href="https://arxiv.org/abs/2504.13538" target="_blank" rel="noopener noreferrer">Machine Learning Informed by Micro and Mesoscopic Statistical Physics Methods for Community Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yijun Ran, Junfan Yi, Wei Si, Michael Small, Ke-ke Shang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Community detection plays a crucial role in understanding the structural organization of complex networks. Previous methods, particularly those from statistical physics, primarily focus on the analysis of mesoscopic network structures and often struggle to integrate fine-grained node similarities. T</span>
                
                <span class="abstract-full" style="display: none;">Community detection plays a crucial role in understanding the structural organization of complex networks. Previous methods, particularly those from statistical physics, primarily focus on the analysis of mesoscopic network structures and often struggle to integrate fine-grained node similarities. To address this limitation, we propose a low-complexity framework that integrates machine learning to embed micro-level node-pair similarities into mesoscopic community structures. By leveraging ensemble learning models, our approach enhances both structural coherence and detection accuracy. Experimental evaluations on artificial and real-world networks demonstrate that our framework consistently outperforms conventional methods, achieving higher modularity and improved accuracy in NMI and ARI. Notably, when ground-truth labels are available, our approach yields the most accurate detection results, effectively recovering real-world community structures while minimizing misclassifications. To further explain our framework's performance, we analyze the correlation between node-pair similarity and evaluation metrics. The results reveal a strong and statistically significant correlation, underscoring the critical role of node-pair similarity in enhancing detection accuracy. Overall, our findings highlight the synergy between machine learning and statistical physics, demonstrating how machine learning techniques can enhance network analysis and uncover complex structural patterns.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.0 -->
                    
                <!-- LLMs: 6.3 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.748
                </span>
                <a href="https://arxiv.org/abs/2502.14427" target="_blank" rel="noopener noreferrer">Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Artem Vazhentsev, Lyudmila Rvanova, Ivan Lazichny, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Artem Shelmanov
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs). To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs. Density-based methods, despite being very effective for UQ in </span>
                
                <span class="abstract-full" style="display: none;">Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs). To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs. Density-based methods, despite being very effective for UQ in text classification with encoder-based models, have not been very successful with generative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a well-established UQ technique in classification tasks - for text generation and introduce a new supervised UQ method. Our method extracts token embeddings from multiple layers of LLMs, computes MD scores for each token, and uses linear regression trained on these features to provide robust uncertainty scores. Through extensive experiments on eleven datasets, we demonstrate that our approach substantially improves over existing UQ methods, providing accurate and computationally efficient uncertainty scores for both sequence-level selective generation and claim-level fact-checking tasks. Our method also exhibits strong generalization to out-of-domain data, making it suitable for a wide range of LLM-based applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 17.6 -->
                    
                <!-- Medicine: 10.4 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- T2I: 2.1 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7732
                </span>
                <a href="https://arxiv.org/abs/2504.13713" target="_blank" rel="noopener noreferrer">SLAM&Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Samuel Cerezo, Gaetano Meli, Tom\'as Berriel Martins, Kirill Safronov, Javier Civera
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Models and methods originally developed for novel view synthesis and scene rendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are increasingly being adopted as representations in Simultaneous Localization and Mapping (SLAM). However, existing datasets fail to include the specif</span>
                
                <span class="abstract-full" style="display: none;">Models and methods originally developed for novel view synthesis and scene rendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are increasingly being adopted as representations in Simultaneous Localization and Mapping (SLAM). However, existing datasets fail to include the specific challenges of both fields, such as multimodality and sequentiality in SLAM or generalization across viewpoints and illumination conditions in neural rendering. To bridge this gap, we introduce SLAM&amp;Render, a novel dataset designed to benchmark methods in the intersection between SLAM and novel view rendering. It consists of 40 sequences with synchronized RGB, depth, IMU, robot kinematic data, and ground-truth pose streams. By releasing robot kinematic data, the dataset also enables the assessment of novel SLAM strategies when applied to robot manipulators. The dataset sequences span five different setups featuring consumer and industrial objects under four different lighting conditions, with separate training and test trajectories per scene, as well as object rearrangements. Our experimental results, obtained with several baselines from the literature, validate SLAM&amp;Render as a relevant benchmark for this emerging research area.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.2 -->
                    
                <!-- LLMs: 5.6 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7758
                </span>
                <a href="https://arxiv.org/abs/2504.13524" target="_blank" rel="noopener noreferrer">OBIFormer: A Fast Attentive Denoising Framework for Oracle Bone Inscriptions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jinhao Li, Zijian Chen, Tingzhu Chen, Zhiji Liu, Changbo Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Oracle bone inscriptions (OBIs) are the earliest known form of Chinese characters and serve as a valuable resource for research in anthropology and archaeology. However, most excavated fragments are severely degraded due to thousands of years of natural weathering, corrosion, and man-made destructio</span>
                
                <span class="abstract-full" style="display: none;">Oracle bone inscriptions (OBIs) are the earliest known form of Chinese characters and serve as a valuable resource for research in anthropology and archaeology. However, most excavated fragments are severely degraded due to thousands of years of natural weathering, corrosion, and man-made destruction, making automatic OBI recognition extremely challenging. Previous methods either focus on pixel-level information or utilize vanilla transformers for glyph-based OBI denoising, which leads to tremendous computational overhead. Therefore, this paper proposes a fast attentive denoising framework for oracle bone inscriptions, i.e., OBIFormer. It leverages channel-wise self-attention, glyph extraction, and selective kernel feature fusion to reconstruct denoised images precisely while being computationally efficient. Our OBIFormer achieves state-of-the-art denoising performance for PSNR and SSIM metrics on synthetic and original OBI datasets. Furthermore, comprehensive experiments on a real oracle dataset demonstrate the great potential of our OBIFormer in assisting automatic OBI recognition. The code will be made available at https://github.com/LJHolyGround/OBIFormer.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.1 -->
                    
                <!-- LLMs: 6.3 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7764
                </span>
                <a href="https://arxiv.org/abs/2408.02657" target="_blank" rel="noopener noreferrer">Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, Peng Gao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. By initializing from multimodal Generative PreTraining (mGPT), we demonstrate that decoder-on</span>
                
                <span class="abstract-full" style="display: none;">We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. By initializing from multimodal Generative PreTraining (mGPT), we demonstrate that decoder-only Autoregressive (AR) model can achieve image generation performance comparable to modern diffusion models with high efficiency through Flexible Progressive Supervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image Representation (UniRep), Lumina-mGPT can flexibly generate high-quality images of varying aspect ratios. Building on the strong image generation capabilities, we further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial attempt to elevate Lumina-mGPT into a unified multi-modal generalist. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like text-to-image/multiview generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multi-turn visual question answering, showing the rosy potential of the technical direction. Codes and checkpoints are available at https://github.com/Alpha-VLLM/Lumina-mGPT.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 12.9 -->
                    
                <!-- Medicine: 11.6 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- T2I: 2.4 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7782
                </span>
                <a href="https://arxiv.org/abs/2504.13614" target="_blank" rel="noopener noreferrer">Adaptive Long-term Embedding with Denoising and Augmentation for Recommendation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zahra Akhlaghi, Mostafa Haghir Chehreghani
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rapid growth of the internet has made personalized recommendation systems indispensable. Graph-based sequential recommendation systems, powered by Graph Neural Networks (GNNs), effectively capture complex user-item interactions but often face challenges such as noise and static representations. </span>
                
                <span class="abstract-full" style="display: none;">The rapid growth of the internet has made personalized recommendation systems indispensable. Graph-based sequential recommendation systems, powered by Graph Neural Networks (GNNs), effectively capture complex user-item interactions but often face challenges such as noise and static representations. In this paper, we introduce the Adaptive Long-term Embedding with Denoising and Augmentation for Recommendation (ALDA4Rec) method, a novel model that constructs an item-item graph, filters noise through community detection, and enriches user-item interactions. Graph Convolutional Networks (GCNs) are then employed to learn short-term representations, while averaging, GRUs, and attention mechanisms are utilized to model long-term embeddings. An MLP-based adaptive weighting strategy is further incorporated to dynamically optimize long-term user preferences. Experiments conducted on four real-world datasets demonstrate that ALDA4Rec outperforms state-of-the-art baselines, delivering notable improvements in both accuracy and robustness. The source code is available at https://github.com/zahraakhlaghi/ALDA4Rec.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.2 -->
                    
                <!-- LLMs: 8.4 -->
                    
                <!-- GNN: 2.8 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8147
                </span>
                <a href="https://arxiv.org/abs/2504.13219" target="_blank" rel="noopener noreferrer">Scaling Laws for Data-Efficient Visual Transfer Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wenxuan Yang, Qingqu Wei, Chenxi Ma, Weimin Tan, Bo Yan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Current scaling laws for visual AI models focus predominantly on large-scale pretraining, leaving a critical gap in understanding how performance scales for data-constrained downstream tasks. To address this limitation, this paper establishes the first practical framework for data-efficient scaling </span>
                
                <span class="abstract-full" style="display: none;">Current scaling laws for visual AI models focus predominantly on large-scale pretraining, leaving a critical gap in understanding how performance scales for data-constrained downstream tasks. To address this limitation, this paper establishes the first practical framework for data-efficient scaling laws in visual transfer learning, addressing two fundamental questions: 1) How do scaling behaviors shift when downstream tasks operate with limited data? 2) What governs the efficacy of knowledge distillation under such constraints? Through systematic analysis of vision tasks across data regimes (1K-1M samples), we propose the distillation boundary theory, revealing a critical turning point in distillation efficiency: 1) Distillation superiority: In data-scarce conditions, distilled models significantly outperform their non-distillation counterparts, efficiently leveraging inherited knowledge to compensate for limited training samples. 2) Pre-training dominance: As pre-training data increases beyond a critical threshold, non-distilled models gradually surpass distilled versions, suggesting diminishing returns from knowledge inheritance when sufficient task-specific data becomes available. Empirical validation across various model scales (2.5M to 38M parameters) and data volumes demonstrate these performance inflection points, with error difference curves transitioning from positive to negative values at critical data thresholds, confirming our theoretical predictions. This work redefines scaling laws for data-limited regimes, bridging the knowledge gap between large-scale pretraining and practical downstream adaptation, addressing a critical barrier to understanding vision model scaling behaviors and optimizing computational resource allocation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.3 -->
                    
                <!-- LLMs: 9.0 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- RAG: 2.4 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.841
                </span>
                <a href="https://arxiv.org/abs/2504.13710" target="_blank" rel="noopener noreferrer">Few-Shot Referring Video Single- and Multi-Object Segmentation via Cross-Modal Affinity with Instance Sequence Matching</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Heng Liu, Guanghui Li, Mingqi Gao, Xiantong Zhen, Feng Zheng, Yang Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Referring video object segmentation (RVOS) aims to segment objects in videos guided by natural language descriptions. We propose FS-RVOS, a Transformer-based model with two key components: a cross-modal affinity module and an instance sequence matching strategy, which extends FS-RVOS to multi-object</span>
                
                <span class="abstract-full" style="display: none;">Referring video object segmentation (RVOS) aims to segment objects in videos guided by natural language descriptions. We propose FS-RVOS, a Transformer-based model with two key components: a cross-modal affinity module and an instance sequence matching strategy, which extends FS-RVOS to multi-object segmentation (FS-RVMOS). Experiments show FS-RVOS and FS-RVMOS outperform state-of-the-art methods across diverse benchmarks, demonstrating superior robustness and accuracy.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.0 -->
                    
                <!-- LLMs: 10.3 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- 3D: 3.2 -->
                    
                <!-- RAG: 2.8 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- T2I: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Attention: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8791
                </span>
                <a href="https://arxiv.org/abs/2504.13439" target="_blank" rel="noopener noreferrer">D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Grace Byun, Jinho Choi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Evaluating generative models with open-ended generation is challenging due to inconsistencies in response formats. Multiple-choice (MC) evaluation mitigates this issue, but generating high-quality distractors is time-consuming and labor-intensive. We introduce D-GEN, the first open-source distractor</span>
                
                <span class="abstract-full" style="display: none;">Evaluating generative models with open-ended generation is challenging due to inconsistencies in response formats. Multiple-choice (MC) evaluation mitigates this issue, but generating high-quality distractors is time-consuming and labor-intensive. We introduce D-GEN, the first open-source distractor generator model that transforms open-ended data into an MC format. To evaluate distractor quality, we propose two novel methods: (1) ranking alignment, ensuring generated distractors retain the discriminatory power of ground-truth distractors, and (2) entropy analysis, comparing model confidence distributions. Our results show that D-GEN preserves ranking consistency (Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy distribution of ground-truth distractors. Human evaluation further confirms the fluency, coherence, distractiveness, and incorrectness. Our work advances robust and efficient distractor generation with automated evaluation, setting a new standard for MC evaluation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.1 -->
                    
                <!-- LLMs: 10.4 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.9275
                </span>
                <a href="https://arxiv.org/abs/2504.13579" target="_blank" rel="noopener noreferrer">HDBFormer: Efficient RGB-D Semantic Segmentation with A Heterogeneous Dual-Branch Framework</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shuobin Wei, Zhuang Zhou, Zhengan Lu, Zizhao Yuan, Binghua Su
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In RGB-D semantic segmentation for indoor scenes, a key challenge is effectively integrating the rich color information from RGB images with the spatial distance information from depth images. However, most existing methods overlook the inherent differences in how RGB and depth images express inform</span>
                
                <span class="abstract-full" style="display: none;">In RGB-D semantic segmentation for indoor scenes, a key challenge is effectively integrating the rich color information from RGB images with the spatial distance information from depth images. However, most existing methods overlook the inherent differences in how RGB and depth images express information. Properly distinguishing the processing of RGB and depth images is essential to fully exploiting their unique and significant characteristics. To address this, we propose a novel heterogeneous dual-branch framework called HDBFormer, specifically designed to handle these modality differences. For RGB images, which contain rich detail, we employ both a basic and detail encoder to extract local and global features. For the simpler depth images, we propose LDFormer, a lightweight hierarchical encoder that efficiently extracts depth features with fewer parameters. Additionally, we introduce the Modality Information Interaction Module (MIIM), which combines transformers with large kernel convolutions to interact global and local information across modalities efficiently. Extensive experiments show that HDBFormer achieves state-of-the-art performance on the NYUDepthv2 and SUN-RGBD datasets. The code is available at: https://github.com/Weishuobin/HDBFormer.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.7 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.9313
                </span>
                <a href="https://arxiv.org/abs/2504.13339" target="_blank" rel="noopener noreferrer">Volume Encoding Gaussians: Transfer Function-Agnostic 3D Gaussians for Volume Rendering</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Landon Dyken, Andres Sewell, Will Usher, Steve Petruzza, Sidharth Kumar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While HPC resources are increasingly being used to produce adaptively refined or unstructured volume datasets, current research in applying machine learning-based representation to visualization has largely ignored this type of data. To address this, we introduce Volume Encoding Gaussians (VEG), a n</span>
                
                <span class="abstract-full" style="display: none;">While HPC resources are increasingly being used to produce adaptively refined or unstructured volume datasets, current research in applying machine learning-based representation to visualization has largely ignored this type of data. To address this, we introduce Volume Encoding Gaussians (VEG), a novel 3D Gaussian-based representation for scientific volume visualization focused on unstructured volumes. Unlike prior 3D Gaussian Splatting (3DGS) methods that store view-dependent color and opacity for each Gaussian, VEG decouple the visual appearance from the data representation by encoding only scalar values, enabling transfer-function-agnostic rendering of 3DGS models for interactive scientific visualization. VEG are directly initialized from volume datasets, eliminating the need for structure-from-motion pipelines like COLMAP. To ensure complete scalar field coverage, we introduce an opacity-guided training strategy, using differentiable rendering with multiple transfer functions to optimize our data representation. This allows VEG to preserve fine features across the full scalar range of a dataset while remaining independent of any specific transfer function. Each Gaussian is scaled and rotated to adapt to local geometry, allowing for efficient representation of unstructured meshes without storing mesh connectivity and while using far fewer primitives. Across a diverse set of data, VEG achieve high reconstruction quality, compress large volume datasets by up to 3600x, and support lightning-fast rendering on commodity GPUs, enabling interactive visualization of large-scale structured and unstructured volumes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.7 -->
                    
                <!-- LLMs: 5.8 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.9635
                </span>
                <a href="https://arxiv.org/abs/2410.09024" target="_blank" rel="noopener noreferrer">AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, Eric Winsor, Jerome Wynne, Yarin Gal, Xander Davies
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greate</span>
                
                <span class="abstract-full" style="display: none;">The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 15.6 -->
                    
                <!-- Medicine: 10.3 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.9818
                </span>
                <a href="https://arxiv.org/abs/2412.11520" target="_blank" rel="noopener noreferrer">EditSplat: Multi-View Fusion and Attention-Guided Optimization for View-Consistent 3D Scene Editing with 3D Gaussian Splatting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dong In Lee, Hyeongcheol Park, Jiyoung Seo, Eunbyung Park, Hyunje Park, Ha Dam Baek, Sangheon Shin, Sangmin Kim, Sangpil Kim
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advancements in 3D editing have highlighted the potential of text-driven methods in real-time, user-friendly AR/VR applications. However, current methods rely on 2D diffusion models without adequately considering multi-view information, resulting in multi-view inconsistency. While 3D Gaussian</span>
                
                <span class="abstract-full" style="display: none;">Recent advancements in 3D editing have highlighted the potential of text-driven methods in real-time, user-friendly AR/VR applications. However, current methods rely on 2D diffusion models without adequately considering multi-view information, resulting in multi-view inconsistency. While 3D Gaussian Splatting (3DGS) significantly improves rendering quality and speed, its 3D editing process encounters difficulties with inefficient optimization, as pre-trained Gaussians retain excessive source information, hindering optimization. To address these limitations, we propose EditSplat, a novel text-driven 3D scene editing framework that integrates Multi-view Fusion Guidance (MFG) and Attention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by incorporating essential multi-view information into the diffusion process, leveraging classifier-free guidance from the text-to-image diffusion model and the geometric structure inherent to 3DGS. Additionally, our AGT utilizes the explicit representation of 3DGS to selectively prune and optimize 3D Gaussians, enhancing optimization efficiency and enabling precise, semantically rich local editing. Through extensive qualitative and quantitative evaluations, EditSplat achieves state-of-the-art performance, establishing a new benchmark for text-driven 3D scene editing.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.9 -->
                    
                <!-- 3D: 9.6 -->
                    
                <!-- LLMs: 6.0 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.9931
                </span>
                <a href="https://arxiv.org/abs/2504.13521" target="_blank" rel="noopener noreferrer">Deep Learning Models Meet Financial Data Modalities</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kasymkhan Khubiev, Michail Semenov
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Algorithmic trading relies on extracting meaningful signals from diverse financial data sources, including candlestick charts, order statistics on put and canceled orders, traded volume data, limit order books, and news flow. While deep learning has demonstrated remarkable success in processing unst</span>
                
                <span class="abstract-full" style="display: none;">Algorithmic trading relies on extracting meaningful signals from diverse financial data sources, including candlestick charts, order statistics on put and canceled orders, traded volume data, limit order books, and news flow. While deep learning has demonstrated remarkable success in processing unstructured data and has significantly advanced natural language processing, its application to structured financial data remains an ongoing challenge. This study investigates the integration of deep learning models with financial data modalities, aiming to enhance predictive performance in trading strategies and portfolio optimization. We present a novel approach to incorporating limit order book analysis into algorithmic trading by developing embedding techniques and treating sequential limit order book snapshots as distinct input channels in an image-based representation. Our methodology for processing limit order book data achieves state-of-the-art performance in high-frequency trading algorithms, underscoring the effectiveness of deep learning in financial applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.6 -->
                    
                <!-- LLMs: 10.7 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- RAG: 2.2 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Attention: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.011
                </span>
                <a href="https://arxiv.org/abs/2504.11634" target="_blank" rel="noopener noreferrer">Doppler-SLAM: Doppler-Aided Radar-Inertial and LiDAR-Inertial Simultaneous Localization and Mapping</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dong Wang, Hannes Haag, Daniel Casado Herraez, Stefan May, Cyrill Stachniss, Andreas Nuechter
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Simultaneous localization and mapping (SLAM) is a critical capability for autonomous systems. Traditional SLAM approaches, which often rely on visual or LiDAR sensors, face significant challenges in adverse conditions such as low light or featureless environments. To overcome these limitations, we p</span>
                
                <span class="abstract-full" style="display: none;">Simultaneous localization and mapping (SLAM) is a critical capability for autonomous systems. Traditional SLAM approaches, which often rely on visual or LiDAR sensors, face significant challenges in adverse conditions such as low light or featureless environments. To overcome these limitations, we propose a novel Doppler-aided radar-inertial and LiDAR-inertial SLAM framework that leverages the complementary strengths of 4D radar, FMCW LiDAR, and inertial measurement units. Our system integrates Doppler velocity measurements and spatial data into a tightly-coupled front-end and graph optimization back-end to provide enhanced ego velocity estimation, accurate odometry, and robust mapping. We also introduce a Doppler-based scan-matching technique to improve front-end odometry in dynamic environments. In addition, our framework incorporates an innovative online extrinsic calibration mechanism, utilizing Doppler velocity and loop closure to dynamically maintain sensor alignment. Extensive evaluations on both public and proprietary datasets show that our system significantly outperforms state-of-the-art radar-SLAM and LiDAR-SLAM frameworks in terms of accuracy and robustness. To encourage further research, the code of our Doppler-SLAM and our dataset are available at: https://github.com/Wayne-DWA/Doppler-SLAM.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.0 -->
                    
                <!-- LLMs: 8.2 -->
                    
                <!-- 3D: 3.0 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0471
                </span>
                <a href="https://arxiv.org/abs/2504.13448" target="_blank" rel="noopener noreferrer">Ascribe New Dimensions to Scientific Data Visualization with VR</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Daniela Ushizima, Guilherme Melo dos Santos, Zineb Sordo, Ronald Pandolfi, Jeffrey Donatelli
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">For over half a century, the computer mouse has been the primary tool for interacting with digital data, yet it remains a limiting factor in exploring complex, multi-scale scientific images. Traditional 2D visualization methods hinder intuitive analysis of inherently 3D structures. Virtual Reality (</span>
                
                <span class="abstract-full" style="display: none;">For over half a century, the computer mouse has been the primary tool for interacting with digital data, yet it remains a limiting factor in exploring complex, multi-scale scientific images. Traditional 2D visualization methods hinder intuitive analysis of inherently 3D structures. Virtual Reality (VR) offers a transformative alternative, providing immersive, interactive environments that enhance data comprehension. This article introduces ASCRIBE-VR, a VR platform of Autonomous Solutions for Computational Research with Immersive Browsing \& Exploration, which integrates AI-driven algorithms with scientific images. ASCRIBE-VR enables multimodal analysis, structural assessments, and immersive visualization, supporting scientific visualization of advanced datasets such as X-ray CT, Magnetic Resonance, and synthetic 3D imaging. Our VR tools, compatible with Meta Quest, can consume the output of our AI-based segmentation and iterative feedback processes to enable seamless exploration of large-scale 3D images. By merging AI-generated results with VR visualization, ASCRIBE-VR enhances scientific discovery, bridging the gap between computational analysis and human intuition in materials research, connecting human-in-the-loop with digital twins.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.2 -->
                    
                <!-- LLMs: 6.6 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- 3D: 2.5 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.057
                </span>
                <a href="https://arxiv.org/abs/2504.13217" target="_blank" rel="noopener noreferrer">Sustainability via LLM Right-sizing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jennifer Haase, Finn Klessascheck, Jan Mendling, Sebastian Pokutta
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language models (LLMs) have become increasingly embedded in organizational workflows. This has raised concerns over their energy consumption, financial costs, and data sovereignty. While performance benchmarks often celebrate cutting-edge models, real-world deployment decisions require a broad</span>
                
                <span class="abstract-full" style="display: none;">Large language models (LLMs) have become increasingly embedded in organizational workflows. This has raised concerns over their energy consumption, financial costs, and data sovereignty. While performance benchmarks often celebrate cutting-edge models, real-world deployment decisions require a broader perspective: when is a smaller, locally deployable model "good enough"? This study offers an empirical answer by evaluating eleven proprietary and open-weight LLMs across ten everyday occupational tasks, including summarizing texts, generating schedules, and drafting emails and proposals. Using a dual-LLM-based evaluation framework, we automated task execution and standardized evaluation across ten criteria related to output quality, factual accuracy, and ethical responsibility. Results show that GPT-4o delivers consistently superior performance but at a significantly higher cost and environmental footprint. Notably, smaller models like Gemma-3 and Phi-4 achieved strong and reliable results on most tasks, suggesting their viability in contexts requiring cost-efficiency, local deployment, or privacy. A cluster analysis revealed three model groups -- premium all-rounders, competent generalists, and limited but safe performers -- highlighting trade-offs between quality, control, and sustainability. Significantly, task type influenced model effectiveness: conceptual tasks challenged most models, while aggregation and transformation tasks yielded better performances. We argue for a shift from performance-maximizing benchmarks to task- and context-aware sufficiency assessments that better reflect organizational priorities. Our approach contributes a scalable method to evaluate AI models through a sustainability lens and offers actionable guidance for responsible LLM deployment in practice.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 24.9 -->
                    
                <!-- Medicine: 11.4 -->
                    
                <!-- 3D: 3.5 -->
                    
                <!-- RAG: 2.4 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Attention: 1.0 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0639
                </span>
                <a href="https://arxiv.org/abs/2504.13805" target="_blank" rel="noopener noreferrer">LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration Benchmark</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Guangyi Liu, Pengxiang Zhao, Liang Liu, Zhiming Chen, Yuxiang Chai, Shuai Ren, Hao Wang, Shibo He, Wenchao Meng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Mobile GUI agents show promise in automating tasks but face generalization challenges in diverse real-world scenarios. Traditional approaches using pre-training or fine-tuning with massive datasets struggle with the diversity of mobile applications and user-specific tasks. We propose enhancing mobil</span>
                
                <span class="abstract-full" style="display: none;">Mobile GUI agents show promise in automating tasks but face generalization challenges in diverse real-world scenarios. Traditional approaches using pre-training or fine-tuning with massive datasets struggle with the diversity of mobile applications and user-specific tasks. We propose enhancing mobile GUI agent capabilities through human demonstrations, focusing on improving performance in unseen scenarios rather than pursuing universal generalization through larger datasets. To realize this paradigm, we introduce LearnGUI, the first comprehensive dataset specifically designed for studying demonstration-based learning in mobile GUI agents, comprising 2,252 offline tasks and 101 online tasks with high-quality human demonstrations. We further develop LearnAct, a sophisticated multi-agent framework that automatically extracts knowledge from demonstrations to enhance task completion. This framework integrates three specialized agents: DemoParser for knowledge extraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for demonstration-enhanced task execution. Our experimental results show significant performance gains in both offline and online evaluations. In offline assessments, a single demonstration improves model performance, increasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online evaluations, our framework enhances UI-TARS-7B-SFT's task success rate from 18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish demonstration-based learning as a promising direction for more adaptable, personalized, and deployable mobile GUI agents.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.2 -->
                    
                <!-- LLMs: 8.4 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0643
                </span>
                <a href="https://arxiv.org/abs/2504.13465" target="_blank" rel="noopener noreferrer">Are you SURE? Enhancing Multimodal Pretraining with Missing Modalities through Uncertainty Estimation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Duy A. Nguyen, Quan Huu Do, Khoa D. Doan, Minh N. Do
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multimodal learning has demonstrated incredible successes by integrating diverse data sources, yet it often relies on the availability of all modalities - an assumption that rarely holds in real-world applications. Pretrained multimodal models, while effective, struggle when confronted with small-sc</span>
                
                <span class="abstract-full" style="display: none;">Multimodal learning has demonstrated incredible successes by integrating diverse data sources, yet it often relies on the availability of all modalities - an assumption that rarely holds in real-world applications. Pretrained multimodal models, while effective, struggle when confronted with small-scale and incomplete datasets (i.e., missing modalities), limiting their practical applicability. Previous studies on reconstructing missing modalities have overlooked the reconstruction's potential unreliability, which could compromise the quality of the final outputs. We present SURE (Scalable Uncertainty and Reconstruction Estimation), a novel framework that extends the capabilities of pretrained multimodal models by introducing latent space reconstruction and uncertainty estimation for both reconstructed modalities and downstream tasks. Our method is architecture-agnostic, reconstructs missing modalities, and delivers reliable uncertainty estimates, improving both interpretability and performance. SURE introduces a unique Pearson Correlation-based loss and applies statistical error propagation in deep networks for the first time, allowing precise quantification of uncertainties from missing data and model predictions. Extensive experiments across tasks such as sentiment analysis, genre classification, and action recognition show that SURE consistently achieves state-of-the-art performance, ensuring robust predictions even in the presence of incomplete data.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.2 -->
                    
                <!-- LLMs: 9.6 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0982
                </span>
                <a href="https://arxiv.org/abs/2504.13442" target="_blank" rel="noopener noreferrer">SatelliteCalculator: A Multi-Task Vision Foundation Model for Quantitative Remote Sensing Inversion</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhenyu Yu, Mohd. Yamani Idna Idris, Pei Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Quantitative remote sensing inversion plays a critical role in environmental monitoring, enabling the estimation of key ecological variables such as vegetation indices, canopy structure, and carbon stock. Although vision foundation models have achieved remarkable progress in classification and segme</span>
                
                <span class="abstract-full" style="display: none;">Quantitative remote sensing inversion plays a critical role in environmental monitoring, enabling the estimation of key ecological variables such as vegetation indices, canopy structure, and carbon stock. Although vision foundation models have achieved remarkable progress in classification and segmentation tasks, their application to physically interpretable regression remains largely unexplored. Furthermore, the multi-spectral nature and geospatial heterogeneity of remote sensing data pose significant challenges for generalization and transferability. To address these issues, we introduce SatelliteCalculator, the first vision foundation model tailored for quantitative remote sensing inversion. By leveraging physically defined index formulas, we automatically construct a large-scale dataset of over one million paired samples across eight core ecological indicators. The model integrates a frozen Swin Transformer backbone with a prompt-guided architecture, featuring cross-attentive adapters and lightweight task-specific MLP decoders. Experiments on the Open-Canopy benchmark demonstrate that SatelliteCalculator achieves competitive accuracy across all tasks while significantly reducing inference cost. Our results validate the feasibility of applying foundation models to quantitative inversion, and provide a scalable framework for task-adaptive remote sensing estimation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.4 -->
                    
                <!-- LLMs: 6.9 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1125
                </span>
                <a href="https://arxiv.org/abs/2504.13220" target="_blank" rel="noopener noreferrer">SSTAF: Spatial-Spectral-Temporal Attention Fusion Transformer for Motor Imagery Classification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ummay Maria Muna, Md. Mehedi Hasan Shawon, Md Jobayer, Sumaiya Akter, Saifur Rahman Sabuj
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Brain-computer interfaces (BCI) in electroencephalography (EEG)-based motor imagery classification offer promising solutions in neurorehabilitation and assistive technologies by enabling communication between the brain and external devices. However, the non-stationary nature of EEG signals and signi</span>
                
                <span class="abstract-full" style="display: none;">Brain-computer interfaces (BCI) in electroencephalography (EEG)-based motor imagery classification offer promising solutions in neurorehabilitation and assistive technologies by enabling communication between the brain and external devices. However, the non-stationary nature of EEG signals and significant inter-subject variability cause substantial challenges for developing robust cross-subject classification models. This paper introduces a novel Spatial-Spectral-Temporal Attention Fusion (SSTAF) Transformer specifically designed for upper-limb motor imagery classification. Our architecture consists of a spectral transformer and a spatial transformer, followed by a transformer block and a classifier network. Each module is integrated with attention mechanisms that dynamically attend to the most discriminative patterns across multiple domains, such as spectral frequencies, spatial electrode locations, and temporal dynamics. The short-time Fourier transform is incorporated to extract features in the time-frequency domain to make it easier for the model to obtain a better feature distinction. We evaluated our SSTAF Transformer model on two publicly available datasets, the EEGMMIDB dataset, and BCI Competition IV-2a. SSTAF Transformer achieves an accuracy of 76.83% and 68.30% in the data sets, respectively, outperforms traditional CNN-based architectures and a few existing transformer-based approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.4 -->
                    
                <!-- LLMs: 4.3 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Attention: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1288
                </span>
                <a href="https://arxiv.org/abs/2504.13189" target="_blank" rel="noopener noreferrer">BASIR: Budget-Assisted Sectoral Impact Ranking -- A Dataset for Sector Identification and Performance Prediction Using Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sohom Ghosh, Sudip Kumar Naskar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Government fiscal policies, particularly annual union budgets, exert significant influence on financial markets. However, real-time analysis of budgetary impacts on sector-specific equity performance remains methodologically challenging and largely unexplored. This study proposes a framework to syst</span>
                
                <span class="abstract-full" style="display: none;">Government fiscal policies, particularly annual union budgets, exert significant influence on financial markets. However, real-time analysis of budgetary impacts on sector-specific equity performance remains methodologically challenging and largely unexplored. This study proposes a framework to systematically identify and rank sectors poised to benefit from India's Union Budget announcements. The framework addresses two core tasks: (1) multi-label classification of excerpts from budget transcripts into 81 predefined economic sectors, and (2) performance ranking of these sectors. Leveraging a comprehensive corpus of Indian Union Budget transcripts from 1947 to 2025, we introduce BASIR (Budget-Assisted Sectoral Impact Ranking), an annotated dataset mapping excerpts from budgetary transcripts to sectoral impacts. Our architecture incorporates fine-tuned embeddings for sector identification, coupled with language models that rank sectors based on their predicted performances. Our results demonstrate 0.605 F1-score in sector classification, and 0.997 NDCG score in predicting ranks of sectors based on post-budget performances. The methodology enables investors and policymakers to quantify fiscal policy impacts through structured, data-driven insights, addressing critical gaps in manual analysis. The annotated dataset has been released under CC-BY-NC-SA-4.0 license to advance computational economics research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 13.8 -->
                    
                <!-- Medicine: 11.0 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1374
                </span>
                <a href="https://arxiv.org/abs/2504.13267" target="_blank" rel="noopener noreferrer">Leveraging Functional Encryption and Deep Learning for Privacy-Preserving Traffic Forecasting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Isaac Adom, Mohammmad Iqbal Hossain, Hassan Mahmoud, Ahmad Alsharif, Mahmoud Nabil Mahmoud, Yang Xiao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Over the past few years, traffic congestion has continuously plagued the nation's transportation system creating several negative impacts including longer travel times, increased pollution rates, and higher collision risks. To overcome these challenges, Intelligent Transportation Systems (ITS) aim t</span>
                
                <span class="abstract-full" style="display: none;">Over the past few years, traffic congestion has continuously plagued the nation's transportation system creating several negative impacts including longer travel times, increased pollution rates, and higher collision risks. To overcome these challenges, Intelligent Transportation Systems (ITS) aim to improve mobility and vehicular systems, ensuring higher levels of safety by utilizing cutting-edge technologies, sophisticated sensing capabilities, and innovative algorithms. Drivers' participatory sensing, current/future location reporting, and machine learning algorithms have considerably improved real-time congestion monitoring and future traffic management. However, each driver's sensitive spatiotemporal location information can create serious privacy concerns. To address these challenges, we propose in this paper a secure, privacy-preserving location reporting and traffic forecasting system that guarantees privacy protection of driver data while maintaining high traffic forecasting accuracy. Our novel k-anonymity scheme utilizes functional encryption to aggregate encrypted location information submitted by drivers while ensuring the privacy of driver location data. Additionally, using the aggregated encrypted location information as input, this research proposes a deep learning model that incorporates a Convolutional-Long Short-Term Memory (Conv-LSTM) module to capture spatial and short-term temporal features and a Bidirectional Long Short-Term Memory (Bi-LSTM) module to recover long-term periodic patterns for traffic forecasting. With extensive evaluation on real datasets, we demonstrate the effectiveness of the proposed scheme with less than 10% mean absolute error for a 60-minute forecasting horizon, all while protecting driver privacy.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.5 -->
                    
                <!-- LLMs: 6.2 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1622
                </span>
                <a href="https://arxiv.org/abs/2504.13275" target="_blank" rel="noopener noreferrer">ChartQA-X: Generating Explanations for Charts</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shamanthak Hegde, Pooyan Fazli, Hasti Seifi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The ability to interpret and explain complex information from visual data in charts is crucial for data-driven decision-making. In this work, we address the challenge of providing explanations alongside answering questions about chart images. We present ChartQA-X, a comprehensive dataset comprising </span>
                
                <span class="abstract-full" style="display: none;">The ability to interpret and explain complex information from visual data in charts is crucial for data-driven decision-making. In this work, we address the challenge of providing explanations alongside answering questions about chart images. We present ChartQA-X, a comprehensive dataset comprising various chart types with 28,299 contextually relevant questions, answers, and detailed explanations. These explanations are generated by prompting six different models and selecting the best responses based on metrics such as faithfulness, informativeness, coherence, and perplexity. Our experiments show that models fine-tuned on our dataset for explanation generation achieve superior performance across various metrics and demonstrate improved accuracy in question-answering tasks on new datasets. By integrating answers with explanatory narratives, our approach enhances the ability of intelligent agents to convey complex information effectively, improve user understanding, and foster trust in the generated responses.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.6 -->
                    
                <!-- LLMs: 8.7 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.2516
                </span>
                <a href="https://arxiv.org/abs/2504.13768" target="_blank" rel="noopener noreferrer">Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vinay Sharma, R\'emi Tanguy Oddon, Pietro Tesini, Jens Ravesloot, Cees Taal, Olga Fink
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate real-time modeling of multi-body dynamical systems is essential for enabling digital twin applications across industries. While many data-driven approaches aim to learn system dynamics, jointly predicting internal loads and system trajectories remains a key challenge. This dual prediction i</span>
                
                <span class="abstract-full" style="display: none;">Accurate real-time modeling of multi-body dynamical systems is essential for enabling digital twin applications across industries. While many data-driven approaches aim to learn system dynamics, jointly predicting internal loads and system trajectories remains a key challenge. This dual prediction is especially important for fault detection and predictive maintenance, where internal loads-such as contact forces-act as early indicators of faults, reflecting wear or misalignment before affecting motion. These forces also serve as inputs to degradation models (e.g., crack growth), enabling damage prediction and remaining useful life estimation. We propose Equi-Euler GraphNet, a physics-informed graph neural network (GNN) that simultaneously predicts internal forces and global trajectories in multi-body systems. In this mesh-free framework, nodes represent system components and edges encode interactions. Equi-Euler GraphNet introduces two inductive biases: (1) an equivariant message-passing scheme, interpreting edge messages as interaction forces consistent under Euclidean transformations; and (2) a temporal-aware iterative node update mechanism, based on Euler integration, to capture influence of distant interactions over time. Tailored for cylindrical roller bearings, it decouples ring dynamics from constrained motion of rolling elements. Trained on high-fidelity multiphysics simulations, Equi-Euler GraphNet generalizes beyond the training distribution, accurately predicting loads and trajectories under unseen speeds, loads, and configurations. It outperforms state-of-the-art GNNs focused on trajectory prediction, delivering stable rollouts over thousands of time steps with minimal error accumulation. Achieving up to a 200x speedup over conventional solvers while maintaining comparable accuracy, it serves as an efficient reduced-order model for digital twins, design, and maintenance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.0 -->
                    
                <!-- LLMs: 6.1 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.2634
                </span>
                <a href="https://arxiv.org/abs/2504.13604" target="_blank" rel="noopener noreferrer">FocusTrack: A Self-Adaptive Local Sampling Algorithm for Efficient Anti-UAV Tracking</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ying Wang, Tingfa Xu, Jianan Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Anti-UAV tracking poses significant challenges, including small target sizes, abrupt camera motion, and cluttered infrared backgrounds. Existing tracking paradigms can be broadly categorized into global- and local-based methods. Global-based trackers, such as SiamDT, achieve high accuracy by scannin</span>
                
                <span class="abstract-full" style="display: none;">Anti-UAV tracking poses significant challenges, including small target sizes, abrupt camera motion, and cluttered infrared backgrounds. Existing tracking paradigms can be broadly categorized into global- and local-based methods. Global-based trackers, such as SiamDT, achieve high accuracy by scanning the entire field of view but suffer from excessive computational overhead, limiting real-world deployment. In contrast, local-based methods, including OSTrack and ROMTrack, efficiently restrict the search region but struggle when targets undergo significant displacements due to abrupt camera motion. Through preliminary experiments, it is evident that a local tracker, when paired with adaptive search region adjustment, can significantly enhance tracking accuracy, narrowing the gap between local and global trackers. To address this challenge, we propose FocusTrack, a novel framework that dynamically refines the search region and strengthens feature representations, achieving an optimal balance between computational efficiency and tracking accuracy. Specifically, our Search Region Adjustment (SRA) strategy estimates the target presence probability and adaptively adjusts the field of view, ensuring the target remains within focus. Furthermore, to counteract feature degradation caused by varying search regions, the Attention-to-Mask (ATM) module is proposed. This module integrates hierarchical information, enriching the target representations with fine-grained details. Experimental results demonstrate that FocusTrack achieves state-of-the-art performance, obtaining 67.7% AUC on AntiUAV and 62.8% AUC on AntiUAV410, outperforming the baseline tracker by 8.5% and 9.1% AUC, respectively. In terms of efficiency, FocusTrack surpasses global-based trackers, requiring only 30G MACs and achieving 143 fps with FocusTrack (SRA) and 44 fps with the full version, both enabling real-time tracking.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.0 -->
                    
                <!-- LLMs: 5.3 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3199
                </span>
                <a href="https://arxiv.org/abs/2504.13825" target="_blank" rel="noopener noreferrer">Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Junjie Yang, Junhao Song, Xudong Han, Ziqian Bi, Tianyang Wang, Chia Xin Liang, Xinyuan Song, Yichao Zhang, Qian Niu, Benji Peng, Keyu Chen, Ming Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Knowledge distillation (KD) is a technique for transferring knowledge from complex teacher models to simpler student models, significantly enhancing model efficiency and accuracy. It has demonstrated substantial advancements in various applications including image classification, object detection, l</span>
                
                <span class="abstract-full" style="display: none;">Knowledge distillation (KD) is a technique for transferring knowledge from complex teacher models to simpler student models, significantly enhancing model efficiency and accuracy. It has demonstrated substantial advancements in various applications including image classification, object detection, language modeling, text classification, and sentiment analysis. Recent innovations in KD methods, such as attention-based approaches, block-wise logit distillation, and decoupling distillation, have notably improved student model performance. These techniques focus on stimulus complexity, attention mechanisms, and global information capture to optimize knowledge transfer. In addition, KD has proven effective in compressing large language models while preserving accuracy, reducing computational overhead, and improving inference speed. This survey synthesizes the latest literature, highlighting key findings, contributions, and future directions in knowledge distillation to provide insights for researchers and practitioners on its evolving role in artificial intelligence and machine learning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 20.7 -->
                    
                <!-- Medicine: 12.6 -->
                    
                <!-- RAG: 3.3 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- T2I: 2.0 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Attention: 1.1 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3773
                </span>
                <a href="https://arxiv.org/abs/2504.13797" target="_blank" rel="noopener noreferrer">Meta-Learning and Knowledge Discovery based Physics-Informed Neural Network for Remaining Useful Life Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yu Wang, Shujie Liu, Shuai Lv, Gengshuo Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Predicting the remaining useful life (RUL) of rotating machinery is critical for industrial safety and maintenance, but existing methods struggle with scarce target-domain data and unclear degradation dynamics. We propose a Meta-Learning and Knowledge Discovery-based Physics-Informed Neural Network </span>
                
                <span class="abstract-full" style="display: none;">Predicting the remaining useful life (RUL) of rotating machinery is critical for industrial safety and maintenance, but existing methods struggle with scarce target-domain data and unclear degradation dynamics. We propose a Meta-Learning and Knowledge Discovery-based Physics-Informed Neural Network (MKDPINN) to address these challenges. The method first maps noisy sensor data to a low-dimensional hidden state space via a Hidden State Mapper (HSM). A Physics-Guided Regulator (PGR) then learns unknown nonlinear PDEs governing degradation evolution, embedding these physical constraints into the PINN framework. This integrates data-driven and physics-based approaches. The framework uses meta-learning, optimizing across source-domain meta-tasks to enable few-shot adaptation to new target tasks. Experiments on industrial data and the C-MAPSS benchmark show MKDPINN outperforms baselines in generalization and accuracy, proving its effectiveness for RUL prediction under data scarcity</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.5 -->
                    
                <!-- LLMs: 5.6 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3777
                </span>
                <a href="https://arxiv.org/abs/2504.13753" target="_blank" rel="noopener noreferrer">Gevrey class regularity for steady-state incompressible Navier-Stokes equations in parametric domains and related models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alexey Chernov, Tung Le
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We investigate parameteric Navier-Stokes equations for a viscous, incompressible flow in bounded domains. The coefficients of the equations are perturbed by high-dimensional random parameters, this fits in particular for modelling flows in domains with uncertain perturbations. Our focus is on derivi</span>
                
                <span class="abstract-full" style="display: none;">We investigate parameteric Navier-Stokes equations for a viscous, incompressible flow in bounded domains. The coefficients of the equations are perturbed by high-dimensional random parameters, this fits in particular for modelling flows in domains with uncertain perturbations. Our focus is on deriving bounds for arbitrary high-order derivatives of the pressure and the velocity fields with respect to the random parameters in the context of incompressible Navier-Stokes equation under a small-data assumption. To achieve this, we analyze mixed and saddle-point problems and employ the alternative-to-factorial technique to establish generalized Gevrey-class regularity for the solution pair. Thereby the analytic regularity follows as a special case. In the numerical experiments, we validate and illustrate our theoretical findings using Gauss-Legendre quadrature and Quasi-Monte Carlo methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.1 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.4145
                </span>
                <a href="https://arxiv.org/abs/2502.16542" target="_blank" rel="noopener noreferrer">Variable transformations in consistent loss functions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hristos Tyralis, Georgia Papacharalampous
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Loss functions constructed by applying transformations to the realization and prediction variables of (strictly) consistent loss functions have been extensively studied empirically, yet their theoretical foundations remain unexplored. To address this gap, we establish formal characterizations of (st</span>
                
                <span class="abstract-full" style="display: none;">Loss functions constructed by applying transformations to the realization and prediction variables of (strictly) consistent loss functions have been extensively studied empirically, yet their theoretical foundations remain unexplored. To address this gap, we establish formal characterizations of (strict) consistency for such transformed loss functions and their corresponding elicitable functionals. Our analysis focuses on two interrelated cases: (a) transformations applied solely to the realization variable and (b) bijective transformations applied jointly to both the realization and prediction variables. These cases extend the well-established framework of transformations applied exclusively to the prediction variable, as formalized by Osband's revelation principle. We further develop analogous characterizations for (strict) identification functions. The resulting theoretical framework is broadly applicable to statistical and machine learning methodologies. When applied to Bregman and expectile loss functions, our framework enables two key advancements: (a) the interpretation of empirical findings from models trained with transformed loss functions and (b) the systematic construction of novel identifiable and elicitable functionals, including the g-transformed expectation and g-transformed expectile. By unifying theoretical insights with practical applications, this work advances principled methodologies for designing loss functions in complex predictive tasks. Applications of the framework to simulated and real-world data illustrate its practical utility in diverse settings.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.2 -->
                    
                <!-- LLMs: 6.8 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.4237
                </span>
                <a href="https://arxiv.org/abs/2504.13762" target="_blank" rel="noopener noreferrer">Models, Methods and Waveforms for Estimation and Prediction of Doubly Sparse Time-Varying Channels</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wissal Benzine, Ali Bemani, Nassar Ksairi, Dirk Slock
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper investigates channel estimation for linear time-varying (LTV) wireless channels under double sparsity, i.e., sparsity in both the delay and Doppler domains. An on-grid approximation is first considered, enabling rigorous hierarchical-sparsity modeling and compressed sensing-based channel </span>
                
                <span class="abstract-full" style="display: none;">This paper investigates channel estimation for linear time-varying (LTV) wireless channels under double sparsity, i.e., sparsity in both the delay and Doppler domains. An on-grid approximation is first considered, enabling rigorous hierarchical-sparsity modeling and compressed sensing-based channel estimation. Guaranteed recovery conditions are provided for affine frequency division multiplexing (AFDM), orthogonal frequency division multiplexing (OFDM) and single-carrier modulation (SCM), highlighting the superiority of AFDM in terms of doubly sparse channel estimation. To address arbitrary Doppler shifts, a relaxed version of the on-grid model is introduced by making use of multiple elementary Expansion Models (BEM) each based on Discrete Prolate Spheroidal Sequences (DPSS). Next, theoretical guarantees are provided for the precision of this off-grid model before further extending it to tackle channel prediction by exploiting the inherent DPSS extrapolation capability. Finally, numerical results are provided to both validate the proposed off-grid model for channel estimation and prediction purposes under the double sparsity assumption and to compare the corresponding mean squared error (MSE) and the overhead performance when the different wireless waveforms are used.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.3 -->
                    
                <!-- LLMs: 5.3 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.45
                </span>
                <a href="https://arxiv.org/abs/2504.13599" target="_blank" rel="noopener noreferrer">ViG3D-UNet: Volumetric Vascular Connectivity-Aware Segmentation via 3D Vision Graph Representation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bowen Liu, Chunlei Meng, Wei Lin, Hongda Zhang, Ziqing Zhou, Zhongxue Gan, Chun Ouyang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate vascular segmentation is essential for coronary visualization and the diagnosis of coronary heart disease. This task involves the extraction of sparse tree-like vascular branches from the volumetric space. However, existing methods have faced significant challenges due to discontinuous vasc</span>
                
                <span class="abstract-full" style="display: none;">Accurate vascular segmentation is essential for coronary visualization and the diagnosis of coronary heart disease. This task involves the extraction of sparse tree-like vascular branches from the volumetric space. However, existing methods have faced significant challenges due to discontinuous vascular segmentation and missing endpoints. To address this issue, a 3D vision graph neural network framework, named ViG3D-UNet, was introduced. This method integrates 3D graph representation and aggregation within a U-shaped architecture to facilitate continuous vascular segmentation. The ViG3D module captures volumetric vascular connectivity and topology, while the convolutional module extracts fine vascular details. These two branches are combined through channel attention to form the encoder feature. Subsequently, a paperclip-shaped offset decoder minimizes redundant computations in the sparse feature space and restores the feature map size to match the original input dimensions. To evaluate the effectiveness of the proposed approach for continuous vascular segmentation, evaluations were performed on two public datasets, ASOCA and ImageCAS. The segmentation results show that the ViG3D-UNet surpassed competing methods in maintaining vascular segmentation connectivity while achieving high segmentation accuracy. Our code will be available soon.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.6 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.4521
                </span>
                <a href="https://arxiv.org/abs/2504.13582" target="_blank" rel="noopener noreferrer">Hysteresis-Aware Neural Network Modeling and Whole-Body Reinforcement Learning Control of Soft Robots</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zongyuan Chen, Yan Xia, Jiayuan Liu, Jijia Liu, Wenhao Tang, Jiayu Chen, Feng Gao, Longfei Ma, Hongen Liao, Yu Wang, Chao Yu, Boyu Zhang, Fei Xing
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Soft robots exhibit inherent compliance and safety, which makes them particularly suitable for applications requiring direct physical interaction with humans, such as surgical procedures. However, their nonlinear and hysteretic behavior, resulting from the properties of soft materials, presents subs</span>
                
                <span class="abstract-full" style="display: none;">Soft robots exhibit inherent compliance and safety, which makes them particularly suitable for applications requiring direct physical interaction with humans, such as surgical procedures. However, their nonlinear and hysteretic behavior, resulting from the properties of soft materials, presents substantial challenges for accurate modeling and control. In this study, we present a soft robotic system designed for surgical applications and propose a hysteresis-aware whole-body neural network model that accurately captures and predicts the soft robot's whole-body motion, including its hysteretic behavior. Building upon the high-precision dynamic model, we construct a highly parallel simulation environment for soft robot control and apply an on-policy reinforcement learning algorithm to efficiently train whole-body motion control strategies. Based on the trained control policy, we developed a soft robotic system for surgical applications and validated it through phantom-based laser ablation experiments in a physical environment. The results demonstrate that the hysteresis-aware modeling reduces the Mean Squared Error (MSE) by 84.95 percent compared to traditional modeling methods. The deployed control algorithm achieved a trajectory tracking error ranging from 0.126 to 0.250 mm on the real soft robot, highlighting its precision in real-world conditions. The proposed method showed strong performance in phantom-based surgical experiments and demonstrates its potential for complex scenarios, including future real-world clinical applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.7 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.4704
                </span>
                <a href="https://arxiv.org/abs/2504.08217" target="_blank" rel="noopener noreferrer">DrivAer Transformer: A high-precision and fast prediction method for vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jiaqi He, Xiangwen Luo, Yiping Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">At the current stage, deep learning-based methods have demonstrated excellent capabilities in evaluating aerodynamic performance, significantly reducing the time and cost required for traditional computational fluid dynamics (CFD) simulations. However, when faced with the task of processing extremel</span>
                
                <span class="abstract-full" style="display: none;">At the current stage, deep learning-based methods have demonstrated excellent capabilities in evaluating aerodynamic performance, significantly reducing the time and cost required for traditional computational fluid dynamics (CFD) simulations. However, when faced with the task of processing extremely complex three-dimensional (3D) vehicle models, the lack of large-scale datasets and training resources, coupled with the inherent diversity and complexity of the geometry of different vehicle models, means that the prediction accuracy and versatility of these networks are still not up to the level required for current production. In view of the remarkable success of Transformer models in the field of natural language processing and their strong potential in the field of image processing, this study innovatively proposes a point cloud learning framework called DrivAer Transformer (DAT). The DAT structure uses the DrivAerNet++ dataset, which contains high-fidelity CFD data of industrial-standard 3D vehicle shapes. enabling accurate estimation of air drag directly from 3D meshes, thus avoiding the limitations of traditional methods such as 2D image rendering or signed distance fields (SDF). DAT enables fast and accurate drag prediction, driving the evolution of the aerodynamic evaluation process and laying the critical foundation for introducing a data-driven approach to automotive design. The framework is expected to accelerate the vehicle design process and improve development efficiency.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.4 -->
                    
                <!-- LLMs: 5.9 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Math: 2.4 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.5041
                </span>
                <a href="https://arxiv.org/abs/2503.21392" target="_blank" rel="noopener noreferrer">HybridoNet-Adapt: A Domain-Adapted Framework for Accurate Lithium-Ion Battery RUL Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Khoa Tran, Bao Huynh, Tri Le, Lam Pham, Vy-Rin Nguyen, Hung-Cuong Trinh, Duong Tran Anh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate prediction of the Remaining Useful Life (RUL) in Lithium ion battery (LIB) health management systems is essential for ensuring operational reliability and safety. However, many existing methods assume that training and testing data follow the same distribution, limiting their ability to gen</span>
                
                <span class="abstract-full" style="display: none;">Accurate prediction of the Remaining Useful Life (RUL) in Lithium ion battery (LIB) health management systems is essential for ensuring operational reliability and safety. However, many existing methods assume that training and testing data follow the same distribution, limiting their ability to generalize to unseen target domains. To address this, we propose a novel RUL prediction framework that incorporates a domain adaptation (DA) technique. Our framework integrates a signal preprocessing pipeline including noise reduction, feature extraction, and normalization with a robust deep learning model called HybridoNet Adapt. The model features a combination of LSTM, Multihead Attention, and Neural ODE layers for feature extraction, followed by two predictor modules with trainable trade-off parameters. To improve generalization, we adopt a DA strategy inspired by Domain Adversarial Neural Networks (DANN), replacing adversarial loss with Maximum Mean Discrepancy (MMD) to learn domain-invariant features. Experimental results show that HybridoNet Adapt significantly outperforms traditional models such as XGBoost and Elastic Net, as well as deep learning baselines like Dual input DNN, demonstrating its potential for scalable and reliable battery health management (BHM).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.0 -->
                    
                <!-- LLMs: 5.0 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- 3D: 2.9 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.5257
                </span>
                <a href="https://arxiv.org/abs/2504.13469" target="_blank" rel="noopener noreferrer">HMPE:HeatMap Embedding for Efficient Transformer-Based Small Object Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: YangChen Zeng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Current Transformer-based methods for small object detection continue emerging, yet they have still exhibited significant shortcomings. This paper introduces HeatMap Position Embedding (HMPE), a novel Transformer Optimization technique that enhances object detection performance by dynamically integr</span>
                
                <span class="abstract-full" style="display: none;">Current Transformer-based methods for small object detection continue emerging, yet they have still exhibited significant shortcomings. This paper introduces HeatMap Position Embedding (HMPE), a novel Transformer Optimization technique that enhances object detection performance by dynamically integrating positional encoding with semantic detection information through heatmap-guided adaptive learning.We also innovatively visualize the HMPE method, offering clear visualization of embedded information for parameter fine-tuning.We then create Multi-Scale ObjectBox-Heatmap Fusion Encoder (MOHFE) and HeatMap Induced High-Quality Queries for Decoder (HIDQ) modules. These are designed for the encoder and decoder, respectively, to generate high-quality queries and reduce background noise queries.Using both heatmap embedding and Linear-Snake Conv(LSConv) feature engineering, we enhance the embedding of massively diverse small object categories and reduced the decoder multihead layers, thereby accelerating both inference and training.In the generalization experiments, our approach outperforme the baseline mAP by 1.9% on the small object dataset (NWPU VHR-10) and by 1.2% on the general dataset (PASCAL VOC). By employing HMPE-enhanced embedding, we are able to reduce the number of decoder layers from eight to a minimum of three, significantly decreasing both inference and training costs.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.8 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.5802
                </span>
                <a href="https://arxiv.org/abs/2504.13373" target="_blank" rel="noopener noreferrer">Geometric adaptive smoothed aggregation multigrid for discontinuous Galerkin discretisations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yulong Pan, Michael Lindsey, Per-Olof Persson
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a geometric multigrid solver based on adaptive smoothed aggregation suitable for Discontinuous Galerkin (DG) discretisations. Mesh hierarchies are formed via domain decomposition techniques, and the method is applicable to fully unstructured meshes using arbitrary element shapes. Furtherm</span>
                
                <span class="abstract-full" style="display: none;">We present a geometric multigrid solver based on adaptive smoothed aggregation suitable for Discontinuous Galerkin (DG) discretisations. Mesh hierarchies are formed via domain decomposition techniques, and the method is applicable to fully unstructured meshes using arbitrary element shapes. Furthermore, the method can be employed for a wide range of commonly used DG numerical fluxes for first- and second-order PDEs including the Interior Penalty and the Local Discontinuous Galerkin methods. We demonstrate excellent and mesh-independent convergence for a range of problems including the Poisson equation, and convection-diffusion for a range of P\'eclet numbers.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.9 -->
                    
                <!-- LLMs: 3.6 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.5943
                </span>
                <a href="https://arxiv.org/abs/2502.19413" target="_blank" rel="noopener noreferrer">Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Christoph Schuhmann, Gollam Rabby, Ameya Prabhu, Tawsif Ahmed, Andreas Hochlehnert, Huu Nguyen, Nick Akinci, Ludwig Schmidt, Robert Kaczmarczyk, S\"oren Auer, Jenia Jitsev, Matthias Bethge
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Paywalls, licenses and copyright rules often restrict the broad dissemination and reuse of scientific knowledge. We take the position that it is both legally and technically feasible to extract the scientific knowledge in scholarly texts. Current methods, like text embeddings, fail to reliably prese</span>
                
                <span class="abstract-full" style="display: none;">Paywalls, licenses and copyright rules often restrict the broad dissemination and reuse of scientific knowledge. We take the position that it is both legally and technically feasible to extract the scientific knowledge in scholarly texts. Current methods, like text embeddings, fail to reliably preserve factual content, and simple paraphrasing may not be legally sound. We propose a new idea for the community to adopt: convert scholarly documents into knowledge preserving, but style agnostic representations we term Knowledge Units using LLMs. These units use structured data capturing entities, attributes and relationships without stylistic content. We provide evidence that Knowledge Units (1) form a legally defensible framework for sharing knowledge from copyrighted research texts, based on legal analyses of German copyright law and U.S. Fair Use doctrine, and (2) preserve most (~95\%) factual knowledge from original text, measured by MCQ performance on facts from the original copyrighted text across four research domains. Freeing scientific knowledge from copyright promises transformative benefits for scientific research and education by allowing language models to reuse important facts from copyrighted text. To support this, we share open-source tools for converting research documents into Knowledge Units. Overall, our work posits the feasibility of democratizing access to scientific knowledge while respecting copyright.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.4 -->
                    
                <!-- LLMs: 10.0 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.6064
                </span>
                <a href="https://arxiv.org/abs/2504.13423" target="_blank" rel="noopener noreferrer">Mixed Fractional Information: Consistency of Dissipation Measures for Stable Laws</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: William Cook
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Symmetric alpha-stable (S alpha S) distributions with alpha<2 lack finite classical Fisher information. Building on Johnson's framework, we define Mixed Fractional Information (MFI) via the initial rate of relative entropy dissipation during interpolation between S alpha S laws with differing scales</span>
                
                <span class="abstract-full" style="display: none;">Symmetric alpha-stable (S alpha S) distributions with alpha<2 lack finite classical Fisher information. Building on Johnson's framework, we define Mixed Fractional Information (MFI) via the initial rate of relative entropy dissipation during interpolation between S alpha S laws with differing scales, v and s. We demonstrate two equivalent formulations for MFI in this specific S alpha S-to-S alpha S setting. The first involves the derivative D'(v) of the relative entropy between the two S alpha S densities. The second uses an integral expectation E_gv[u(x,0) (pF_v(x) - pF_s(x))] involving the difference between Fisher scores (pF_v, pF_s) and a specific MMSE-related score function u(x,0) derived from the interpolation dynamics. Our central contribution is a rigorous proof of the consistency identity: D'(v) = (1/(alpha v)) E_gv[X (pF_v(X) - pF_s(X))]. This identity mathematically validates the equivalence of the two MFI formulations for S alpha S inputs, establishing MFI's internal coherence and directly linking entropy dissipation rates to score function differences. We further establish MFI's non-negativity (zero if and only if v=s), derive its closed-form expression for the Cauchy case (alpha=1), and numerically validate the consistency identity. MFI provides a finite, coherent, and computable information-theoretic measure for comparing S alpha S distributions where classical Fisher information fails, connecting entropy dynamics to score functions and estimation concepts. This work lays a foundation for exploring potential fractional I-MMSE relations and new functional inequalities tailored to heavy-tailed systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.8 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.6745
                </span>
                <a href="https://arxiv.org/abs/2504.13330" target="_blank" rel="noopener noreferrer">The Future of Work is Blended, Not Hybrid</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Marios Constantinides, Himanshu Verma, Shadan Sadeghian, Abdallah El Ali
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The way we work is no longer hybrid -- it is blended with AI co-workers, automated decisions, and virtual presence reshaping human roles, agency, and expertise. We now work through AI, with our outputs shaped by invisible algorithms. AI's infiltration into knowledge, creative, and service work is no</span>
                
                <span class="abstract-full" style="display: none;">The way we work is no longer hybrid -- it is blended with AI co-workers, automated decisions, and virtual presence reshaping human roles, agency, and expertise. We now work through AI, with our outputs shaped by invisible algorithms. AI's infiltration into knowledge, creative, and service work is not just about automation, but concerns redistribution of agency, creativity, and control. How do we deal with physical and distributed AI-mediated workspaces? What happens when algorithms co-author reports, and draft our creative work? In this provocation, we argue that hybrid work is obsolete. Blended work is the future, not just in physical and virtual spaces but in how human effort and AI output become inseparable. We argue this shift demands urgent attention to AI-mediated work practices, work-life boundaries, physical-digital interactions, and AI transparency and accountability. The question is not whether we accept it, but whether we actively shape it before it shapes us.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.0 -->
                    
                <!-- LLMs: 12.9 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.6812
                </span>
                <a href="https://arxiv.org/abs/2504.13229" target="_blank" rel="noopener noreferrer">PSG-MAE: Robust Multitask Sleep Event Monitoring using Multichannel PSG Reconstruction and Inter-channel Contrastive Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yifei Wang, Qi Liu, Fuli Min, Honghao Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Polysomnography (PSG) signals are essential for studying sleep processes and diagnosing sleep disorders. Analyzing PSG data through deep neural networks (DNNs) for automated sleep monitoring has become increasingly feasible. However, the limited availability of datasets for certain sleep events ofte</span>
                
                <span class="abstract-full" style="display: none;">Polysomnography (PSG) signals are essential for studying sleep processes and diagnosing sleep disorders. Analyzing PSG data through deep neural networks (DNNs) for automated sleep monitoring has become increasingly feasible. However, the limited availability of datasets for certain sleep events often leads to DNNs focusing on a single task with a single-sourced training dataset. As a result, these models struggle to transfer to new sleep events and lack robustness when applied to new datasets. To address these challenges, we propose PSG-MAE, a mask autoencoder (MAE) based pre-training framework. By performing self-supervised learning on a large volume of unlabeled PSG data, PSG-MAE develops a robust feature extraction network that can be broadly applied to various sleep event monitoring tasks. Unlike conventional MAEs, PSG-MAE generates complementary masks across PSG channels, integrates a multichannel signal reconstruction method, and employs a self-supervised inter-channel contrastive learning (ICCL) strategy. This approach enables the encoder to capture temporal features from each channel while simultaneously learning latent relationships between channels, thereby enhancing the utilization of multichannel information. Experimental results show that PSG-MAE effectively captures both temporal details and inter-channel information from PSG signals. When the encoder pre-trained through PSG-MAE is fine-tuned with downstream feature decomposition networks, it achieves an accuracy of 83.7% for sleep staging and 90.45% for detecting obstructive sleep apnea, which highlights the framework's robustness and broad applicability.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.3 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.7459
                </span>
                <a href="https://arxiv.org/abs/2504.13402" target="_blank" rel="noopener noreferrer">CytoFM: The first cytology foundation model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vedrana Ivezi\'c, Ashwath Radhachandran, Ekaterina Redekop, Shreeram Athreya, Dongwoo Lee, Vivek Sant, Corey Arnold, William Speier
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Cytology is essential for cancer diagnostics and screening due to its minimally invasive nature. However, the development of robust deep learning models for digital cytology is challenging due to the heterogeneity in staining and preparation methods of samples, differences across organs, and the lim</span>
                
                <span class="abstract-full" style="display: none;">Cytology is essential for cancer diagnostics and screening due to its minimally invasive nature. However, the development of robust deep learning models for digital cytology is challenging due to the heterogeneity in staining and preparation methods of samples, differences across organs, and the limited availability of large, diverse, annotated datasets. Developing a task-specific model for every cytology application is impractical and non-cytology-specific foundation models struggle to generalize to tasks in this domain where the emphasis is on cell morphology. To address these challenges, we introduce CytoFM, the first cytology self-supervised foundation model. Using iBOT, a self-supervised Vision Transformer (ViT) training framework incorporating masked image modeling and self-distillation, we pretrain CytoFM on a diverse collection of cytology datasets to learn robust, transferable representations. We evaluate CytoFM on multiple downstream cytology tasks, including breast cancer classification and cell type identification, using an attention-based multiple instance learning framework. Our results demonstrate that CytoFM performs better on two out of three downstream tasks than existing foundation models pretrained on histopathology (UNI) or natural images (iBOT-Imagenet). Visualizations of learned representations demonstrate our model is able to attend to cytologically relevant features. Despite a small pre-training dataset, CytoFM's promising results highlight the ability of task-agnostic pre-training approaches to learn robust and generalizable features from cytology data.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.5 -->
                    
                <!-- LLMs: 6.0 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.7586
                </span>
                <a href="https://arxiv.org/abs/2504.13348" target="_blank" rel="noopener noreferrer">Physical Reservoir Computing in Hook-Shaped Rover Wheel Spokes for Real-Time Terrain Identification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiao Jin, Zihan Wang, Zhenhua Yu, Changrak Choi, Kalind Carpenter, Thrishantha Nanayakkara
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Effective terrain detection in unknown environments is crucial for safe and efficient robotic navigation. Traditional methods often rely on computationally intensive data processing, requiring extensive onboard computational capacity and limiting real-time performance for rovers. This study presents</span>
                
                <span class="abstract-full" style="display: none;">Effective terrain detection in unknown environments is crucial for safe and efficient robotic navigation. Traditional methods often rely on computationally intensive data processing, requiring extensive onboard computational capacity and limiting real-time performance for rovers. This study presents a novel approach that combines physical reservoir computing with piezoelectric sensors embedded in rover wheel spokes for real-time terrain identification. By leveraging wheel dynamics, terrain-induced vibrations are transformed into high-dimensional features for machine learning-based classification. Experimental results show that strategically placing three sensors on the wheel spokes achieves 90$\%$ classification accuracy, which demonstrates the accuracy and feasibility of the proposed method. The experiment results also showed that the system can effectively distinguish known terrains and identify unknown terrains by analyzing their similarity to learned categories. This method provides a robust, low-power framework for real-time terrain classification and roughness estimation in unstructured environments, enhancing rover autonomy and adaptability.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.0 -->
                    
                <!-- LLMs: 4.8 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.7712
                </span>
                <a href="https://arxiv.org/abs/2504.13735" target="_blank" rel="noopener noreferrer">Orientation and mobility test in virtual reality, a tool for quantitative assessment of functional vision: dataset and evaluation in healthy subjects</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yujie Huang, Audrey Crozet, Toinon Vigier, Alexandre Bruckert, Patrick Le Callet, Pierre Lebranchu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The purpose of this study was to develop and evaluate a novel virtual reality seated orientation and mobility (VR-S-O&amp;M) test protocol designed to assess functional vision. This study aims to provide a dataset of healthy subjects using this protocol and preliminary analyses. We introduced a VR-b</span>
                
                <span class="abstract-full" style="display: none;">The purpose of this study was to develop and evaluate a novel virtual reality seated orientation and mobility (VR-S-O&amp;M) test protocol designed to assess functional vision. This study aims to provide a dataset of healthy subjects using this protocol and preliminary analyses. We introduced a VR-based O&amp;M test protocol featuring a novel seated displacement method, diverse lighting conditions, and varying course configurations within a virtual environment. Normally sighted participants (N=42) completed the test, which required them to navigate a path and destroy identified obstacles. We assessed basic performance metrics, including time duration, number of missed objects, and time before the first step, under different environmental conditions to verify ecological validity. Additionally, we analyzed participants' behaviors regarding missed objects, demonstrating the potential of integrating behavioral and interactive data for a more precise functional vision assessment. Our VR-S-O&amp;M test protocol, along with the first O&amp;M behavior dataset, presents significant opportunities for developing more refined performance metrics for assessing functional vision and enhancing the quality of life.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.1 -->
                    
                <!-- LLMs: 5.9 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8244
                </span>
                <a href="https://arxiv.org/abs/2504.13597" target="_blank" rel="noopener noreferrer">FocusNet: Transformer-enhanced Polyp Segmentation with Local and Pooling Attention</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jun Zeng, KC Santosh, Deepak Rajan Nayak, Thomas de Lange, Jonas Varkey, Tyler Berzin, Debesh Jha
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Colonoscopy is vital in the early diagnosis of colorectal polyps. Regular screenings can effectively prevent benign polyps from progressing to CRC. While deep learning has made impressive strides in polyp segmentation, most existing models are trained on single-modality and single-center data, makin</span>
                
                <span class="abstract-full" style="display: none;">Colonoscopy is vital in the early diagnosis of colorectal polyps. Regular screenings can effectively prevent benign polyps from progressing to CRC. While deep learning has made impressive strides in polyp segmentation, most existing models are trained on single-modality and single-center data, making them less effective in real-world clinical environments. To overcome these limitations, we propose FocusNet, a Transformer-enhanced focus attention network designed to improve polyp segmentation. FocusNet incorporates three essential modules: the Cross-semantic Interaction Decoder Module (CIDM) for generating coarse segmentation maps, the Detail Enhancement Module (DEM) for refining shallow features, and the Focus Attention Module (FAM), to balance local detail and global context through local and pooling attention mechanisms. We evaluate our model on PolypDB, a newly introduced dataset with multi-modality and multi-center data for building more reliable segmentation methods. Extensive experiments showed that FocusNet consistently outperforms existing state-of-the-art approaches with a high dice coefficients of 82.47% on the BLI modality, 88.46% on FICE, 92.04% on LCI, 82.09% on the NBI and 93.42% on WLI modality, demonstrating its accuracy and robustness across five different modalities. The source code for FocusNet is available at https://github.com/JunZengz/FocusNet.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.6 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8304
                </span>
                <a href="https://arxiv.org/abs/2504.06587" target="_blank" rel="noopener noreferrer">SigChord: Sniffing Wide Non-sparse Multiband Signals for Terrestrial and Non-terrestrial Wireless Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jinbo Peng, Junwen Duan, Zheng Lin, Haoxuan Yuan, Yue Gao, Zhe Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While unencrypted information inspection in physical layer (e.g., open headers) can provide deep insights for optimizing wireless networks, the state-of-the-art (SOTA) methods heavily depend on full sampling rate (a.k.a Nyquist rate), and high-cost radios, due to terrestrial and non-terrestrial netw</span>
                
                <span class="abstract-full" style="display: none;">While unencrypted information inspection in physical layer (e.g., open headers) can provide deep insights for optimizing wireless networks, the state-of-the-art (SOTA) methods heavily depend on full sampling rate (a.k.a Nyquist rate), and high-cost radios, due to terrestrial and non-terrestrial networks densely occupying multiple bands across large bandwidth (e.g., from 4G/5G at 0.4-7 GHz to LEO satellite at 4-40 GHz). To this end, we present SigChord, an efficient physical layer inspection system built on low-cost and sub-Nyquist sampling radios. We first design a deep and rule-based interleaving algorithm based on Transformer network to perform spectrum sensing and signal recovery under sub-Nyquist sampling rate, and second, cascade protocol identifier and decoder based on Transformer neural networks to help physical layer packets analysis. We implement SigChord using software-defined radio platforms, and extensively evaluate it on over-the-air terrestrial and non-terrestrial wireless signals. The experiments demonstrate that SigChord delivers over 99% accuracy in detecting and decoding, while still decreasing 34% sampling rate, compared with the SOTA approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.0 -->
                    
                <!-- LLMs: 6.4 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- 3D: 2.4 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8339
                </span>
                <a href="https://arxiv.org/abs/2408.08070" target="_blank" rel="noopener noreferrer">MambaMIM: Pre-training Mamba with State Space Token Interpolation and its Application to Medical Image Segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fenghe Tang, Bingkun Nian, Yingtai Li, Zihang Jiang, Jie Yang, Wei Liu, S. Kevin Zhou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recently, the state space model Mamba has demonstrated efficient long-sequence modeling capabilities, particularly for addressing long-sequence visual tasks in 3D medical imaging. However, existing generative self-supervised learning methods have not yet fully unleashed Mamba's potential for handlin</span>
                
                <span class="abstract-full" style="display: none;">Recently, the state space model Mamba has demonstrated efficient long-sequence modeling capabilities, particularly for addressing long-sequence visual tasks in 3D medical imaging. However, existing generative self-supervised learning methods have not yet fully unleashed Mamba's potential for handling long-range dependencies because they overlook the inherent causal properties of state space sequences in masked modeling. To address this challenge, we propose a general-purpose pre-training framework called MambaMIM, a masked image modeling method based on a novel TOKen-Interpolation strategy (TOKI) for the selective structure state space sequence, which learns causal relationships of state space within the masked sequence. Further, MambaMIM introduces a bottom-up 3D hybrid masking strategy to maintain a masking consistency across different architectures and can be used on any single or hybrid Mamba architecture to enhance its multi-scale and long-range representation capability. We pre-train MambaMIM on a large-scale dataset of 6.8K CT scans and evaluate its performance across eight public medical segmentation benchmarks. Extensive downstream experiments reveal the feasibility and advancement of using Mamba for medical image pre-training. In particular, when we apply the MambaMIM to a customized architecture that hybridizes MedNeXt and Vision Mamba, we consistently obtain the state-of-the-art segmentation performance. The code is available at: https://github.com/FengheTan9/MambaMIM.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.1 -->
                    
                <!-- LLMs: 4.5 -->
                    
                <!-- 3D: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8409
                </span>
                <a href="https://arxiv.org/abs/2504.13754" target="_blank" rel="noopener noreferrer">Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhu Zhu, Shuo Jiang, Jingyuan Zheng, Yawen Li, Yifei Chen, Manli Zhao, Weizhong Gu, Feiwei Qin, Jinhu Wang, Gang Yu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Neuroblastoma, adrenal-derived, is among the most common pediatric solid malignancies, characterized by significant clinical heterogeneity. Timely and accurate pathological diagnosis from hematoxylin and eosin-stained whole slide images is critical for patient prognosis. However, current diagnostic </span>
                
                <span class="abstract-full" style="display: none;">Neuroblastoma, adrenal-derived, is among the most common pediatric solid malignancies, characterized by significant clinical heterogeneity. Timely and accurate pathological diagnosis from hematoxylin and eosin-stained whole slide images is critical for patient prognosis. However, current diagnostic practices primarily rely on subjective manual examination by pathologists, leading to inconsistent accuracy. Existing automated whole slide image classification methods encounter challenges such as poor interpretability, limited feature extraction capabilities, and high computational costs, restricting their practical clinical deployment. To overcome these limitations, we propose CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model tailored for pathological image classification, which enhances the Swin Transformer architecture by integrating a Kernel Activation Network within its multilayer perceptron and classification head modules, significantly improving both interpretability and accuracy. By fusing multi-scale features and leveraging contrastive learning strategies, CMSwinKAN mimics clinicians' comprehensive approach, effectively capturing global and local tissue characteristics. Additionally, we introduce a heuristic soft voting mechanism guided by clinical insights to seamlessly bridge patch-level predictions to whole slide image-level classifications. We validate CMSwinKAN on the PpNTs dataset, which was collaboratively established with our partner hospital and the publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN performs better than existing state-of-the-art pathology-specific models pre-trained on large datasets. Our source code is available at https://github.com/JSLiam94/CMSwinKAN.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.4 -->
                    
                <!-- LLMs: 5.3 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- 3D: 2.7 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8439
                </span>
                <a href="https://arxiv.org/abs/2504.02812" target="_blank" rel="noopener noreferrer">BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Van Nguyen Nguyen, Stephen Tyree, Andrew Guo, Mederic Fourmy, Anas Gouda, Taeyeop Lee, Sungphill Moon, Hyeontae Son, Lukas Ranftl, Jonathan Tremblay, Eric Brachmann, Bertram Drost, Vincent Lepetit, Carsten Rother, Stan Birchfield, Jiri Matas, Yann Labbe, Martin Sundermeyer, Tomas Hodan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the 6th in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scena</span>
                
                <span class="abstract-full" style="display: none;">We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the 6th in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 13% more accurate than GenFlow. Methods have similar rankings on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21--29% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still -35% behind the accuracy for seen objects (GDet2023), and the 2D detection stage is consequently the main bottleneck of existing pipelines for 6D localization/detection of unseen objects. The online evaluation system stays open and is available at http://bop.felk.cvut.cz/</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.3 -->
                    
                <!-- LLMs: 4.6 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- 3D: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8641
                </span>
                <a href="https://arxiv.org/abs/2410.11682" target="_blank" rel="noopener noreferrer">SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jaeseong Lee, Taewoong Kang, Marcel C. B\"uhler, Min-Jung Kim, Sungwon Hwang, Junha Hyung, Hyojin Jang, Jaegul Choo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometry is crucial for applications like mesh reconstruction and relighting, current methods struggle to capture intricate geometric details and render uns</span>
                
                <span class="abstract-full" style="display: none;">Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometry is crucial for applications like mesh reconstruction and relighting, current methods struggle to capture intricate geometric details and render unseen poses due to their reliance on similarity transformations, which cannot handle stretch and shear transforms essential for detailed deformations of geometry. To address this, we propose SurFhead, a novel method that reconstructs riggable head geometry from RGB videos using 2D Gaussian surfels, which offer well-defined geometric properties, such as precise depth from fixed ray intersections and normals derived from their surface orientation, making them advantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of both normals and images, even in extreme poses, by leveraging classical mesh-based deformation transfer and affine transformation interpolation. SurFhead introduces precise geometric deformation and blends surfels through polar decomposition of transformations, including those affecting normals. Our key contribution lies in bridging classical graphics techniques, such as mesh-based deformation, with modern Gaussian primitives, achieving state-of-the-art geometry reconstruction and rendering quality. Unlike previous avatar rendering approaches, SurFhead enables efficient reconstruction driven by Gaussian primitives while preserving high-fidelity geometry.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.9 -->
                    
                <!-- LLMs: 8.9 -->
                    
                <!-- 3D: 4.9 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- RAG: 2.2 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.1099
                </span>
                <a href="https://arxiv.org/abs/2504.13370" target="_blank" rel="noopener noreferrer">Multi-Sensor Fusion-Based Mobile Manipulator Remote Control for Intelligent Smart Home Assistance</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiao Jin, Bo Xiao, Huijiang Wang, Wendong Wang, Zhenhua Yu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper proposes a wearable-controlled mobile manipulator system for intelligent smart home assistance, integrating MEMS capacitive microphones, IMU sensors, vibration motors, and pressure feedback to enhance human-robot interaction. The wearable device captures forearm muscle activity and conver</span>
                
                <span class="abstract-full" style="display: none;">This paper proposes a wearable-controlled mobile manipulator system for intelligent smart home assistance, integrating MEMS capacitive microphones, IMU sensors, vibration motors, and pressure feedback to enhance human-robot interaction. The wearable device captures forearm muscle activity and converts it into real-time control signals for mobile manipulation. The wearable device achieves an offline classification accuracy of 88.33\%\ across six distinct movement-force classes for hand gestures by using a CNN-LSTM model, while real-world experiments involving five participants yield a practical accuracy of 83.33\%\ with an average system response time of 1.2 seconds. In Human-Robot synergy in navigation and grasping tasks, the robot achieved a 98\%\ task success rate with an average trajectory deviation of only 3.6 cm. Finally, the wearable-controlled mobile manipulator system achieved a 93.3\%\ gripping success rate, a transfer success of 95.6\%\, and a full-task success rate of 91.1\%\ during object grasping and transfer tests, in which a total of 9 object-texture combinations were evaluated. These three experiments' results validate the effectiveness of MEMS-based wearable sensing combined with multi-sensor fusion for reliable and intuitive control of assistive robots in smart home scenarios.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.7 -->
                    
                <!-- LLMs: 6.0 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.1192
                </span>
                <a href="https://arxiv.org/abs/2408.11363" target="_blank" rel="noopener noreferrer">ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure Understanding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yijia Xiao, Edward Sun, Yiqiao Jin, Qifan Wang, Wei Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Understanding biological processes, drug development, and biotechnological advancements requires a detailed analysis of protein structures and functions, a task that is inherently complex and time-consuming in traditional protein research. To streamline this process, we introduce ProteinGPT, a state</span>
                
                <span class="abstract-full" style="display: none;">Understanding biological processes, drug development, and biotechnological advancements requires a detailed analysis of protein structures and functions, a task that is inherently complex and time-consuming in traditional protein research. To streamline this process, we introduce ProteinGPT, a state-of-the-art multimodal large language model for proteins that enables users to upload protein sequences and/or structures for comprehensive analysis and responsive inquiries. ProteinGPT integrates protein sequence and structure encoders with linear projection layers to ensure precise representation adaptation and leverages a large language model (LLM) to generate accurate, contextually relevant responses. To train ProteinGPT, we constructed a large-scale dataset of 132,092 proteins, each annotated with 20-30 property tags and 5-10 QA pairs per protein, and optimized the instruction-tuning process using GPT-4o. Experiments demonstrate that ProteinGPT effectively generates informative responses to protein-related questions, achieving high performance on both semantic and lexical metrics and significantly outperforming baseline models and general-purpose LLMs in understanding and responding to protein-related queries. Our code and data are available at https://github.com/ProteinGPT/ProteinGPT.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.7 -->
                    
                <!-- LLMs: 12.8 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.193
                </span>
                <a href="https://arxiv.org/abs/2504.13193" target="_blank" rel="noopener noreferrer">HEAT:History-Enhanced Dual-phase Actor-Critic Algorithm with A Shared Transformer</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hong Yang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">For a single-gateway LoRaWAN network, this study proposed a history-enhanced two-phase actor-critic algorithm with a shared transformer algorithm (HEAT) to improve network performance. HEAT considers uplink parameters and often neglected downlink parameters, and effectively integrates offline and on</span>
                
                <span class="abstract-full" style="display: none;">For a single-gateway LoRaWAN network, this study proposed a history-enhanced two-phase actor-critic algorithm with a shared transformer algorithm (HEAT) to improve network performance. HEAT considers uplink parameters and often neglected downlink parameters, and effectively integrates offline and online reinforcement learning, using historical data and real-time interaction to improve model performance. In addition, this study developed an open source LoRaWAN network simulator LoRaWANSim. The simulator considers the demodulator lock effect and supports multi-channel, multi-demodulator and bidirectional communication. Simulation experiments show that compared with the best results of all compared algorithms, HEAT improves the packet success rate and energy efficiency by 15% and 95%, respectively.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.4 -->
                    
                <!-- LLMs: 6.6 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.1997
                </span>
                <a href="https://arxiv.org/abs/2503.11101" target="_blank" rel="noopener noreferrer">A Survey on Self-supervised Contrastive Learning for Multimodal Text-Image Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Asifullah Khan, Laiba Asmatullah, Anza Malik, Shahzaib Khan, Hamna Asif
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Self-supervised learning is a machine learning approach that generates implicit labels by learning underlined patterns and extracting discriminative features from unlabeled data without manual labelling. Contrastive learning introduces the concept of "positive" and "negative" samples, where positive</span>
                
                <span class="abstract-full" style="display: none;">Self-supervised learning is a machine learning approach that generates implicit labels by learning underlined patterns and extracting discriminative features from unlabeled data without manual labelling. Contrastive learning introduces the concept of "positive" and "negative" samples, where positive pairs (e.g., variation of the same image/object) are brought together in the embedding space, and negative pairs (e.g., views from different images/objects) are pushed farther away. This methodology has shown significant improvements in image understanding and image text analysis without much reliance on labeled data. In this paper, we comprehensively discuss the terminologies, recent developments and applications of contrastive learning with respect to text-image models. Specifically, we provide an overview of the approaches of contrastive learning in text-image models in recent years. Secondly, we categorize the approaches based on different model structures. Thirdly, we further introduce and discuss the latest advances of the techniques used in the process such as pretext tasks for both images and text, architectural structures, and key trends. Lastly, we discuss the recent state-of-art applications of self-supervised contrastive learning Text-Image based models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.2 -->
                    
                <!-- LLMs: 6.0 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.2663
                </span>
                <a href="https://arxiv.org/abs/2502.13407" target="_blank" rel="noopener noreferrer">JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ziyuan Liu, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, Yuantao Gu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, comprehensive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images wi</span>
                
                <span class="abstract-full" style="display: none;">Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, comprehensive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. To address these issues, we introduce the JL1-CD dataset, which consists of 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5 to 0.75 meters. This all-inclusive dataset covers a wide range of human-induced and natural changes, including buildings, roads, hardened surfaces, woodlands, grasslands, croplands, water bodies, and photovoltaic panels, among others. Additionally, we propose a novel multi-teacher knowledge distillation (MTKD) framework that leverages the Origin-Partition (O-P) strategy to enhance CD performance. In the O-P strategy, we partition the training data based on the Change Area Ratio (CAR) to train separate models for small, medium, and large CAR values, alleviating the learning burden on each model and improving their performance within their respective partitions. Building upon this, our MTKD framework distills knowledge from multiple teacher models trained on different CAR partitions into a single student model,enabling the student model to achieve superior detection results across diverse CAR scenarios without incurring additional computational or time overhead during the inference phase. Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results. The JL1-CD dataset and code are available at https://github.com/circleLZY/MTKD-CD.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.8 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.2712
                </span>
                <a href="https://arxiv.org/abs/2503.05992" target="_blank" rel="noopener noreferrer">Psycholinguistic Analyses in Software Engineering Text: A Systematic Literature Review</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Amirali Sajadi, Kostadin Damevski, Preetha Chatterjee
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Context: A deeper understanding of human factors in software engineering (SE) is essential for improving team collaboration, decision-making, and productivity. Communication channels like code reviews and chats provide insights into developers' psychological and emotional states. While large languag</span>
                
                <span class="abstract-full" style="display: none;">Context: A deeper understanding of human factors in software engineering (SE) is essential for improving team collaboration, decision-making, and productivity. Communication channels like code reviews and chats provide insights into developers' psychological and emotional states. While large language models excel at text analysis, they often lack transparency and precision. Psycholinguistic tools like Linguistic Inquiry and Word Count (LIWC) offer clearer, interpretable insights into cognitive and emotional processes exhibited in text. Despite its wide use in SE research, no comprehensive review of LIWC's use has been conducted. Objective: We examine the importance of psycholinguistic tools, particularly LIWC, and provide a thorough analysis of its current and potential future applications in SE research. Methods: We conducted a systematic review of six prominent databases, identifying 43 SE-related papers using LIWC. Our analysis focuses on five research questions. Results: Our findings reveal a wide range of applications, including analyzing team communication to detect developer emotions and personality, developing ML models to predict deleted Stack Overflow posts, and more recently comparing AI-generated and human-written text. LIWC has been primarily used with data from project management platforms (e.g., GitHub) and Q&amp;A forums (e.g., Stack Overflow). Key BSE concepts include Communication, Organizational Climate, and Positive Psychology. 26 of 43 papers did not formally evaluate LIWC. Concerns were raised about some limitations, including difficulty handling SE-specific vocabulary. Conclusion: We highlight the potential of psycholinguistic tools and their limitations, and present new use cases for advancing the research of human factors in SE (e.g., bias in human-LLM conversations).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 20.0 -->
                    
                <!-- Medicine: 12.1 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Networks: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.2792
                </span>
                <a href="https://arxiv.org/abs/2504.13560" target="_blank" rel="noopener noreferrer">Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: SoYoung Park, Hyewon Lee, Mingyu Choi, Seunghoon Han, Jong-Ryul Lee, Sungsu Lim, Tae-Ho Kim
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Anomaly segmentation is essential for industrial quality, maintenance, and stability. Existing text-guided zero-shot anomaly segmentation models are effective but rely on fixed prompts, limiting adaptability in diverse industrial scenarios. This highlights the need for flexible, context-aware prompt</span>
                
                <span class="abstract-full" style="display: none;">Anomaly segmentation is essential for industrial quality, maintenance, and stability. Existing text-guided zero-shot anomaly segmentation models are effective but rely on fixed prompts, limiting adaptability in diverse industrial scenarios. This highlights the need for flexible, context-aware prompting strategies. We propose Image-Aware Prompt Anomaly Segmentation (IAP-AS), which enhances anomaly segmentation by generating dynamic, context-aware prompts using an image tagging model and a large language model (LLM). IAP-AS extracts object attributes from images to generate context-aware prompts, improving adaptability and generalization in dynamic and unstructured industrial environments. In our experiments, IAP-AS improves the F1-max metric by up to 10%, demonstrating superior adaptability and generalization. It provides a scalable solution for anomaly segmentation across industries</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.1 -->
                    
                <!-- LLMs: 8.2 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- RAG: 2.3 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.3503
                </span>
                <a href="https://arxiv.org/abs/2504.13234" target="_blank" rel="noopener noreferrer">Non-Uniform Class-Wise Coreset Selection: Characterizing Category Difficulty for Data-Efficient Transfer Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hanyu Zhang, Zhen Xing, Wenxuan Yang, Chenxi Ma, Weimin Tan, Bo Yan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As transfer learning models and datasets grow larger, efficient adaptation and storage optimization have become critical needs. Coreset selection addresses these challenges by identifying and retaining the most informative samples, constructing a compact subset for target domain training. However, c</span>
                
                <span class="abstract-full" style="display: none;">As transfer learning models and datasets grow larger, efficient adaptation and storage optimization have become critical needs. Coreset selection addresses these challenges by identifying and retaining the most informative samples, constructing a compact subset for target domain training. However, current methods primarily rely on instance-level difficulty assessments, overlooking crucial category-level characteristics and consequently under-representing minority classes. To overcome this limitation, we propose Non-Uniform Class-Wise Coreset Selection (NUCS), a novel framework that integrates both class-level and instance-level criteria. NUCS automatically allocates data selection budgets for each class based on intrinsic category difficulty and adaptively selects samples within optimal difficulty ranges. By explicitly incorporating category-specific insights, our approach achieves a more balanced and representative coreset, addressing key shortcomings of prior methods. Comprehensive theoretical analysis validates the rationale behind adaptive budget allocation and sample selection, while extensive experiments across 14 diverse datasets and model architectures demonstrate NUCS's consistent improvements over state-of-the-art methods, achieving superior accuracy and computational efficiency. Notably, on CIFAR100 and Food101, NUCS matches full-data training accuracy while retaining just 30% of samples and reducing computation time by 60%. Our work highlights the importance of characterizing category difficulty in coreset selection, offering a robust and data-efficient solution for transfer learning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.4 -->
                    
                <!-- LLMs: 7.8 -->
                    
                <!-- 3D: 2.6 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.4247
                </span>
                <a href="https://arxiv.org/abs/2504.13621" target="_blank" rel="noopener noreferrer">Visual Intention Grounding for Egocentric Assistants</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pengzhan Sun, Junbin Xiao, Tze Ho Elden Tse, Yicong Li, Arjun Akula, Angela Yao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Visual grounding associates textual descriptions with objects in an image. Conventional methods target third-person image inputs and named object queries. In applications such as AI assistants, the perspective shifts -- inputs are egocentric, and objects may be referred to implicitly through needs a</span>
                
                <span class="abstract-full" style="display: none;">Visual grounding associates textual descriptions with objects in an image. Conventional methods target third-person image inputs and named object queries. In applications such as AI assistants, the perspective shifts -- inputs are egocentric, and objects may be referred to implicitly through needs and intentions. To bridge this gap, we introduce EgoIntention, the first dataset for egocentric visual intention grounding. EgoIntention challenges multimodal LLMs to 1) understand and ignore unintended contextual objects and 2) reason about uncommon object functionalities. Benchmark results show that current models misidentify context objects and lack affordance understanding in egocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it enables hybrid training with normal descriptions and egocentric intentions with a chained intention reasoning and object grounding mechanism. RoG significantly outperforms naive finetuning and hybrid training on EgoIntention, while maintaining or slightly improving naive description grounding. This advancement enables unified visual grounding for egocentric and exocentric visual inputs while handling explicit object queries and implicit human intentions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.9 -->
                    
                <!-- LLMs: 10.3 -->
                    
                <!-- 3D: 2.6 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- T2I: 2.0 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.5522
                </span>
                <a href="https://arxiv.org/abs/2312.11128" target="_blank" rel="noopener noreferrer">Unleashing the Power of CNN and Transformer for Balanced RGB-Event Video Recognition</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiao Wang, Yao Rong, Shiao Wang, Yuan Chen, Zhe Wu, Bo Jiang, Yonghong Tian, Jin Tang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Pattern recognition based on RGB-Event data is a newly arising research topic and previous works usually learn their features using CNN or Transformer. As we know, CNN captures the local features well and the cascaded self-attention mechanisms are good at extracting the long-range global relations. </span>
                
                <span class="abstract-full" style="display: none;">Pattern recognition based on RGB-Event data is a newly arising research topic and previous works usually learn their features using CNN or Transformer. As we know, CNN captures the local features well and the cascaded self-attention mechanisms are good at extracting the long-range global relations. It is intuitive to combine them for high-performance RGB-Event based video recognition, however, existing works fail to achieve a good balance between the accuracy and model parameters, as shown in Fig.~\ref{firstimage}. In this work, we propose a novel RGB-Event based recognition framework termed TSCFormer, which is a relatively lightweight CNN-Transformer model. Specifically, we mainly adopt the CNN as the backbone network to first encode both RGB and Event data. Meanwhile, we initialize global tokens as the input and fuse them with RGB and Event features using the BridgeFormer module. It captures the global long-range relations well between both modalities and maintains the simplicity of the whole model architecture at the same time. The enhanced features will be projected and fused into the RGB and Event CNN blocks, respectively, in an interactive manner using F2E and F2V modules. Similar operations are conducted for other CNN blocks to achieve adaptive fusion and local-global feature enhancement under different resolutions. Finally, we concatenate these three features and feed them into the classification head for pattern recognition. Extensive experiments on two large-scale RGB-Event benchmark datasets (PokerEvent and HARDVS) fully validated the effectiveness of our proposed TSCFormer. The source code and pre-trained models will be released at https://github.com/Event-AHU/TSCFormer.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.7 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.8943
                </span>
                <a href="https://arxiv.org/abs/2504.13647" target="_blank" rel="noopener noreferrer">Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class Trajectory Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yushen He, Lei Zhao, Tianchen Deng, Zipeng Fang, Weidong Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Service mobile robots are often required to avoid dynamic objects while performing their tasks, but they usually have only limited computational resources. So we present a lightweight multi-modal framework for 3D object detection and trajectory prediction. Our system synergistically integrates LiDAR</span>
                
                <span class="abstract-full" style="display: none;">Service mobile robots are often required to avoid dynamic objects while performing their tasks, but they usually have only limited computational resources. So we present a lightweight multi-modal framework for 3D object detection and trajectory prediction. Our system synergistically integrates LiDAR and camera inputs to achieve real-time perception of pedestrians, vehicles, and riders in 3D space. The framework proposes two novel modules: 1) a Cross-Modal Deformable Transformer (CMDT) for object detection with high accuracy and acceptable amount of computation, and 2) a Reference Trajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse trajectory prediction of mult-class objects with flexible trajectory lengths. Evaluations on the CODa benchmark demonstrate superior performance over existing methods across detection (+2.03% in mAP) and trajectory prediction (-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits exceptional deployability - when implemented on a wheelchair robot with an entry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To facilitate reproducibility and practical deployment, we release the related code of the method at https://github.com/TossherO/3D_Perception and its ROS inference version at https://github.com/TossherO/ros_packages.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.3 -->
                    
                <!-- LLMs: 6.8 -->
                    
                <!-- 3D: 3.3 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.9102
                </span>
                <a href="https://arxiv.org/abs/2504.13186" target="_blank" rel="noopener noreferrer">Advanced Deep Learning and Large Language Models: Comprehensive Insights for Cancer Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yassine Habchi, Hamza Kheddar, Yassine Himeur, Adel Belouchrani, Erchin Serpedin, Fouad Khelifi, Muhammad E. H. Chowdhury
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rapid advancement of deep learning (DL) has transformed healthcare, particularly in cancer detection and diagnosis. DL surpasses traditional machine learning and human accuracy, making it a critical tool for identifying diseases. Despite numerous reviews on DL in healthcare, a comprehensive anal</span>
                
                <span class="abstract-full" style="display: none;">The rapid advancement of deep learning (DL) has transformed healthcare, particularly in cancer detection and diagnosis. DL surpasses traditional machine learning and human accuracy, making it a critical tool for identifying diseases. Despite numerous reviews on DL in healthcare, a comprehensive analysis of its role in cancer detection remains limited. Existing studies focus on specific aspects, leaving gaps in understanding its broader impact. This paper addresses these gaps by reviewing advanced DL techniques, including transfer learning (TL), reinforcement learning (RL), federated learning (FL), Transformers, and large language models (LLMs). These approaches enhance accuracy, tackle data scarcity, and enable decentralized learning while maintaining data privacy. TL adapts pre-trained models to new datasets, improving performance with limited labeled data. RL optimizes diagnostic pathways and treatment strategies, while FL fosters collaborative model development without sharing sensitive data. Transformers and LLMs, traditionally used in natural language processing, are now applied to medical data for improved interpretability. Additionally, this review examines these techniques' efficiency in cancer diagnosis, addresses challenges like data imbalance, and proposes solutions. It serves as a resource for researchers and practitioners, providing insights into current trends and guiding future research in advanced DL for cancer detection.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 20.9 -->
                    
                <!-- LLMs: 12.6 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- RAG: 2.6 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.9873
                </span>
                <a href="https://arxiv.org/abs/2504.13440" target="_blank" rel="noopener noreferrer">Temporal Propagation of Asymmetric Feature Pyramid for Surgical Scene Segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Cheng Yuan, Yutong Ban
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Surgical scene segmentation is crucial for robot-assisted laparoscopic surgery understanding. Current approaches face two challenges: (i) static image limitations including ambiguous local feature similarities and fine-grained structural details, and (ii) dynamic video complexities arising from rapi</span>
                
                <span class="abstract-full" style="display: none;">Surgical scene segmentation is crucial for robot-assisted laparoscopic surgery understanding. Current approaches face two challenges: (i) static image limitations including ambiguous local feature similarities and fine-grained structural details, and (ii) dynamic video complexities arising from rapid instrument motion and persistent visual occlusions. While existing methods mainly focus on spatial feature extraction, they fundamentally overlook temporal dependencies in surgical video streams. To address this, we present temporal asymmetric feature propagation network, a bidirectional attention architecture enabling cross-frame feature propagation. The proposed method contains a temporal query propagator that integrates multi-directional consistency constraints to enhance frame-specific feature representation, and an aggregated asymmetric feature pyramid module that preserves discriminative features for anatomical structures and surgical instruments. Our framework uniquely enables both temporal guidance and contextual reasoning for surgical scene understanding. Comprehensive evaluations on two public benchmarks show the proposed method outperforms the current SOTA methods by a large margin, with +16.4\% mIoU on EndoVis2018 and +3.3\% mAP on Endoscapes2023. The code will be publicly available after paper acceptance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 19.9 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- 3D: 3.3 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.0085
                </span>
                <a href="https://arxiv.org/abs/2504.13207" target="_blank" rel="noopener noreferrer">BEV-GS: Feed-forward Gaussian Splatting in Bird's-Eye-View for Road Reconstruction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wenhua Wu, Tong Zhao, Chensheng Peng, Lei Yang, Yintao Wei, Zhe Liu, Hesheng Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Road surface is the sole contact medium for wheels or robot feet. Reconstructing road surface is crucial for unmanned vehicles and mobile robots. Recent studies on Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have achieved remarkable results in scene reconstruction. However, they typica</span>
                
                <span class="abstract-full" style="display: none;">Road surface is the sole contact medium for wheels or robot feet. Reconstructing road surface is crucial for unmanned vehicles and mobile robots. Recent studies on Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have achieved remarkable results in scene reconstruction. However, they typically rely on multi-view image inputs and require prolonged optimization times. In this paper, we propose BEV-GS, a real-time single-frame road surface reconstruction method based on feed-forward Gaussian splatting. BEV-GS consists of a prediction module and a rendering module. The prediction module introduces separate geometry and texture networks following Bird's-Eye-View paradigm. Geometric and texture parameters are directly estimated from a single frame, avoiding per-scene optimization. In the rendering module, we utilize grid Gaussian for road surface representation and novel view synthesis, which better aligns with road surface characteristics. Our method achieves state-of-the-art performance on the real-world dataset RSRD. The road elevation error reduces to 1.73 cm, and the PSNR of novel view synthesis reaches 28.36 dB. The prediction and rendering FPS is 26, and 2061, respectively, enabling high-accuracy and real-time applications. The code will be available at: \href{https://github.com/cat-wwh/BEV-GS}{\texttt{https://github.com/cat-wwh/BEV-GS}}</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 18.4 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- 3D: 4.0 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.0301
                </span>
                <a href="https://arxiv.org/abs/2504.13553" target="_blank" rel="noopener noreferrer">A Novel Hybrid Approach for Retinal Vessel Segmentation with Dynamic Long-Range Dependency and Multi-Scale Retinal Edge Fusion Enhancement</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yihao Ouyang, Xunheng Kuang, Mengjia Xiong, Zhida Wang, Yuanquan Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate retinal vessel segmentation provides essential structural information for ophthalmic image analysis. However, existing methods struggle with challenges such as multi-scale vessel variability, complex curvatures, and ambiguous boundaries. While Convolutional Neural Networks (CNNs), Transform</span>
                
                <span class="abstract-full" style="display: none;">Accurate retinal vessel segmentation provides essential structural information for ophthalmic image analysis. However, existing methods struggle with challenges such as multi-scale vessel variability, complex curvatures, and ambiguous boundaries. While Convolutional Neural Networks (CNNs), Transformer-based models and Mamba-based architectures have advanced the field, they often suffer from vascular discontinuities or edge feature ambiguity. To address these limitations, we propose a novel hybrid framework that synergistically integrates CNNs and Mamba for high-precision retinal vessel segmentation. Our approach introduces three key innovations: 1) The proposed High-Resolution Edge Fuse Network is a high-resolution preserving hybrid segmentation framework that combines a multi-scale backbone with the Multi-scale Retina Edge Fusion (MREF) module to enhance edge features, ensuring accurate and robust vessel segmentation. 2) The Dynamic Snake Visual State Space block combines Dynamic Snake Convolution with Mamba to adaptively capture vessel curvature details and long-range dependencies. An improved eight-directional 2D Snake-Selective Scan mechanism and a dynamic weighting strategy enhance the perception of complex vascular topologies. 3) The MREF module enhances boundary precision through multi-scale edge feature aggregation, suppressing noise while emphasizing critical vessel structures across scales. Experiments on three public datasets demonstrate that our method achieves state-of-the-art performance, particularly in maintaining vascular continuity and effectively segmenting vessels in low-contrast regions. This work provides a robust method for clinical applications requiring accurate retinal vessel analysis. The code is available at https://github.com/frank-oy/HREFNet.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 18.4 -->
                    
                <!-- LLMs: 6.5 -->
                    
                <!-- 3D: 3.6 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.0939
                </span>
                <a href="https://arxiv.org/abs/2412.17041" target="_blank" rel="noopener noreferrer">An OpenMind for 3D medical vision self-supervised learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tassilo Wald, Constantin Ulrich, Jonathan Suprijadi, Sebastian Ziegler, Michal Nohel, Robin Peretzke, Gregor K\"ohler, Klaus H. Maier-Hein
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The field of self-supervised learning (SSL) for 3D medical images lacks consistency and standardization. While many methods have been developed, it is impossible to identify the current state-of-the-art, due to i) varying and small pretraining datasets, ii) varying architectures, and iii) being eval</span>
                
                <span class="abstract-full" style="display: none;">The field of self-supervised learning (SSL) for 3D medical images lacks consistency and standardization. While many methods have been developed, it is impossible to identify the current state-of-the-art, due to i) varying and small pretraining datasets, ii) varying architectures, and iii) being evaluated on differing downstream datasets. In this paper, we bring clarity to this field and lay the foundation for further method advancements through three key contributions: We a) publish the largest publicly available pre-training dataset comprising 114k 3D brain MRI volumes, enabling all practitioners to pre-train on a large-scale dataset. We b) benchmark existing 3D self-supervised learning methods on this dataset for a state-of-the-art CNN and Transformer architecture, clarifying the state of 3D SSL pre-training. Among many findings, we show that pre-trained methods can exceed a strong from-scratch nnU-Net ResEnc-L baseline. Lastly, we c) publish the code of our pre-training and fine-tuning frameworks and provide the pre-trained models created during the benchmarking process to facilitate rapid adoption and reproduction.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.3 -->
                    
                <!-- LLMs: 4.2 -->
                    
                <!-- 3D: 2.9 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.1598
                </span>
                <a href="https://arxiv.org/abs/2504.13765" target="_blank" rel="noopener noreferrer">Modeling L1 Influence on L2 Pronunciation: An MFCC-Based Framework for Explainable Machine Learning and Pedagogical Feedback</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Peyman Jahanbin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study investigates the extent to which Mel-Frequency Cepstral Coefficients (MFCCs) capture first language (L1) transfer in extended second language (L2) English speech. Speech samples from Mandarin and American English L1 speakers were extracted from the GMU Speech Accent Archive, converted to </span>
                
                <span class="abstract-full" style="display: none;">This study investigates the extent to which Mel-Frequency Cepstral Coefficients (MFCCs) capture first language (L1) transfer in extended second language (L2) English speech. Speech samples from Mandarin and American English L1 speakers were extracted from the GMU Speech Accent Archive, converted to WAV format, and processed to obtain thirteen MFCCs per speaker. A multi-method analytic framework combining inferential statistics (t-tests, MANOVA, Canonical Discriminant Analysis) and machine learning (Random Forest classification) identified MFCC-1 (broadband energy), MFCC-2 (first formant region), and MFCC-5 (voicing and fricative energy) as the most discriminative features for distinguishing L1 backgrounds. A reduced-feature model using these MFCCs significantly outperformed the full-feature model, as confirmed by McNemar's test and non-overlapping confidence intervals. The findings empirically support the Perceptual Assimilation Model for L2 (PAM-L2) and the Speech Learning Model (SLM), demonstrating that L1-conditioned variation in L2 speech is both perceptually grounded and acoustically quantifiable. Methodologically, the study contributes to applied linguistics and explainable AI by proposing a transparent, data-efficient pipeline for L2 pronunciation modeling. The results also offer pedagogical implications for ESL/EFL instruction by highlighting L1-specific features that can inform intelligibility-oriented instruction, curriculum design, and speech assessment tools.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.5 -->
                    
                <!-- LLMs: 5.8 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.1726
                </span>
                <a href="https://arxiv.org/abs/2504.13200" target="_blank" rel="noopener noreferrer">Efficient Brain Tumor Segmentation Using a Dual-Decoder 3D U-Net with Attention Gates (DDUNet)</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mohammad Mahdi Danesh Pajouh
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Cancer remains one of the leading causes of mortality worldwide, and among its many forms, brain tumors are particularly notorious due to their aggressive nature and the critical challenges involved in early diagnosis. Recent advances in artificial intelligence have shown great promise in assisting </span>
                
                <span class="abstract-full" style="display: none;">Cancer remains one of the leading causes of mortality worldwide, and among its many forms, brain tumors are particularly notorious due to their aggressive nature and the critical challenges involved in early diagnosis. Recent advances in artificial intelligence have shown great promise in assisting medical professionals with precise tumor segmentation, a key step in timely diagnosis and treatment planning. However, many state-of-the-art segmentation methods require extensive computational resources and prolonged training times, limiting their practical application in resource-constrained settings. In this work, we present a novel dual-decoder U-Net architecture enhanced with attention-gated skip connections, designed specifically for brain tumor segmentation from MRI scans. Our approach balances efficiency and accuracy by achieving competitive segmentation performance while significantly reducing training demands. Evaluated on the BraTS 2020 dataset, the proposed model achieved Dice scores of 85.06% for Whole Tumor (WT), 80.61% for Tumor Core (TC), and 71.26% for Enhancing Tumor (ET) in only 50 epochs, surpassing several commonly used U-Net variants. Our model demonstrates that high-quality brain tumor segmentation is attainable even under limited computational resources, thereby offering a viable solution for researchers and clinicians operating with modest hardware. This resource-efficient model has the potential to improve early detection and diagnosis of brain tumors, ultimately contributing to better patient outcomes</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 20.7 -->
                    
                <!-- LLMs: 7.0 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.1746
                </span>
                <a href="https://arxiv.org/abs/2309.01321" target="_blank" rel="noopener noreferrer">Joint Oscillation Damping and Inertia Provision Service for Converter-Interfaced Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Cheng Feng, Linbin Huang, Xiuqiang He, Yi Wang, Florian D\"orfler, Chongqing Kang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Power systems dominated by converter-interfaced distributed energy resources (DERs) typically exhibit weaker damping capabilities and lower inertia, compromising system stability. Although individual DER controllers are evolving to provide superior oscillation damping capabilities and inertia suppor</span>
                
                <span class="abstract-full" style="display: none;">Power systems dominated by converter-interfaced distributed energy resources (DERs) typically exhibit weaker damping capabilities and lower inertia, compromising system stability. Although individual DER controllers are evolving to provide superior oscillation damping capabilities and inertia supports, there is a lack of network-wide coordinated management measures for multiple DERs, potentially leading to unexpected instability and cost-effectiveness problems. To address this gap, this paper introduces a hybrid oscillation damping and inertia management strategy for multiple DERs, considering network coupling effects, and seeks to encourage DERs to provide enhanced damping and inertia with appropriate economic incentives. We first formulate an optimization problem to tune and allocate damping and inertia coefficients for DERs, minimizing associated power and energy costs while ensuring hard constraints for system frequency stability and small-signal stability. The problem is built upon a novel convex parametric formulation that integrates oscillation mode location and frequency trajectory requirements, equipped with a theoretical guarantee, and eliminating the need for iterative tuning and computation burdens. Furthermore, to increase the willingness of DERs to cooperate, we further design appropriate economic incentives to compensate for DERs' costs based on the proposed cost minimization problem, and assess its impact on system cost-efficiency. Numerical tests highlight the effectiveness of the proposed method in promoting system stability and offer insights into potential economic benefits.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.5 -->
                    
                <!-- LLMs: 4.5 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.2395
                </span>
                <a href="https://arxiv.org/abs/2504.13196" target="_blank" rel="noopener noreferrer">Investigating cybersecurity incidents using large language models in latest-generation wireless networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Leonid Legashev, Arthur Zhigalov
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The purpose of research: Detection of cybersecurity incidents and analysis of decision support and assessment of the effectiveness of measures to counter information security threats based on modern generative models. The methods of research: Emulation of signal propagation data in MIMO systems, syn</span>
                
                <span class="abstract-full" style="display: none;">The purpose of research: Detection of cybersecurity incidents and analysis of decision support and assessment of the effectiveness of measures to counter information security threats based on modern generative models. The methods of research: Emulation of signal propagation data in MIMO systems, synthesis of adversarial examples, execution of adversarial attacks on machine learning models, fine tuning of large language models for detecting adversarial attacks, explainability of decisions on detecting cybersecurity incidents based on the prompts technique. Scientific novelty: A binary classification of data poisoning attacks was performed using large language models, and the possibility of using large language models for investigating cybersecurity incidents in the latest generation wireless networks was investigated. The result of research: Fine-tuning of large language models was performed on the prepared data of the emulated wireless network segment. Six large language models were compared for detecting adversarial attacks, and the capabilities of explaining decisions made by a large language model were investigated. The Gemma-7b model showed the best results according to the metrics Precision = 0.89, Recall = 0.89 and F1-Score = 0.89. Based on various explainability prompts, the Gemma-7b model notes inconsistencies in the compromised data under study, performs feature importance analysis and provides various recommendations for mitigating the consequences of adversarial attacks. Large language models integrated with binary classifiers of network threats have significant potential for practical application in the field of cybersecurity incident investigation, decision support and assessing the effectiveness of measures to counter information security threats.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 25.7 -->
                    
                <!-- Medicine: 14.9 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.3068
                </span>
                <a href="https://arxiv.org/abs/2504.09258" target="_blank" rel="noopener noreferrer">PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jianyu Wu, Hao Yang, Xinhua Zeng, Guibing He, Zhiyu Chen, Zihui Li, Xiaochuan Zhang, Yangyang Ma, Run Fang, Yang Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The diagnosis of pathological images is often limited by expert availability and regional disparities, highlighting the importance of automated diagnosis using Vision-Language Models (VLMs). Traditional multimodal models typically emphasize outcomes over the reasoning process, compromising the relia</span>
                
                <span class="abstract-full" style="display: none;">The diagnosis of pathological images is often limited by expert availability and regional disparities, highlighting the importance of automated diagnosis using Vision-Language Models (VLMs). Traditional multimodal models typically emphasize outcomes over the reasoning process, compromising the reliability of clinical decisions. To address the weak reasoning abilities and lack of supervised processes in pathological VLMs, we have innovatively proposed PathVLM-R1, a visual language model designed specifically for pathological images. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its performance for pathological tasks through meticulously designed post-training strategies. Firstly, we conduct supervised fine-tuning guided by pathological data to imbue the model with foundational pathological knowledge, forming a new pathological base model. Subsequently, we introduce Group Relative Policy Optimization (GRPO) and propose a dual reward-driven reinforcement learning optimization, ensuring strict constraint on logical supervision of the reasoning process and accuracy of results via cross-modal process reward and outcome accuracy reward. In the pathological image question-answering tasks, the testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy compared to baseline methods, and it demonstrated superior performance compared to the Qwen2.5-VL-32B version despite having a significantly smaller parameter size. Furthermore, in out-domain data evaluation involving four medical imaging modalities: Computed Tomography (CT), dermoscopy, fundus photography, and Optical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance improved by an average of 17.3% compared to traditional SFT methods. These results clearly indicate that PathVLM-R1 not only enhances accuracy but also possesses broad applicability and expansion potential.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.2 -->
                    
                <!-- LLMs: 8.9 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.3272
                </span>
                <a href="https://arxiv.org/abs/2504.13645" target="_blank" rel="noopener noreferrer">Efficient Parameter Adaptation for Multi-Modal Medical Image Segmentation and Prognosis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Numan Saeed, Shahad Hardan, Muhammad Ridzuan, Nada Saadi, Karthik Nandakumar, Mohammad Yaqub
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Cancer detection and prognosis relies heavily on medical imaging, particularly CT and PET scans. Deep Neural Networks (DNNs) have shown promise in tumor segmentation by fusing information from these modalities. However, a critical bottleneck exists: the dependency on CT-PET data concurrently for tra</span>
                
                <span class="abstract-full" style="display: none;">Cancer detection and prognosis relies heavily on medical imaging, particularly CT and PET scans. Deep Neural Networks (DNNs) have shown promise in tumor segmentation by fusing information from these modalities. However, a critical bottleneck exists: the dependency on CT-PET data concurrently for training and inference, posing a challenge due to the limited availability of PET scans. Hence, there is a clear need for a flexible and efficient framework that can be trained with the widely available CT scans and can be still adapted for PET scans when they become available. In this work, we propose a parameter-efficient multi-modal adaptation (PEMMA) framework for lightweight upgrading of a transformer-based segmentation model trained only on CT scans such that it can be efficiently adapted for use with PET scans when they become available. This framework is further extended to perform prognosis task maintaining the same efficient cross-modal fine-tuning approach. The proposed approach is tested with two well-known segementation backbones, namely UNETR and Swin UNETR. Our approach offers two main advantages. Firstly, we leverage the inherent modularity of the transformer architecture and perform low-rank adaptation (LoRA) as well as decomposed low-rank adaptation (DoRA) of the attention weights to achieve parameter-efficient adaptation. Secondly, by minimizing cross-modal entanglement, PEMMA allows updates using only one modality without causing catastrophic forgetting in the other. Our method achieves comparable performance to early fusion, but with only 8% of the trainable parameters, and demonstrates a significant +28% Dice score improvement on PET scans when trained with a single modality. Furthermore, in prognosis, our method improves the concordance index by +10% when adapting a CT-pretrained model to include PET scans, and by +23% when adapting for both PET and EHR data.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.2 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.3725
                </span>
                <a href="https://arxiv.org/abs/2504.13648" target="_blank" rel="noopener noreferrer">Enhancing Pothole Detection and Characterization: Integrated Segmentation and Depth Estimation in Road Anomaly Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Uthman Baroudi, Alala BaHamid, Yasser Elalfy, Ziad Al Alami
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Road anomaly detection plays a crucial role in road maintenance and in enhancing the safety of both drivers and vehicles. Recent machine learning approaches for road anomaly detection have overcome the tedious and time-consuming process of manual analysis and anomaly counting; however, they often fa</span>
                
                <span class="abstract-full" style="display: none;">Road anomaly detection plays a crucial role in road maintenance and in enhancing the safety of both drivers and vehicles. Recent machine learning approaches for road anomaly detection have overcome the tedious and time-consuming process of manual analysis and anomaly counting; however, they often fall short in providing a complete characterization of road potholes. In this paper, we leverage transfer learning by adopting a pre-trained YOLOv8-seg model for the automatic characterization of potholes using digital images captured from a dashboard-mounted camera. Our work includes the creation of a novel dataset, comprising both images and their corresponding depth maps, collected from diverse road environments in Al-Khobar city and the KFUPM campus in Saudi Arabia. Our approach performs pothole detection and segmentation to precisely localize potholes and calculate their area. Subsequently, the segmented image is merged with its depth map to extract detailed depth information about the potholes. This integration of segmentation and depth data offers a more comprehensive characterization compared to previous deep learning-based road anomaly detection systems. Overall, this method not only has the potential to significantly enhance autonomous vehicle navigation by improving the detection and characterization of road hazards but also assists road maintenance authorities in responding more effectively to road damage.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.7 -->
                    
                <!-- LLMs: 8.9 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Networks: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.6016
                </span>
                <a href="https://arxiv.org/abs/2501.06019" target="_blank" rel="noopener noreferrer">BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, Weihao Xuan, Junjue Wang, Xinlei Shao, Yimin Wei, Junshi Xia, Cuiling Lan, Konrad Schindler, Naoto Yokoya
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Disaster events occur around the world and cause significant damage to human life and property. Earth observation (EO) data enables rapid and comprehensive building damage assessment (BDA), an essential capability in the aftermath of a disaster to reduce human casualties and to inform disaster relie</span>
                
                <span class="abstract-full" style="display: none;">Disaster events occur around the world and cause significant damage to human life and property. Earth observation (EO) data enables rapid and comprehensive building damage assessment (BDA), an essential capability in the aftermath of a disaster to reduce human casualties and to inform disaster relief efforts. Recent research focuses on the development of AI models to achieve accurate mapping of unseen disaster events, mostly using optical EO data. However, solutions based on optical data are limited to clear skies and daylight hours, preventing a prompt response to disasters. Integrating multimodal (MM) EO data, particularly the combination of optical and SAR imagery, makes it possible to provide all-weather, day-and-night disaster responses. Despite this potential, the development of robust multimodal AI models has been constrained by the lack of suitable benchmark datasets. In this paper, we present a BDA dataset using veRy-hIGH-resoluTion optical and SAR imagery (BRIGHT) to support AI-based all-weather disaster response. To the best of our knowledge, BRIGHT is the first open-access, globally distributed, event-diverse MM dataset specifically curated to support AI-based disaster response. It covers five types of natural disasters and two types of man-made disasters across 14 regions worldwide, with a particular focus on developing countries where external assistance is most needed. The optical and SAR imagery in BRIGHT, with a spatial resolution between 0.3-1 meters, provides detailed representations of individual buildings, making it ideal for precise BDA. In our experiments, we have tested seven advanced AI models trained with our BRIGHT to validate the transferability and robustness. The dataset and code are available at https://github.com/ChenHongruixuan/BRIGHT. BRIGHT also serves as the official dataset for the 2025 IEEE GRSS Data Fusion Contest.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.2 -->
                    
                <!-- LLMs: 8.2 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Networks: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.63
                </span>
                <a href="https://arxiv.org/abs/2504.13314" target="_blank" rel="noopener noreferrer">On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion Management</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Timothy Tjhay, Ricardo J. Bessa, Jose Paulos
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The European Union's Artificial Intelligence (AI) Act defines robustness, resilience, and security requirements for high-risk sectors but lacks detailed methodologies for assessment. This paper introduces a novel framework for quantitatively evaluating the robustness and resilience of reinforcement </span>
                
                <span class="abstract-full" style="display: none;">The European Union's Artificial Intelligence (AI) Act defines robustness, resilience, and security requirements for high-risk sectors but lacks detailed methodologies for assessment. This paper introduces a novel framework for quantitatively evaluating the robustness and resilience of reinforcement learning agents in congestion management. Using the AI-friendly digital environment Grid2Op, perturbation agents simulate natural and adversarial disruptions by perturbing the input of AI systems without altering the actual state of the environment, enabling the assessment of AI performance under various scenarios. Robustness is measured through stability and reward impact metrics, while resilience quantifies recovery from performance degradation. The results demonstrate the framework's effectiveness in identifying vulnerabilities and improving AI robustness and resilience for critical applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.9 -->
                    
                <!-- LLMs: 5.6 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.6518
                </span>
                <a href="https://arxiv.org/abs/2504.13308" target="_blank" rel="noopener noreferrer">Acoustic to Articulatory Inversion of Speech; Data Driven Approaches, Challenges, Applications, and Future Scope</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Leena G Pillai, D. Muhammad Noorul Mubarak
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This review is focused on the data-driven approaches applied in different applications of Acoustic-to-Articulatory Inversion (AAI) of speech. This review paper considered the relevant works published in the last ten years (2011-2021). The selection criteria includes (a) type of AAI - Speaker Depende</span>
                
                <span class="abstract-full" style="display: none;">This review is focused on the data-driven approaches applied in different applications of Acoustic-to-Articulatory Inversion (AAI) of speech. This review paper considered the relevant works published in the last ten years (2011-2021). The selection criteria includes (a) type of AAI - Speaker Dependent and Speaker Independent AAI, (b) objectives of the work - Articulatory approximation, Articulatory Feature space selection and Automatic Speech Recognition (ASR), explore the correlation between acoustic and articulatory features, and framework for Computer-assisted language training, (c) Corpus - Simultaneously recorded speech (wav) and medical imaging models such as ElectroMagnetic Articulography (EMA), Electropalatography (EPG), Laryngography, Electroglottography (EGG), X-ray Cineradiography, Ultrasound, and real-time Magnetic Resonance Imaging (rtMRI), (d) Methods or models - recent works are considered, and therefore all the works are based on machine learning, (e) Evaluation - as AAI is a non-linear regression problem, the performance evaluation is mostly done by Correlation Coefficient (CC), Root Mean Square Error (RMSE), and also considered Mean Square Error (MSE), and Mean Format Error (MFE). The practical application of the AAI model can provide a better and user-friendly interpretable image feedback system of articulatory positions, especially tongue movement. Such trajectory feedback system can be used to provide phonetic, language, and speech therapy for pathological subjects.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.1 -->
                    
                <!-- LLMs: 7.8 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.7545
                </span>
                <a href="https://arxiv.org/abs/2504.13820" target="_blank" rel="noopener noreferrer">CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yang Yue, Yulin Wang, Chenxin Tao, Pan Liu, Shiji Song, Gao Huang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Humans can develop internal world models that encode common sense knowledge, telling them how the world works and predicting the consequences of their actions. This concept has emerged as a promising direction for establishing general-purpose machine-learning models in recent preliminary works, e.g.</span>
                
                <span class="abstract-full" style="display: none;">Humans can develop internal world models that encode common sense knowledge, telling them how the world works and predicting the consequences of their actions. This concept has emerged as a promising direction for establishing general-purpose machine-learning models in recent preliminary works, e.g., for visual representation learning. In this paper, we present CheXWorld, the first effort towards a self-supervised world model for radiographic images. Specifically, our work develops a unified framework that simultaneously models three aspects of medical knowledge essential for qualified radiologists, including 1) local anatomical structures describing the fine-grained characteristics of local tissues (e.g., architectures, shapes, and textures); 2) global anatomical layouts describing the global organization of the human body (e.g., layouts of organs and skeletons); and 3) domain variations that encourage CheXWorld to model the transitions across different appearance domains of radiographs (e.g., varying clarity, contrast, and exposure caused by collecting radiographs from different hospitals, devices, or patients). Empirically, we design tailored qualitative and quantitative analyses, revealing that CheXWorld successfully captures these three dimensions of medical knowledge. Furthermore, transfer learning experiments across eight medical image classification and segmentation benchmarks showcase that CheXWorld significantly outperforms existing SSL methods and large-scale medical foundation models. Code & pre-trained models are available at https://github.com/LeapLabTHU/CheXWorld.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.3 -->
                    
                <!-- LLMs: 9.6 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.7593
                </span>
                <a href="https://arxiv.org/abs/2501.19140" target="_blank" rel="noopener noreferrer">Transformation trees -- documentation of multimodal image registration</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Agnieszka Anna Tomaka, Dariusz Pojda, Micha{\l} Tarnawski, Leszek Luchowski
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multimodal image registration plays a key role in creating digital patient models by combining data from different imaging techniques into a single coordinate system. This process often involves multiple sequential and interconnected transformations, which must be well-documented to ensure transpare</span>
                
                <span class="abstract-full" style="display: none;">Multimodal image registration plays a key role in creating digital patient models by combining data from different imaging techniques into a single coordinate system. This process often involves multiple sequential and interconnected transformations, which must be well-documented to ensure transparency and reproducibility. In this paper, we propose the use of transformation trees as a method for structured recording and management of these transformations. This approach has been implemented in the dpVision software and uses a dedicated .dpw file format to store hierarchical relationships between images, transformations, and motion data. Transformation trees allow precise tracking of all image processing steps, reduce the need to store multiple copies of the same data, and enable the indirect registration of images that do not share common reference points. This improves the reproducibility of the analyses and facilitates later processing and integration of images from different sources. The practical application of this method is demonstrated with examples from orthodontics, including the integration of 3D face scans, intraoral scans, and CBCT images, as well as the documentation of mandibular motion. Beyond orthodontics, this method can be applied in other fields that require systematic management of image registration processes, such as maxillofacial surgery, oncology, and biomechanical analysis. Maintaining long-term data consistency is essential for both scientific research and clinical practice. It enables easier comparison of results in longitudinal studies, improves retrospective analysis, and supports the development of artificial intelligence algorithms by providing standardized and well-documented datasets. The proposed approach enhances data organization, allows for efficient analysis, and facilitates the reuse of information in future studies and diagnostic procedures.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.1 -->
                    
                <!-- LLMs: 11.0 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.9093
                </span>
                <a href="https://arxiv.org/abs/2504.13576" target="_blank" rel="noopener noreferrer">MSTIM: A MindSpore-Based Model for Traffic Flow Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Weiqi Qin, Yuxin Liu, Dongze Wu, Zhenkai Qin, Qining Luo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Aiming at the problems of low accuracy and large error fluctuation of traditional traffic flow predictionmodels when dealing with multi-scale temporal features and dynamic change patterns. this paperproposes a multi-scale time series information modelling model MSTIM based on the Mindspore framework</span>
                
                <span class="abstract-full" style="display: none;">Aiming at the problems of low accuracy and large error fluctuation of traditional traffic flow predictionmodels when dealing with multi-scale temporal features and dynamic change patterns. this paperproposes a multi-scale time series information modelling model MSTIM based on the Mindspore framework, which integrates long and short-term memory networks (LSTMs), convolutional neural networks (CNN), and the attention mechanism to improve the modelling accuracy and stability. The Metropolitan Interstate Traffic Volume (MITV) dataset was used for the experiments and compared and analysed with typical LSTM-attention models, CNN-attention models and LSTM-CNN models. The experimental results show that the MSTIM model achieves better results in the metrics of Mean Absolute Error (MAE), Mean Square Error (MSE), and Root Mean Square Error (RMSE), which significantly improves the accuracy and stability of the traffic volume prediction.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 19.7 -->
                    
                <!-- LLMs: 5.4 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Networks: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.0004
                </span>
                <a href="https://arxiv.org/abs/2504.13408" target="_blank" rel="noopener noreferrer">OpCode-Based Malware Classification Using Machine Learning and Deep Learning Techniques</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Varij Saini, Rudraksh Gupta, Neel Soni
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This technical report presents a comprehensive analysis of malware classification using OpCode sequences. Two distinct approaches are evaluated: traditional machine learning using n-gram analysis with Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers; and a deep </span>
                
                <span class="abstract-full" style="display: none;">This technical report presents a comprehensive analysis of malware classification using OpCode sequences. Two distinct approaches are evaluated: traditional machine learning using n-gram analysis with Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers; and a deep learning approach employing a Convolutional Neural Network (CNN). The traditional machine learning approach establishes a baseline using handcrafted 1-gram and 2-gram features from disassembled malware samples. The deep learning methodology builds upon the work proposed in "Deep Android Malware Detection" by McLaughlin et al. and evaluates the performance of a CNN model trained to automatically extract features from raw OpCode data. Empirical results are compared using standard performance metrics (accuracy, precision, recall, and F1-score). While the SVM classifier outperforms other traditional techniques, the CNN model demonstrates competitive performance with the added benefit of automated feature extraction.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 20.0 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.4457
                </span>
                <a href="https://arxiv.org/abs/2504.13363" target="_blank" rel="noopener noreferrer">AI-Empowered Integrated Sensing and Communications</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mojtaba Vaezi, Gayan Aruma Baduge, Esa Ollila, Sergiy A. Vorobyov
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Integrating sensing and communication (ISAC) can help overcome the challenges of limited spectrum and expensive hardware, leading to improved energy and cost efficiency. While full cooperation between sensing and communication can result in significant performance gains, achieving optimal performanc</span>
                
                <span class="abstract-full" style="display: none;">Integrating sensing and communication (ISAC) can help overcome the challenges of limited spectrum and expensive hardware, leading to improved energy and cost efficiency. While full cooperation between sensing and communication can result in significant performance gains, achieving optimal performance requires efficient designs of unified waveforms and beamformers for joint sensing and communication. Sophisticated statistical signal processing and multi-objective optimization techniques are necessary to balance the competing design requirements of joint sensing and communication tasks. Since model-based analytical approaches may be suboptimal or overly complex, deep learning emerges as a powerful tool for developing data-driven signal processing algorithms, particularly when optimal algorithms are unknown or when known algorithms are too complex for real-time implementation. Unified waveform and beamformer design problems for ISAC fall into this category, where fundamental design trade-offs exist between sensing and communication performance metrics, and the underlying models may be inadequate or incomplete. This article explores the application of artificial intelligence (AI) in ISAC designs to enhance efficiency and reduce complexity. We emphasize the integration benefits through AI-driven ISAC designs, prioritizing the development of unified waveforms, constellations, and beamforming strategies for both sensing and communication. To illustrate the practical potential of AI-driven ISAC, we present two case studies on waveform and beamforming design, demonstrating how unsupervised learning and neural network-based optimization can effectively balance performance, complexity, and implementation constraints.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.2 -->
                    
                <!-- LLMs: 7.0 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.5539
                </span>
                <a href="https://arxiv.org/abs/2504.13545" target="_blank" rel="noopener noreferrer">Enhancing Multilingual Sentiment Analysis with Explainability for Sinhala, English, and Code-Mixed Content</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Azmarah Rizvi, Navojith Thamindu, A. M. N. H. Adhikari, W. P. U. Senevirathna, Dharshana Kasthurirathna, Lakmini Abeywardhana
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Sentiment analysis is crucial for brand reputation management in the banking sector, where customer feedback spans English, Sinhala, Singlish, and code-mixed text. Existing models struggle with low-resource languages like Sinhala and lack interpretability for practical use. This research develops a </span>
                
                <span class="abstract-full" style="display: none;">Sentiment analysis is crucial for brand reputation management in the banking sector, where customer feedback spans English, Sinhala, Singlish, and code-mixed text. Existing models struggle with low-resource languages like Sinhala and lack interpretability for practical use. This research develops a hybrid aspect-based sentiment analysis framework that enhances multilingual capabilities with explainable outputs. Using cleaned banking customer reviews, we fine-tune XLM-RoBERTa for Sinhala and code-mixed text, integrate domain-specific lexicon correction, and employ BERT-base-uncased for English. The system classifies sentiment (positive, neutral, negative) with confidence scores, while SHAP and LIME improve interpretability by providing real-time sentiment explanations. Experimental results show that our approaches outperform traditional transformer-based classifiers, achieving 92.3 percent accuracy and an F1-score of 0.89 in English and 88.4 percent in Sinhala and code-mixed content. An explainability analysis reveals key sentiment drivers, improving trust and transparency. A user-friendly interface delivers aspect-wise sentiment insights, ensuring accessibility for businesses. This research contributes to robust, transparent sentiment analysis for financial applications by bridging gaps in multilingual, low-resource NLP and explainability.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 26.2 -->
                    
                <!-- LLMs: 9.8 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- RAG: 1.9 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Datasets: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.6305
                </span>
                <a href="https://arxiv.org/abs/2504.13208" target="_blank" rel="noopener noreferrer">Intelligent road crack detection and analysis based on improved YOLOv8</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Haomin Zuo, Zhengyang Li, Jiangchuan Gong, Zhen Tian
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As urbanization speeds up and traffic flow increases, the issue of pavement distress is becoming increasingly pronounced, posing a severe threat to road safety and service life. Traditional methods of pothole detection rely on manual inspection, which is not only inefficient but also costly. This pa</span>
                
                <span class="abstract-full" style="display: none;">As urbanization speeds up and traffic flow increases, the issue of pavement distress is becoming increasingly pronounced, posing a severe threat to road safety and service life. Traditional methods of pothole detection rely on manual inspection, which is not only inefficient but also costly. This paper proposes an intelligent road crack detection and analysis system, based on the enhanced YOLOv8 deep learning framework. A target segmentation model has been developed through the training of 4029 images, capable of efficiently and accurately recognizing and segmenting crack regions in roads. The model also analyzes the segmented regions to precisely calculate the maximum and minimum widths of cracks and their exact locations. Experimental results indicate that the incorporation of ECA and CBAM attention mechanisms substantially enhances the model's detection accuracy and efficiency, offering a novel solution for road maintenance and safety monitoring.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 19.9 -->
                    
                <!-- LLMs: 7.8 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Networks: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.7777
                </span>
                <a href="https://arxiv.org/abs/2504.13415" target="_blank" rel="noopener noreferrer">DADU: Dual Attention-based Deep Supervised UNet for Automated Semantic Segmentation of Cardiac Images</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Racheal Mukisa, Arvind K. Bansal
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose an enhanced deep learning-based model for image segmentation of the left and right ventricles and myocardium scar tissue from cardiac magnetic resonance (CMR) images. The proposed technique integrates UNet, channel and spatial attention, edge-detection based skip-connection and deep super</span>
                
                <span class="abstract-full" style="display: none;">We propose an enhanced deep learning-based model for image segmentation of the left and right ventricles and myocardium scar tissue from cardiac magnetic resonance (CMR) images. The proposed technique integrates UNet, channel and spatial attention, edge-detection based skip-connection and deep supervised learning to improve the accuracy of the CMR image-segmentation. Images are processed using multiple channels to generate multiple feature-maps. We built a dual attention-based model to integrate channel and spatial attention. The use of extracted edges in skip connection improves the reconstructed images from feature-maps. The use of deep supervision reduces vanishing gradient problems inherent in classification based on deep neural networks. The algorithms for dual attention-based model, corresponding implementation and performance results are described. The performance results show that this approach has attained high accuracy: 98% Dice Similarity Score (DSC) and significantly lower Hausdorff Distance (HD). The performance results outperform other leading techniques both in DSC and HD.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 20.3 -->
                    
                <!-- LLMs: 6.2 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.2763
                </span>
                <a href="https://arxiv.org/abs/2405.13341" target="_blank" rel="noopener noreferrer">Wealth inequality and utility: Effect evaluation of redistribution and consumption morals using the macro-econophysical coupled approach</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Takeshi Kato, Yosuke Tanabe, Mohammad Rezoanul Hoque
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reducing wealth inequality and increasing utility are critical issues. This study reveals the effects of redistribution and consumption morals on wealth inequality and utility. To this end, we present a novel approach that couples the dynamic model of capital, consumption, and utility in macroeconom</span>
                
                <span class="abstract-full" style="display: none;">Reducing wealth inequality and increasing utility are critical issues. This study reveals the effects of redistribution and consumption morals on wealth inequality and utility. To this end, we present a novel approach that couples the dynamic model of capital, consumption, and utility in macroeconomics with the interaction model of joint business and redistribution in econophysics. With this approach, we calculate the capital (wealth), the utility based on consumption, and the Gini index of these inequality using redistribution and consumption thresholds as moral parameters. The results show that: under-redistribution and waste exacerbate inequality; conversely, over-redistribution and stinginess reduce utility; and a balanced moderate moral leads to achieve both reduced inequality and increased utility. These findings provide renewed economic and numerical support for the moral importance known from philosophy, anthropology, and religion. The revival of redistribution and consumption morals should promote the transformation to a human mutual-aid economy, as indicated by philosopher and anthropologist, instead of the capitalist economy that has produced the current inequality. The practical challenge is to implement bottom-up social business, on a foothold of worker coops and platform cooperatives as a community against the state and the market, with moral consensus and its operation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.6 -->
                    
                <!-- LLMs: 5.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.7104
                </span>
                <a href="https://arxiv.org/abs/2503.17140" target="_blank" rel="noopener noreferrer">Adiabatic Fine-Tuning of Neural Quantum States Enables Detection of Phase Transitions in Weight Space</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vinicius Hernandes, Thomas Spriggs, Saqar Khaleefah, Eliska Greplova
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Neural quantum states (NQS) have emerged as a powerful tool for approximating quantum wavefunctions using deep learning. While these models achieve remarkable accuracy, understanding how they encode physical information remains an open challenge. In this work, we introduce adiabatic fine-tuning, a s</span>
                
                <span class="abstract-full" style="display: none;">Neural quantum states (NQS) have emerged as a powerful tool for approximating quantum wavefunctions using deep learning. While these models achieve remarkable accuracy, understanding how they encode physical information remains an open challenge. In this work, we introduce adiabatic fine-tuning, a scheme that trains NQS across a phase diagram, leading to strongly correlated weight representations across different models. This correlation in weight space enables the detection of phase transitions in quantum systems by analyzing the trained network weights alone. We validate our approach on the transverse field Ising model and the J1-J2 Heisenberg model, demonstrating that phase transitions manifest as distinct structures in weight space. Our results establish a connection between physical phase transitions and the geometry of neural network parameters, opening new directions for the interpretability of machine learning models in physics.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 10.6 -->
                    
                <!-- LLMs: 8.2 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.7283
                </span>
                <a href="https://arxiv.org/abs/2406.16039" target="_blank" rel="noopener noreferrer">CholecInstanceSeg: A Tool Instance Segmentation Dataset for Laparoscopic Surgery</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Oluwatosin Alabi, Ko Ko Zayar Toe, Zijian Zhou, Charlie Budd, Nicholas Raison, Miaojing Shi, Tom Vercauteren
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In laparoscopic and robotic surgery, precise tool instance segmentation is an essential technology for advanced computer-assisted interventions. Although publicly available procedures of routine surgeries exist, they often lack comprehensive annotations for tool instance segmentation. Additionally, </span>
                
                <span class="abstract-full" style="display: none;">In laparoscopic and robotic surgery, precise tool instance segmentation is an essential technology for advanced computer-assisted interventions. Although publicly available procedures of routine surgeries exist, they often lack comprehensive annotations for tool instance segmentation. Additionally, the majority of standard datasets for tool segmentation are derived from porcine(pig) surgeries. To address this gap, we introduce CholecInstanceSeg, the largest open-access tool instance segmentation dataset to date. Derived from the existing CholecT50 and Cholec80 datasets, CholecInstanceSeg provides novel annotations for laparoscopic cholecystectomy procedures in patients. Our dataset comprises 41.9k annotated frames extracted from 85 clinical procedures and 64.4k tool instances, each labelled with semantic masks and instance IDs. To ensure the reliability of our annotations, we perform extensive quality control, conduct label agreement statistics, and benchmark the segmentation results with various instance segmentation baselines. CholecInstanceSeg aims to advance the field by offering a comprehensive and high-quality open-access dataset for the development and evaluation of tool instance segmentation algorithms.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 28.3 -->
                    
                <!-- LLMs: 4.8 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.9841
                </span>
                <a href="https://arxiv.org/abs/2504.13391" target="_blank" rel="noopener noreferrer">Cardiac MRI Semantic Segmentation for Ventricles and Myocardium using Deep Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Racheal Mukisa, Arvind K. Bansal
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Automated noninvasive cardiac diagnosis plays a critical role in the early detection of cardiac disorders and cost-effective clinical management. Automated diagnosis involves the automated segmentation and analysis of cardiac images. Precise delineation of cardiac substructures and extraction of the</span>
                
                <span class="abstract-full" style="display: none;">Automated noninvasive cardiac diagnosis plays a critical role in the early detection of cardiac disorders and cost-effective clinical management. Automated diagnosis involves the automated segmentation and analysis of cardiac images. Precise delineation of cardiac substructures and extraction of their morphological attributes are essential for evaluating the cardiac function, and diagnosing cardiovascular disease such as cardiomyopathy, valvular diseases, abnormalities related to septum perforations, and blood-flow rate. Semantic segmentation labels the CMR image at the pixel level, and localizes its subcomponents to facilitate the detection of abnormalities, including abnormalities in cardiac wall motion in an aging heart with muscle abnormalities, vascular abnormalities, and valvular abnormalities. In this paper, we describe a model to improve semantic segmentation of CMR images. The model extracts edge-attributes and context information during down-sampling of the U-Net and infuses this information during up-sampling to localize three major cardiac structures: left ventricle cavity (LV); right ventricle cavity (RV); and LV myocardium (LMyo). We present an algorithm and performance results. A comparison of our model with previous leading models, using similarity metrics between actual image and segmented image, shows that our approach improves Dice similarity coefficient (DSC) by 2%-11% and lowers Hausdorff distance (HD) by 1.6 to 5.7 mm.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 18.3 -->
                    
                <!-- LLMs: 10.9 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -8.7044
                </span>
                <a href="https://arxiv.org/abs/2504.13397" target="_blank" rel="noopener noreferrer">Quantum repeaters enhanced by vacuum beam guides</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yu Gan, Mohadeseh Azar, Nitish Kumar Chandra, Xin Jin, Jinglei Cheng, Kaushik P. Seshadreesan, Junyu Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The development of large-scale quantum communication networks faces critical challenges due to photon loss and decoherence in optical fiber channels. These fundamentally limit transmission distances and demand dense networks of repeater stations. This work investigates using vacuum beam guides (VBGs</span>
                
                <span class="abstract-full" style="display: none;">The development of large-scale quantum communication networks faces critical challenges due to photon loss and decoherence in optical fiber channels. These fundamentally limit transmission distances and demand dense networks of repeater stations. This work investigates using vacuum beam guides (VBGs)-a promising ultra-low-loss transmission platform-as an alternative to traditional fiber links. By incorporating VBGs into repeater-based architectures, we demonstrate that the inter-repeater spacing can be substantially extended, resulting in fewer required nodes and significantly reducing hardware and operational complexity. We perform a cost-function analysis to quantify performance trade-offs across first, second, and third-generation repeaters. Our results show that first-generation repeaters reduce costs dramatically by eliminating entanglement purification. Third-generation repeaters benefit from improved link transmission success, which is crucial for quantum error correction. In contrast, second-generation repeaters exhibit a more nuanced response; although transmission loss is reduced, their performance remains primarily limited by logical gate errors rather than channel loss. These findings highlight that while all repeater generations benefit from reduced photon loss, the magnitude of improvement depends critically on the underlying error mechanisms. Vacuum beam guides thus emerge as a powerful enabler for scalable, high-performance quantum networks, particularly in conjunction with near-term quantum hardware capabilities.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 13.1 -->
                    
                <!-- LLMs: 7.0 -->
                    
                <!-- Medicine: 5.9 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -8.8089
                </span>
                <a href="https://arxiv.org/abs/2504.13331" target="_blank" rel="noopener noreferrer">Wearable-Derived Behavioral and Physiological Biomarkers for Classifying Unipolar and Bipolar Depression Severity</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yassine Ouzar, Cl\'emence Nineuil, Fouad Boutaleb, Emery Pierson, Ali Amad, Mohamed Daoudi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Depression is a complex mental disorder characterized by a diverse range of observable and measurable indicators that go beyond traditional subjective assessments. Recent research has increasingly focused on objective, passive, and continuous monitoring using wearable devices to gain more precise in</span>
                
                <span class="abstract-full" style="display: none;">Depression is a complex mental disorder characterized by a diverse range of observable and measurable indicators that go beyond traditional subjective assessments. Recent research has increasingly focused on objective, passive, and continuous monitoring using wearable devices to gain more precise insights into the physiological and behavioral aspects of depression. However, most existing studies primarily distinguish between healthy and depressed individuals, adopting a binary classification that fails to capture the heterogeneity of depressive disorders. In this study, we leverage wearable devices to predict depression subtypes-specifically unipolar and bipolar depression-aiming to identify distinctive biomarkers that could enhance diagnostic precision and support personalized treatment strategies. To this end, we introduce the CALYPSO dataset, designed for non-invasive detection of depression subtypes and symptomatology through physiological and behavioral signals, including blood volume pulse, electrodermal activity, body temperature, and three-axis acceleration. Additionally, we establish a benchmark on the dataset using well-known features and standard machine learning methods. Preliminary results indicate that features related to physical activity, extracted from accelerometer data, are the most effective in distinguishing between unipolar and bipolar depression, achieving an accuracy of $96.77\%$. Temperature-based features also showed high discriminative power, reaching an accuracy of $93.55\%$. These findings highlight the potential of physiological and behavioral monitoring for improving the classification of depressive subtypes, paving the way for more tailored clinical interventions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.6 -->
                    
                <!-- LLMs: 8.5 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.1293
                </span>
                <a href="https://arxiv.org/abs/2409.17425" target="_blank" rel="noopener noreferrer">Website visits can predict angler presence using machine learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Julia S. Schmid (Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, Alberta, Canada), Sean Simmons (Anglers Atlas, Goldstream Publishing, Prince George, British Columbia, Canada), Mark A. Lewis (Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, Alberta, Canada, Department of Mathematics and Statistics, University of Victoria, Victoria, British Columbia, Canada, Department of Biology, University of Victoria, Victoria, British Columbia, Canada, Department of Biological Sciences, University of Alberta, Edmonton, Alberta, Canada), Mark S. Poesch (Department of Biological Sciences, University of Alberta, Edmonton, Alberta, Canada), Pouria Ramazi (Department of Mathematics and Statistics, Brock University, St. Catharines, Ontario, Canada)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Understanding and predicting recreational angler effort is important for sustainable fisheries management. However, conventional methods of measuring angler effort, such as surveys, can be costly and limited in both time and spatial extent. Models that predict angler effort based on environmental or</span>
                
                <span class="abstract-full" style="display: none;">Understanding and predicting recreational angler effort is important for sustainable fisheries management. However, conventional methods of measuring angler effort, such as surveys, can be costly and limited in both time and spatial extent. Models that predict angler effort based on environmental or economic factors typically rely on historical data, which often limits their spatial and temporal generalizability due to data scarcity. In this study, high-resolution data from an online fishing platform and easily accessible auxiliary data were tested to predict daily boat presence and aerial counts of boats at almost 200 lakes over five years in Ontario, Canada. Lake-information website visits alone enabled predicting daily angler boat presence with 78% accuracy. While incorporating additional environmental, socio-ecological, weather and angler-reported features into machine learning models did not remarkably improve prediction performance of boat presence, they were substantial for the prediction of boat counts. Models achieved an R2 of up to 0.77 at known lakes included in the model training, but they performed poorly for unknown lakes (R2 = 0.21). The results demonstrate the value of integrating data from online fishing platforms into predictive models and highlight the potential of machine learning models to enhance fisheries management.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 20.7 -->
                    
                <!-- LLMs: 17.7 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -10.3667
                </span>
                <a href="https://arxiv.org/abs/2504.13232" target="_blank" rel="noopener noreferrer">A Quantum of Learning: Using Quaternion Algebra to Model Learning on Quantum Devices</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sayed Pouria Talebi, Clive Cheong Took, Danilo P. Mandic
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This article considers the problem of designing adaption and optimisation techniques for training quantum learning machines. To this end, the division algebra of quaternions is used to derive an effective model for representing computation and measurement operations on qubits. In turn, the derived m</span>
                
                <span class="abstract-full" style="display: none;">This article considers the problem of designing adaption and optimisation techniques for training quantum learning machines. To this end, the division algebra of quaternions is used to derive an effective model for representing computation and measurement operations on qubits. In turn, the derived model, serves as the foundation for formulating an adaptive learning problem on principal quantum learning units, thereby establishing quantum information processing units akin to that of neurons in classical approaches. Then, leveraging the modern HR-calculus, a comprehensive training framework for learning on quantum machines is developed. The quaternion-valued model accommodates mathematical tractability and establishment of performance criteria, such as convergence conditions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 14.3 -->
                    
                <!-- Medicine: 8.2 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Evolutionary Algorithms: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -10.6903
                </span>
                <a href="https://arxiv.org/abs/2504.13717" target="_blank" rel="noopener noreferrer">Human-aligned Deep Learning: Explainability, Causality, and Biological Inspiration</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gianluca Carloni
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This work aligns deep learning (DL) with human reasoning capabilities and needs to enable more efficient, interpretable, and robust image classification. We approach this from three perspectives: explainability, causality, and biological vision. Introduction and background open this work before divi</span>
                
                <span class="abstract-full" style="display: none;">This work aligns deep learning (DL) with human reasoning capabilities and needs to enable more efficient, interpretable, and robust image classification. We approach this from three perspectives: explainability, causality, and biological vision. Introduction and background open this work before diving into operative chapters. First, we assess neural networks' visualization techniques for medical images and validate an explainable-by-design method for breast mass classification. A comprehensive review at the intersection of XAI and causality follows, where we introduce a general scaffold to organize past and future research, laying the groundwork for our second perspective. In the causality direction, we propose novel modules that exploit feature co-occurrence in medical images, leading to more effective and explainable predictions. We further introduce CROCODILE, a general framework that integrates causal concepts, contrastive learning, feature disentanglement, and prior knowledge to enhance generalization. Lastly, we explore biological vision, examining how humans recognize objects, and propose CoCoReco, a connectivity-inspired network with context-aware attention mechanisms. Overall, our key findings include: (i) simple activation maximization lacks insight for medical imaging DL models; (ii) prototypical-part learning is effective and radiologically aligned; (iii) XAI and causal ML are deeply connected; (iv) weak causal signals can be leveraged without a priori information to improve performance and interpretability; (v) our framework generalizes across medical domains and out-of-distribution data; (vi) incorporating biological circuit motifs improves human-aligned recognition. This work contributes toward human-aligned DL and highlights pathways to bridge the gap between research and clinical adoption, with implications for improved trust, diagnostic accuracy, and safe deployment.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 32.1 -->
                    
                <!-- LLMs: 7.5 -->
                    
                <!-- 3D: 2.7 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Networks: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -10.9609
                </span>
                <a href="https://arxiv.org/abs/2503.11450" target="_blank" rel="noopener noreferrer">The Road to Hybrid Quantum Programs: Characterizing the Evolution from Classical to Hybrid Quantum Software</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vincenzo De Maio, Ivona Brandic, Ewa Deelman, J\"urgen Cito
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Quantum computing exhibits the unique capability to natively and efficiently encode various natural phenomena, promising theoretical speedups of several orders of magnitude. However, not all computational tasks can be efficiently executed on quantum machines, giving rise to hybrid systems, where som</span>
                
                <span class="abstract-full" style="display: none;">Quantum computing exhibits the unique capability to natively and efficiently encode various natural phenomena, promising theoretical speedups of several orders of magnitude. However, not all computational tasks can be efficiently executed on quantum machines, giving rise to hybrid systems, where some portions of an application run on classical machines, while others utilize quantum resources. Efforts to identify quantum candidate code fragments that can meaningfully execute on quantum machines primarily rely on static code analysis. Yet, the state-of-the-art in static code analysis for quantum candidates remains in its infancy, with limited applicability to specific frameworks and languages, and a lack of generalizability. Existing methods often involve a trial-and-error approach, relying on the intuition and expertise of computer scientists, resulting in varying identification durations ranging from minutes to days for a single application.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 16.4 -->
                    
                <!-- LLMs: 4.7 -->
                    
                <!-- Medicine: 4.3 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -11.6434
                </span>
                <a href="https://arxiv.org/abs/2503.22071" target="_blank" rel="noopener noreferrer">Quantum error correction for long chains of trapped ions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Min Ye, Nicolas Delfosse
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a model for quantum computing with long chains of trapped ions and we design quantum error correction schemes for this model. The main components of a quantum error correction scheme are the quantum code and a quantum circuit called the syndrome extraction circuit, which is executed to pe</span>
                
                <span class="abstract-full" style="display: none;">We propose a model for quantum computing with long chains of trapped ions and we design quantum error correction schemes for this model. The main components of a quantum error correction scheme are the quantum code and a quantum circuit called the syndrome extraction circuit, which is executed to perform error correction with this code. In this work, we design syndrome extraction circuits tailored to our ion chain model, a syndrome extraction tuning protocol to optimize these circuits, and we construct new quantum codes that outperform the state-of-the-art for chains of about $50$ qubits. To establish a baseline under the ion chain model, we simulate the performance of surface codes and bivariate bicycle (BB) codes equipped with our optimized syndrome extraction circuits. Then, we propose a new variant of BB codes defined by weight-five measurements, that we refer to as BB5 codes and we identify BB5 codes that achieve a better minimum distance than any BB codes with the same number of logical qubits and data qubits, such as a $[[48, 4, 7]]$ BB5 code. For a physical error rate of $10^{-3}$, the $[[48, 4, 7]]$ BB5 code achieves a logical error rate per logical qubit of $5 \cdot 10^{-5}$, which is four times smaller than the best BB code in our baseline family. It also achieves the same logical error rate per logical qubit as the distance-7 surface code but using four times fewer physical qubits per logical qubit.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 13.1 -->
                    
                <!-- Medicine: 3.8 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                <!-- Evolutionary Algorithms: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -11.7269
                </span>
                <a href="https://arxiv.org/abs/2504.13836" target="_blank" rel="noopener noreferrer">Outlier-Robust Multi-Model Fitting on Quantum Annealers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Saurabh Pandey, Luca Magri, Federica Arrigoni, Vladislav Golyanik
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multi-model fitting (MMF) presents a significant challenge in Computer Vision, particularly due to its combinatorial nature. While recent advancements in quantum computing offer promise for addressing NP-hard problems, existing quantum-based approaches for model fitting are either limited to a singl</span>
                
                <span class="abstract-full" style="display: none;">Multi-model fitting (MMF) presents a significant challenge in Computer Vision, particularly due to its combinatorial nature. While recent advancements in quantum computing offer promise for addressing NP-hard problems, existing quantum-based approaches for model fitting are either limited to a single model or consider multi-model scenarios within outlier-free datasets. This paper introduces a novel approach, the robust quantum multi-model fitting (R-QuMF) algorithm, designed to handle outliers effectively. Our method leverages the intrinsic capabilities of quantum hardware to tackle combinatorial challenges inherent in MMF tasks, and it does not require prior knowledge of the exact number of models, thereby enhancing its practical applicability. By formulating the problem as a maximum set coverage task for adiabatic quantum computers (AQC), R-QuMF outperforms existing quantum techniques, demonstrating superior performance across various synthetic and real-world 3D datasets. Our findings underscore the potential of quantum computing in addressing the complexities of MMF, especially in real-world scenarios with noisy and outlier-prone data.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 17.6 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -12.1658
                </span>
                <a href="https://arxiv.org/abs/2504.13782" target="_blank" rel="noopener noreferrer">Robust Decentralized Quantum Kernel Learning for Noisy and Adversarial Environment</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wenxuan Ma, Kuan-Cheng Chen, Shang Yu, Mengxiang Liu, Ruilong Deng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper proposes a general decentralized framework for quantum kernel learning (QKL). It has robustness against quantum noise and can also be designed to defend adversarial information attacks forming a robust approach named RDQKL. We analyze the impact of noise on QKL and study the robustness of</span>
                
                <span class="abstract-full" style="display: none;">This paper proposes a general decentralized framework for quantum kernel learning (QKL). It has robustness against quantum noise and can also be designed to defend adversarial information attacks forming a robust approach named RDQKL. We analyze the impact of noise on QKL and study the robustness of decentralized QKL to the noise. By integrating robust decentralized optimization techniques, our method is able to mitigate the impact of malicious data injections across multiple nodes. Experimental results demonstrate that our approach maintains high accuracy under noisy quantum operations and effectively counter adversarial modifications, offering a promising pathway towards the future practical, scalable and secure quantum machine learning (QML).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 16.7 -->
                    
                <!-- Medicine: 6.2 -->
                    
                <!-- LLMs: 5.0 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -14.936
                </span>
                <a href="https://arxiv.org/abs/2504.13532" target="_blank" rel="noopener noreferrer">Quantum Walks-Based Adaptive Distribution Generation with Efficient CUDA-Q Acceleration</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yen-Jui Chang, Wei-Ting Wang, Chen-Yu Liu, Yun-Yuan Wang, Ching-Ray Chang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a novel Adaptive Distribution Generator that leverages a quantum walks-based approach to generate high precision and efficiency of target probability distributions. Our method integrates variational quantum circuits with discrete-time quantum walks, specifically, split-step quantum walks </span>
                
                <span class="abstract-full" style="display: none;">We present a novel Adaptive Distribution Generator that leverages a quantum walks-based approach to generate high precision and efficiency of target probability distributions. Our method integrates variational quantum circuits with discrete-time quantum walks, specifically, split-step quantum walks and their entangled extensions, to dynamically tune coin parameters and drive the evolution of quantum states towards desired distributions. This enables accurate one-dimensional probability modeling for applications such as financial simulation and structured two-dimensional pattern generation exemplified by digit representations(0~9). Implemented within the CUDA-Q framework, our approach exploits GPU acceleration to significantly reduce computational overhead and improve scalability relative to conventional methods. Extensive benchmarks demonstrate that our Quantum Walks-Based Adaptive Distribution Generator achieves high simulation fidelity and bridges the gap between theoretical quantum algorithms and practical high-performance computation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 22.4 -->
                    
                <!-- LLMs: 6.3 -->
                    
                <!-- Medicine: 5.1 -->
                    
                <!-- 3D: 2.4 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -18.1209
                </span>
                <a href="https://arxiv.org/abs/2210.06723" target="_blank" rel="noopener noreferrer">Stochastic noise can be helpful for variational quantum algorithms</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Junyu Liu, Frederik Wilde, Antonio Anna Mele, Xin Jin, Liang Jiang, Jens Eisert
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Saddle points constitute a crucial challenge for first-order gradient descent algorithms. In notions of classical machine learning, they are avoided for example by means of stochastic gradient descent methods. In this work, we provide evidence that the saddle points problem can be naturally avoided </span>
                
                <span class="abstract-full" style="display: none;">Saddle points constitute a crucial challenge for first-order gradient descent algorithms. In notions of classical machine learning, they are avoided for example by means of stochastic gradient descent methods. In this work, we provide evidence that the saddle points problem can be naturally avoided in variational quantum algorithms by exploiting the presence of stochasticity. We prove convergence guarantees and present practical examples in numerical simulations and on quantum hardware. We argue that the natural stochasticity of variational algorithms can be beneficial for avoiding strict saddle points, i.e., those saddle points with at least one negative Hessian eigenvalue. This insight that some levels of shot noise could help is expected to add a new perspective to notions of near-term variational quantum algorithms.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 20.4 -->
                    
                <!-- LLMs: 8.5 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
    </div>
    
    
    <div id="jsonPopup" class="json-popup">
        <pre id="jsonContent"></pre>
        <button onclick="copyJson()">Copy to Clipboard</button>
        <button onclick="closePopup()">Close</button>
    </div>

    <script>
        function extractPaperData(paperElement) {
            const titleElement = paperElement.querySelector('.paper-title a');
            const metaElement = paperElement.querySelector('.paper-meta');
            const abstractElement = paperElement.querySelector('.paper-abstract');
            const tagsElement = paperElement.querySelector('.paper-tags');
            
            // Get the date from the parent date-section header
            const dateSection = paperElement.closest('.date-section');
            const dateText = dateSection.querySelector('.date-header').textContent.trim();
            
            const authorsText = metaElement.textContent.replace('Authors:', '').trim();
            
            const paperData = {
                title: titleElement.textContent,
                url: titleElement.href,
                authors: authorsText.split(',').map(author => author.trim()),
                created: dateText,
                abstract: abstractElement.querySelector('.abstract-full').textContent
            };
            
            return paperData;
        }

        function showJson(paperElement) {
            const popup = document.getElementById('jsonPopup');
            const content = document.getElementById('jsonContent');
            const paperData = extractPaperData(paperElement);
            content.textContent = JSON.stringify(paperData, null, null);
            popup.style.display = 'block';
            document.addEventListener('click', function closePopupOnClick(event) {
                if (!popup.contains(event.target)) {
                    popup.style.display = 'none';
                    document.removeEventListener('click', closePopupOnClick);
                }
            });
        }
        function toggleAbstract(element) {
            const abstract = element.parentElement;
            const short = abstract.querySelector('.abstract-short');
            const full = abstract.querySelector('.abstract-full');
            const lowConfidenceTags = abstract.parentElement.querySelectorAll('.tag-badge.low-confidence');
            
            if (element.textContent === '... more') {
                short.style.display = 'none';
                full.style.display = 'inline';
                element.textContent = ' less';
                lowConfidenceTags.forEach(tag => tag.style.display = 'inline-block');
            } else {
                short.style.display = 'inline';
                full.style.display = 'none';
                element.textContent = '... more';
                lowConfidenceTags.forEach(tag => tag.style.display = 'none');
            }
        }

        function closePopup() {
            document.getElementById('jsonPopup').style.display = 'none';
        }

        function copyJson() {
            const content = document.getElementById('jsonContent').textContent;
            navigator.clipboard.writeText(content).catch(() => {
                // If clipboard API is not available, just show the popup
                alert('Could not copy to clipboard. JSON is displayed in the popup.');
            });
        }

        // Close popup when clicking outside
        window.onclick = function(event) {
            const popup = document.getElementById('jsonPopup');
            if (event.target === popup) {
                popup.style.display = 'none';
            }
        }
    </script>
</body>
</html> 