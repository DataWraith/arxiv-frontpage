<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Frontpage</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .paper {
            margin-bottom: 30px;
            margin-top: 30px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .paper-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .paper-meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        .paper-abstract {
            margin-bottom: 10px;
        }
        .abstract-short {
            display: inline;
        }
        .abstract-full {
            display: none;
        }
        .more-link {
            color: blue;
            cursor: pointer;
            text-decoration: underline;
        }
        .tag-badge {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 5px;
            margin-bottom: 5px;
            border-radius: 3px;
            font-size: 0.8em;
            color: white;
        }
        .tag-badge.high-confidence {
            opacity: 1;
        }
        .tag-badge.low-confidence {
            opacity: 0.6;
            display: none;
        }
        .interestingness-score {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 10px;
            color: white;
            border-radius: 3px;
            font-weight: bold;
        }
        .interestingness-positive {
            background-color: #4CAF50;
        }
        .interestingness-negative {
            background-color: #f44336;
        }
        .last-updated {
            text-align: right;
            color: #666;
            font-size: 0.9em;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .intro {
            text-align: center;
            max-width: 60em;
            margin: 0 auto;
            color: #888;
        }
        .copy-icon {
            display: inline-block;
            width: 16px;
            height: 16px;
            cursor: pointer;
            margin-left: 5px;
            opacity: 0.5;
        }
        .copy-icon:hover {
            opacity: 1;
        }
        .json-popup {
            display: none;
            position: fixed;
            top: 20px;
            right: 20px;
            background: white;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            max-width: 500px;
            max-height: 300px;
            overflow: auto;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        a {
            color: inherit;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h1 {
            text-align: center;
        }
        h1 a {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>
        <a href="https://github.com/DataWraith/arxiv-frontpage">DataWraith's</a> ArXiv Frontpage
    </h1>

    <div class="last-updated">
        Last updated: 2025-04-15
    </div>

    <p class="intro">
        This frontpage is made by scraping ArXiv's computer science RSS feed and tagging papers with a classifier.
    </p>

    <p class="intro">
        Each tag is weighted according to my preferences to compute a paper's <i>interestingness</i> score.
    </p>
    
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                45.5958
            </span>
            <a href="https://arxiv.org/abs/2504.08057" target="_blank" rel="noopener noreferrer">Vector Quantized-Elites: Unsupervised and Problem-Agnostic Quality-Diversity Optimization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Constantinos Tsakonas, Konstantinos Chatzilygeroudis | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Quality-Diversity algorithms have transformed optimization by prioritizing the discovery of diverse, high-performing solutions over a single optimal result. However, traditional Quality-Diversity methods, such as MAP-Elites, rely heavily on predefined behavioral descriptors and complete prior knowle</span>
            
            <span class="abstract-full" style="display: none;">Quality-Diversity algorithms have transformed optimization by prioritizing the discovery of diverse, high-performing solutions over a single optimal result. However, traditional Quality-Diversity methods, such as MAP-Elites, rely heavily on predefined behavioral descriptors and complete prior knowledge of the task to define the behavioral space grid, limiting their flexibility and applicability. In this work, we introduce Vector Quantized-Elites (VQ-Elites), a novel Quality-Diversity algorithm that autonomously constructs a structured behavioral space grid using unsupervised learning, eliminating the need for prior task-specific knowledge. At the core of VQ-Elites is the integration of Vector Quantized Variational Autoencoders, which enables the dynamic learning of behavioral descriptors and the generation of a structured, rather than unstructured, behavioral space grid - a significant advancement over existing unsupervised Quality-Diversity approaches. This design establishes VQ-Elites as a flexible, robust, and task-agnostic optimization framework. To further enhance the performance of unsupervised Quality-Diversity algorithms, we introduce two key components: behavioral space bounding and cooperation mechanisms, which significantly improve convergence and performance. We validate VQ-Elites on robotic arm pose-reaching and mobile robot space-covering tasks. The results demonstrate its ability to efficiently generate diverse, high-quality solutions, emphasizing its adaptability, scalability, robustness to hyperparameters, and potential to extend Quality-Diversity optimization to complex, previously inaccessible domains.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #82c40e" title="Confidence: 66.4%">
                        Quality Diversity
                    </span>
            <!-- Medicine: 7.8 -->
                
            <!-- LLMs: 6.0 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                30.9692
            </span>
            <a href="https://arxiv.org/abs/2504.08359" target="_blank" rel="noopener noreferrer">Kernel-Level Energy-Efficient Neural Architecture Search for Tabular Dataset</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hoang-Loc La, Phuong Hoai Ha | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neur</span>
            
            <span class="abstract-full" style="display: none;">Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neural Architecture Search (NAS) method that directly focuses on identifying architectures that minimize energy consumption while maintaining acceptable accuracy. Unlike previous methods that primarily target vision and language tasks, the approach proposed here specifically addresses tabular datasets. Remarkably, the optimal architecture suggested by this method can reduce energy consumption by up to 92% compared to architectures recommended by conventional NAS.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #19d3a1" title="Confidence: 59.2%">
                        HPO and AutoML
                    </span>
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- LLMs: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                3.9993
            </span>
            <a href="https://arxiv.org/abs/2504.05108" target="_blank" rel="noopener noreferrer">Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, Caglar Gulcehre | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating t</span>
            
            <span class="abstract-full" style="display: none;">Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #ae668e" title="Confidence: 63.5%">
                        Evolutionary Algorithms
                    </span>
            <!-- LLMs: 8.0 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                1.1665
            </span>
            <a href="https://arxiv.org/abs/2412.14865" target="_blank" rel="noopener noreferrer">Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anthony Kobanda, R\'emy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Su</span>
            
            <span class="abstract-full" style="display: none;">We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Such issues prevail in autonomous robotics and video game simulations, notably for navigation tasks prone to topological or kinematic changes. To address these issues, we introduce HiSPO, a novel hierarchical framework designed specifically for continual learning in navigation settings from offline data. Our method leverages distinct policy subspaces of neural networks to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like navigation simulations, showcasing competitive performances and satisfying adaptability with respect to classical continual learning metrics, in particular regarding the memory usage and efficiency.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #44f899" title="Confidence: 63.5%">
                        Reinforcement Learning
                    </span>
            <!-- Medicine: 8.3 -->
                
            <!-- LLMs: 7.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                0.0674
            </span>
            <a href="https://arxiv.org/abs/2504.08667" target="_blank" rel="noopener noreferrer">Faster shortest-path algorithms using the acyclic-connected tree</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Elis Stefansson, Oliver Biggar, Karl H. Johansson | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper gives a fixed-parameter linear algorithm for the single-source shortest path problem (SSSP) on directed graphs. The parameter in question is the nesting width, a measure of the extent to which a graph can be represented as a nested collection of graphs. We present a novel directed graph d</span>
            
            <span class="abstract-full" style="display: none;">This paper gives a fixed-parameter linear algorithm for the single-source shortest path problem (SSSP) on directed graphs. The parameter in question is the nesting width, a measure of the extent to which a graph can be represented as a nested collection of graphs. We present a novel directed graph decomposition called the acyclic-connected tree (A-C tree), which breaks the graph into a recursively nested sequence of strongly connected components in topological order. We prove that the A-C tree is optimal in the sense that its width, the size of the largest nested graph, is equal to the nesting width of the graph. We then provide a linear-time algorithm for constructing the A-C tree of any graph. Finally, we show how the A-C tree allows us to construct a simple variant of Dijkstra's algorithm which achieves a time complexity of $O(e+n\log w)$, where $n$ ($e$) is the number of nodes (arcs) in the graph and $w$ is the nesting width. The idea is to apply the shortest path algorithm separately to each component in the order dictated by the A-C tree. We obtain an asymptotic improvement over Dijkstra's algorithm: when $w=n$, our algorithm reduces to Dijkstra's algorithm, but it is faster when $w \in o(n)$, and linear-time for classes of graphs with bounded width, such as directed acyclic graphs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5caaa5" title="Confidence: 69.6%">
                        Pathfinding
                    </span>
            <!-- Medicine: 5.8 -->
                
            <!-- LLMs: 3.9 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- Math: 2.9 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.1402
            </span>
            <a href="https://arxiv.org/abs/2504.08003" target="_blank" rel="noopener noreferrer">Have we unified image generation and understanding yet? An empirical study of GPT-4o's image generation ability</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ning Li, Jingran Zhang, Justin Cui | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we s</span>
            
            <span class="abstract-full" style="display: none;">OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #5f6936" title="Confidence: 82.0%">
                        LLMs
                    </span>
            <span class="tag-badge high-confidence" style="background-color: #827745" title="Confidence: 76.2%">
                        T2I
                    </span>
            <!-- GNN: 3.8 -->
                
            <!-- 3D: 3.2 -->
                
            <!-- Robotics: 3.0 -->
                
            <!-- RAG: 2.0 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Attention: 1.3 -->
                
            <!-- Bayesian Optimization: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Medicine: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2406
            </span>
            <a href="https://arxiv.org/abs/2502.18791" target="_blank" rel="noopener noreferrer">Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jungsoo Park, Junmo Kang, Gabriel Stanovsky, Alan Ritter | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The surge of LLM studies makes synthesizing their findings challenging. Analysis of experimental results from literature can uncover important trends across studies, but the time-consuming nature of manual data extraction limits its use. Our study presents a semi-automated approach for literature an</span>
            
            <span class="abstract-full" style="display: none;">The surge of LLM studies makes synthesizing their findings challenging. Analysis of experimental results from literature can uncover important trends across studies, but the time-consuming nature of manual data extraction limits its use. Our study presents a semi-automated approach for literature analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset, LLMEvalDB. We then conduct an automated literature analysis of frontier LLMs, reducing the effort of paper surveying and data extraction by more than 93% compared to manual approaches. We validate LLMEvalDB by showing that it reproduces key findings from a recent manual analysis of Chain-of-Thought (CoT) reasoning and also uncovers new insights that go beyond it, showing, for example, that in-context examples benefit coding and multimodal tasks but offer limited gains in math reasoning tasks compared to zero-shot CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through LLMEvalDB and empirical analysis, we provide insights into LLMs while facilitating ongoing literature analyses of their behavior.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 44.8 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Medicine: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.4809
            </span>
            <a href="https://arxiv.org/abs/2503.17365" target="_blank" rel="noopener noreferrer">How Effective Is Constitutional AI in Small LLMs? A Study on DeepSeek-R1 and Its Peers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Antonio-Gabriel Chac\'on Menke (Shibaura Institute of Technology, Kempten University of Applied Sciences), Phan Xuan Tan (Shibaura Institute of Technology) | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent incidents highlight safety risks in Large Language Models (LLMs), motivating research into alignment methods like Constitutional AI (CAI). This paper explores CAI's self-critique mechanism on small, uncensored 7-9B parameter models: DeepSeek-R1-8B, Gemma-2-9B, Llama 3.1-8B, and Qwen2.5-7B. We</span>
            
            <span class="abstract-full" style="display: none;">Recent incidents highlight safety risks in Large Language Models (LLMs), motivating research into alignment methods like Constitutional AI (CAI). This paper explores CAI's self-critique mechanism on small, uncensored 7-9B parameter models: DeepSeek-R1-8B, Gemma-2-9B, Llama 3.1-8B, and Qwen2.5-7B. We show that while Llama-based models exhibited significant harm reduction through self-critique, other architectures demonstrated less improvement in harm detection after abliteration. These results suggest CAI's effectiveness may vary depending on model architecture and reasoning capabilities.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #5f6936" title="Confidence: 81.8%">
                        LLMs
                    </span>
            <!-- GNN: 3.8 -->
                
            <!-- 3D: 3.4 -->
                
            <!-- Robotics: 3.0 -->
                
            <!-- T2I: 2.3 -->
                
            <!-- RAG: 1.9 -->
                
            <!-- Bayesian Optimization: 1.3 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Attention: 1.3 -->
                
            <!-- Medicine: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.4848
            </span>
            <a href="https://arxiv.org/abs/2504.08231" target="_blank" rel="noopener noreferrer">Out of Style: RAG's Fragility to Linguistic Variation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tianyu Cao, Neel Bhandari, Akhila Yerukola, Akari Asai, Maarten Sap | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Despite the impressive performance of Retrieval-augmented Generation (RAG) systems across various NLP benchmarks, their robustness in handling real-world user-LLM interaction queries remains largely underexplored. This presents a critical gap for practical deployment, where user queries exhibit grea</span>
            
            <span class="abstract-full" style="display: none;">Despite the impressive performance of Retrieval-augmented Generation (RAG) systems across various NLP benchmarks, their robustness in handling real-world user-LLM interaction queries remains largely underexplored. This presents a critical gap for practical deployment, where user queries exhibit greater linguistic variations and can trigger cascading errors across interdependent RAG components. In this work, we systematically analyze how varying four linguistic dimensions (formality, readability, politeness, and grammatical correctness) impact RAG performance. We evaluate two retrieval models and nine LLMs, ranging from 3 to 72 billion parameters, across four information-seeking Question Answering (QA) datasets. Our results reveal that linguistic reformulations significantly impact both retrieval and generation stages, leading to a relative performance drop of up to 40.41% in Recall@5 scores for less formal queries and 38.86% in answer match scores for queries containing grammatical errors. Notably, RAG systems exhibit greater sensitivity to such variations compared to LLM-only generations, highlighting their vulnerability to error propagation due to linguistic shifts. These findings highlight the need for improved robustness techniques to enhance reliability in diverse user interactions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- RAG: 43.3 -->
                
            <!-- LLMs: 11.3 -->
                
            <!-- GNN: 2.4 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Medicine: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.4269
            </span>
            <a href="https://arxiv.org/abs/2504.08098" target="_blank" rel="noopener noreferrer">Semicontinuity bounds for the von Neumann entropy and partial majorization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: M. E. Shirokov | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We consider families of tight upper bounds on the difference $S(\rho)-S(\sigma)$ with the rank/energy constraint imposed on the state $\rho$ which are valid provided that the state $\rho$ partially majorizes the state $\sigma$ and is close to the state $\sigma$ w.r.t. the trace norm.</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.7 -->
                
            <!-- LLMs: 5.9 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Math: 2.8 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Multi-armed Bandit: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.6301
            </span>
            <a href="https://arxiv.org/abs/2504.10090" target="_blank" rel="noopener noreferrer">CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: I-Sheng Fang, Jun-Cheng Chen | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) and multimodal large language models (MLLMs) have significantly advanced artificial intelligence. However, visual reasoning, reasoning involving both visual and textual inputs, remains underexplored. Recent advancements, including the reasoning models like OpenAI o1 and </span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) and multimodal large language models (MLLMs) have significantly advanced artificial intelligence. However, visual reasoning, reasoning involving both visual and textual inputs, remains underexplored. Recent advancements, including the reasoning models like OpenAI o1 and Gemini 2.0 Flash Thinking, which incorporate image inputs, have opened this capability. In this ongoing work, we focus specifically on photography-related tasks because a photo is a visual snapshot of the physical world where the underlying physics (i.e., illumination, blur extent, etc.) interplay with the camera parameters. Successfully reasoning from the visual information of a photo to identify these numerical camera settings requires the MLLMs to have a deeper understanding of the underlying physics for precise visual comprehension, representing a challenging and intelligent capability essential for practical applications like photography assistant agents. We aim to evaluate MLLMs on their ability to distinguish visual differences related to numerical camera settings, extending a methodology previously proposed for vision-language models (VLMs). Our preliminary results demonstrate the importance of visual reasoning in photography-related tasks. Moreover, these results show that no single MLLM consistently dominates across all evaluation tasks, demonstrating ongoing challenges and opportunities in developing MLLMs with better visual reasoning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.7 -->
                
            <!-- Medicine: 6.6 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.6507
            </span>
            <a href="https://arxiv.org/abs/2504.08525" target="_blank" rel="noopener noreferrer">Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ye Ye | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, fr</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. A reference implementation of the core TME components is available at https://github.com/biubiutomato/TME-Agent, including basic examples and structured memory integration. While the current implementation uses a tree-based structure, TME is designed to be graph-aware, supporting reusable substeps, converging task paths, and shared dependencies. This lays the groundwork for future DAG-based memory architectures.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 49.0 -->
                
            <!-- Medicine: 5.3 -->
                
            <!-- GNN: 2.6 -->
                
            <!-- Quantum Computing: 2.4 -->
                
            <!-- Robotics: 2.0 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Networks: 1.7 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Reinforcement Learning: 1.3 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.666
            </span>
            <a href="https://arxiv.org/abs/2409.16163" target="_blank" rel="noopener noreferrer">The anonymization problem in social networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rachel G. de Jong, Mark P. J. van der Loo, Frank W. Takes | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper we introduce a general version of the anonymization problem in social networks, in which the goal is to maximize the number of anonymous nodes by altering a given graph. We define three variants of this optimization problem being full, partial and budgeted anonymization. In each, the o</span>
            
            <span class="abstract-full" style="display: none;">In this paper we introduce a general version of the anonymization problem in social networks, in which the goal is to maximize the number of anonymous nodes by altering a given graph. We define three variants of this optimization problem being full, partial and budgeted anonymization. In each, the objective is to maximize the number of k-anonymous nodes, i.e., nodes for which there are at least k-1 equivalent nodes, according to a particular anonymity measure of structural node equivalence. We propose four new heuristic algorithms for solving the anonymization problem which we implement into a reusable computational framework. As a baseline, we use an edge sampling method introduced in previous work. Experiments on both graph models and 23 real-world network datasets result in three empirical findings. First, we demonstrate that edge deletion is the most effective graph alteration operation. Second, we compare four commonly used anonymity measures from the literature and highlight how the choice of anonymity measure has a tremendous effect on both the initial anonymity as well as the difficulty of solving the anonymization problem. Third, we find that the proposed algorithm that preferentially deletes edges with a larger effect on nodes at a structurally unique position consistently outperforms heuristics solely based on network structure. Our best performing algorithm retains on average 14 times more edges in full anonymization, and overall ensures a better trade-off between anonymity and data utility. In the budgeted variant, it achieves 4.8 times more anonymous nodes than the baseline. This work lays foundations for future development of algorithms for anonymizing social networks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.7 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.4 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.6978
            </span>
            <a href="https://arxiv.org/abs/2504.07596" target="_blank" rel="noopener noreferrer">Boosting Universal LLM Reward Design through Heuristic Reward Observation Space Evolution</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zen Kit Heng, Zimeng Zhao, Tianhao Wu, Yuanfei Wang, Mingdong Wu, Yangang Wang, Hao Dong | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation. By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation. By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting relevant environment states and defining their internal operations. However, existing frameworks have not effectively leveraged historical exploration data or manual task descriptions to iteratively evolve this space. In this paper, we propose a novel heuristic framework that enhances LLM-driven reward design by evolving the ROS through a table-based exploration caching mechanism and a text-code reconciliation strategy. Our framework introduces a state execution table, which tracks the historical usage and success rates of environment states, overcoming the Markovian constraint typically found in LLM dialogues and facilitating more effective exploration. Furthermore, we reconcile user-provided task descriptions with expert-defined success criteria using structured prompts, ensuring alignment in reward design objectives. Comprehensive evaluations on benchmark RL tasks demonstrate the effectiveness and stability of the proposed framework. Code and video demos are available at jingjjjjjie.github.io/LLM2Reward.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 18.2 -->
                
            <!-- Medicine: 6.3 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Math: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.8448
            </span>
            <a href="https://arxiv.org/abs/2503.15854" target="_blank" rel="noopener noreferrer">Persistent Stiefel-Whitney Classes of Tangent Bundles</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dongwoo Gang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Stiefel-Whitney classes are invariants of the tangent bundle of a smooth manifold, represented as cohomology classes of the base manifold. These classes are essential in obstruction theory, embedding problems, and cobordism theory. In this work, we first reestablish an appropriate notion of vector b</span>
            
            <span class="abstract-full" style="display: none;">Stiefel-Whitney classes are invariants of the tangent bundle of a smooth manifold, represented as cohomology classes of the base manifold. These classes are essential in obstruction theory, embedding problems, and cobordism theory. In this work, we first reestablish an appropriate notion of vector bundles in a persistent setting, allowing characteristic classes to be interpreted through topological data analysis. Next, we propose a concrete algorithm to compute persistent cohomology classes that represent the Stiefel-Whitney classes of the tangent bundle of a smooth manifold. Given a point cloud, we construct a \v{C}ech or alpha filtration. By applying the Wu formula in this setting, we derive a sequence of persistent cohomology classes from the filtration. We show that if the filtration is homotopy equivalent to a smooth manifold, then one of these persistent cohomology classes corresponds to the $k$-th Stiefel-Whitney class of the tangent bundle of that manifold. To demonstrate the effectiveness of our approach, we present experiments on real-world datasets, including applications to complex manifolds, image patches, and molecular conformation space.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 6.4 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.8586
            </span>
            <a href="https://arxiv.org/abs/2501.17468" target="_blank" rel="noopener noreferrer">Solving Inverse Problems using Diffusion with Iterative Colored Renoising</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Matt C. Bendel, Saurav K. Shastri, Rizwan Ahmad, Philip Schniter | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Imaging inverse problems can be solved in an unsupervised manner using pre-trained diffusion models, but doing so requires approximating the gradient of the measurement-conditional score function in the diffusion reverse process. We show that the approximations produced by existing methods are relat</span>
            
            <span class="abstract-full" style="display: none;">Imaging inverse problems can be solved in an unsupervised manner using pre-trained diffusion models, but doing so requires approximating the gradient of the measurement-conditional score function in the diffusion reverse process. We show that the approximations produced by existing methods are relatively poor, especially early in the reverse process, and so we propose a new approach that iteratively reestimates and "renoises" the estimate several times per diffusion step. This iterative approach, which we call Fast Iterative REnoising (FIRE), injects colored noise that is shaped to ensure that the pre-trained diffusion model always sees white noise, in accordance with how it was trained. We then embed FIRE into the DDIM reverse process and show that the resulting "DDfire" offers state-of-the-art accuracy and runtime on several linear inverse problems, as well as phase retrieval. Our implementation is at https://github.com/matt-bendel/DDfire</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.9 -->
                
            <!-- Medicine: 6.4 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 2.1 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.8625
            </span>
            <a href="https://arxiv.org/abs/2504.10227" target="_blank" rel="noopener noreferrer">Probing then Editing Response Personality of Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tianjie Ju, Zhenyu Shao, Bowen Wang, Yujia Chen, Zhuosheng Zhang, Hao Fei, Mong-Li Lee, Wynne Hsu, Sufeng Duan, Gongshen Liu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have demonstrated promising capabilities to generate responses that exhibit consistent personality traits. Despite the major attempts to analyze personality expression through output-based evaluations, little is known about how such traits are internally encoded within L</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have demonstrated promising capabilities to generate responses that exhibit consistent personality traits. Despite the major attempts to analyze personality expression through output-based evaluations, little is known about how such traits are internally encoded within LLM parameters. In this paper, we introduce a layer-wise probing framework to systematically investigate the layer-wise capability of LLMs in encoding personality for responding. We conduct probing experiments on 11 open-source LLMs over the PersonalityEdit benchmark and find that LLMs predominantly encode personality for responding in their middle and upper layers, with instruction-tuned models demonstrating a slightly clearer separation of personality traits. Furthermore, by interpreting the trained probing hyperplane as a layer-wise boundary for each personality category, we propose a layer-wise perturbation method to edit the personality expressed by LLMs during inference. Our results show that even when the prompt explicitly specifies a particular personality, our method can still successfully alter the response personality of LLMs. Interestingly, the difficulty of converting between certain personality traits varies substantially, which aligns with the representational distances in our probing experiments. Finally, we conduct a comprehensive MMLU benchmark evaluation and time overhead analysis, demonstrating that our proposed personality editing method incurs only minimal degradation in general capabilities while maintaining low training costs and acceptable inference latency. Our code is publicly available at https://github.com/universe-sky/probing-then-editing-personality.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 19.7 -->
                
            <!-- Medicine: 6.1 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Networks: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.9303
            </span>
            <a href="https://arxiv.org/abs/2501.11515" target="_blank" rel="noopener noreferrer">UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zixuan Chen, Yujin Wang, Xin Cai, Zhiyuan You, Zheming Lu, Fan Zhang, Shi Guo, Tianfan Xue | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Capturing high dynamic range (HDR) scenes is one of the most important issues in camera design. Majority of cameras use exposure fusion, which fuses images captured by different exposure levels, to increase dynamic range. However, this approach can only handle images with limited exposure difference</span>
            
            <span class="abstract-full" style="display: none;">Capturing high dynamic range (HDR) scenes is one of the most important issues in camera design. Majority of cameras use exposure fusion, which fuses images captured by different exposure levels, to increase dynamic range. However, this approach can only handle images with limited exposure difference, normally 3-4 stops. When applying to very high dynamic range scenes where a large exposure difference is required, this approach often fails due to incorrect alignment or inconsistent lighting between inputs, or tone mapping artifacts. In this work, we propose \model, the first exposure fusion technique that can merge inputs with 9 stops differences. The key idea is that we model exposure fusion as a guided inpainting problem, where the under-exposed image is used as a guidance to fill the missing information of over-exposed highlights in the over-exposed region. Using an under-exposed image as a soft guidance, instead of a hard constraint, our model is robust to potential alignment issue or lighting variations. Moreover, by utilizing the image prior of the generative model, our model also generates natural tone mapping, even for very high-dynamic range scenes. Our approach outperforms HDR-Transformer on latest HDR benchmarks. Moreover, to test its performance in ultra high dynamic range scenes, we capture a new real-world exposure fusion benchmark, UltraFusion dataset, with exposure differences up to 9 stops, and experiments show that UltraFusion can generate beautiful and high-quality fusion results under various scenarios. Code and data will be available at https://openimaginglab.github.io/UltraFusion.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.4 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.939
            </span>
            <a href="https://arxiv.org/abs/2410.10370" target="_blank" rel="noopener noreferrer">Innovative Thinking, Infinite Humor: Humor Research of Large Language Models through Structured Thought Leaps</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Han Wang, Yilin Zhao, Dian Li, Xiaohan Wang, Gang Liu, Xuguang Lan, Hui Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Humor is previously regarded as a gift exclusive to humans for the following reasons. Humor is a culturally nuanced aspect of human language, presenting challenges for its understanding and generation. Humor generation necessitates a multi-hop reasoning process, with each hop founded on proper ratio</span>
            
            <span class="abstract-full" style="display: none;">Humor is previously regarded as a gift exclusive to humans for the following reasons. Humor is a culturally nuanced aspect of human language, presenting challenges for its understanding and generation. Humor generation necessitates a multi-hop reasoning process, with each hop founded on proper rationales. Although many studies, such as those related to GPT-o1, focus on logical reasoning with reflection and correction, they still fall short in humor generation. Due to the sparsity of the knowledge graph in creative thinking, it is arduous to achieve multi-hop reasoning. Consequently, in this paper, we propose a more robust framework for addressing the humor reasoning task, named LoL. LoL aims to inject external information to mitigate the sparsity of the knowledge graph, thereby enabling multi-hop reasoning. In the first stage of LoL, we put forward an automatic instruction-evolution method to incorporate the deeper and broader thinking processes underlying humor. Judgment-oriented instructions are devised to enhance the model's judgment capability, dynamically supplementing and updating the sparse knowledge graph. Subsequently, through reinforcement learning, the reasoning logic for each online-generated response is extracted using GPT-4o. In this process, external knowledge is re-introduced to aid the model in logical reasoning and the learning of human preferences. Finally, experimental results indicate that the combination of these two processes can enhance both the model's judgment ability and its generative capacity. These findings deepen our comprehension of the creative capabilities of large language models (LLMs) and offer approaches to boost LLMs' creative abilities for cross-domain innovative applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 24.8 -->
                
            <!-- Medicine: 5.0 -->
                
            <!-- Quantum Computing: 2.9 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Federated Learning: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.9536
            </span>
            <a href="https://arxiv.org/abs/2504.08525" target="_blank" rel="noopener noreferrer">Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ye Ye | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, fr</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. The full implementation of TME is available at https://github.com/biubiutomato/TME-Agent.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 55.8%">
                        LLMs
                    </span>
            <!-- Medicine: 5.1 -->
                
            <!-- GNN: 2.6 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.9569
            </span>
            <a href="https://arxiv.org/abs/2504.09648" target="_blank" rel="noopener noreferrer">RANSAC Revisited: An Improved Algorithm for Robust Subspace Recovery under Adversarial and Noisy Corruptions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Guixian Chen, Jianhao Ma, Salar Fattahi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we study the problem of robust subspace recovery (RSR) in the presence of both strong adversarial corruptions and Gaussian noise. Specifically, given a limited number of noisy samples -- some of which are tampered by an adaptive and strong adversary -- we aim to recover a low-dimensio</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we study the problem of robust subspace recovery (RSR) in the presence of both strong adversarial corruptions and Gaussian noise. Specifically, given a limited number of noisy samples -- some of which are tampered by an adaptive and strong adversary -- we aim to recover a low-dimensional subspace that approximately contains a significant fraction of the uncorrupted samples, up to an error that scales with the Gaussian noise. Existing approaches to this problem often suffer from high computational costs or rely on restrictive distributional assumptions, limiting their applicability in truly adversarial settings. To address these challenges, we revisit the classical random sample consensus (RANSAC) algorithm, which offers strong robustness to adversarial outliers, but sacrifices efficiency and robustness against Gaussian noise and model misspecification in the process. We propose a two-stage algorithm, RANSAC+, that precisely pinpoints and remedies the failure modes of standard RANSAC. Our method is provably robust to both Gaussian and adversarial corruptions, achieves near-optimal sample complexity without requiring prior knowledge of the subspace dimension, and is more efficient than existing RANSAC-type methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.2 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.9685
            </span>
            <a href="https://arxiv.org/abs/2504.09474" target="_blank" rel="noopener noreferrer">MigGPT: Harnessing Large Language Models for Automated Migration of Out-of-Tree Linux Kernel Patches Across Versions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pucheng Dang, Di Huang, Dong Li, Kang Chen, Yuanbo Wen, Qi Guo, Xing Hu, Ninghui Sun | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Out-of-tree kernel patches are essential for adapting the Linux kernel to new hardware or enabling specific functionalities. Maintaining and updating these patches across different kernel versions demands significant effort from experienced engineers. Large language models (LLMs) have shown remarkab</span>
            
            <span class="abstract-full" style="display: none;">Out-of-tree kernel patches are essential for adapting the Linux kernel to new hardware or enabling specific functionalities. Maintaining and updating these patches across different kernel versions demands significant effort from experienced engineers. Large language models (LLMs) have shown remarkable progress across various domains, suggesting their potential for automating out-of-tree kernel patch migration. However, our findings reveal that LLMs, while promising, struggle with incomplete code context understanding and inaccurate migration point identification. In this work, we propose MigGPT, a framework that employs a novel code fingerprint structure to retain code snippet information and incorporates three meticulously designed modules to improve the migration accuracy and efficiency of out-of-tree kernel patches. Furthermore, we establish a robust benchmark using real-world out-of-tree kernel patch projects to evaluate LLM capabilities. Evaluations show that MigGPT significantly outperforms the direct application of vanilla LLMs, achieving an average completion rate of 72.59% (50.74% improvement) for migration tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.3 -->
                
            <!-- Medicine: 6.4 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0065
            </span>
            <a href="https://arxiv.org/abs/2504.10286" target="_blank" rel="noopener noreferrer">Characterizing LLM-driven Social Network: The Chirper.ai Case</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yiming Zhu, Yupeng He, Ehsan-Ul Haq, Gareth Tyson, Pan Hui | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) demonstrate the ability to simulate human decision-making processes, enabling their use as agents in modeling sophisticated social networks, both offline and online. Recent research has explored collective behavioral patterns and structural characteristics of LLM agents </span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) demonstrate the ability to simulate human decision-making processes, enabling their use as agents in modeling sophisticated social networks, both offline and online. Recent research has explored collective behavioral patterns and structural characteristics of LLM agents within simulated networks. However, empirical comparisons between LLM-driven and human-driven online social networks remain scarce, limiting our understanding of how LLM agents differ from human users. This paper presents a large-scale analysis of Chirper.ai, an X/Twitter-like social network entirely populated by LLM agents, comprising over 65,000 agents and 7.7 million AI-generated posts. For comparison, we collect a parallel dataset from Mastodon, a human-driven decentralized social network, with over 117,000 users and 16 million posts. We examine key differences between LLM agents and humans in posting behaviors, abusive content, and social network structures. Our findings provide critical insights into the evolving landscape of online social network analysis in the AI era, offering a comprehensive profile of LLM agents in social simulations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.0 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0171
            </span>
            <a href="https://arxiv.org/abs/2504.10136" target="_blank" rel="noopener noreferrer">Uncertainty Propagation in the Fast Fourier Transform</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Luca Schmid, Charlotte Muth, Laurent Schmalen | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We address the problem of uncertainty propagation in the discrete Fourier transform by modeling the fast Fourier transform as a factor graph. Building on this representation, we propose an efficient framework for approximate Bayesian inference using belief propagation (BP) and expectation propagatio</span>
            
            <span class="abstract-full" style="display: none;">We address the problem of uncertainty propagation in the discrete Fourier transform by modeling the fast Fourier transform as a factor graph. Building on this representation, we propose an efficient framework for approximate Bayesian inference using belief propagation (BP) and expectation propagation, extending its applicability beyond Gaussian assumptions. By leveraging an appropriate BP message representation and a suitable schedule, our method achieves stable convergence with accurate mean and variance estimates. Numerical experiments in representative scenarios from communications demonstrate the practical potential of the proposed framework for uncertainty-aware inference in probabilistic systems operating across both time and frequency domain.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0352
            </span>
            <a href="https://arxiv.org/abs/2504.07986" target="_blank" rel="noopener noreferrer">SEAL: Steerable Reasoning Calibration of Large Language Models for Free</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Runjin Chen, Zhenyu Zhang, Junyuan Hong, Souvik Kundu, Zhangyang Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated compelling capabilities for complex reasoning tasks via the extended chain-of-thought (CoT) reasoning mechanism. However, recent studies reveal substantial redundancy in the CoT reasoning traces, which not only increases infe</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated compelling capabilities for complex reasoning tasks via the extended chain-of-thought (CoT) reasoning mechanism. However, recent studies reveal substantial redundancy in the CoT reasoning traces, which not only increases inference latency but also negatively impacts model performance by diverting attention to unnecessary reasoning paths. To address this issue, we investigate the internal reasoning structures of LLMs and categorize them into three primary thought types: execution, reflection, and transition thoughts. Moreover, our analysis reveals that excessive reflection and transition thoughts are strongly correlated with failure cases and these thought categories exhibit clear separation in the latent space. Based on these, we introduce SEAL (Steerable reasoning calibration), a training-free approach that seamlessly calibrates the CoT process, improving accuracy while demonstrating significant efficiency gains. SEAL consists of an offline stage for extracting the reasoning steering vector in the latent space, followed by an on-the-fly calibration of the reasoning trace through representation intervention using the steering vector. Notably, the steering vector exhibits strong transferability across various tasks. Extensive experiments across multiple models (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500, GSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11% improvement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our code is publicly available at https://github.com/VITA-Group/SEAL.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.1 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0435
            </span>
            <a href="https://arxiv.org/abs/2504.09554" target="_blank" rel="noopener noreferrer">HD-RAG: Retrieval-Augmented Generation for Hybrid Documents Containing Text and Hierarchical Tables</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chi Zhang, Qiyang Chen | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">With the rapid advancement of large language models (LLMs), Retrieval-Augmented Generation (RAG) effectively combines LLMs generative capabilities with external retrieval-based information. The Hybrid Document RAG task aims to integrate textual and hierarchical tabular data for more comprehensive re</span>
            
            <span class="abstract-full" style="display: none;">With the rapid advancement of large language models (LLMs), Retrieval-Augmented Generation (RAG) effectively combines LLMs generative capabilities with external retrieval-based information. The Hybrid Document RAG task aims to integrate textual and hierarchical tabular data for more comprehensive retrieval and generation in complex scenarios. However, there is no existing dataset specifically designed for this task that includes both text and tabular data. Additionally, existing methods struggle to retrieve relevant tabular data and integrate it with text. Semantic similarity-based retrieval lacks accuracy, while table-specific methods fail to handle complex hierarchical structures effectively. Furthermore, the QA task requires complex reasoning and calculations, further complicating the challenge. In this paper, we propose a new large-scale dataset, DocRAGLib, specifically designed for the question answering (QA) task scenario under Hybrid Document RAG. To tackle these challenges, we introduce HD-RAG, a novel framework that incorporates a row-and-column level (RCL) table representation, employs a two-stage process combining ensemble and LLM-based retrieval, and integrates RECAP, which is designed for multi-step reasoning and complex calculations in Document-QA tasks. We conduct comprehensive experiments with DocRAGLib, showing that HD-RAG outperforms existing baselines in both retrieval accuracy and QA performance, demonstrating its effectiveness.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.5 -->
                
            <!-- Medicine: 6.4 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- GNN: 2.5 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- Math: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0789
            </span>
            <a href="https://arxiv.org/abs/2504.10112" target="_blank" rel="noopener noreferrer">Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Andreas Happe, J\"urgen Cito | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have emerged as a powerful approach for driving offensive penetration-testing tooling. This paper analyzes the methodology and benchmarking practices used for evaluating Large Language Model (LLM)-driven attacks, focusing on offensive uses of LLMs in cybersecurity. We re</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have emerged as a powerful approach for driving offensive penetration-testing tooling. This paper analyzes the methodology and benchmarking practices used for evaluating Large Language Model (LLM)-driven attacks, focusing on offensive uses of LLMs in cybersecurity. We review 16 research papers detailing 15 prototypes and their respective testbeds.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 19.0 -->
                
            <!-- Medicine: 6.2 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0831
            </span>
            <a href="https://arxiv.org/abs/2504.09942" target="_blank" rel="noopener noreferrer">Fully-Adaptive and Semi-Adaptive Frequency Sweep Algorithm Exploiting Loewner-State Model for EM Simulation of Multiport Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shilpa T. N., Rakesh Sinha | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper employs a fully adaptive and semi-adaptive frequency sweep algorithm using the Loewner matrix-based state model for the electromagnetic simulation. The proposed algorithms use two Loewner matrix models with different or the same orders with small frequency perturbation for adaptive freque</span>
            
            <span class="abstract-full" style="display: none;">This paper employs a fully adaptive and semi-adaptive frequency sweep algorithm using the Loewner matrix-based state model for the electromagnetic simulation. The proposed algorithms use two Loewner matrix models with different or the same orders with small frequency perturbation for adaptive frequency selection. The error between the two models is calculated in each iteration, and the next frequency points are selected to minimize maximum error. With the help of memory, the algorithm terminates when the error between the model and the simulation result is reached within the specified error tolerance. In the fully adaptive frequency sweep algorithm, the method starts with the minimum and maximum frequency of simulation. In the semi-adaptive algorithm, a novel approach has been proposed to determine the initial number of frequency points necessary for system interpolation based on the electrical size of the structure. The proposed algorithms have been compared with the Stoer-Bulirsch algorithm and Pradovera's minimal sampling algorithm for electromagnetic simulation. Four examples are presented using MATLAB R2024b. The results show that the proposed methods offer better performance in terms of speed, accuracy and the requirement of the minimum number of frequency samples. The proposed method shows remarkable consistency with full-wave simulation data, and the algorithm can be effectively applicable to electromagnetic simulations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.8 -->
                
            <!-- LLMs: 6.3 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- Math: 2.6 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0908
            </span>
            <a href="https://arxiv.org/abs/2504.09402" target="_blank" rel="noopener noreferrer">Question Tokens Deserve More Attention: Enhancing Large Language Models without Training through Step-by-Step Reading and Question Attention Recalibration</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Feijiang Han, Licheng Guo, Hengtao Cui, Zhiyuan Lyu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) often struggle with tasks that require a deep understanding of complex questions, especially when faced with long-range dependencies or multi-step reasoning. This work investigates the limitations of current LLMs in question comprehension and identifies three insights: (</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) often struggle with tasks that require a deep understanding of complex questions, especially when faced with long-range dependencies or multi-step reasoning. This work investigates the limitations of current LLMs in question comprehension and identifies three insights: (1) repeating question tokens improves comprehension by increasing attention to question regions, (2) increased backward dependencies negatively affect performance due to unidirectional attentional constraints, and (3) recalibrating attentional mechanisms to prioritize question-relevant regions improves performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 20.1 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1218
            </span>
            <a href="https://arxiv.org/abs/2504.08961" target="_blank" rel="noopener noreferrer">A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kseniia Petukhova, Ekaterina Kochmar | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent advances in Large Language Models (LLMs) have shown promise in automating discourse annotation for conversations. While manually designing tree annotation schemes significantly improves annotation quality for humans and models, their creation remains time-consuming and requires expert knowled</span>
            
            <span class="abstract-full" style="display: none;">Recent advances in Large Language Models (LLMs) have shown promise in automating discourse annotation for conversations. While manually designing tree annotation schemes significantly improves annotation quality for humans and models, their creation remains time-consuming and requires expert knowledge. We propose a fully automated pipeline that uses LLMs to construct such schemes and perform annotation. We evaluate our approach on speech functions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our experiments compare various design choices, and we show that frequency-guided decision trees, paired with an advanced LLM for annotation, can outperform previously manually designed trees and even match or surpass human annotators while significantly reducing the time required for annotation. We release all code and resultant schemes and annotations to facilitate future research on discourse annotation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 18.4 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- 3D: 2.0 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.126
            </span>
            <a href="https://arxiv.org/abs/2504.09798" target="_blank" rel="noopener noreferrer">ReadMe.LLM: A Framework to Help LLMs Understand Your Library</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sandya Wijaya, Jacob Bolano, Alejandro Gomez Soteres, Shriyanshu Kode, Yue Huang, Anant Sahai | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) often struggle with code generation tasks involving niche software libraries. Existing code generation techniques with only human-oriented documentation can fail -- even when the LLM has access to web search and the library is documented online. To address this challenge</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) often struggle with code generation tasks involving niche software libraries. Existing code generation techniques with only human-oriented documentation can fail -- even when the LLM has access to web search and the library is documented online. To address this challenge, we propose ReadMe.LLM, LLM-oriented documentation for software libraries. By attaching the contents of ReadMe.LLM to a query, performance consistently improves to near-perfect accuracy, with one case study demonstrating up to 100% success across all tested models. We propose a software development lifecycle where LLM-specific documentation is maintained alongside traditional software updates. In this study, we present two practical applications of the ReadMe.LLM idea with diverse software libraries, highlighting that our proposed approach could generalize across programming domains.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.4 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1332
            </span>
            <a href="https://arxiv.org/abs/2504.09265" target="_blank" rel="noopener noreferrer">Mixture of Group Experts for Learning Invariant Representations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lei Kang, Jia Li, Mi Tian, Hua Huang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Sparsely activated Mixture-of-Experts (MoE) models effectively increase the number of parameters while maintaining consistent computational costs per token. However, vanilla MoE models often suffer from limited diversity and specialization among experts, constraining their performance and scalabilit</span>
            
            <span class="abstract-full" style="display: none;">Sparsely activated Mixture-of-Experts (MoE) models effectively increase the number of parameters while maintaining consistent computational costs per token. However, vanilla MoE models often suffer from limited diversity and specialization among experts, constraining their performance and scalability, especially as the number of experts increases. In this paper, we present a novel perspective on vanilla MoE with top-$k$ routing inspired by sparse representation. This allows us to bridge established theoretical insights from sparse representation into MoE models. Building on this foundation, we propose a group sparse regularization approach for the input of top-$k$ routing, termed Mixture of Group Experts (MoGE). MoGE indirectly regularizes experts by imposing structural constraints on the routing inputs, while preserving the original MoE architecture. Furthermore, we organize the routing input into a 2D topographic map, spatially grouping neighboring elements. This structure enables MoGE to capture representations invariant to minor transformations, thereby significantly enhancing expert diversity and specialization. Comprehensive evaluations across various Transformer models for image classification and language modeling tasks demonstrate that MoGE substantially outperforms its MoE counterpart, with minimal additional memory and computation overhead. Our approach provides a simple yet effective solution to scale the number of experts and reduce redundancy among them. The source code is included in the supplementary material and will be publicly released.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1342
            </span>
            <a href="https://arxiv.org/abs/2504.08682" target="_blank" rel="noopener noreferrer">Bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Paul Saves, Nathalie Bartoli, Youssef Diouane, Thierry Lefebvre, Joseph Morlier, Christophe David, Eric Nguyen Van, S\'ebastien Defoort | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Multidisciplinary design optimization methods aim at adapting numerical optimization techniques to the design of engineering systems involving multiple disciplines. In this context, a large number of mixed continuous, integer and categorical variables might arise during the optimization process and </span>
            
            <span class="abstract-full" style="display: none;">Multidisciplinary design optimization methods aim at adapting numerical optimization techniques to the design of engineering systems involving multiple disciplines. In this context, a large number of mixed continuous, integer and categorical variables might arise during the optimization process and practical applications involve a large number of design variables. Recently, there has been a growing interest in mixed variables constrained Bayesian optimization but most existing approaches severely increase the number of the hyperparameters related to the surrogate model. In this paper, we address this issue by constructing surrogate models using less hyperparameters. The reduction process is based on the partial least squares method. An adaptive procedure for choosing the number of hyperparameters is proposed. The performance of the proposed approach is confirmed on analytical tests as well as two real applications related to aircraft design. A significant improvement is obtained compared to genetic algorithms.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.6 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- Robotics: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1415
            </span>
            <a href="https://arxiv.org/abs/2503.02268" target="_blank" rel="noopener noreferrer">AppAgentX: Evolving GUI Agents as Proficient Smartphone Users</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wenjia Jiang, Yangyang Zhuang, Chenxi Song, Xu Yang, Joey Tianyi Zhou, Chi Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required</span>
            
            <span class="abstract-full" style="display: none;">Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required predefined rules. However, the reliance on step-by-step reasoning in LLM-based agents often results in inefficiencies, particularly for routine tasks. In contrast, traditional rule-based systems excel in efficiency but lack the intelligence and flexibility to adapt to novel scenarios. To address this challenge, we propose a novel evolutionary framework for GUI agents that enhances operational efficiency while retaining intelligence and flexibility. Our approach incorporates a memory mechanism that records the agent's task execution history. By analyzing this history, the agent identifies repetitive action sequences and evolves high-level actions that act as shortcuts, replacing these low-level operations and improving efficiency. This allows the agent to focus on tasks requiring more complex reasoning, while simplifying routine actions. Experimental results on multiple benchmark tasks demonstrate that our approach significantly outperforms existing methods in both efficiency and accuracy. The code will be open-sourced to support further research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.1 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- 3D: 2.1 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Math: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1637
            </span>
            <a href="https://arxiv.org/abs/2011.08159" target="_blank" rel="noopener noreferrer">On the performance of downlink NOMA in underlay spectrum sharing</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vaibhav Kumar, Zhiguo Ding, Mark F. Flanagan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Non-orthogonal multiple access (NOMA) and spectrum sharing are two potential technologies for providing massive connectivity in beyond fifth-generation (B5G) networks. In this paper, we present the performance analysis of a multi-antenna-assisted two-user downlink NOMA system in an underlay spectrum</span>
            
            <span class="abstract-full" style="display: none;">Non-orthogonal multiple access (NOMA) and spectrum sharing are two potential technologies for providing massive connectivity in beyond fifth-generation (B5G) networks. In this paper, we present the performance analysis of a multi-antenna-assisted two-user downlink NOMA system in an underlay spectrum sharing system. We derive closed-form expressions for the average achievable sum-rate and outage probability of the secondary network under a peak interference constraint and/or peak power constraint, depending on the availability of channel state information (CSI) of the interference link between secondary transmitter (ST) and primary receiver (PR). For the case where the ST has a fixed power budget, we show that performance can be divided into two specific regimes, where either the interference constraint or the power constraint primarily dictates the performance. Our results confirm that the NOMA-based underlay spectrum sharing system significantly outperforms its orthogonal multiple access (OMA) based counterpart, by achieving higher average sum-rate and lower outage probability. We also show the effect of information loss at the ST in terms of CSI of the link between the ST and PR on the system performance. Moreover, we also present closed-form expressions for the optimal power allocation coefficient that minimizes the outage probability of the NOMA system for the special case where the secondary users are each equipped with a single antenna. A close agreement between the simulation and analytical results confirms the correctness of the presented analysis.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.7 -->
                
            <!-- LLMs: 5.4 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Math: 2.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Pathfinding: 1.6 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Hardware: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1652
            </span>
            <a href="https://arxiv.org/abs/2504.03786" target="_blank" rel="noopener noreferrer">Do "New Snow Tablets" Contain Snow? Large Language Models Over-Rely on Names to Identify Ingredients of Chinese Drugs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sifan Li, Yujun Cai, Bryan Hooi, Nanyun Peng, Yiwei Wang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Traditional Chinese Medicine (TCM) has seen increasing adoption in healthcare, with specialized Large Language Models (LLMs) emerging to support clinical applications. A fundamental requirement for these models is accurate identification of TCM drug ingredients. In this paper, we evaluate how genera</span>
            
            <span class="abstract-full" style="display: none;">Traditional Chinese Medicine (TCM) has seen increasing adoption in healthcare, with specialized Large Language Models (LLMs) emerging to support clinical applications. A fundamental requirement for these models is accurate identification of TCM drug ingredients. In this paper, we evaluate how general and TCM-specialized LLMs perform when identifying ingredients of Chinese drugs. Our systematic analysis reveals consistent failure patterns: models often interpret drug names literally, overuse common herbs regardless of relevance, and exhibit erratic behaviors when faced with unfamiliar formulations. LLMs also fail to understand the verification task. These findings demonstrate that current LLMs rely primarily on drug names rather than possessing systematic pharmacological knowledge. To address these limitations, we propose a Retrieval Augmented Generation (RAG) approach focused on ingredient names. Experiments across 220 TCM formulations show our method significantly improves accuracy from approximately 50% to 82% in ingredient verification tasks. Our work highlights critical weaknesses in current TCM-specific LLMs and offers a practical solution for enhancing their clinical reliability.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 18.2 -->
                
            <!-- Medicine: 6.7 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- GNN: 2.4 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Math: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.189
            </span>
            <a href="https://arxiv.org/abs/2504.08272" target="_blank" rel="noopener noreferrer">Palmprint De-Identification Using Diffusion Model for High-Quality and Diverse Synthesis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Licheng Yan, Bob Zhang, Andrew Beng Jin Teoh, Lu Leng, Shuyi Li, Yuqi Wang, Ziyuan Yang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Palmprint recognition techniques have advanced significantly in recent years, enabling reliable recognition even when palmprints are captured in uncontrolled or challenging environments. However, this strength also introduces new risks, as publicly available palmprint images can be misused by advers</span>
            
            <span class="abstract-full" style="display: none;">Palmprint recognition techniques have advanced significantly in recent years, enabling reliable recognition even when palmprints are captured in uncontrolled or challenging environments. However, this strength also introduces new risks, as publicly available palmprint images can be misused by adversaries for malicious activities. Despite this growing concern, research on methods to obscure or anonymize palmprints remains largely unexplored. Thus, it is essential to develop a palmprint de-identification technique capable of removing identity-revealing features while retaining the image's utility and preserving non-sensitive information. In this paper, we propose a training-free framework that utilizes pre-trained diffusion models to generate diverse, high-quality palmprint images that conceal identity features for de-identification purposes. To ensure greater stability and controllability in the synthesis process, we incorporate a semantic-guided embedding fusion alongside a prior interpolation mechanism. We further propose the de-identification ratio, a novel metric for intuitive de-identification assessment. Extensive experiments across multiple palmprint datasets and recognition methods demonstrate that our method effectively conceals identity-related traits with significant diversity across de-identified samples. The de-identified samples preserve high visual fidelity and maintain excellent usability, achieving a balance between de-identification and retaining non-identity information.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.8 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1971
            </span>
            <a href="https://arxiv.org/abs/2504.09710" target="_blank" rel="noopener noreferrer">DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhenting Wang, Guofeng Cui, Kun Wan, Wentian Zhao | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking </span>
            
            <span class="abstract-full" style="display: none;">Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.0 -->
                
            <!-- Medicine: 6.8 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1994
            </span>
            <a href="https://arxiv.org/abs/2504.09606" target="_blank" rel="noopener noreferrer">Early-Bird Diffusion: Investigating and Leveraging Timestep-Aware Early-Bird Tickets in Diffusion Models for Efficient Training</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lexington Whalen (Celine), Zhenbang Du (Celine), Haoran You (Celine), Chaojian Li (Celine), Sixu Li (Celine), Yingyan (Celine), Lin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Training diffusion models (DMs) requires substantial computational resources due to multiple forward and backward passes across numerous timesteps, motivating research into efficient training techniques. In this paper, we propose EB-Diff-Train, a new efficient DM training approach that is orthogonal</span>
            
            <span class="abstract-full" style="display: none;">Training diffusion models (DMs) requires substantial computational resources due to multiple forward and backward passes across numerous timesteps, motivating research into efficient training techniques. In this paper, we propose EB-Diff-Train, a new efficient DM training approach that is orthogonal to other methods of accelerating DM training, by investigating and leveraging Early-Bird (EB) tickets -- sparse subnetworks that manifest early in the training process and maintain high generation quality.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.2 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Math: 1.2 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2002
            </span>
            <a href="https://arxiv.org/abs/2504.09358" target="_blank" rel="noopener noreferrer">DoorBot: Closed-Loop Task Planning and Manipulation for Door Opening in the Wild with Haptic Feedback</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhi Wang, Yuchen Mo, Shengmiao Jin, Wenzhen Yuan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Robots operating in unstructured environments face significant challenges when interacting with everyday objects like doors. They particularly struggle to generalize across diverse door types and conditions. Existing vision-based and open-loop planning methods often lack the robustness to handle var</span>
            
            <span class="abstract-full" style="display: none;">Robots operating in unstructured environments face significant challenges when interacting with everyday objects like doors. They particularly struggle to generalize across diverse door types and conditions. Existing vision-based and open-loop planning methods often lack the robustness to handle varying door designs, mechanisms, and push/pull configurations. In this work, we propose a haptic-aware closed-loop hierarchical control framework that enables robots to explore and open different unseen doors in the wild. Our approach leverages real-time haptic feedback, allowing the robot to adjust its strategy dynamically based on force feedback during manipulation. We test our system on 20 unseen doors across different buildings, featuring diverse appearances and mechanical types. Our framework achieves a 90% success rate, demonstrating its ability to generalize and robustly handle varied door-opening tasks. This scalable solution offers potential applications in broader open-world articulated object manipulation tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.8 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Math: 1.2 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2062
            </span>
            <a href="https://arxiv.org/abs/2503.17604" target="_blank" rel="noopener noreferrer">OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretra</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.3 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2071
            </span>
            <a href="https://arxiv.org/abs/2504.08957" target="_blank" rel="noopener noreferrer">Factors Influencing Gender Representation in IT Faculty Programmes: Insights with a Focus on Software Engineering in a Nordic Context</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Cristina Martinez Montes, Jonna Johansson, Emrik Dunvald | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Software engineering remains male-dominated despite efforts to attract and retain women. Many leave the field due to limited opportunities, unfair treatment, and challenging workplace cultures. Examining university life and choices is important, as these formative experiences shape career aspiration</span>
            
            <span class="abstract-full" style="display: none;">Software engineering remains male-dominated despite efforts to attract and retain women. Many leave the field due to limited opportunities, unfair treatment, and challenging workplace cultures. Examining university life and choices is important, as these formative experiences shape career aspirations and can help address the root causes of underrepresentation in the industry. The study aimed to deepen understanding of the motivations behind women's choice of a career in IT, their experiences in academic life, and how these experiences influence their career decisions, all within a Nordic context. We used a combination of surveys in the bachelor programmes in the IT faculty and interviews with only women from software engineering (SE) to provide a comprehensive view of population experiences and a closer exploration of the experiences of a smaller sample with a focus on SE.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.6 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2115
            </span>
            <a href="https://arxiv.org/abs/2504.08820" target="_blank" rel="noopener noreferrer">CAReDiO: Cultural Alignment of LLM via Representativeness and Distinctiveness Guided Data Optimization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jing Yao, Xiaoyuan Yi, Jindong Wang, Zhicheng Dou, Xing Xie | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As Large Language Models (LLMs) more deeply integrate into human life across various regions, aligning them with pluralistic cultures is crucial for improving user experience and mitigating cultural conflicts. Existing approaches develop culturally aligned LLMs primarily through fine-tuning with mas</span>
            
            <span class="abstract-full" style="display: none;">As Large Language Models (LLMs) more deeply integrate into human life across various regions, aligning them with pluralistic cultures is crucial for improving user experience and mitigating cultural conflicts. Existing approaches develop culturally aligned LLMs primarily through fine-tuning with massive carefully curated culture-specific corpora. Nevertheless, inspired by culture theories, we identify two key challenges faced by these datasets: (1) Representativeness: These corpora fail to fully capture the target culture's core characteristics with redundancy, causing computation waste; (2) Distinctiveness: They struggle to distinguish the unique nuances of a given culture from shared patterns across other relevant ones, hindering precise cultural modeling. To handle these challenges, we introduce CAReDiO, a novel cultural data construction framework. Specifically, CAReDiO utilizes powerful LLMs to automatically generate cultural conversation data, where both the queries and responses are further optimized by maximizing representativeness and distinctiveness. Using CAReDiO, we construct a small yet effective dataset, covering five cultures, and compare it with several recent cultural corpora. Extensive experiments demonstrate that our method generates more effective data and enables cultural alignment with as few as 100 training samples, enhancing both performance and efficiency.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.2 -->
                
            <!-- Medicine: 6.8 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.229
            </span>
            <a href="https://arxiv.org/abs/2504.10322" target="_blank" rel="noopener noreferrer">Efficient Prompt Tuning for Hierarchical Ingredient Recognition</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yinxuan Gui, Bin Zhu, Jingjing Chen, Chong-Wah Ngo | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Fine-grained ingredient recognition presents a significant challenge due to the diverse appearances of ingredients, resulting from different cutting and cooking methods. While existing approaches have shown promising results, they still require extensive training costs and focus solely on fine-grain</span>
            
            <span class="abstract-full" style="display: none;">Fine-grained ingredient recognition presents a significant challenge due to the diverse appearances of ingredients, resulting from different cutting and cooking methods. While existing approaches have shown promising results, they still require extensive training costs and focus solely on fine-grained ingredient recognition. In this paper, we address these limitations by introducing an efficient prompt-tuning framework that adapts pretrained visual-language models (VLMs), such as CLIP, to the ingredient recognition task without requiring full model finetuning. Additionally, we introduce three-level ingredient hierarchies to enhance both training performance and evaluation robustness. Specifically, we propose a hierarchical ingredient recognition task, designed to evaluate model performance across different hierarchical levels (e.g., chicken chunks, chicken, meat), capturing recognition capabilities from coarse- to fine-grained categories. Our method leverages hierarchical labels, training prompt-tuned models with both fine-grained and corresponding coarse-grained labels. Experimental results on the VireoFood172 dataset demonstrate the effectiveness of prompt-tuning with hierarchical labels, achieving superior performance. Moreover, the hierarchical ingredient recognition task provides valuable insights into the model's ability to generalize across different levels of ingredient granularity.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2316
            </span>
            <a href="https://arxiv.org/abs/2504.10106" target="_blank" rel="noopener noreferrer">SoccerNet-v3D: Leveraging Sports Broadcast Replays for 3D Scene Understanding</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Marc Guti\'errez-P\'erez, Antonio Agudo | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Sports video analysis is a key domain in computer vision, enabling detailed spatial understanding through multi-view correspondences. In this work, we introduce SoccerNet-v3D and ISSIA-3D, two enhanced and scalable datasets designed for 3D scene understanding in soccer broadcast analysis. These data</span>
            
            <span class="abstract-full" style="display: none;">Sports video analysis is a key domain in computer vision, enabling detailed spatial understanding through multi-view correspondences. In this work, we introduce SoccerNet-v3D and ISSIA-3D, two enhanced and scalable datasets designed for 3D scene understanding in soccer broadcast analysis. These datasets extend SoccerNet-v3 and ISSIA by incorporating field-line-based camera calibration and multi-view synchronization, enabling 3D object localization through triangulation. We propose a monocular 3D ball localization task built upon the triangulation of ground-truth 2D ball annotations, along with several calibration and reprojection metrics to assess annotation quality on demand. Additionally, we present a single-image 3D ball localization method as a baseline, leveraging camera calibration and ball size priors to estimate the ball's position from a monocular viewpoint. To further refine 2D annotations, we introduce a bounding box optimization technique that ensures alignment with the 3D scene representation. Our proposed datasets establish new benchmarks for 3D soccer scene understanding, enhancing both spatial and temporal analysis in sports analytics. Finally, we provide code to facilitate access to our annotations and the generation pipelines for the datasets.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2525
            </span>
            <a href="https://arxiv.org/abs/2504.09936" target="_blank" rel="noopener noreferrer">KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yuxuan Tian, Zihan Wang, Yebo Peng, Aomufei Yuan, Zhiming Wang, Bairen Yi, Xin Liu, Yong Cui, Tong Yang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to i</span>
            
            <span class="abstract-full" style="display: none;">Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.8 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- RAG: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2564
            </span>
            <a href="https://arxiv.org/abs/2504.09424" target="_blank" rel="noopener noreferrer">Comparing Performance of Preprocessing Techniques for Traffic Sign Recognition Using a HOG-SVM</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Luis Vieira | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This study compares the performance of various preprocessing techniques for Traffic Sign Recognition (TSR) using Histogram of Oriented Gradients (HOG) and Support Vector Machine (SVM) on the German Traffic Sign Recognition Benchmark (GTSRB) dataset. Techniques such as CLAHE, HUE, and YUV were evalua</span>
            
            <span class="abstract-full" style="display: none;">This study compares the performance of various preprocessing techniques for Traffic Sign Recognition (TSR) using Histogram of Oriented Gradients (HOG) and Support Vector Machine (SVM) on the German Traffic Sign Recognition Benchmark (GTSRB) dataset. Techniques such as CLAHE, HUE, and YUV were evaluated for their impact on classification accuracy. Results indicate that YUV in particular significantly enhance the performance of the HOG-SVM classifier (improving accuracy from 89.65% to 91.25%), providing insights into improvements for preprocessing pipeline of TSR applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.0 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 4.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Blockchain: 1.5 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2788
            </span>
            <a href="https://arxiv.org/abs/2402.01677" target="_blank" rel="noopener noreferrer">Embedding Ontologies via Incorporating Extensional and Intensional Knowledge</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Keyu Wang, Guilin Qi, Jiaoyan Chen, Yi Huang, Tianxing Wu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge det</span>
            
            <span class="abstract-full" style="display: none;">Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can capture both structure information and textual information. Experimental results show that EIKE significantly outperforms state-of-the-art methods in three datasets for both triple classification and link prediction, indicating that EIKE provides a more comprehensive and representative perspective of the domain.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.0 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2813
            </span>
            <a href="https://arxiv.org/abs/2504.09072" target="_blank" rel="noopener noreferrer">MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vikas Natesh, H. T. Kung, David Kong | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We offer a novel approach, MGS (Markov Greedy Sums), to improve the accuracy of low-bitwidth floating-point dot products in neural network computations. In conventional 32-bit floating-point summation, adding values with different exponents may lead to loss of precision in the mantissa of the smalle</span>
            
            <span class="abstract-full" style="display: none;">We offer a novel approach, MGS (Markov Greedy Sums), to improve the accuracy of low-bitwidth floating-point dot products in neural network computations. In conventional 32-bit floating-point summation, adding values with different exponents may lead to loss of precision in the mantissa of the smaller term, which is right-shifted to align with the larger term's exponent. Such shifting (a.k.a. 'swamping') is a significant source of numerical errors in accumulation when implementing low-bitwidth dot products (e.g., 8-bit floating point) as the mantissa has a small number of bits. We avoid most swamping errors by arranging the terms in dot product summation based on their exponents and summing the mantissas without overflowing the low-bitwidth accumulator. We design, analyze, and implement the algorithm to minimize 8-bit floating point error at inference time for several neural networks. In contrast to traditional sequential summation, our method has significantly lowered numerical errors, achieving classification accuracy on par with high-precision floating-point baselines for multiple image classification tasks. Our dMAC hardware units can reduce power consumption by up to 34.1\% relative to conventional MAC units.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.0 -->
                
            <!-- LLMs: 7.7 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2869
            </span>
            <a href="https://arxiv.org/abs/2504.09209" target="_blank" rel="noopener noreferrer">EchoMask: Speech-Queried Attention-based Mask Modeling for Holistic Co-Speech Motion Generation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xiangyue Zhang, Jianfang Li, Jiaxu Zhang, Jianqiang Ren, Liefeng Bo, Zhigang Tu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Masked modeling framework has shown promise in co-speech motion generation. However, it struggles to identify semantically significant frames for effective motion masking. In this work, we propose a speech-queried attention-based mask modeling framework for co-speech motion generation. Our key insig</span>
            
            <span class="abstract-full" style="display: none;">Masked modeling framework has shown promise in co-speech motion generation. However, it struggles to identify semantically significant frames for effective motion masking. In this work, we propose a speech-queried attention-based mask modeling framework for co-speech motion generation. Our key insight is to leverage motion-aligned speech features to guide the masked motion modeling process, selectively masking rhythm-related and semantically expressive motion frames. Specifically, we first propose a motion-audio alignment module (MAM) to construct a latent motion-audio joint space. In this space, both low-level and high-level speech features are projected, enabling motion-aligned speech representation using learnable speech queries. Then, a speech-queried attention mechanism (SQA) is introduced to compute frame-level attention scores through interactions between motion keys and speech queries, guiding selective masking toward motion frames with high attention scores. Finally, the motion-aligned speech features are also injected into the generation network to facilitate co-speech motion generation. Qualitative and quantitative evaluations confirm that our method outperforms existing state-of-the-art approaches, successfully producing high-quality co-speech motion.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.2 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.1 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4134
            </span>
            <a href="https://arxiv.org/abs/2504.08001" target="_blank" rel="noopener noreferrer">Linguistic Interpretability of Transformer-based Language Models: a systematic review</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Miguel L\'opez-Otal, Jorge Gracia, Jordi Bernad, Carlos Bobed, Luc\'ia Pitarch-Ballesteros, Emma Angl\'es-Herrero | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Language models based on the Transformer architecture achieve excellent results in many language-related tasks, such as text classification or sentiment analysis. However, despite the architecture of these models being well-defined, little is known about how their internal computations help them ach</span>
            
            <span class="abstract-full" style="display: none;">Language models based on the Transformer architecture achieve excellent results in many language-related tasks, such as text classification or sentiment analysis. However, despite the architecture of these models being well-defined, little is known about how their internal computations help them achieve their results. This renders these models, as of today, a type of 'black box' systems. There is, however, a line of research -- 'interpretability' -- aiming to learn how information is encoded inside these models. More specifically, there is work dedicated to studying whether Transformer-based models possess knowledge of linguistic phenomena similar to human speakers -- an area we call 'linguistic interpretability' of these models. In this survey we present a comprehensive analysis of 160 research works, spread across multiple languages and models -- including multilingual ones -- that attempt to discover linguistic information from the perspective of several traditional Linguistics disciplines: Syntax, Morphology, Lexico-Semantics and Discourse. Our survey fills a gap in the existing interpretability literature, which either not focus on linguistic knowledge in these models or present some limitations -- e.g. only studying English-based models. Our survey also focuses on Pre-trained Language Models not further specialized for a downstream task, with an emphasis on works that use interpretability techniques that explore models' internal representations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.415
            </span>
            <a href="https://arxiv.org/abs/2504.09440" target="_blank" rel="noopener noreferrer">Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: MingShan Liu, Shi Bo, Jialing Fang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explor</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.5 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4291
            </span>
            <a href="https://arxiv.org/abs/2501.04870" target="_blank" rel="noopener noreferrer">Deep Transfer $Q$-Learning for Offline Non-Stationary Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jinhang Chai, Elynn Chen, Jianqing Fan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In dynamic decision-making scenarios across business and healthcare, leveraging sample trajectories from diverse populations can significantly enhance reinforcement learning (RL) performance for specific target populations, especially when sample sizes are limited. While existing transfer learning m</span>
            
            <span class="abstract-full" style="display: none;">In dynamic decision-making scenarios across business and healthcare, leveraging sample trajectories from diverse populations can significantly enhance reinforcement learning (RL) performance for specific target populations, especially when sample sizes are limited. While existing transfer learning methods primarily focus on linear regression settings, they lack direct applicability to reinforcement learning algorithms. This paper pioneers the study of transfer learning for dynamic decision scenarios modeled by non-stationary finite-horizon Markov decision processes, utilizing neural networks as powerful function approximators and backward inductive learning. We demonstrate that naive sample pooling strategies, effective in regression settings, fail in Markov decision processes.To address this challenge, we introduce a novel ``re-weighted targeting procedure'' to construct ``transferable RL samples'' and propose ``transfer deep $Q^*$-learning'', enabling neural network approximation with theoretical guarantees. We assume that the reward functions are transferable and deal with both situations in which the transition densities are transferable or nontransferable. Our analytical techniques for transfer learning in neural network approximation and transition density transfers have broader implications, extending to supervised transfer learning with neural networks and domain shift scenarios. Empirical experiments on both synthetic and real datasets corroborate the advantages of our method, showcasing its potential for improving decision-making through strategically constructing transferable RL samples in non-stationary reinforcement learning contexts.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.467
            </span>
            <a href="https://arxiv.org/abs/2408.05886" target="_blank" rel="noopener noreferrer">Online-Score-Aided Federated Learning: Taming the Resource Constraints in Wireless Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Md-Ferdous Pervej, Minseok Choi, Andreas F. Molisch | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">While federated learning (FL) is a widely popular distributed machine learning (ML) strategy that protects data privacy, time-varying wireless network parameters and heterogeneous configurations of the wireless devices pose significant challenges. Although the limited radio and computational resourc</span>
            
            <span class="abstract-full" style="display: none;">While federated learning (FL) is a widely popular distributed machine learning (ML) strategy that protects data privacy, time-varying wireless network parameters and heterogeneous configurations of the wireless devices pose significant challenges. Although the limited radio and computational resources of the network and the clients, respectively, are widely acknowledged, two critical yet often ignored aspects are (a) wireless devices can only dedicate a small chunk of their limited storage for the FL task and (b) new training samples may arrive in an online manner in many practical wireless applications. Therefore, we propose a new FL algorithm called online-score-aided federated learning (OSAFL), specifically designed to learn tasks relevant to wireless applications under these practical considerations. Since clients' local training steps differ under resource constraints, which may lead to client drift under statistically heterogeneous data distributions, we leverage normalized gradient similarities and exploit weighting clients' updates based on optimized scores that facilitate the convergence rate of the proposed OSAFL algorithm without incurring any communication overheads to the clients or requiring any statistical data information from them. Our extensive simulation results on two different datasets with four popular ML models validate the effectiveness of OSAFL compared to five modified state-of-the-art FL baselines.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.2 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.5714
            </span>
            <a href="https://arxiv.org/abs/2502.03206" target="_blank" rel="noopener noreferrer">A Unified and General Humanoid Whole-Body Controller for Versatile Locomotion</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yufei Xue, Wentao Dong, Minghuan Liu, Weinan Zhang, Jiangmiao Pang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Locomotion is a fundamental skill for humanoid robots. However, most existing works make locomotion a single, tedious, unextendable, and unconstrained movement. This limits the kinematic capabilities of humanoid robots. In contrast, humans possess versatile athletic abilities-running, jumping, hoppi</span>
            
            <span class="abstract-full" style="display: none;">Locomotion is a fundamental skill for humanoid robots. However, most existing works make locomotion a single, tedious, unextendable, and unconstrained movement. This limits the kinematic capabilities of humanoid robots. In contrast, humans possess versatile athletic abilities-running, jumping, hopping, and finely adjusting gait parameters such as frequency and foot height. In this paper, we investigate solutions to bring such versatility into humanoid locomotion and thereby propose HugWBC: a unified and general humanoid whole-body controller for versatile locomotion. By designing a general command space in the aspect of tasks and behaviors, along with advanced techniques like symmetrical loss and intervention training for learning a whole-body humanoid controlling policy in simulation, HugWBC enables real-world humanoid robots to produce various natural gaits, including walking, jumping, standing, and hopping, with customizable parameters such as frequency, foot swing height, further combined with different body height, waist rotation, and body pitch. Beyond locomotion, HugWBC also supports real-time interventions from external upper-body controllers like teleoperation, enabling loco-manipulation with precision under any locomotive behavior. Extensive experiments validate the high tracking accuracy and robustness of HugWBC with/without upper-body intervention for all commands, and we further provide an in-depth analysis of how the various commands affect humanoid movement and offer insights into the relationships between these commands. To our knowledge, HugWBC is the first humanoid whole-body controller that supports such versatile locomotion behaviors with high robustness and flexibility.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6044
            </span>
            <a href="https://arxiv.org/abs/2409.19465" target="_blank" rel="noopener noreferrer">Construction of the Sparsest Maximally r-Robust Graphs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Haejoon Lee, Dimitra Panagou | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent years, the notion of r-robustness for the communication graph of the network has been introduced to address the challenge of achieving consensus in the presence of misbehaving agents. Higher r-robustness typically implies higher tolerance to malicious information towards achieving resilien</span>
            
            <span class="abstract-full" style="display: none;">In recent years, the notion of r-robustness for the communication graph of the network has been introduced to address the challenge of achieving consensus in the presence of misbehaving agents. Higher r-robustness typically implies higher tolerance to malicious information towards achieving resilient consensus, but it also implies more edges for the communication graph. This in turn conflicts with the need to minimize communication due to limited resources in real-world applications (e.g., multi-robot networks). In this paper, our contributions are twofold. (a) We provide the necessary subgraph structures and tight lower bounds on the number of edges required for graphs with a given number of nodes to achieve maximum robustness. (b) We then use the results of (a) to introduce two classes of graphs that maintain maximum robustness with the least number of edges. Our work is validated through a series of simulations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.7 -->
                
            <!-- LLMs: 6.2 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Math: 2.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.7 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6524
            </span>
            <a href="https://arxiv.org/abs/2504.08608" target="_blank" rel="noopener noreferrer">Discretization Error Analysis of a High Order Unfitted Space-Time Method for moving domain problems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Fabian Heimann, Christoph Lehrenfeld, Janosch Preu{\ss} | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a numerical analysis of a higher order unfitted space-time Finite Element method applied to a convection-diffusion model problem posed on a moving bulk domain. The method uses isoparametric space-time mappings for the geometry approximation of level set domains and has been presented and </span>
            
            <span class="abstract-full" style="display: none;">We present a numerical analysis of a higher order unfitted space-time Finite Element method applied to a convection-diffusion model problem posed on a moving bulk domain. The method uses isoparametric space-time mappings for the geometry approximation of level set domains and has been presented and investigated computationally in [Heimann, Lehrenfeld, Preu{\ss}, SIAM J. Sci. Comp. 45(2), 2023, B139 - B165]. Recently, in [Heimann, Lehrenfeld, IMA J. Numer. Anal., 2025] error bounds for the geometry approximation have been proven. In this paper we prove stability and accuracy including the influence of the geometry approximation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.7 -->
                
            <!-- Medicine: 6.6 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6941
            </span>
            <a href="https://arxiv.org/abs/2503.08643" target="_blank" rel="noopener noreferrer">Rethinking Diffusion Model in High Dimension</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhenxin Zheng, Zhenjie Zheng | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Curse of Dimensionality is an unavoidable challenge in statistical probability models, yet diffusion models seem to overcome this limitation, achieving impressive results in high-dimensional data generation. Diffusion models assume that they can learn the statistical properties of the underlying pro</span>
            
            <span class="abstract-full" style="display: none;">Curse of Dimensionality is an unavoidable challenge in statistical probability models, yet diffusion models seem to overcome this limitation, achieving impressive results in high-dimensional data generation. Diffusion models assume that they can learn the statistical properties of the underlying probability distribution, enabling sampling from this distribution to generate realistic samples. But is this really how they work? To address this question, this paper conducts a detailed analysis of the objective function and inference methods of diffusion models, leading to several important conclusions that help answer the above question: 1) In high-dimensional sparse scenarios, the target of the objective function fitting degrades from a weighted sum of multiple samples to a single sample. 2) The mainstream inference methods can all be represented within a simple unified framework, without requiring statistical concepts such as Markov chains and SDE, while aligning with the degraded objective function. 3) Guided by this simple framework, more efficient inference methods can be discovered.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.0 -->
                
            <!-- Medicine: 6.7 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.7384
            </span>
            <a href="https://arxiv.org/abs/2504.08247" target="_blank" rel="noopener noreferrer">Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Liu Xiao, Li Zhiyuan, Lin Yueyu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">State-based sequence models like RWKV-7 offer a compelling alternative to Transformer architectures, achieving linear complexity while demonstrating greater expressive power in short-context scenarios and enabling state tracking beyond the \(\text{TC}^0\) complexity class. However, RWKV-7 lacks mech</span>
            
            <span class="abstract-full" style="display: none;">State-based sequence models like RWKV-7 offer a compelling alternative to Transformer architectures, achieving linear complexity while demonstrating greater expressive power in short-context scenarios and enabling state tracking beyond the \(\text{TC}^0\) complexity class. However, RWKV-7 lacks mechanisms for token-parameter interactions and native scalability, limiting its adaptability and growth without retraining. In this paper, we propose \textbf{Meta-State}, a novel extension to RWKV-7 that replaces attention mechanisms with a fully state-driven approach, integrating token-parameter interactions through a \textbf{Self-State Encoder} (SSE) mechanism. The SSE repurposes a portion of the RWKV-7 Weighted Key-Value (WKV) state as transformation weights to encode token-parameter interactions in a linear, state-driven manner without introducing new trainable matrices or softmax operations, while preserving the autoregressive property of token processing. Meta-State supports progressive model scaling by expanding the WKV state and parameter tokens, reusing existing parameters without retraining. Our approach bridges the gap between state-based modeling, token-parameter interactions, and scalable architectures, offering a flexible framework for efficient and adaptable sequence modeling with linear complexity and constant memory usage.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.5 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.7481
            </span>
            <a href="https://arxiv.org/abs/2504.10248" target="_blank" rel="noopener noreferrer">Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for Dynamic Data Acquisition in Digital Twins</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Collins O. Ogbodo, Timothy J. Rogers, Mattia Dal Borgo, David J. Wagg | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper introduces a sensor steering methodology based on deep reinforcement learning to enhance the predictive accuracy and decision support capabilities of digital twins by optimising the data acquisition process. Traditional sensor placement techniques are often constrained by one-off optimisa</span>
            
            <span class="abstract-full" style="display: none;">This paper introduces a sensor steering methodology based on deep reinforcement learning to enhance the predictive accuracy and decision support capabilities of digital twins by optimising the data acquisition process. Traditional sensor placement techniques are often constrained by one-off optimisation strategies, which limit their applicability for online applications requiring continuous informative data assimilation. The proposed approach addresses this limitation by offering an adaptive framework for sensor placement within the digital twin paradigm. The sensor placement problem is formulated as a Markov decision process, enabling the training and deployment of an agent capable of dynamically repositioning sensors in response to the evolving conditions of the physical structure as represented by the digital twin. This ensures that the digital twin maintains a highly representative and reliable connection to its physical counterpart. The proposed framework is validated through a series of comprehensive case studies involving a cantilever plate structure subjected to diverse conditions, including healthy and damaged conditions. The results demonstrate the capability of the deep reinforcement learning agent to adaptively reposition sensors improving the quality of data acquisition and hence enhancing the overall accuracy of digital twins.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.0 -->
                
            <!-- LLMs: 7.3 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.7689
            </span>
            <a href="https://arxiv.org/abs/2504.09881" target="_blank" rel="noopener noreferrer">Focus on Local: Finding Reliable Discriminative Regions for Visual Place Recognition</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Changwei Wang, Shunpeng Chen, Yukun Song, Rongtao Xu, Zherui Zhang, Jiguang Zhang, Haoran Yang, Yu Zhang, Kexue Fu, Shide Du, Zhiwei Xu, Longxiang Gao, Li Guo, Shibiao Xu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Visual Place Recognition (VPR) is aimed at predicting the location of a query image by referencing a database of geotagged images. For VPR task, often fewer discriminative local regions in an image produce important effects while mundane background regions do not contribute or even cause perceptual </span>
            
            <span class="abstract-full" style="display: none;">Visual Place Recognition (VPR) is aimed at predicting the location of a query image by referencing a database of geotagged images. For VPR task, often fewer discriminative local regions in an image produce important effects while mundane background regions do not contribute or even cause perceptual aliasing because of easy overlap. However, existing methods lack precisely modeling and full exploitation of these discriminative regions. In this paper, we propose the Focus on Local (FoL) approach to stimulate the performance of image retrieval and re-ranking in VPR simultaneously by mining and exploiting reliable discriminative local regions in images and introducing pseudo-correlation supervision. First, we design two losses, Extraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background Contrast Enhancement Loss (CEL), to explicitly model reliable discriminative local regions and use them to guide the generation of global representations and efficient re-ranking. Second, we introduce a weakly-supervised local feature training strategy based on pseudo-correspondences obtained from aggregating global features to alleviate the lack of local correspondences ground truth for the VPR task. Third, we suggest an efficient re-ranking pipeline that is efficiently and precisely based on discriminative region guidance. Finally, experimental results show that our FoL achieves the state-of-the-art on multiple VPR benchmarks in both image retrieval and re-ranking stages and also significantly outperforms existing two-stage VPR methods in terms of computational efficiency. Code and models are available at https://github.com/chenshunpeng/FoL</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.8069
            </span>
            <a href="https://arxiv.org/abs/2504.08626" target="_blank" rel="noopener noreferrer">Task-conditioned Ensemble of Expert Models for Continuous Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Renu Sharma, Debasmita Pal, Arun Ross | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed mode</span>
            
            <span class="abstract-full" style="display: none;">One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed model with new data could be one remedy. However, the question arises as to how we should update the model with new training data so that it retains its accuracy on the old data while adapting to the new data. In this work, we propose a task-conditioned ensemble of models to maintain the performance of the existing model. The method involves an ensemble of expert models based on task membership information. The in-domain models-based on the local outlier concept (different from the expert models) provide task membership information dynamically at run-time to each probe sample. To evaluate the proposed method, we experiment with three setups: the first represents distribution shift between tasks (LivDet-Iris-2017), the second represents distribution shift both between and within tasks (LivDet-Iris-2020), and the third represents disjoint distribution between tasks (Split MNIST). The experiments highlight the benefits of the proposed method. The source code is available at https://github.com/iPRoBe-lab/Continuous_Learning_FE_DM.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.5 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.8136
            </span>
            <a href="https://arxiv.org/abs/2504.09634" target="_blank" rel="noopener noreferrer">Evaluating Machine Learning-Driven Intrusion Detection Systems in IoT: Performance and Energy Consumption</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Saeid Jamshidi, Kawser Wazed Nafi, Amin Nikanjam, Foutse Khomh | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In the evolving landscape of the Internet of Things (IoT), Machine Learning (ML)-based Intrusion Detection Systems (IDS) represent a significant advancement, especially when integrated with Software-Defined Networking (SDN). These systems play a critical role in enhancing security infrastructure wit</span>
            
            <span class="abstract-full" style="display: none;">In the evolving landscape of the Internet of Things (IoT), Machine Learning (ML)-based Intrusion Detection Systems (IDS) represent a significant advancement, especially when integrated with Software-Defined Networking (SDN). These systems play a critical role in enhancing security infrastructure within resource-constrained IoT systems. Despite their growing adoption, limited research has explored the impact of ML-based IDS on key performance metrics, such as CPU load, CPU usage, and energy consumption, particularly under real-time cyber threats. This study bridges that gap through an empirical evaluation of cutting-edge ML-based IDSs deployed at the edge of IoT networks under both benign and attack scenarios. Additionally, we investigate how SDN's centralized control and dynamic resource management influence IDS performance. Our experimental framework compares traditional ML-based IDS with deep learning (DL)-based counterparts, both with and without SDN integration. Results reveal that edge-deployed ML-based IDSs significantly impact system performance during cyber threats, with marked increases in resource consumption. SDN integration further influences these outcomes, emphasizing the need for optimized architectural design. Statistical analysis using ANOVA confirms the significance of our findings. This research provides critical insights into the performance and trade-offs of deploying ML-based IDSs in edge-based IoT systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.7 -->
                
            <!-- LLMs: 8.5 -->
                
            <!-- Quantum Computing: 4.7 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.842
            </span>
            <a href="https://arxiv.org/abs/2504.09339" target="_blank" rel="noopener noreferrer">Towards Optimal Differentially Private Regret Bounds in Linear MDPs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sharan Sahu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We study regret minimization under privacy constraints in episodic inhomogeneous linear Markov Decision Processes (MDPs), motivated by the growing use of reinforcement learning (RL) in personalized decision-making systems that rely on sensitive user data. In this setting, both transition probabiliti</span>
            
            <span class="abstract-full" style="display: none;">We study regret minimization under privacy constraints in episodic inhomogeneous linear Markov Decision Processes (MDPs), motivated by the growing use of reinforcement learning (RL) in personalized decision-making systems that rely on sensitive user data. In this setting, both transition probabilities and reward functions are assumed to be linear in a feature mapping $\phi(s, a)$, and we aim to ensure privacy through joint differential privacy (JDP), a relaxation of differential privacy suited to online learning. Prior work has established suboptimal regret bounds by privatizing the LSVI-UCB algorithm, which achieves $\widetilde{O}(\sqrt{d^3 H^4 K})$ regret in the non-private setting. Building on recent advances that improve this to minimax optimal regret $\widetilde{O}(HD\sqrt{K})$ via LSVI-UCB++ with Bernstein-style bonuses, we design a new differentially private algorithm by privatizing LSVI-UCB++ and adapting techniques for variance-aware analysis from offline RL. Our algorithm achieves a regret bound of $\widetilde{O}(d \sqrt{H^3 K} + H^{4.5} d^{7/6} K^{1/2} / \epsilon)$, improving over previous private methods. Empirical results show that our algorithm retains near-optimal utility compared to non-private baselines, indicating that privacy can be achieved with minimal performance degradation in this setting.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.9 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.8816
            </span>
            <a href="https://arxiv.org/abs/2504.10451" target="_blank" rel="noopener noreferrer">Minimizing Functions of Age of Incorrect Information for Remote Estimation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ismail Cosandal, Sennur Ulukus, Nail Akar | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The age of incorrect information (AoII) process which keeps track of the time since the source and monitor processes are in sync, has been extensively used in remote estimation problems. In this paper, we consider a push-based remote estimation system with a discrete-time Markov chain (DTMC) informa</span>
            
            <span class="abstract-full" style="display: none;">The age of incorrect information (AoII) process which keeps track of the time since the source and monitor processes are in sync, has been extensively used in remote estimation problems. In this paper, we consider a push-based remote estimation system with a discrete-time Markov chain (DTMC) information source transmitting status update packets towards the monitor once the AoII process exceeds a certain estimation-based threshold. In this paper, the time average of an arbitrary function of AoII is taken as the AoII cost, as opposed to using the average AoII as the mismatch metric, whereas this function is also allowed to depend on the estimation value. In this very general setting, our goal is to minimize a weighted sum of AoII and transmission costs. For this purpose, we formulate a discrete-time semi-Markov decision process (SMDP) regarding the multi-threshold status update policy. We propose a novel tool in discrete-time called 'dual-regime absorbing Markov chain' (DR-AMC) and its corresponding absorption time distribution named as 'dual-regime phase-type' (DR-PH) distribution, to obtain the characterizing parameters of the SMDP, which allows us to obtain the distribution of the AoII process for a given policy, and hence the average of any function of AoII. The proposed method is validated with numerical results by which we compare our proposed method against other policies obtained by exhaustive-search, and also various benchmark policies.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.0 -->
                
            <!-- Medicine: 6.6 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9214
            </span>
            <a href="https://arxiv.org/abs/2504.09804" target="_blank" rel="noopener noreferrer">BO-SA-PINNs: Self-adaptive physics-informed neural networks based on Bayesian optimization for automatically designing PDE solvers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rui Zhang, Liang Li, St\'ephane Lanteri, Hao Kang, Jiaqi Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Physics-informed neural networks (PINNs) is becoming a popular alternative method for solving partial differential equations (PDEs). However, they require dedicated manual modifications to the hyperparameters of the network, the sampling methods and loss function weights for different PDEs, which re</span>
            
            <span class="abstract-full" style="display: none;">Physics-informed neural networks (PINNs) is becoming a popular alternative method for solving partial differential equations (PDEs). However, they require dedicated manual modifications to the hyperparameters of the network, the sampling methods and loss function weights for different PDEs, which reduces the efficiency of the solvers. In this paper, we pro- pose a general multi-stage framework, i.e. BO-SA-PINNs to alleviate this issue. In the first stage, Bayesian optimization (BO) is used to select hyperparameters for the training process, and based on the results of the pre-training, the network architecture, learning rate, sampling points distribution and loss function weights suitable for the PDEs are automatically determined. The proposed hyperparameters search space based on experimental results can enhance the efficiency of BO in identifying optimal hyperparameters. After selecting the appropriate hyperparameters, we incorporate a global self-adaptive (SA) mechanism the second stage. Using the pre-trained model and loss information in the second-stage training, the exponential moving average (EMA) method is employed to optimize the loss function weights, and residual-based adaptive refinement with distribution (RAR-D) is used to optimize the sampling points distribution. In the third stage, L-BFGS is used for stable training. In addition, we introduce a new activation function that enables BO-SA-PINNs to achieve higher accuracy. In numerical experiments, we conduct comparative and ablation experiments to verify the performance of the model on Helmholtz, Maxwell, Burgers and high-dimensional Poisson equations. The comparative experiment results show that our model can achieve higher accuracy and fewer iterations in test cases, and the ablation experiments demonstrate the positive impact of every improvement.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.1 -->
                
            <!-- Medicine: 5.6 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Federated Learning: 2.2 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9271
            </span>
            <a href="https://arxiv.org/abs/2504.08752" target="_blank" rel="noopener noreferrer">Patience is all you need! An agentic system for performing scientific literature review</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: David Brett, Anniek Myatt | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) have grown in their usage to provide support for question answering across numerous disciplines. The models on their own have already shown promise for answering basic questions, however fail quickly where expert domain knowledge is required or the question is nuanced. S</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) have grown in their usage to provide support for question answering across numerous disciplines. The models on their own have already shown promise for answering basic questions, however fail quickly where expert domain knowledge is required or the question is nuanced. Scientific research often involves searching for relevant literature, distilling pertinent information from that literature and analysing how the findings support or contradict one another. The information is often encapsulated in the full text body of research articles, rather than just in the abstracts. Statements within these articles frequently require the wider article context to be fully understood. We have built an LLM-based system that performs such search and distillation of information encapsulated in scientific literature, and we evaluate our keyword based search and information distillation system against a set of biology related questions from previously released literature benchmarks. We demonstrate sparse retrieval methods exhibit results close to state of the art without the need for dense retrieval, with its associated infrastructure and complexity overhead. We also show how to increase the coverage of relevant documents for literature review generation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.7 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.959
            </span>
            <a href="https://arxiv.org/abs/2504.08946" target="_blank" rel="noopener noreferrer">Incremental Bidirectional Typing via Order Maintenance</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Thomas J. Porter, Marisa Kirisame, Ivan Wei, Pavel Panchekha, Cyrus Omar | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Live programming environments provide various semantic services, including type checking and evaluation, continuously as the user is editing the program. The live paradigm promises to improve the developer experience, but liveness is an implementation challenge particularly when working with large p</span>
            
            <span class="abstract-full" style="display: none;">Live programming environments provide various semantic services, including type checking and evaluation, continuously as the user is editing the program. The live paradigm promises to improve the developer experience, but liveness is an implementation challenge particularly when working with large programs. This paper specifies and efficiently implements a system the is able to incrementally update type information for a live program in response to fine-grained program edits. This information includes type error marks and information about the expected and actual type on every expression. The system is specified type-theoretically as a small-step dynamics that propagates updates through the marked and annotated program. Most updates flow according to a base bidirectional type system. Additional pointers are maintained to connect bound variables to their binding locations, with edits traversing these pointers directly. Order maintenance data structures are employed to efficiently maintain these pointers and to prioritize the order of update propagation. We prove this system is equivalent to naive re-analysis in the Agda theorem prover, along with other important metatheoretic properties. We then implement it efficiently in OCaml, detailing a number of impactful optimizations. We evaluate this implementation's performance with a large stress-test and find that it is able to achieve dramatic speed-ups of 275.96$\times$ compared to from-scratch reanalysis.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.3 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9651
            </span>
            <a href="https://arxiv.org/abs/2504.09620" target="_blank" rel="noopener noreferrer">Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yuta Matsui, Ryosuke Yamaki, Ryo Ueda, Seitaro Shinagawa, Tadahiro Taniguchi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse knowledge of multiple vision-language models (VLMs) by learning from each other. Although existing methods that combine multiple models suffer from inference costs and architectural constraints, MHCG avoids these problems by</span>
            
            <span class="abstract-full" style="display: none;">We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse knowledge of multiple vision-language models (VLMs) by learning from each other. Although existing methods that combine multiple models suffer from inference costs and architectural constraints, MHCG avoids these problems by performing decentralized Bayesian inference through a process resembling a language game. The knowledge fusion process establishes communication between two VLM agents alternately captioning images and learning from each other. We conduct two image-captioning experiments with two VLMs, each pre-trained on a different dataset. The first experiment demonstrates that MHCG achieves consistent improvement in reference-free evaluation metrics. The second experiment investigates how MHCG contributes to sharing VLMs' category-level vocabulary by observing the occurrence of the vocabulary in the generated captions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9758
            </span>
            <a href="https://arxiv.org/abs/2504.09514" target="_blank" rel="noopener noreferrer">Capturing Longitudinal Changes in Brain Morphology Using Temporally Parameterized Neural Displacement Fields</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Aisha L. Shuaibu, Kieran A. Gibb, Peter A. Wijeratne, Ivor J. A. Simpson | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Longitudinal image registration enables studying temporal changes in brain morphology which is useful in applications where monitoring the growth or atrophy of specific structures is important. However this task is challenging due to; noise/artifacts in the data and quantifying small anatomical chan</span>
            
            <span class="abstract-full" style="display: none;">Longitudinal image registration enables studying temporal changes in brain morphology which is useful in applications where monitoring the growth or atrophy of specific structures is important. However this task is challenging due to; noise/artifacts in the data and quantifying small anatomical changes between sequential scans. We propose a novel longitudinal registration method that models structural changes using temporally parameterized neural displacement fields. Specifically, we implement an implicit neural representation (INR) using a multi-layer perceptron that serves as a continuous coordinate-based approximation of the deformation field at any time point. In effect, for any N scans of a particular subject, our model takes as input a 3D spatial coordinate location x, y, z and a corresponding temporal representation t and learns to describe the continuous morphology of structures for both observed and unobserved points in time. Furthermore, we leverage the analytic derivatives of the INR to derive a new regularization function that enforces monotonic rate of change in the trajectory of the voxels, which is shown to provide more biologically plausible patterns. We demonstrate the effectiveness of our method on 4D brain MR registration.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.4 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9993
            </span>
            <a href="https://arxiv.org/abs/2504.09570" target="_blank" rel="noopener noreferrer">LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Biao Fu, Minpeng Liao, Kai Fan, Chengxi Li, Liang Zhang, Yidong Chen, Xiaodong Shi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">When the complete source sentence is provided, Large Language Models (LLMs) perform excellently in offline machine translation even with a simple prompt "Translate the following sentence from [src lang] into [tgt lang]:". However, in many real scenarios, the source tokens arrive in a streaming manne</span>
            
            <span class="abstract-full" style="display: none;">When the complete source sentence is provided, Large Language Models (LLMs) perform excellently in offline machine translation even with a simple prompt "Translate the following sentence from [src lang] into [tgt lang]:". However, in many real scenarios, the source tokens arrive in a streaming manner and simultaneous machine translation (SiMT) is required, then the efficiency and performance of decoder-only LLMs are significantly limited by their auto-regressive nature. To enable LLMs to achieve high-quality SiMT as efficiently as offline translation, we propose a novel paradigm that includes constructing supervised fine-tuning (SFT) data for SiMT, along with new training and inference strategies. To replicate the token input/output stream in SiMT, the source and target tokens are rearranged into an interleaved sequence, separated by special tokens according to varying latency requirements. This enables powerful LLMs to learn read and write operations adaptively, based on varying latency prompts, while still maintaining efficient auto-regressive decoding. Experimental results show that, even with limited SFT data, our approach achieves state-of-the-art performance across various SiMT benchmarks, and preserves the original abilities of offline translation. Moreover, our approach generalizes well to document-level SiMT setting without requiring specific fine-tuning, even beyond the offline translation model.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.3 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0102
            </span>
            <a href="https://arxiv.org/abs/2504.09006" target="_blank" rel="noopener noreferrer">Learning in Structured Stackelberg Games</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Maria-Florina Balcan, Kiriaki Fragkia, Keegan Harris | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We study structured Stackelberg games, in which both players (the leader and the follower) observe information about the state of the world at time of play. Importantly, this information may contain information about the follower, which the leader may use when deciding her strategy. Under this setti</span>
            
            <span class="abstract-full" style="display: none;">We study structured Stackelberg games, in which both players (the leader and the follower) observe information about the state of the world at time of play. Importantly, this information may contain information about the follower, which the leader may use when deciding her strategy. Under this setting, we show that no-regret learning is possible if and only if the set of mappings from contexts to follower types that the leader uses to learn is not ``too complex''. Specifically, we find that standard learning theoretic measures of complexity do not characterize learnability in our setting and we give a new dimension which does, which we term the Stackelberg-Littlestone dimension. In the distributional setting, we give analogous results by showing that standard complexity measures do not characterize the sample complexity of learning, but a new dimension called the Stackelberg-Natarajan dimension does. We then show that an appropriate empirical risk minimization procedure achieves the corresponding sample complexity.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 6.7 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0126
            </span>
            <a href="https://arxiv.org/abs/2504.08254" target="_blank" rel="noopener noreferrer">Understanding the Impact of Data Domain Extraction on Synthetic Data Privacy</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Georgi Ganev, Meenatchi Sundaram Muthu Selva Annamalai, Sofiane Mahiou, Emiliano De Cristofaro | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Privacy attacks, particularly membership inference attacks (MIAs), are widely used to assess the privacy of generative models for tabular synthetic data, including those with Differential Privacy (DP) guarantees. These attacks often exploit outliers, which are especially vulnerable due to their posi</span>
            
            <span class="abstract-full" style="display: none;">Privacy attacks, particularly membership inference attacks (MIAs), are widely used to assess the privacy of generative models for tabular synthetic data, including those with Differential Privacy (DP) guarantees. These attacks often exploit outliers, which are especially vulnerable due to their position at the boundaries of the data domain (e.g., at the minimum and maximum values). However, the role of data domain extraction in generative models and its impact on privacy attacks have been overlooked. In this paper, we examine three strategies for defining the data domain: assuming it is externally provided (ideally from public data), extracting it directly from the input data, and extracting it with DP mechanisms. While common in popular implementations and libraries, we show that the second approach breaks end-to-end DP guarantees and leaves models vulnerable. While using a provided domain (if representative) is preferable, extracting it with DP can also defend against popular MIAs, even at high privacy budgets.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0213
            </span>
            <a href="https://arxiv.org/abs/2504.04430" target="_blank" rel="noopener noreferrer">AGITB: A Signal-Level Benchmark for Evaluating Artificial General Intelligence</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Matej \v{S}progar | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Despite remarkable progress in machine learning, current AI systems continue to fall short of true human-like intelligence. While Large Language Models (LLMs) excel in pattern recognition and response generation, they lack genuine understanding - an essential hallmark of Artificial General Intellige</span>
            
            <span class="abstract-full" style="display: none;">Despite remarkable progress in machine learning, current AI systems continue to fall short of true human-like intelligence. While Large Language Models (LLMs) excel in pattern recognition and response generation, they lack genuine understanding - an essential hallmark of Artificial General Intelligence (AGI). Existing AGI evaluation methods fail to offer a practical, gradual, and informative metric. This paper introduces the Artificial General Intelligence Test Bed (AGITB), comprising twelve rigorous tests that form a signal-processing-level foundation for the potential emergence of cognitive capabilities. AGITB evaluates intelligence through a model's ability to predict binary signals across time without relying on symbolic representations or pretraining. Unlike high-level tests grounded in language or perception, AGITB focuses on core computational invariants reflective of biological intelligence, such as determinism, sensitivity, and generalisation. The test bed assumes no prior bias, operates independently of semantic meaning, and ensures unsolvability through brute force or memorization. While humans pass AGITB by design, no current AI system has met its criteria, making AGITB a compelling benchmark for guiding and recognizing progress toward AGI.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.5 -->
                
            <!-- Medicine: 6.5 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- RAG: 1.2 -->
                
            <!-- Federated Learning: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0235
            </span>
            <a href="https://arxiv.org/abs/2504.09047" target="_blank" rel="noopener noreferrer">Multi-Robot Coordination with Adversarial Perception</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rayan Bahrami, Hamidreza Jafarnejadsani | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper investigates the resilience of perception-based multi-robot coordination with wireless communication to online adversarial perception. A systematic study of this problem is essential for many safety-critical robotic applications that rely on the measurements from learned perception module</span>
            
            <span class="abstract-full" style="display: none;">This paper investigates the resilience of perception-based multi-robot coordination with wireless communication to online adversarial perception. A systematic study of this problem is essential for many safety-critical robotic applications that rely on the measurements from learned perception modules. We consider a (small) team of quadrotor robots that rely only on an Inertial Measurement Unit (IMU) and the visual data measurements obtained from a learned multi-task perception module (e.g., object detection) for downstream tasks, including relative localization and coordination. We focus on a class of adversarial perception attacks that cause misclassification, mislocalization, and latency. We propose that the effects of adversarial misclassification and mislocalization can be modeled as sporadic (intermittent) and spurious measurement data for the downstream tasks. To address this, we present a framework for resilience analysis of multi-robot coordination with adversarial measurements. The framework integrates data from Visual-Inertial Odometry (VIO) and the learned perception model for robust relative localization and state estimation in the presence of adversarially sporadic and spurious measurements. The framework allows for quantifying the degradation in system observability and stability in relation to the success rate of adversarial perception. Finally, experimental results on a multi-robot platform demonstrate the real-world applicability of our methodology for resource-constrained robotic platforms.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0311
            </span>
            <a href="https://arxiv.org/abs/2504.08242" target="_blank" rel="noopener noreferrer">Jupiter: Fast and Resource-Efficient Collaborative Inference of Generative LLMs on Edge Devices</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shengyuan Ye, Bei Ouyang, Liekang Zeng, Tianyi Qian, Xiaowen Chu, Jian Tang, Xu Chen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative large language models (LLMs) have garnered significant attention due to their exceptional capabilities in various AI tasks. Traditionally deployed in cloud datacenters, LLMs are now increasingly moving towards more accessible edge platforms to protect sensitive user data and ensure privac</span>
            
            <span class="abstract-full" style="display: none;">Generative large language models (LLMs) have garnered significant attention due to their exceptional capabilities in various AI tasks. Traditionally deployed in cloud datacenters, LLMs are now increasingly moving towards more accessible edge platforms to protect sensitive user data and ensure privacy preservation. The limited computational resources of individual edge devices, however, can result in excessively prolonged inference latency and overwhelmed memory usage. While existing research has explored collaborative edge computing to break the resource wall of individual devices, these solutions yet suffer from massive communication overhead and under-utilization of edge resources. Furthermore, they focus exclusively on optimizing the prefill phase, neglecting the crucial autoregressive decoding phase for generative LLMs. To address that, we propose Jupiter, a fast, scalable, and resource-efficient collaborative edge AI system for generative LLM inference. Jupiter introduces a flexible pipelined architecture as a principle and differentiates its system design according to the differentiated characteristics of the prefill and decoding phases. For prefill phase, Jupiter submits a novel intra-sequence pipeline parallelism and develops a meticulous parallelism planning strategy to maximize resource efficiency; For decoding, Jupiter devises an effective outline-based pipeline parallel decoding mechanism combined with speculative decoding, which further magnifies inference acceleration. Extensive evaluation based on realistic implementation demonstrates that Jupiter remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving up to 26.1x end-to-end latency reduction while rendering on-par generation quality.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0423
            </span>
            <a href="https://arxiv.org/abs/2504.03036" target="_blank" rel="noopener noreferrer">IPA-CHILDES & G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Z\'ebulon Goriely, Paula Buttery | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we introduce two resources: (i) G2P+, a tool for converting orthographic datasets to a consistent phonemic representation; and (ii) IPA CHILDES, a phonemic dataset of child-centered speech across 31 languages. Prior tools for grapheme-to-phoneme conversion result in phonemic vocabular</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we introduce two resources: (i) G2P+, a tool for converting orthographic datasets to a consistent phonemic representation; and (ii) IPA CHILDES, a phonemic dataset of child-centered speech across 31 languages. Prior tools for grapheme-to-phoneme conversion result in phonemic vocabularies that are inconsistent with established phonemic inventories, an issue which G2P+ addresses by leveraging the inventories in the Phoible database. Using this tool, we augment CHILDES with phonemic transcriptions to produce IPA CHILDES. This new resource fills several gaps in existing phonemic datasets, which often lack multilingual coverage, spontaneous speech, and a focus on child-directed language. We demonstrate the utility of this dataset for phonological research by training phoneme language models on 11 languages and probing them for distinctive features, finding that the distributional properties of phonemes are sufficient to learn major class and place features cross-lingually.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0702
            </span>
            <a href="https://arxiv.org/abs/2504.09449" target="_blank" rel="noopener noreferrer">aweSOM: a CPU/GPU-accelerated Self-organizing Map and Statistically Combined Ensemble Framework for Machine-learning Clustering Analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Trung Ha, Joonas N\"attil\"a, Jordy Davelaar | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We introduce aweSOM, an open-source Python package for machine learning (ML) clustering and classification, using a Self-organizing Maps (SOM) algorithm that incorporates CPU/GPU acceleration to accommodate large ($N > 10^6$, where $N$ is the number of data points), multidimensional datasets. aweSOM</span>
            
            <span class="abstract-full" style="display: none;">We introduce aweSOM, an open-source Python package for machine learning (ML) clustering and classification, using a Self-organizing Maps (SOM) algorithm that incorporates CPU/GPU acceleration to accommodate large ($N > 10^6$, where $N$ is the number of data points), multidimensional datasets. aweSOM consists of two main modules, one that handles the initialization and training of the SOM, and another that stacks the results of multiple SOM realizations to obtain more statistically robust clusters. Existing Python-based SOM implementations (e.g., POPSOM, Yuan (2018); MiniSom, Vettigli (2018); sklearn-som) primarily serve as proof-of-concept demonstrations, optimized for smaller datasets, but lacking scalability for large, multidimensional data. aweSOM provides a solution for this gap in capability, with good performance scaling up to $\sim 10^8$ individual points, and capable of utilizing multiple features per point. We compare the code performance against the legacy implementations it is based on, and find a 10-100x speed up, as well as significantly improved memory efficiency, due to several built-in optimizations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.4 -->
                
            <!-- LLMs: 7.8 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0747
            </span>
            <a href="https://arxiv.org/abs/2501.07824" target="_blank" rel="noopener noreferrer">Real-time Verification and Refinement of Language Model Text Generation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Joonho Ko, Jinheon Baek, Sung Ju Hwang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their genera</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their generation and further refining them, they are slow in deployment since they are designed to verify the response from LLMs only after their entire generation (from the first to last tokens) is done. Further, we observe that once LLMs generate incorrect tokens early on, there is a higher likelihood that subsequent tokens will also be factually incorrect. To this end, in this work, we propose Streaming-VR (Streaming Verification and Refinement), a novel approach designed to enhance the efficiency of verification and refinement of LLM outputs. Specifically, the proposed Streaming-VR enables on-the-fly verification and correction of tokens as they are being generated, similar to a streaming process, ensuring that each subset of tokens is checked and refined in real-time by another LLM as the LLM constructs its response. Through comprehensive evaluations on multiple datasets, we demonstrate that our approach not only enhances the factual accuracy of LLMs, but also offers a more efficient solution compared to prior refinement methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.8 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.0 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0839
            </span>
            <a href="https://arxiv.org/abs/2503.03953" target="_blank" rel="noopener noreferrer">GeoDEN: A Visual Exploration Tool for Analysing the Geographic Spread of Dengue Serotypes</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Aidan Marler, Yannik Roell, Steffen Knoblauch, Jane P. Messina, Thomas Jaenisch, Morteza Karimzadeh | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Static maps and animations remain popular in spatial epidemiology of dengue, limiting the analytical depth and scope of visualisations. Over half of the global population live in dengue endemic regions. Understanding the spatiotemporal dynamics of the four closely related dengue serotypes, and their</span>
            
            <span class="abstract-full" style="display: none;">Static maps and animations remain popular in spatial epidemiology of dengue, limiting the analytical depth and scope of visualisations. Over half of the global population live in dengue endemic regions. Understanding the spatiotemporal dynamics of the four closely related dengue serotypes, and their immunological interactions, remains a challenge at a global scale. To facilitate this understanding, we worked with dengue epidemiologists in a user-centered design framework to create GeoDEN, an exploratory visualisation tool that empowers experts to investigate spatiotemporal patterns in dengue serotype reports. The tool has several linked visualisations and filtering mechanisms, enabling analysis at a range of spatial and temporal scales. To identify successes and failures, we present both insight-based and value-driven evaluations. Our domain experts found GeoDEN valuable, verifying existing hypotheses and uncovering novel insights that warrant further investigation by the epidemiology community. The developed visual exploration approach can be adapted for exploring other epidemiology and disease incident datasets.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0921
            </span>
            <a href="https://arxiv.org/abs/2502.00896" target="_blank" rel="noopener noreferrer">LoR-VP: Low-Rank Visual Prompting for Efficient Vision Model Adaptation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Can Jin, Ying Li, Mingyu Zhao, Shiyu Zhao, Zhenting Wang, Xiaoxiao He, Ligong Han, Tong Che, Dimitris N. Metaxas | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Visual prompting has gained popularity as a method for adapting pre-trained models to specific tasks, particularly in the realm of parameter-efficient tuning. However, existing visual prompting techniques often pad the prompt parameters around the image, limiting the interaction between the visual p</span>
            
            <span class="abstract-full" style="display: none;">Visual prompting has gained popularity as a method for adapting pre-trained models to specific tasks, particularly in the realm of parameter-efficient tuning. However, existing visual prompting techniques often pad the prompt parameters around the image, limiting the interaction between the visual prompts and the original image to a small set of patches while neglecting the inductive bias present in shared information across different patches. In this study, we conduct a thorough preliminary investigation to identify and address these limitations. We propose a novel visual prompt design, introducing Low-Rank matrix multiplication for Visual Prompting (LoR-VP), which enables shared and patch-specific information across rows and columns of image pixels. Extensive experiments across seven network architectures and four datasets demonstrate significant improvements in both performance and efficiency compared to state-of-the-art visual prompting methods, achieving up to 6 times faster training times, utilizing 18 times fewer visual prompt parameters, and delivering a 3.1% improvement in performance. The code is available as https://github.com/jincan333/LoR-VP.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1038
            </span>
            <a href="https://arxiv.org/abs/2504.09664" target="_blank" rel="noopener noreferrer">Adapting to the Unknown: Robust Meta-Learning for Zero-Shot Financial Time Series Forecasting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anxian Liu, Junying Ma, Guang Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Financial time series forecasting in the zero-shot setting is essential for risk management and investment decision-making, particularly during abrupt market regime shifts or in emerging markets with limited historical data. While Model-Agnostic Meta-Learning (MAML)-based approaches have shown promi</span>
            
            <span class="abstract-full" style="display: none;">Financial time series forecasting in the zero-shot setting is essential for risk management and investment decision-making, particularly during abrupt market regime shifts or in emerging markets with limited historical data. While Model-Agnostic Meta-Learning (MAML)-based approaches have shown promise in this domain, existing meta task construction strategies often lead to suboptimal performance, especially when dealing with highly turbulent financial time series. To address this challenge, we propose a novel task construction method that leverages learned embeddings for more effective meta-learning in the zero-shot setting. Specifically, we construct two complementary types of meta-tasks based on the learned embeddings: intra-cluster tasks and inter-cluster tasks. To capture diverse fine-grained patterns, we apply stochastic projection matrices to the learned embeddings and use clustering algorithm to form the tasks. Additionally, to improve generalization capabilities, we employ hard task mining strategies and leverage inter-cluster tasks to identify invariant patterns across different time series. Extensive experiments on the real world financial dataset demonstrate that our method significantly outperforms existing approaches, showing better generalization ability in the zero-shot scenario.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1085
            </span>
            <a href="https://arxiv.org/abs/2409.11267" target="_blank" rel="noopener noreferrer">Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Caio Fabio Oliveira da Silva, Azita Dabiri, Bart De Schutter | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work proposes an approach that integrates reinforcement learning and model predictive control (MPC) to solve finite-horizon optimal control problems in mixed-logical dynamical systems efficiently. Optimization-based control of such systems with discrete and continuous decision variables entails</span>
            
            <span class="abstract-full" style="display: none;">This work proposes an approach that integrates reinforcement learning and model predictive control (MPC) to solve finite-horizon optimal control problems in mixed-logical dynamical systems efficiently. Optimization-based control of such systems with discrete and continuous decision variables entails the online solution of mixed-integer linear programs, which suffer from the curse of dimensionality. Our approach aims to mitigate this issue by decoupling the decision on the discrete variables from the decision on the continuous variables. In the proposed approach, reinforcement learning determines the discrete decision variables and simplifies the online optimization problem of the MPC controller from a mixed-integer linear program to a linear program, significantly reducing the computational time. A fundamental contribution of this work is the definition of the decoupled Q-function, which plays a crucial role in making the learning problem tractable in a combinatorial action space. We motivate the use of recurrent neural networks to approximate the decoupled Q-function and show how they can be employed in a reinforcement learning setting. Simulation experiments on a microgrid system using real-world data demonstrate that the proposed method substantially reduces the online computation time of MPC while maintaining high feasibility and low suboptimality.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.2 -->
                
            <!-- LLMs: 6.4 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 2.5 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1498
            </span>
            <a href="https://arxiv.org/abs/2504.10150" target="_blank" rel="noopener noreferrer">HistLLM: A Unified Framework for LLM-Based Multimodal Recommendation with User History Encoding and Compression</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chen Zhang, Bo Hu, Weidong Chen, Zhendong Mao | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">While large language models (LLMs) have proven effective in leveraging textual data for recommendations, their application to multimodal recommendation tasks remains relatively underexplored. Although LLMs can process multimodal information through projection functions that map visual features into </span>
            
            <span class="abstract-full" style="display: none;">While large language models (LLMs) have proven effective in leveraging textual data for recommendations, their application to multimodal recommendation tasks remains relatively underexplored. Although LLMs can process multimodal information through projection functions that map visual features into their semantic space, recommendation tasks often require representing users' history interactions through lengthy prompts combining text and visual elements, which not only hampers training and inference efficiency but also makes it difficult for the model to accurately capture user preferences from complex and extended prompts, leading to reduced recommendation performance. To address this challenge, we introduce HistLLM, an innovative multimodal recommendation framework that integrates textual and visual features through a User History Encoding Module (UHEM), compressing multimodal user history interactions into a single token representation, effectively facilitating LLMs in processing user preferences. Extensive experiments demonstrate the effectiveness and efficiency of our proposed mechanism.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.3 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1539
            </span>
            <a href="https://arxiv.org/abs/2403.16478" target="_blank" rel="noopener noreferrer">Real-World Evaluation of two Cooperative Intersection Management Approaches</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Marvin Klimke, Max Bastian Mertens, Benjamin V\"olz, Michael Buchholz | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Cooperative maneuver planning promises to significantly improve traffic efficiency at unsignalized intersections by leveraging connected automated vehicles. Previous works on this topic have been mostly developed for completely automated traffic in a simple simulated environment. In contrast, our pr</span>
            
            <span class="abstract-full" style="display: none;">Cooperative maneuver planning promises to significantly improve traffic efficiency at unsignalized intersections by leveraging connected automated vehicles. Previous works on this topic have been mostly developed for completely automated traffic in a simple simulated environment. In contrast, our previously introduced planning approaches are specifically designed to handle real-world mixed traffic. The two methods are based on multi-scenario prediction and graph-based reinforcement learning, respectively. This is the first study to perform evaluations in a novel mixed traffic simulation framework as well as real-world drives with prototype connected automated vehicles in public traffic. The simulation features the same connected automated driving software stack as deployed on one of the automated vehicles. Our quantitative evaluations show that cooperative maneuver planning achieves a substantial reduction in crossing times and the number of stops. In a realistic environment with few automated vehicles, there are noticeable efficiency gains with only slightly increasing criticality metrics.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1566
            </span>
            <a href="https://arxiv.org/abs/2412.10423" target="_blank" rel="noopener noreferrer">Look Before You Leap: Enhancing Attention and Vigilance Regarding Harmful Content with GuidelineLLM</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Rongxiang Weng, Muyun Yang, Tiejun Zhao, Min Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Despite being empowered with alignment mechanisms, large language models (LLMs) are increasingly vulnerable to emerging jailbreak attacks that can compromise their alignment mechanisms. This vulnerability poses significant risks to real-world applications. Existing work faces challenges in both trai</span>
            
            <span class="abstract-full" style="display: none;">Despite being empowered with alignment mechanisms, large language models (LLMs) are increasingly vulnerable to emerging jailbreak attacks that can compromise their alignment mechanisms. This vulnerability poses significant risks to real-world applications. Existing work faces challenges in both training efficiency and generalization capabilities (i.e., Reinforcement Learning from Human Feedback and Red-Teaming). Developing effective strategies to enable LLMs to resist continuously evolving jailbreak attempts represents a significant challenge. To address this challenge, we propose a novel defensive paradigm called GuidelineLLM, which assists LLMs in recognizing queries that may have harmful content. Before LLMs respond to a query, GuidelineLLM first identifies potential risks associated with the query, summarizes these risks into guideline suggestions, and then feeds these guidelines to the responding LLMs. Importantly, our approach eliminates the necessity for additional safety fine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning. This characteristic enhances the general applicability of GuidelineLLM across various LLMs. Experimental results demonstrate that GuidelineLLM can significantly reduce the attack success rate (ASR) against LLM (an average reduction of 34.17\% ASR) while maintaining the usefulness of LLM in handling benign queries. The code is available at https://github.com/sqzhang-lazy/GuidelineLLM.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.3 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1658
            </span>
            <a href="https://arxiv.org/abs/2501.15145" target="_blank" rel="noopener noreferrer">PromptShield: Deployable Detection for Prompt Injection Attacks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dennis Jacob, Hend Alzahrani, Zhanhao Hu, Basel Alomair, David Wagner | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Application designers have moved to integrate large language models (LLMs) into their products. However, many LLM-integrated applications are vulnerable to prompt injections. While attempts have been made to address this problem by building prompt injection detectors, many are not yet suitable for p</span>
            
            <span class="abstract-full" style="display: none;">Application designers have moved to integrate large language models (LLMs) into their products. However, many LLM-integrated applications are vulnerable to prompt injections. While attempts have been made to address this problem by building prompt injection detectors, many are not yet suitable for practical deployment. To support research in this area, we introduce PromptShield, a benchmark for training and evaluating deployable prompt injection detectors. Our benchmark is carefully curated and includes both conversational and application-structured data. In addition, we use insights from our curation process to fine-tune a new prompt injection detector that achieves significantly higher performance in the low false positive rate (FPR) evaluation regime compared to prior schemes. Our work suggests that careful curation of training data and larger models can contribute to strong detector performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.1 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1834
            </span>
            <a href="https://arxiv.org/abs/2504.08590" target="_blank" rel="noopener noreferrer">Playpen: An Environment for Exploring Learning Through Conversational Interaction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Nicola Horst, Davide Mazzaccara, Antonia Schmidt, Michael Sullivan, Filippo Moment\`e, Luca Franceschetti, Philipp Sadler, Sherzod Hakimov, Alberto Testoni, Raffaella Bernardi, Raquel Fern\'andez, Alexander Koller, Oliver Lemon, David Schlangen, Mario Giulianelli, Alessandro Suglia | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Are we running out of learning signal? Predicting the next word in an existing text has turned out to be a powerful signal, at least at scale. But there are signs that we are running out of this resource. In recent months, interaction between learner and feedback-giver has come into focus, both for </span>
            
            <span class="abstract-full" style="display: none;">Are we running out of learning signal? Predicting the next word in an existing text has turned out to be a powerful signal, at least at scale. But there are signs that we are running out of this resource. In recent months, interaction between learner and feedback-giver has come into focus, both for "alignment" (with a reward model judging the quality of instruction following attempts) and for improving "reasoning" (process- and outcome-based verifiers judging reasoning steps). In this paper, we explore to what extent synthetic interaction in what we call Dialogue Games -- goal-directed and rule-governed activities driven predominantly by verbal actions -- can provide a learning signal, and how this signal can be used. We introduce an environment for producing such interaction data (with the help of a Large Language Model as counterpart to the learner model), both offline and online. We investigate the effects of supervised fine-tuning on this data, as well as reinforcement learning setups such as DPO, and GRPO; showing that all of these approaches achieve some improvements in in-domain games, but only GRPO demonstrates the ability to generalise to out-of-domain games as well as retain competitive performance in reference-based tasks. We release the framework and the baseline training setups in the hope that this can foster research in this promising new direction.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.2 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1913
            </span>
            <a href="https://arxiv.org/abs/2504.09812" target="_blank" rel="noopener noreferrer">Efficient Multi-Task Modeling through Automated Fusion of Trained Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jingxuan Zhou, Weidong Bao, Ji Wang, Zhengyi Zhong, Dayu Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Although multi-task learning is widely applied in intelligent services, traditional multi-task modeling methods often require customized designs based on specific task combinations, resulting in a cumbersome modeling process. Inspired by the rapid development and excellent performance of single-task</span>
            
            <span class="abstract-full" style="display: none;">Although multi-task learning is widely applied in intelligent services, traditional multi-task modeling methods often require customized designs based on specific task combinations, resulting in a cumbersome modeling process. Inspired by the rapid development and excellent performance of single-task models, this paper proposes an efficient multi-task modeling method that can automatically fuse trained single-task models with different structures and tasks to form a multi-task model. As a general framework, this method allows modelers to simply prepare trained models for the required tasks, simplifying the modeling process while fully utilizing the knowledge contained in the trained models. This eliminates the need for excessive focus on task relationships and model structure design. To achieve this goal, we consider the structural differences among various trained models and employ model decomposition techniques to hierarchically decompose them into multiple operable model components. Furthermore, we have designed an Adaptive Knowledge Fusion (AKF) module based on Transformer, which adaptively integrates intra-task and inter-task knowledge based on model components. Through the proposed method, we achieve efficient and automated construction of multi-task models, and its effectiveness is verified through extensive experiments on three datasets.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1922
            </span>
            <a href="https://arxiv.org/abs/2411.06770" target="_blank" rel="noopener noreferrer">Sketched Adaptive Federated Deep Learning: A Sharp Convergence Analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhijie Chen, Qiaobo Li, Arindam Banerjee | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Combining gradient compression methods (e.g., CountSketch, quantization) and adaptive optimizers (e.g., Adam, AMSGrad) is a desirable goal in federated learning (FL), with potential benefits on both fewer communication rounds and less per-round communication. In spite of the preliminary empirical su</span>
            
            <span class="abstract-full" style="display: none;">Combining gradient compression methods (e.g., CountSketch, quantization) and adaptive optimizers (e.g., Adam, AMSGrad) is a desirable goal in federated learning (FL), with potential benefits on both fewer communication rounds and less per-round communication. In spite of the preliminary empirical success of sketched adaptive methods, existing convergence analyses show the communication cost to have a linear dependence on the ambient dimension, i.e., number of parameters, which is prohibitively high for modern deep learning models. In this work, we introduce specific sketched adaptive federated learning (SAFL) algorithms and, as our main contribution, provide theoretical convergence analyses in different FL settings with guarantees on communication cost depending only logarithmically (instead of linearly) on the ambient dimension. Unlike existing analyses, we show that the entry-wise sketching noise existent in the preconditioners and the first moments of SAFL can be implicitly addressed by leveraging the recently-popularized anisotropic curvatures in deep learning losses, e.g., fast decaying loss Hessian eigen-values. In the i.i.d. client setting of FL, we show that SAFL achieves asymptotic $O(1/\sqrt{T})$ convergence, and converges faster in the initial epochs. In the non-i.i.d. client setting, where non-adaptive methods lack convergence guarantees, we show that SACFL (SAFL with clipping) algorithms can provably converge in spite of the additional heavy-tailed noise. Our theoretical claims are supported by empirical studies on vision and language tasks, and in both fine-tuning and training-from-scratch regimes. Surprisingly, as a by-product of our analysis, the proposed SAFL methods are competitive with the state-of-the-art communication-efficient federated learning algorithms based on error feedback.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.3 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1926
            </span>
            <a href="https://arxiv.org/abs/2503.15793" target="_blank" rel="noopener noreferrer">DNR Bench: Benchmarking Over-Reasoning in Reasoning LLMs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Masoud Hashemi, Oluwanifemi Bamgbose, Sathwik Tejaswi Madhusudhan, Jishnu Sethumadhavan Nair, Aman Tiwari, Vikas Yadav | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Test-time scaling has significantly improved large language model performance, enabling deeper reasoning to solve complex problems. However, this increased reasoning capability also leads to excessive token generation and unnecessary problem-solving attempts. We introduce Don\'t Answer Bench (DNA Be</span>
            
            <span class="abstract-full" style="display: none;">Test-time scaling has significantly improved large language model performance, enabling deeper reasoning to solve complex problems. However, this increased reasoning capability also leads to excessive token generation and unnecessary problem-solving attempts. We introduce Don\'t Answer Bench (DNA Bench), a new benchmark designed to evaluate LLMs ability to robustly understand the tricky reasoning triggers and avoiding unnecessary generation. DNA Bench consists of 150 adversarially designed prompts that are easy for humans to understand and respond to, but surprisingly not for many of the recent prominent LLMs. DNA Bench tests models abilities across different capabilities, such as instruction adherence, hallucination avoidance, redundancy filtering, and unanswerable question recognition. We evaluate reasoning LLMs (RLMs), including DeepSeek-R1, OpenAI O3-mini, Claude-3.7-sonnet and compare them against a powerful non-reasoning model, e.g., GPT-4o. Our experiments reveal that RLMs generate up to 70x more tokens than necessary, often failing at tasks that simpler non-reasoning models handle efficiently with higher accuracy. Our findings underscore the need for more effective training and inference strategies in RLMs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.6 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2057
            </span>
            <a href="https://arxiv.org/abs/2504.09155" target="_blank" rel="noopener noreferrer">Evolved Hierarchical Masking for Self-Supervised Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhanzhou Feng, Shiliang Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Existing Masked Image Modeling methods apply fixed mask patterns to guide the self-supervised training. As those mask patterns resort to different criteria to depict image contents, sticking to a fixed pattern leads to a limited vision cues modeling capability.This paper introduces an evolved hierar</span>
            
            <span class="abstract-full" style="display: none;">Existing Masked Image Modeling methods apply fixed mask patterns to guide the self-supervised training. As those mask patterns resort to different criteria to depict image contents, sticking to a fixed pattern leads to a limited vision cues modeling capability.This paper introduces an evolved hierarchical masking method to pursue general visual cues modeling in self-supervised learning. The proposed method leverages the vision model being trained to parse the input visual cues into a hierarchy structure, which is hence adopted to generate masks accordingly. The accuracy of hierarchy is on par with the capability of the model being trained, leading to evolved mask patterns at different training stages. Initially, generated masks focus on low-level visual cues to grasp basic textures, then gradually evolve to depict higher-level cues to reinforce the learning of more complicated object semantics and contexts. Our method does not require extra pre-trained models or annotations and ensures training efficiency by evolving the training difficulty. We conduct extensive experiments on seven downstream tasks including partial-duplicate image retrieval relying on low-level details, as well as image classification and semantic segmentation that require semantic parsing capability. Experimental results demonstrate that it substantially boosts performance across these tasks. For instance, it surpasses the recent MAE by 1.1\% in imageNet-1K classification and 1.4\% in ADE20K segmentation with the same training epochs. We also align the proposed method with the current research focus on LLMs. The proposed approach bridges the gap with large-scale pre-training on semantic demanding tasks and enhances intricate detail perception in tasks requiring low-level feature recognition.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2131
            </span>
            <a href="https://arxiv.org/abs/2411.18094" target="_blank" rel="noopener noreferrer">Comprehensive Kernel Safety in the Spectre Era: Mitigations and Performance Evaluation (Extended Version)</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Davide Davoli, Martin Avanzini, Tamara Rezk | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The efficacy of address space layout randomization has been formally demonstrated in a shared-memory model by Abadi et al., contingent on specific assumptions about victim programs. However, modern operating systems, implementing layout randomization in the kernel, diverge from these assumptions and</span>
            
            <span class="abstract-full" style="display: none;">The efficacy of address space layout randomization has been formally demonstrated in a shared-memory model by Abadi et al., contingent on specific assumptions about victim programs. However, modern operating systems, implementing layout randomization in the kernel, diverge from these assumptions and operate on a separate memory model with communication through system calls. In this work, we relax Abadi et al.'s language assumptions while demonstrating that layout randomization offers a comparable safety guarantee in a system with memory separation. However, in practice, speculative execution and side-channels are recognized threats to layout randomization. We show that kernel safety cannot be restored for attackers capable of using side-channels and speculative execution, and introduce enforcement mechanisms that can guarantee speculative kernel safety for safe system calls in the Spectre era. We implement three suitable mechanisms and we evaluate their performance overhead on the Linux kernel.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.5 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2228
            </span>
            <a href="https://arxiv.org/abs/2504.08694" target="_blank" rel="noopener noreferrer">TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hang Ni, Fan Liu, Xinyu Ma, Lixin Su, Shuaiqiang Wang, Dawei Yin, Hui Xiong, Hao Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptab</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMs' intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.3 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2275
            </span>
            <a href="https://arxiv.org/abs/2504.08850" target="_blank" rel="noopener noreferrer">SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiaming Xu, Jiayi Pan, Yongkang Zhou, Siming Chen, Jinhao Li, Yaoxiu Lian, Junyi Wu, Guohao Dai | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Early exiting has recently emerged as a promising technique for accelerating large language models (LLMs) by effectively reducing the hardware computation and memory access. In this paper, we present SpecEE, a fast LLM inference engine with speculative early exiting. (1) At the algorithm level, we p</span>
            
            <span class="abstract-full" style="display: none;">Early exiting has recently emerged as a promising technique for accelerating large language models (LLMs) by effectively reducing the hardware computation and memory access. In this paper, we present SpecEE, a fast LLM inference engine with speculative early exiting. (1) At the algorithm level, we propose the speculation-based lightweight predictor design by exploiting the probabilistic correlation between the speculative tokens and the correct results and high parallelism of GPUs. (2) At the system level, we point out that not all layers need a predictor and design the two-level heuristic predictor scheduling engine based on skewed distribution and contextual similarity. (3) At the mapping level, we point out that different decoding methods share the same essential characteristics, and propose the context-aware merged mapping for predictor with efficient GPU implementations to support speculative decoding, and form a framework for various existing orthogonal acceleration techniques (e.g., quantization and sparse activation) on cloud and personal computer (PC) scenarios, successfully pushing the Pareto frontier of accuracy and speedup. It is worth noting that SpecEE can be applied to any LLM by negligible training overhead in advance without affecting the model original parameters. Extensive experiments show that SpecEE achieves 2.25x and 2.43x speedup with Llama2-7B on cloud and PC scenarios respectively.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.2 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2305
            </span>
            <a href="https://arxiv.org/abs/2504.08977" target="_blank" rel="noopener noreferrer">Robust Steganography from Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Neil Perry, Sanket Gupte, Nishant Pitta, Lior Rotem | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent steganographic schemes, starting with Meteor (CCS'21), rely on leveraging large language models (LLMs) to resolve a historically-challenging task of disguising covert communication as ``innocent-looking'' natural-language communication. However, existing methods are vulnerable to ``re-randomi</span>
            
            <span class="abstract-full" style="display: none;">Recent steganographic schemes, starting with Meteor (CCS'21), rely on leveraging large language models (LLMs) to resolve a historically-challenging task of disguising covert communication as ``innocent-looking'' natural-language communication. However, existing methods are vulnerable to ``re-randomization attacks,'' where slight changes to the communicated text, that might go unnoticed, completely destroy any hidden message. This is also a vulnerability in more traditional encryption-based stegosystems, where adversaries can modify the randomness of an encryption scheme to destroy the hidden message while preserving an acceptable covertext to ordinary users. In this work, we study the problem of robust steganography. We introduce formal definitions of weak and strong robust LLM-based steganography, corresponding to two threat models in which natural language serves as a covertext channel resistant to realistic re-randomization attacks. We then propose two constructions satisfying these notions. We design and implement our steganographic schemes that embed arbitrary secret messages into natural language text generated by LLMs, ensuring recoverability even under adversarial paraphrasing and rewording attacks. To support further research and real-world deployment, we release our implementation and datasets for public use.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.4 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2315
            </span>
            <a href="https://arxiv.org/abs/2504.09946" target="_blank" rel="noopener noreferrer">Assessing Judging Bias in Large Reasoning Models: An Empirical Study</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Qian Wang, Zhanzhi Lou, Zhenheng Tang, Nuo Chen, Xuandong Zhao, Wenxuan Zhang, Dawn Song, Bingsheng He | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have demonstrated remarkable reasoning capabilities, raising important questions about their biases in LLM-as-a-judge settings. We present a comprehensive benchmark comparing judging biases between LLMs and LRMs across both subjective pref</span>
            
            <span class="abstract-full" style="display: none;">Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have demonstrated remarkable reasoning capabilities, raising important questions about their biases in LLM-as-a-judge settings. We present a comprehensive benchmark comparing judging biases between LLMs and LRMs across both subjective preference-alignment datasets and objective fact-based datasets. Through investigation of bandwagon, authority, position, and distraction biases, we uncover four key findings: (1) despite their advanced reasoning capabilities, LRMs remain susceptible to the above biases; (2) LRMs demonstrate better robustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit notable position bias, preferring options in later positions; and (4) we identify a novel "superficial reflection bias" where phrases mimicking reasoning (e.g., "wait, let me think...") significantly influence model judgments. To address these biases, we design and evaluate three mitigation strategies: specialized system prompts that reduce judging biases by up to 19\% in preference alignment datasets and 14\% in fact-related datasets, in-context learning that provides up to 27\% improvement on preference tasks but shows inconsistent results on factual tasks, and a self-reflection mechanism that reduces biases by up to 10\% in preference datasets and 16\% in fact-related datasets, with self-reflection proving particularly effective for LRMs. Our work provides crucial insights for developing more reliable LLM-as-a-Judge frameworks, especially as LRMs become increasingly deployed as automated judges.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.7 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2322
            </span>
            <a href="https://arxiv.org/abs/2504.10405" target="_blank" rel="noopener noreferrer">Performance of Large Language Models in Supporting Medical Diagnosis and Treatment</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Diogo Sousa, Guilherme Barbosa, Catarina Rocha, Dulce Oliveira | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The integration of Large Language Models (LLMs) into healthcare holds significant potential to enhance diagnostic accuracy and support medical treatment planning. These AI-driven systems can analyze vast datasets, assisting clinicians in identifying diseases, recommending treatments, and predicting </span>
            
            <span class="abstract-full" style="display: none;">The integration of Large Language Models (LLMs) into healthcare holds significant potential to enhance diagnostic accuracy and support medical treatment planning. These AI-driven systems can analyze vast datasets, assisting clinicians in identifying diseases, recommending treatments, and predicting patient outcomes. This study evaluates the performance of a range of contemporary LLMs, including both open-source and closed-source models, on the 2024 Portuguese National Exam for medical specialty access (PNA), a standardized medical knowledge assessment. Our results highlight considerable variation in accuracy and cost-effectiveness, with several models demonstrating performance exceeding human benchmarks for medical students on this specific task. We identify leading models based on a combined score of accuracy and cost, discuss the implications of reasoning methodologies like Chain-of-Thought, and underscore the potential for LLMs to function as valuable complementary tools aiding medical professionals in complex clinical decision-making.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.2 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2481
            </span>
            <a href="https://arxiv.org/abs/2504.07158" target="_blank" rel="noopener noreferrer">Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ling Team, Caizhi Tang, Chilin Fu, Chunwei Wu, Jia Guo, Jianwen Wang, Jingyu Hu, Liang Jiang, Meng Li, Peng Jiao, Pingping Liu, Shaomian Zheng, Shiwei Liang, Shuaicheng Li, Yalin Zhang, Yingting Wu, Yongkang Liu, Zhenyu Huang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE m</span>
            
            <span class="abstract-full" style="display: none;">This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at https://huggingface.co/inclusionAI</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.9 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2542
            </span>
            <a href="https://arxiv.org/abs/2504.08412" target="_blank" rel="noopener noreferrer">Boosting the Class-Incremental Learning in 3D Point Clouds via Zero-Collection-Cost Basic Shape Pre-Training</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chao Qi, Jianqin Yin, Meng Chen, Yingchun Niu, Yuan Sun | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Existing class-incremental learning methods in 3D point clouds rely on exemplars (samples of former classes) to resist the catastrophic forgetting of models, and exemplar-free settings will greatly degrade the performance. For exemplar-free incremental learning, the pre-trained model methods have ac</span>
            
            <span class="abstract-full" style="display: none;">Existing class-incremental learning methods in 3D point clouds rely on exemplars (samples of former classes) to resist the catastrophic forgetting of models, and exemplar-free settings will greatly degrade the performance. For exemplar-free incremental learning, the pre-trained model methods have achieved state-of-the-art results in 2D domains. However, these methods cannot be migrated to the 3D domains due to the limited pre-training datasets and insufficient focus on fine-grained geometric details. This paper breaks through these limitations, proposing a basic shape dataset with zero collection cost for model pre-training. It helps a model obtain extensive knowledge of 3D geometries. Based on this, we propose a framework embedded with 3D geometry knowledge for incremental learning in point clouds, compatible with exemplar-free (-based) settings. In the incremental stage, the geometry knowledge is extended to represent objects in point clouds. The class prototype is calculated by regularizing the data representation with the same category and is kept adjusting in the learning process. It helps the model remember the shape features of different categories. Experiments show that our method outperforms other baseline methods by a large margin on various benchmark datasets, considering both exemplar-free (-based) settings.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2577
            </span>
            <a href="https://arxiv.org/abs/2504.07140" target="_blank" rel="noopener noreferrer">Secure Text Mail Encryption with Generative Adversarial Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alexej Schelle | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work presents an encryption model based on Generative Adversarial Networks (GANs). Encryption of RTF-8 data is realized by dynamically generating decimal numbers that lead to the encryption and decryption of alphabetic strings in integer representation by simple addition rules, the modulus of t</span>
            
            <span class="abstract-full" style="display: none;">This work presents an encryption model based on Generative Adversarial Networks (GANs). Encryption of RTF-8 data is realized by dynamically generating decimal numbers that lead to the encryption and decryption of alphabetic strings in integer representation by simple addition rules, the modulus of the dimension of the considered alphabet. The binary numbers for the private dynamic keys correspond to the binary numbers of public reference keys, as defined by a specific GAN configuration. For reversible encryption with a bijective mapping between dynamic and reference keys, as defined by the GAN encryptor, secure text encryption can be achieved by transferring a GAN-encrypted public key along with the encrypted text from a sender to a receiver. Using the technique described above, secure text mail transfer can be realized through component-wise encryption and decryption of text mail strings, with total key sizes of up to $10^{8}$ bits that define random decimal numbers generated by the GAN. From the present model, we assert that encrypted texts can be transmitted more efficiently and securely than from RSA encryption, as long as users of the specific configuration of the GAN encryption model are unaware of the GAN encryptor circuit and configuration, respectively.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.9 -->
                
            <!-- LLMs: 6.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Math: 2.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- 3D: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.28
            </span>
            <a href="https://arxiv.org/abs/2504.08222" target="_blank" rel="noopener noreferrer">F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhaoyu Liu, Kan Jiang, Murong Ma, Zhe Hou, Yun Lin, Jin Song Dong | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Analyzing Fast, Frequent, and Fine-grained (F$^3$) events presents a significant challenge in video analytics and multi-modal LLMs. Current methods struggle to identify events that satisfy all the F$^3$ criteria with high accuracy due to challenges such as motion blur and subtle visual discrepancies</span>
            
            <span class="abstract-full" style="display: none;">Analyzing Fast, Frequent, and Fine-grained (F$^3$) events presents a significant challenge in video analytics and multi-modal LLMs. Current methods struggle to identify events that satisfy all the F$^3$ criteria with high accuracy due to challenges such as motion blur and subtle visual discrepancies. To advance research in video understanding, we introduce F$^3$Set, a benchmark that consists of video datasets for precise F$^3$ event detection. Datasets in F$^3$Set are characterized by their extensive scale and comprehensive detail, usually encompassing over 1,000 event types with precise timestamps and supporting multi-level granularity. Currently, F$^3$Set contains several sports datasets, and this framework may be extended to other applications as well. We evaluated popular temporal action understanding methods on F$^3$Set, revealing substantial challenges for existing techniques. Additionally, we propose a new method, F$^3$ED, for F$^3$ event detections, achieving superior performance. The dataset, model, and benchmark code are available at https://github.com/F3Set/F3Set.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2926
            </span>
            <a href="https://arxiv.org/abs/2504.09282" target="_blank" rel="noopener noreferrer">VideoAds for Fast-Paced Video Understanding: Where Opensource Foundation Models Beat GPT-4o & Gemini-1.5 Pro</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zheyuan Zhang, Monica Dou, Linkai Peng, Hongyi Pan, Ulas Bagci, Boqing Gong | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Advertisement videos serve as a rich and valuable source of purpose-driven information, encompassing high-quality visual, textual, and contextual cues designed to engage viewers. They are often more complex than general videos of similar duration due to their structured narratives and rapid scene tr</span>
            
            <span class="abstract-full" style="display: none;">Advertisement videos serve as a rich and valuable source of purpose-driven information, encompassing high-quality visual, textual, and contextual cues designed to engage viewers. They are often more complex than general videos of similar duration due to their structured narratives and rapid scene transitions, posing significant challenges to multi-modal large language models (MLLMs). In this work, we introduce VideoAds, the first dataset tailored for benchmarking the performance of MLLMs on advertisement videos. VideoAds comprises well-curated advertisement videos with complex temporal structures, accompanied by \textbf{manually} annotated diverse questions across three core tasks: visual finding, video summary, and visual reasoning. We propose a quantitative measure to compare VideoAds against existing benchmarks in terms of video complexity. Through extensive experiments, we find that Qwen2.5-VL-72B, an opensource MLLM, achieves 73.35\% accuracy on VideoAds, outperforming GPT-4o (66.82\%) and Gemini-1.5 Pro (69.66\%); the two proprietary models especially fall behind the opensource model in video summarization and reasoning, but perform the best in visual finding. Notably, human experts easily achieve a remarkable accuracy of 94.27\%. These results underscore the necessity of advancing MLLMs' temporal modeling capabilities and highlight VideoAds as a potentially pivotal benchmark for future research in understanding video that requires high FPS sampling. The dataset and evaluation code will be publicly available at https://videoadsbenchmark.netlify.app.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.295
            </span>
            <a href="https://arxiv.org/abs/2504.01332" target="_blank" rel="noopener noreferrer">When to Truncate the Archive? On the Effect of the Truncation Frequency in Multi-Objective Optimisation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhiji Cui, Zimin Liang, Lie Meng Pang, Hisao Ishibuchi, Miqing Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Using an archive to store nondominated solutions found during the search of a multi-objective evolutionary algorithm (MOEA) is a useful practice. However, as nondominated solutions of a multi-objective optimisation problem can be enormous or infinitely many, it is desirable to provide the decision-m</span>
            
            <span class="abstract-full" style="display: none;">Using an archive to store nondominated solutions found during the search of a multi-objective evolutionary algorithm (MOEA) is a useful practice. However, as nondominated solutions of a multi-objective optimisation problem can be enormous or infinitely many, it is desirable to provide the decision-maker with only a small, representative portion of all the nondominated solutions in the archive, thus entailing a truncation operation. Then, an important issue is when to truncate the archive. This can be done once a new solution generated, a batch of new solutions generated, or even using an unbounded archive to keep all nondominated solutions generated and truncate it later. Intuitively, the last approach may lead to a better result since we have all the information in hand before performing the truncation. In this paper, we study this issue and investigate the effect of the timing of truncating the archive. We apply well-established truncation criteria that are commonly used in the population maintenance procedure of MOEAs (e.g., crowding distance, hypervolume indicator, and decomposition). We show that, interestingly, truncating the archive once a new solution generated tends to be the best, whereas considering an unbounded archive is often the worst. We analyse and discuss this phenomenon. Our results highlight the importance of developing effective subset selection techniques (rather than employing the population maintenance methods in MOEAs) when using a large archive.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.5 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2975
            </span>
            <a href="https://arxiv.org/abs/2504.08972" target="_blank" rel="noopener noreferrer">Improving municipal responsiveness through AI-powered image analysis in E-Government</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Catalin Vrabie | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Integration of Machine Learning (ML) techniques into public administration marks a new and transformative era for e-government systems. While traditionally e-government studies were focusing on text-based interactions, this one explores the innovative application of ML for image analysis, an approac</span>
            
            <span class="abstract-full" style="display: none;">Integration of Machine Learning (ML) techniques into public administration marks a new and transformative era for e-government systems. While traditionally e-government studies were focusing on text-based interactions, this one explores the innovative application of ML for image analysis, an approach that enables governments to address citizen petitions more efficiently. By using image classification and object detection algorithms, the model proposed in this article supports public institutions in identifying and fast responding to evidence submitted by citizens in picture format, such as infrastructure issues, environmental concerns or other urban issues that citizens might face. The research also highlights the Jevons Paradox as a critical factor, wherein increased efficiency from the citizen side (especially using mobile platforms and apps) may generate higher demand which should lead to scalable and robust solutions. Using as a case study a Romanian municipality who provided datasets of citizen-submitted images, the author analysed and proved that ML can improve accuracy and responsiveness of public institutions. The findings suggest that adopting ML for e-petition systems can not only enhance citizen participation but also speeding up administrative processes, paving the way for more transparent and effective governance. This study contributes to the discourse on e-government 3.0 by showing the potential of Artificial Intelligence (AI) to transform public service delivery, ensuring sustainable (and scalable) solutions for the growing demands of modern urban governance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3008
            </span>
            <a href="https://arxiv.org/abs/2504.08874" target="_blank" rel="noopener noreferrer">Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Roshan Patel, Saeed Moayedpour, Louis De Lescure, Lorenzo Kogler-Anele, Alan Cherney, Sven Jager, Yasser Jangjou | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Machine learning and Bayesian optimization (BO) algorithms can significantly accelerate the optimization of chemical reactions. Transfer learning can bolster the effectiveness of BO algorithms in low-data regimes by leveraging pre-existing chemical information or data outside the direct optimization</span>
            
            <span class="abstract-full" style="display: none;">Machine learning and Bayesian optimization (BO) algorithms can significantly accelerate the optimization of chemical reactions. Transfer learning can bolster the effectiveness of BO algorithms in low-data regimes by leveraging pre-existing chemical information or data outside the direct optimization task (i.e., source data). Large language models (LLMs) have demonstrated that chemical information present in foundation training data can give them utility for processing chemical data. Furthermore, they can be augmented with and help synthesize potentially multiple modalities of source chemical data germane to the optimization task. In this work, we examine how chemical information from LLMs can be elicited and used for transfer learning to accelerate the BO of reaction conditions to maximize yield. Specifically, we show that a survey-like prompting scheme and preference learning can be used to infer a utility function which models prior chemical information embedded in LLMs over a chemical parameter space; we find that the utility function shows modest correlation to true experimental measurements (yield) over the parameter space despite operating in a zero-shot setting. Furthermore, we show that the utility function can be leveraged to focus BO efforts in promising regions of the parameter space, improving the yield of the initial BO query and enhancing optimization in 4 of the 6 datasets studied. Overall, we view this work as a step towards bridging the gap between the chemistry knowledge embedded in LLMs and the capabilities of principled BO methods to accelerate reaction optimization.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 6.2 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Math: 2.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3016
            </span>
            <a href="https://arxiv.org/abs/2504.04840" target="_blank" rel="noopener noreferrer">Unsupervised Ego- and Exo-centric Dense Procedural Activity Captioning via Gaze Consensus Adaptation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhaofeng Shi, Heqian Qiu, Lanxiao Wang, Qingbo Wu, Fanman Meng, Hongliang Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Even from an early age, humans naturally adapt between exocentric (Exo) and egocentric (Ego) perspectives to understand daily procedural activities. Inspired by this cognitive ability, we propose a novel Unsupervised Ego-Exo Dense Procedural Activity Captioning (UE$^{2}$DPAC) task, which aims to tra</span>
            
            <span class="abstract-full" style="display: none;">Even from an early age, humans naturally adapt between exocentric (Exo) and egocentric (Ego) perspectives to understand daily procedural activities. Inspired by this cognitive ability, we propose a novel Unsupervised Ego-Exo Dense Procedural Activity Captioning (UE$^{2}$DPAC) task, which aims to transfer knowledge from the labeled source view to predict the time segments and descriptions of action sequences for the target view without annotations. Despite previous works endeavoring to address the fully-supervised single-view or cross-view dense video captioning, they lapse in the proposed task due to the significant inter-view gap caused by temporal misalignment and irrelevant object interference. Hence, we propose a Gaze Consensus-guided Ego-Exo Adaptation Network (GCEAN) that injects the gaze information into the learned representations for the fine-grained Ego-Exo alignment. Specifically, we propose a Score-based Adversarial Learning Module (SALM) that incorporates a discriminative scoring network and compares the scores of distinct views to learn unified view-invariant representations from a global level. Then, the Gaze Consensus Construction Module (GCCM) utilizes the gaze to progressively calibrate the learned representations to highlight the regions of interest and extract the corresponding temporal contexts. Moreover, we adopt hierarchical gaze-guided consistency losses to construct gaze consensus for the explicit temporal and spatial adaptation between the source and target views. To support our research, we propose a new EgoMe-UE$^{2}$DPAC benchmark, and extensive experiments demonstrate the effectiveness of our method, which outperforms many related methods by a large margin. The code will be released.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.6 -->
                
            <!-- LLMs: 8.3 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3124
            </span>
            <a href="https://arxiv.org/abs/2504.08419" target="_blank" rel="noopener noreferrer">GeoTexBuild: 3D Building Model Generation from Map Footprints</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ruizhe Wang, Junyan Yang, Qiao Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We introduce GeoTexBuild, a modular generative framework for creating 3D building models from map footprints. The proposed framework employs a three-stage process comprising height map generation, geometry reconstruction, and appearance stylization, culminating in building models with intricate geom</span>
            
            <span class="abstract-full" style="display: none;">We introduce GeoTexBuild, a modular generative framework for creating 3D building models from map footprints. The proposed framework employs a three-stage process comprising height map generation, geometry reconstruction, and appearance stylization, culminating in building models with intricate geometry and appearance attributes. By integrating customized ControlNet and Text2Mesh models, we explore effective methods for controlling both geometric and visual attributes during the generation process. By this, we eliminate the problem of structural variations behind a single facade photo of the existing 3D generation techniques. Experimental results at each stage validate the capability of GeoTexBuild to generate detailed and accurate building models from footprints derived from site planning or map designs. Our framework significantly reduces manual labor in modeling buildings and can offer inspiration for designers.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3126
            </span>
            <a href="https://arxiv.org/abs/2504.09740" target="_blank" rel="noopener noreferrer">Customer Validation, Feedback and Collaboration in Large-Scale Continuous Software Development</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: David Molamphy | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The importance of continuously incorporating customer feedback in the software development process is well established and firmly grounded in concepts such as agile and DevOps. In large-scale organizations such as Dell Technologies however, an array of challenges remain unsolved relating to this cru</span>
            
            <span class="abstract-full" style="display: none;">The importance of continuously incorporating customer feedback in the software development process is well established and firmly grounded in concepts such as agile and DevOps. In large-scale organizations such as Dell Technologies however, an array of challenges remain unsolved relating to this crucial aspect of software development. Despite a wide variety of tools and techniques available for collecting and analyzing customer feedback, in large-scale organizations implementing agile and continuous software development practices, harmful disconnects, discrepancies and processes exist. Such challenges negatively impact on an organizations ability to regularly deploy incremental improvements to their software products which meet customer needs. In this Professional Doctorate research program, wherein the researcher is a practitioner within Dell Technologies, we explore the challenges of continuously integrating customer feedback in a large scale global organization with over 100,000 employees and hundreds of software products. Leveraging an Action Research approach, we will propose a model to enhance the continuous incorporation of customer feedback and validation, providing organizations with the ability to frequently deliver incremental software improvements which satisfy the needs of its customers, measurable by metrics such as customer satisfaction, product adoption, bugs/defect escapes, production incidents and deployment frequency.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.4 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3152
            </span>
            <a href="https://arxiv.org/abs/2504.10025" target="_blank" rel="noopener noreferrer">Progressive Transfer Learning for Multi-Pass Fundus Image Restoration</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Uyen Phan, Ozer Can Devecioglu, Serkan Kiranyaz, Moncef Gabbouj | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Diabetic retinopathy is a leading cause of vision impairment, making its early diagnosis through fundus imaging critical for effective treatment planning. However, the presence of poor quality fundus images caused by factors such as inadequate illumination, noise, blurring and other motion artifacts</span>
            
            <span class="abstract-full" style="display: none;">Diabetic retinopathy is a leading cause of vision impairment, making its early diagnosis through fundus imaging critical for effective treatment planning. However, the presence of poor quality fundus images caused by factors such as inadequate illumination, noise, blurring and other motion artifacts yields a significant challenge for accurate DR screening. In this study, we propose progressive transfer learning for multi pass restoration to iteratively enhance the quality of degraded fundus images, ensuring more reliable DR screening. Unlike previous methods that often focus on a single pass restoration, multi pass restoration via PTL can achieve a superior blind restoration performance that can even improve most of the good quality fundus images in the dataset. Initially, a Cycle GAN model is trained to restore low quality images, followed by PTL induced restoration passes over the latest restored outputs to improve overall quality in each pass. The proposed method can learn blind restoration without requiring any paired data while surpassing its limitations by leveraging progressive learning and fine tuning strategies to minimize distortions and preserve critical retinal features. To evaluate PTL's effectiveness on multi pass restoration, we conducted experiments on DeepDRiD, a large scale fundus imaging dataset specifically curated for diabetic retinopathy detection. Our result demonstrates state of the art performance, showcasing PTL's potential as a superior approach to iterative image quality restoration.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3167
            </span>
            <a href="https://arxiv.org/abs/2504.09712" target="_blank" rel="noopener noreferrer">The Structural Safety Generalization Problem</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Julius Broomfield, Tom Gibbs, Ethan Kosak-Hine, George Ingebretsen, Tia Nasir, Jason Zhang, Reihaneh Iranmanesh, Sara Pieri, Reihaneh Rabbany, Kellin Pelrine | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically equivalent inputs. We further focus the target by requiring desirable tractability properties of atta</span>
            
            <span class="abstract-full" style="display: none;">LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically equivalent inputs. We further focus the target by requiring desirable tractability properties of attacks to study: explainability, transferability between models, and transferability between goals. We perform red-teaming within this framework by uncovering new vulnerabilities to multi-turn, multi-image, and translation-based attacks. These attacks are semantically equivalent by our design to their single-turn, single-image, or untranslated counterparts, enabling systematic comparisons; we show that the different structures yield different safety outcomes. We then demonstrate the potential for this framework to enable new defenses by proposing a Structure Rewriting Guardrail, which converts an input to a structure more conducive to safety assessment. This guardrail significantly improves refusal of harmful inputs, without over-refusing benign ones. Thus, by framing this intermediate challenge - more tractable than universal defenses but essential for long-term safety - we highlight a critical milestone for AI safety research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.317
            </span>
            <a href="https://arxiv.org/abs/2412.13478" target="_blank" rel="noopener noreferrer">Efficient Fine-Tuning of Single-Cell Foundation Models Enables Zero-Shot Molecular Perturbation Prediction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sepideh Maleki, Jan-Christian Huetter, Kangway V. Chuang, David Richmond, Gabriele Scalia, Tommaso Biancalani | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Predicting transcriptional responses to novel drugs provides a unique opportunity to accelerate biomedical research and advance drug discovery efforts. However, the inherent complexity and high dimensionality of cellular responses, combined with the extremely limited available experimental data, mak</span>
            
            <span class="abstract-full" style="display: none;">Predicting transcriptional responses to novel drugs provides a unique opportunity to accelerate biomedical research and advance drug discovery efforts. However, the inherent complexity and high dimensionality of cellular responses, combined with the extremely limited available experimental data, makes the task challenging. In this study, we leverage single-cell foundation models (FMs) pre-trained on tens of millions of single cells, encompassing multiple cell types, states, and disease annotations, to address molecular perturbation prediction. We introduce a drug-conditional adapter that allows efficient fine-tuning by training less than 1% of the original foundation model, thus enabling molecular conditioning while preserving the rich biological representation learned during pre-training. The proposed strategy allows not only the prediction of cellular responses to novel drugs, but also the zero-shot generalization to unseen cell lines. We establish a robust evaluation framework to assess model performance across different generalization tasks, demonstrating state-of-the-art results across all settings, with significant improvements in the few-shot and zero-shot generalization to new cell lines compared to existing baselines.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.1 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3216
            </span>
            <a href="https://arxiv.org/abs/2408.10201" target="_blank" rel="noopener noreferrer">LEAD: Towards Learning-Based Equity-Aware Decarbonization in Ridesharing Platforms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mahsa Sahebdel, Ali Zeynali, Noman Bashir, Prashant Shenoy, Mohammad Hajiesmaili | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Ridesharing platforms such as Uber, Lyft, and DiDi have grown in popularity due to their on-demand availability, ease of use, and commute cost reductions, among other benefits. However, not all ridesharing promises have panned out. Recent studies demonstrate that the expected drop in traffic congest</span>
            
            <span class="abstract-full" style="display: none;">Ridesharing platforms such as Uber, Lyft, and DiDi have grown in popularity due to their on-demand availability, ease of use, and commute cost reductions, among other benefits. However, not all ridesharing promises have panned out. Recent studies demonstrate that the expected drop in traffic congestion and reduction in greenhouse gas (GHG) emissions have not materialized. This is primarily due to the substantial distances traveled by the ridesharing vehicles without passengers between rides, known as deadhead miles. Recent work has focused on reducing the impact of deadhead miles while considering additional metrics such as rider waiting time, GHG emissions from deadhead miles, or driver earnings. However, most prior studies consider these environmental and equity-based metrics individually despite them being interrelated. In this paper, we propose a Learning-based Equity-Aware Decarabonization approach, LEAD, for ridesharing platforms. LEAD targets minimizing emissions while ensuring that the driver's utility, defined as the difference between the trip distance and the deadhead miles, is fairly distributed. LEAD uses reinforcement learning to match riders with drivers based on the expected future utility of drivers and the expected carbon emissions of the platform without increasing the rider waiting times. Extensive experiments based on a real-world ridesharing dataset show that LEAD improves the defined notion of fairness by 150% when compared to emission-aware ride-assignment and reduces emissions by 14.6% while ensuring fairness within 28--52% of the fairness-focused baseline. It also reduces the rider wait time, by at least 32.1%, compared to a fairness-focused baseline.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.9 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.328
            </span>
            <a href="https://arxiv.org/abs/2504.09602" target="_blank" rel="noopener noreferrer">Fine-tuning an Large Language Model for Automating Computational Fluid Dynamics Simulations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhehao Dong, Zhen Lu, Yue Yang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Configuring computational fluid dynamics (CFD) simulations typically demands extensive domain expertise, limiting broader access. Although large language models (LLMs) have advanced scientific computing, their use in automating CFD workflows is underdeveloped. We introduce a novel approach centered </span>
            
            <span class="abstract-full" style="display: none;">Configuring computational fluid dynamics (CFD) simulations typically demands extensive domain expertise, limiting broader access. Although large language models (LLMs) have advanced scientific computing, their use in automating CFD workflows is underdeveloped. We introduce a novel approach centered on domain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM, our custom dataset of 28716 natural language-to-OpenFOAM configuration pairs with chain-of-thought (CoT) annotations, we enable direct translation from natural language descriptions to executable CFD setups. A multi-agent framework orchestrates the process, autonomously verifying inputs, generating configurations, running simulations, and correcting errors. Evaluation on a benchmark of 21 diverse flow cases demonstrates state-of-the-art performance, achieving 88.7% solution accuracy and 82.6% first-attempt success rate. This significantly outperforms larger general-purpose models like Qwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also requiring fewer correction iterations and maintaining high computational efficiency. The results highlight the critical role of domain-specific adaptation in deploying LLM assistants for complex engineering workflows.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.5 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.333
            </span>
            <a href="https://arxiv.org/abs/2405.13795" target="_blank" rel="noopener noreferrer">A Parametrizable Algorithm for Distributed Approximate Similarity Search with Arbitrary Distances</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Elena Garcia-Morato, Maria Jesus Algar, Cesar Alfaro, Felipe Ortega, Javier Gomez, Javier M. Moguerza | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent studies have explored alternative distance measures for similarity search in spaces with diverse topologies, emphasizing the importance of selecting an appropriate distance function to improve the performance of k-Nearest Neighbour search algorithms. However, a critical gap remains in accommo</span>
            
            <span class="abstract-full" style="display: none;">Recent studies have explored alternative distance measures for similarity search in spaces with diverse topologies, emphasizing the importance of selecting an appropriate distance function to improve the performance of k-Nearest Neighbour search algorithms. However, a critical gap remains in accommodating such diverse similarity measures, as most existing methods for exact or approximate similarity search are explicitly designed for metric spaces.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 9.3 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3455
            </span>
            <a href="https://arxiv.org/abs/2504.09904" target="_blank" rel="noopener noreferrer">LiteTracker: Leveraging Temporal Causality for Accurate Low-latency Tissue Tracking</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mert Asim Karaoglu, Wenbo Ji, Ahmed Abbas, Nassir Navab, Benjamin Busam, Alexander Ladikos | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Tissue tracking plays a critical role in various surgical navigation and extended reality (XR) applications. While current methods trained on large synthetic datasets achieve high tracking accuracy and generalize well to endoscopic scenes, their runtime performances fail to meet the low-latency requ</span>
            
            <span class="abstract-full" style="display: none;">Tissue tracking plays a critical role in various surgical navigation and extended reality (XR) applications. While current methods trained on large synthetic datasets achieve high tracking accuracy and generalize well to endoscopic scenes, their runtime performances fail to meet the low-latency requirements necessary for real-time surgical applications. To address this limitation, we propose LiteTracker, a low-latency method for tissue tracking in endoscopic video streams. LiteTracker builds on a state-of-the-art long-term point tracking method, and introduces a set of training-free runtime optimizations. These optimizations enable online, frame-by-frame tracking by leveraging a temporal memory buffer for efficient feature reuse and utilizing prior motion for accurate track initialization. LiteTracker demonstrates significant runtime improvements being around 7x faster than its predecessor and 2x than the state-of-the-art. Beyond its primary focus on efficiency, LiteTracker delivers high-accuracy tracking and occlusion prediction, performing competitively on both the STIR and SuPer datasets. We believe LiteTracker is an important step toward low-latency tissue tracking for real-time surgical applications in the operating room.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3488
            </span>
            <a href="https://arxiv.org/abs/2504.09886" target="_blank" rel="noopener noreferrer">Investigating Syntactic Biases in Multilingual Transformers with RC Attachment Ambiguities in Italian and English</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Michael Kamerath, Aniello De Santo | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper leverages past sentence processing studies to investigate whether monolingual and multilingual LLMs show human-like preferences when presented with examples of relative clause attachment ambiguities in Italian and English. Furthermore, we test whether these preferences can be modulated by</span>
            
            <span class="abstract-full" style="display: none;">This paper leverages past sentence processing studies to investigate whether monolingual and multilingual LLMs show human-like preferences when presented with examples of relative clause attachment ambiguities in Italian and English. Furthermore, we test whether these preferences can be modulated by lexical factors (the type of verb/noun in the matrix clause) which have been shown to be tied to subtle constraints on syntactic and semantic relations. Our results overall showcase how LLM behavior varies interestingly across models, but also general failings of these models in correctly capturing human-like preferences. In light of these results, we argue that RC attachment is the ideal benchmark for cross-linguistic investigations of LLMs' linguistic knowledge and biases.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.4 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3602
            </span>
            <a href="https://arxiv.org/abs/2504.08583" target="_blank" rel="noopener noreferrer">AstroLLaVA: towards the unification of astronomical data and natural language</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sharaf Zaman, Michael J. Smith, Pranav Khetarpal, Rishabh Chakrabarty, Michele Ginolfi, Marc Huertas-Company, Maja Jab{\l}o\'nska, Sandor Kruk, Matthieu Le Lain, Sergio Jos\'e Rodr\'iguez M\'endez, Dimitrios Tanoglidis | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present AstroLLaVA, a vision language model for astronomy that enables interaction with astronomical imagery through natural dialogue. By fine-tuning the LLaVA model on a diverse dataset of $\sim$30k images with captions and question-answer pairs sourced from NASA's `Astronomy Picture of the Day'</span>
            
            <span class="abstract-full" style="display: none;">We present AstroLLaVA, a vision language model for astronomy that enables interaction with astronomical imagery through natural dialogue. By fine-tuning the LLaVA model on a diverse dataset of $\sim$30k images with captions and question-answer pairs sourced from NASA's `Astronomy Picture of the Day', the European Southern Observatory, and the NASA/ESA Hubble Space Telescope, we create a model capable of answering open-ended questions about astronomical concepts depicted visually. Our two-stage fine-tuning process adapts the model to both image captioning and visual question answering in the astronomy domain. We demonstrate AstroLLaVA's performance on an astronomical visual question answering benchmark and release the model weights, code, and training set to encourage further open source work in this space. Finally, we suggest a roadmap towards general astronomical data alignment with pre-trained language models, and provide an open space for collaboration towards this end for interested researchers.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3729
            </span>
            <a href="https://arxiv.org/abs/2411.13055" target="_blank" rel="noopener noreferrer">Hardware Scaling Trends and Diminishing Returns in Large-Scale Distributed Training</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jared Fernandez, Luca Wehrstedt, Leonid Shamis, Mostafa Elhoushi, Kalyan Saladi, Yonatan Bisk, Emma Strubell, Jacob Kahn | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Dramatic increases in the capabilities of neural network models in recent years are driven by scaling model size, training data, and corresponding computational resources. To develop the exceedingly large networks required in modern applications, such as large language models (LLMs), model training </span>
            
            <span class="abstract-full" style="display: none;">Dramatic increases in the capabilities of neural network models in recent years are driven by scaling model size, training data, and corresponding computational resources. To develop the exceedingly large networks required in modern applications, such as large language models (LLMs), model training is distributed across tens of thousands of hardware accelerators (e.g. GPUs), requiring orchestration of computation and communication across large computing clusters. In this work, we demonstrate that careful consideration of hardware configuration and parallelization strategy is critical for effective (i.e. compute- and cost-efficient) scaling of model size, training data, and total computation. We conduct an extensive empirical study of the performance of large-scale LLM training workloads across model size, hardware configurations, and distributed parallelization strategies. We demonstrate that: (1) beyond certain scales, overhead incurred from certain distributed communication strategies leads parallelization strategies previously thought to be sub-optimal in fact become preferable; and (2) scaling the total number of accelerators for large model training quickly yields diminishing returns even when hardware and parallelization strategies are properly optimized, implying poor marginal performance per additional unit of power or GPU-hour.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.2 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3819
            </span>
            <a href="https://arxiv.org/abs/2504.08445" target="_blank" rel="noopener noreferrer">A Systematic Evaluation of Knowledge Graph Embeddings for Gene-Disease Association Prediction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Catarina Canastra, C\'atia Pesquita | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Discovery gene-disease links is important in biology and medicine areas, enabling disease identification and drug repurposing. Machine learning approaches accelerate this process by leveraging biological knowledge represented in ontologies and the structure of knowledge graphs. Still, many existing </span>
            
            <span class="abstract-full" style="display: none;">Discovery gene-disease links is important in biology and medicine areas, enabling disease identification and drug repurposing. Machine learning approaches accelerate this process by leveraging biological knowledge represented in ontologies and the structure of knowledge graphs. Still, many existing works overlook ontologies explicitly representing diseases, missing causal and semantic relationships between them. The gene-disease association problem naturally frames itself as a link prediction task, where embedding algorithms directly predict associations by exploring the structure and properties of the knowledge graph. Some works frame it as a node-pair classification task, combining embedding algorithms with traditional machine learning algorithms. This strategy aligns with the logic of a machine learning pipeline. However, the use of negative examples and the lack of validated gene-disease associations to train embedding models may constrain its effectiveness. This work introduces a novel framework for comparing the performance of link prediction versus node-pair classification tasks, analyses the performance of state of the art gene-disease association approaches, and compares the different order-based formalizations of gene-disease association prediction. It also evaluates the impact of the semantic richness through a disease-specific ontology and additional links between ontologies. The framework involves five steps: data splitting, knowledge graph integration, embedding, modeling and prediction, and method evaluation. Results show that enriching the semantic representation of diseases slightly improves performance, while additional links generate a greater impact. Link prediction methods better explore the semantic richness encoded in knowledge graphs. Although node-pair classification methods identify all true positives, link prediction methods outperform overall.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.5 -->
                
            <!-- LLMs: 6.1 -->
                
            <!-- Quantum Computing: 5.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3846
            </span>
            <a href="https://arxiv.org/abs/2504.10445" target="_blank" rel="noopener noreferrer">RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Suyu Ye, Haojun Shi, Darren Shih, Hyokun Yun, Tanya Roosta, Tianmin Shu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">To achieve successful assistance with long-horizon web-based tasks, AI agents must be able to sequentially follow real-world user instructions over a long period. Unlike existing web-based agent benchmarks, sequential instruction following in the real world poses significant challenges beyond perfor</span>
            
            <span class="abstract-full" style="display: none;">To achieve successful assistance with long-horizon web-based tasks, AI agents must be able to sequentially follow real-world user instructions over a long period. Unlike existing web-based agent benchmarks, sequential instruction following in the real world poses significant challenges beyond performing a single, clearly defined task. For instance, real-world human instructions can be ambiguous, require different levels of AI assistance, and may evolve over time, reflecting changes in the user's mental state. To address this gap, we introduce RealWebAssist, a novel benchmark designed to evaluate sequential instruction-following in realistic scenarios involving long-horizon interactions with the web, visual GUI grounding, and understanding ambiguous real-world user instructions. RealWebAssist includes a dataset of sequential instructions collected from real-world human users. Each user instructs a web-based assistant to perform a series of tasks on multiple websites. A successful agent must reason about the true intent behind each instruction, keep track of the mental state of the user, understand user-specific routines, and ground the intended tasks to actions on the correct GUI elements. Our experimental results show that state-of-the-art models struggle to understand and ground user instructions, posing critical challenges in following real-world user instructions for long-horizon web assistance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3889
            </span>
            <a href="https://arxiv.org/abs/2504.09841" target="_blank" rel="noopener noreferrer">StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yang Feng, Xudong Pan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The proliferation of autonomous agents powered by large language models (LLMs) has revolutionized popular business applications dealing with tabular data, i.e., tabular agents. Although LLMs are observed to be vulnerable against prompt injection attacks from external data sources, tabular agents imp</span>
            
            <span class="abstract-full" style="display: none;">The proliferation of autonomous agents powered by large language models (LLMs) has revolutionized popular business applications dealing with tabular data, i.e., tabular agents. Although LLMs are observed to be vulnerable against prompt injection attacks from external data sources, tabular agents impose strict data formats and predefined rules on the attacker's payload, which are ineffective unless the agent navigates multiple layers of structural data to incorporate the payload. To address the challenge, we present a novel attack termed StruPhantom which specifically targets black-box LLM-powered tabular agents. Our attack designs an evolutionary optimization procedure which continually refines attack payloads via the proposed constrained Monte Carlo Tree Search augmented by an off-topic evaluator. StruPhantom helps systematically explore and exploit the weaknesses of target applications to achieve goal hijacking. Our evaluation validates the effectiveness of StruPhantom across various LLM-based agents, including those on real-world platforms, and attack scenarios. Our attack achieves over 50% higher success rates than baselines in enforcing the application's response to contain phishing links or malicious codes.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3989
            </span>
            <a href="https://arxiv.org/abs/2503.03506" target="_blank" rel="noopener noreferrer">Opinion: Revisiting synthetic data classifications from a privacy perspective</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vibeke Binz Vallevik, Serena Elizabeth Marshall, Aleksandar Babic, Jan Franz Nygaard | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Synthetic data is emerging as a cost-effective solution necessary to meet the increasing data demands of AI development, created either from existing knowledge or derived from real data. The traditional classification of synthetic data types into hybrid, partial or fully synthetic datasets has limit</span>
            
            <span class="abstract-full" style="display: none;">Synthetic data is emerging as a cost-effective solution necessary to meet the increasing data demands of AI development, created either from existing knowledge or derived from real data. The traditional classification of synthetic data types into hybrid, partial or fully synthetic datasets has limited value and does not reflect the ever-increasing methods to generate synthetic data. The generation method and their source jointly shape the characteristics of synthetic data, which in turn determines its practical applications. We make a case for an alternative approach to grouping synthetic data types that better reflect privacy perspectives in order to facilitate regulatory guidance in the generation and processing of synthetic data. This approach to classification provides flexibility to new advancements like deep generative methods and offers a more practical framework for future applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.7 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4006
            </span>
            <a href="https://arxiv.org/abs/2404.09247" target="_blank" rel="noopener noreferrer">Generalization Error Bounds for Learning under Censored Feedback</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yifan Yang, Ali Payani, Parinaz Naghizadeh | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generalization error bounds from learning theory provide statistical guarantees on how well an algorithm will perform on previously unseen data. In this paper, we characterize the impacts of data non-IIDness due to censored feedback (a.k.a. selective labeling bias) on such bounds. Censored feedback </span>
            
            <span class="abstract-full" style="display: none;">Generalization error bounds from learning theory provide statistical guarantees on how well an algorithm will perform on previously unseen data. In this paper, we characterize the impacts of data non-IIDness due to censored feedback (a.k.a. selective labeling bias) on such bounds. Censored feedback is ubiquitous in many real-world online selection and classification tasks (e.g., hiring, lending, recommendation systems) where the true label of a data point is only revealed if a favorable decision is made (e.g., accepting a candidate, approving a loan, displaying an ad), and remains unknown otherwise. We first derive an extension of the well-known Dvoretzky-Kiefer-Wolfowitz (DKW) inequality, which characterizes the gap between empirical and theoretical data distribution CDFs learned from IID data, to problems with non-IID data due to censored feedback. We then use this CDF error bound to provide a bound on the generalization error guarantees of a classifier trained on such non-IID data. We show that existing generalization error bounds (which do not account for censored feedback) fail to correctly capture the model's generalization guarantees, verifying the need for our bounds. We further analyze the effectiveness of (pure and bounded) exploration techniques, proposed by recent literature as a way to alleviate censored feedback, on improving our error bounds. Together, our findings illustrate how a decision maker should account for the trade-off between strengthening the generalization guarantees of an algorithm and the costs incurred in data collection when future data availability is limited by censored feedback.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.1 -->
                
            <!-- LLMs: 7.6 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4039
            </span>
            <a href="https://arxiv.org/abs/2504.09167" target="_blank" rel="noopener noreferrer">Stable Determination and Reconstruction of a Quasilinear Term in an Elliptic Equation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jason Choy, Maolin Deng, Bangti Jin, Yavar Kian | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this work, we investigate the inverse problem of determining a quasilinear term appearing in a nonlinear elliptic equation from the measurement of the conormal derivative on the boundary. This problem arises in several practical applications, e.g., heat conduction. We derive novel H\"older stabil</span>
            
            <span class="abstract-full" style="display: none;">In this work, we investigate the inverse problem of determining a quasilinear term appearing in a nonlinear elliptic equation from the measurement of the conormal derivative on the boundary. This problem arises in several practical applications, e.g., heat conduction. We derive novel H\"older stability estimates for both multi- and one-dimensional cases: in the multi-dimensional case, the stability estimates are stated with one single boundary measurement, whereas in the one-dimensional case, due to dimensionality limitation, the stability results are stated for the Dirichlet boundary condition varying in a space of dimension one. We derive these estimates using different properties of solution representations. We complement the theoretical results with numerical reconstructions of the quasilinear term, which illustrate the stable recovery of the quasilinear term in the presence of data noise.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.4 -->
                
            <!-- LLMs: 6.4 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Pathfinding: 1.7 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4187
            </span>
            <a href="https://arxiv.org/abs/2503.15199" target="_blank" rel="noopener noreferrer">Radon: a Programming Model and Platform for Computing Continuum Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Luca De Martini, Dario d'Abate, Alessandro Margara, Gianpaolo Cugola | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Emerging compute continuum environments pose new challenges that traditional cloud-centric architectures struggle to address. Latency, bandwidth constraints, and the heterogeneity of edge environments hinder the efficiency of centralized cloud solutions. While major cloud providers extend their plat</span>
            
            <span class="abstract-full" style="display: none;">Emerging compute continuum environments pose new challenges that traditional cloud-centric architectures struggle to address. Latency, bandwidth constraints, and the heterogeneity of edge environments hinder the efficiency of centralized cloud solutions. While major cloud providers extend their platforms to the edge, these approaches often overlook its unique characteristics, limiting its potential.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.5 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4214
            </span>
            <a href="https://arxiv.org/abs/2504.08306" target="_blank" rel="noopener noreferrer">STSeg-Complex Video Object Segmentation: The 1st Solution for 4th PVUW MOSE Challenge</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kehuan Song, Xinglin Xie, Kexin Zhang, Licheng Jiao, Lingling Li, Shuyuan Yang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Segmentation of video objects in complex scenarios is highly challenging, and the MOSE dataset has significantly contributed to the development of this field. This technical report details the STSeg solution proposed by the "imaplus" team.By finetuning SAM2 and the unsupervised model TMO on the MOSE</span>
            
            <span class="abstract-full" style="display: none;">Segmentation of video objects in complex scenarios is highly challenging, and the MOSE dataset has significantly contributed to the development of this field. This technical report details the STSeg solution proposed by the "imaplus" team.By finetuning SAM2 and the unsupervised model TMO on the MOSE dataset, the STSeg solution demonstrates remarkable advantages in handling complex object motions and long-video sequences. In the inference phase, an Adaptive Pseudo-labels Guided Model Refinement Pipeline is adopted to intelligently select appropriate models for processing each video. Through finetuning the models and employing the Adaptive Pseudo-labels Guided Model Refinement Pipeline in the inference phase, the STSeg solution achieved a J&amp;F score of 87.26% on the test set of the 2025 4th PVUW Challenge MOSE Track, securing the 1st place and advancing the technology for video object segmentation in complex scenarios.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.9 -->
                
            <!-- LLMs: 7.7 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4231
            </span>
            <a href="https://arxiv.org/abs/2504.09219" target="_blank" rel="noopener noreferrer">Generation of Musical Timbres using a Text-Guided Diffusion Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Weixuan Yuan, Qadeer Khan, Vladimir Golkov | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent years, text-to-audio systems have achieved remarkable success, enabling the generation of complete audio segments directly from text descriptions. While these systems also facilitate music creation, the element of human creativity and deliberate expression is often limited. In contrast, th</span>
            
            <span class="abstract-full" style="display: none;">In recent years, text-to-audio systems have achieved remarkable success, enabling the generation of complete audio segments directly from text descriptions. While these systems also facilitate music creation, the element of human creativity and deliberate expression is often limited. In contrast, the present work allows composers, arrangers, and performers to create the basic building blocks for music creation: audio of individual musical notes for use in electronic instruments and DAWs. Through text prompts, the user can specify the timbre characteristics of the audio. We introduce a system that combines a latent diffusion model and multi-modal contrastive learning to generate musical timbres conditioned on text descriptions. By jointly generating the magnitude and phase of the spectrogram, our method eliminates the need for subsequently running a phase retrieval algorithm, as related methods do.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.6 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4308
            </span>
            <a href="https://arxiv.org/abs/2504.04843" target="_blank" rel="noopener noreferrer">Data Augmentation as Free Lunch: Exploring the Test-Time Augmentation for Sequential Recommendation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yizhou Dang, Yuting Liu, Enneng Yang, Minhan Huang, Guibing Guo, Jianzhe Zhao, Xingwei Wang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Data augmentation has become a promising method of mitigating data sparsity in sequential recommendation. Existing methods generate new yet effective data during model training to improve performance. However, deploying them requires retraining, architecture modification, or introducing additional l</span>
            
            <span class="abstract-full" style="display: none;">Data augmentation has become a promising method of mitigating data sparsity in sequential recommendation. Existing methods generate new yet effective data during model training to improve performance. However, deploying them requires retraining, architecture modification, or introducing additional learnable parameters. The above steps are time-consuming and costly for well-trained models, especially when the model scale becomes large. In this work, we explore the test-time augmentation (TTA) for sequential recommendation, which augments the inputs during the model inference and then aggregates the model's predictions for augmented data to improve final accuracy. It avoids significant time and cost overhead from loss calculation and backward propagation. We first experimentally disclose the potential of existing augmentation operators for TTA and find that the Mask and Substitute consistently achieve better performance. Further analysis reveals that these two operators are effective because they retain the original sequential pattern while adding appropriate perturbations. Meanwhile, we argue that these two operators still face time-consuming item selection or interference information from mask tokens. Based on the analysis and limitations, we present TNoise and TMask. The former injects uniform noise into the original representation, avoiding the computational overhead of item selection. The latter blocks mask token from participating in model calculations or directly removes interactions that should have been replaced with mask tokens. Comprehensive experiments demonstrate the effectiveness, efficiency, and generalizability of our method. We provide an anonymous implementation at https://github.com/KingGugu/TTA4SR.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4311
            </span>
            <a href="https://arxiv.org/abs/2411.17161" target="_blank" rel="noopener noreferrer">Enhancing Lane Segment Perception and Topology Reasoning with Crowdsourcing Trajectory Priors</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Peijin Jia, Ziang Luo, Tuopu Wen, Mengmeng Yang, Kun Jiang, Le Cui, Diange Yang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In autonomous driving, recent advances in lane segment perception provide autonomous vehicles with a comprehensive understanding of driving scenarios. Moreover, incorporating prior information input into such perception model represents an effective approach to ensure the robustness and accuracy. Ho</span>
            
            <span class="abstract-full" style="display: none;">In autonomous driving, recent advances in lane segment perception provide autonomous vehicles with a comprehensive understanding of driving scenarios. Moreover, incorporating prior information input into such perception model represents an effective approach to ensure the robustness and accuracy. However, utilizing diverse sources of prior information still faces three key challenges: the acquisition of high-quality prior information, alignment between prior and online perception, efficient integration. To address these issues, we investigate prior augmentation from a novel perspective of trajectory priors. In this paper, we initially extract crowdsourcing trajectory data from Argoverse2 motion forecasting dataset and encode trajectory data into rasterized heatmap and vectorized instance tokens, then we incorporate such prior information into the online mapping model through different ways. Besides, with the purpose of mitigating the misalignment between prior and online perception, we design a confidence-based fusion module that takes alignment into account during the fusion process. We conduct extensive experiments on OpenLane-V2 dataset. The results indicate that our method's performance significantly outperforms the current state-of-the-art methods. Code is released is at https://github.com/wowlza/TrajTopo</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.9 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4376
            </span>
            <a href="https://arxiv.org/abs/2504.07987" target="_blank" rel="noopener noreferrer">mixEEG: Enhancing EEG Federated Learning for Cross-subject EEG Classification with Tailored mixup</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xuan-Hao Liu, Bao-Liang Lu, Wei-Long Zheng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The cross-subject electroencephalography (EEG) classification exhibits great challenges due to the diversity of cognitive processes and physiological structures between different subjects. Modern EEG models are based on neural networks, demanding a large amount of data to achieve high performance an</span>
            
            <span class="abstract-full" style="display: none;">The cross-subject electroencephalography (EEG) classification exhibits great challenges due to the diversity of cognitive processes and physiological structures between different subjects. Modern EEG models are based on neural networks, demanding a large amount of data to achieve high performance and generalizability. However, privacy concerns associated with EEG pose significant limitations to data sharing between different hospitals and institutions, resulting in the lack of large dataset for most EEG tasks. Federated learning (FL) enables multiple decentralized clients to collaboratively train a global model without direct communication of raw data, thus preserving privacy. For the first time, we investigate the cross-subject EEG classification in the FL setting. In this paper, we propose a simple yet effective framework termed mixEEG. Specifically, we tailor the vanilla mixup considering the unique properties of the EEG modality. mixEEG shares the unlabeled averaged data of the unseen subject rather than simply sharing raw data under the domain adaptation setting, thus better preserving privacy and offering an averaged label as pseudo-label. Extensive experiments are conducted on an epilepsy detection and an emotion recognition dataset. The experimental result demonstrates that our mixEEG enhances the transferability of global model for cross-subject EEG classification consistently across different datasets and model architectures. Code is published at: https://github.com/XuanhaoLiu/mixEEG.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Federated Learning: 2.2 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4454
            </span>
            <a href="https://arxiv.org/abs/2504.09526" target="_blank" rel="noopener noreferrer">Super-Exponential Approximation of the Riemann-Liouville Fractional Integral via Shifted Gegenbauer Pseudospectral Methods</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kareem T. Elgindy | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper introduces a shifted Gegenbauer pseudospectral (SGPS) method for high-precision approximation of the left Riemann-Liouville fractional integral (RLFI). By using precomputable fractional-order shifted Gegenbauer integration matrices (FSGIMs), the method achieves super-exponential convergen</span>
            
            <span class="abstract-full" style="display: none;">This paper introduces a shifted Gegenbauer pseudospectral (SGPS) method for high-precision approximation of the left Riemann-Liouville fractional integral (RLFI). By using precomputable fractional-order shifted Gegenbauer integration matrices (FSGIMs), the method achieves super-exponential convergence for smooth functions, delivering near machine-precision accuracy with minimal computational cost. Tunable shifted Gegenbauer (SG) parameters enable flexible optimization across diverse problems, while rigorous error analysis confirms rapid error decay under optimal settings. Numerical experiments demonstrate that the SGPS method outperforms MATLAB's integral, MATHEMATICA's NIntegrate, and existing techniques by up to two orders of magnitude in accuracy, with superior efficiency for varying fractional orders 0 < \alpha < 1. Its adaptability and precision make the SGPS method a transformative tool for fractional calculus, ideal for modeling complex systems with memory and non-local behavior.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Medicine: 6.6 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4476
            </span>
            <a href="https://arxiv.org/abs/2504.08678" target="_blank" rel="noopener noreferrer">From "Worse is Better" to Better: Lessons from a Mixed Methods Study of Ansible's Challenges</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Carolina Carreira, Nuno Saavedra, Alexandra Mendes, Jo\~ao F. Ferreira | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Infrastructure as Code (IaC) tools have transformed the way IT infrastructure is automated and managed, but their growing adoption has also exposed numerous challenges for practitioners. In this paper, we investigate these challenges through the lens of Ansible, a popular IaC tool. Using a mixed met</span>
            
            <span class="abstract-full" style="display: none;">Infrastructure as Code (IaC) tools have transformed the way IT infrastructure is automated and managed, but their growing adoption has also exposed numerous challenges for practitioners. In this paper, we investigate these challenges through the lens of Ansible, a popular IaC tool. Using a mixed methods approach, we investigate challenges, obstacles, and issues faced by practitioners. We analyze 59,157 posts from Stack Overflow, Reddit, and the Ansible Forum to identify common pain points, complemented by 16 semi-structured interviews with practitioners of varying expertise levels.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4507
            </span>
            <a href="https://arxiv.org/abs/2504.08361" target="_blank" rel="noopener noreferrer">SN-LiDAR: Semantic Neural Fields for Novel Space-time View LiDAR Synthesis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yi Chen, Tianchen Deng, Wentao Zhao, Xiaoning Wang, Wenqian Xi, Weidong Chen, Jingchuan Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent research has begun exploring novel view synthesis (NVS) for LiDAR point clouds, aiming to generate realistic LiDAR scans from unseen viewpoints. However, most existing approaches do not reconstruct semantic labels, which are crucial for many downstream applications such as autonomous driving </span>
            
            <span class="abstract-full" style="display: none;">Recent research has begun exploring novel view synthesis (NVS) for LiDAR point clouds, aiming to generate realistic LiDAR scans from unseen viewpoints. However, most existing approaches do not reconstruct semantic labels, which are crucial for many downstream applications such as autonomous driving and robotic perception. Unlike images, which benefit from powerful segmentation models, LiDAR point clouds lack such large-scale pre-trained models, making semantic annotation time-consuming and labor-intensive. To address this challenge, we propose SN-LiDAR, a method that jointly performs accurate semantic segmentation, high-quality geometric reconstruction, and realistic LiDAR synthesis. Specifically, we employ a coarse-to-fine planar-grid feature representation to extract global features from multi-frame point clouds and leverage a CNN-based encoder to extract local semantic features from the current frame point cloud. Extensive experiments on SemanticKITTI and KITTI-360 demonstrate the superiority of SN-LiDAR in both semantic and geometric reconstruction, effectively handling dynamic objects and large-scale scenes. Codes will be available on https://github.com/dtc111111/SN-Lidar.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.0 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- 3D: 2.0 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4525
            </span>
            <a href="https://arxiv.org/abs/2504.08666" target="_blank" rel="noopener noreferrer">Variability-Driven User-Story Generation using LLM and Triadic Concept Analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alexandre Bazin (Huaxi), Alain Gutierrez (Huaxi), Marianne Huchard (Huaxi), Pierre Martin (Huaxi), Yulin (Huaxi), Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A widely used Agile practice for requirements is to produce a set of user stories (also called ``agile product backlog''), which roughly includes a list of pairs (role, feature), where the role handles the feature for a certain purpose. In the context of Software Product Lines, the requirements for </span>
            
            <span class="abstract-full" style="display: none;">A widely used Agile practice for requirements is to produce a set of user stories (also called ``agile product backlog''), which roughly includes a list of pairs (role, feature), where the role handles the feature for a certain purpose. In the context of Software Product Lines, the requirements for a family of similar systems is thus a family of user-story sets, one per system, leading to a 3-dimensional dataset composed of sets of triples (system, role, feature). In this paper, we combine Triadic Concept Analysis (TCA) and Large Language Model (LLM) prompting to suggest the user-story set required to develop a new system relying on the variability logic of an existing system family. This process consists in 1) computing 3-dimensional variability expressed as a set of TCA implications, 2) providing the designer with intelligible design options, 3) capturing the designer's selection of options, 4) proposing a first user-story set corresponding to this selection, 5) consolidating its validity according to the implications identified in step 1, while completing it if necessary, and 6) leveraging LLM to have a more comprehensive website. This process is evaluated with a dataset comprising the user-story sets of 67 similar-purpose websites.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.5 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4533
            </span>
            <a href="https://arxiv.org/abs/2503.00771" target="_blank" rel="noopener noreferrer">Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yupu Hao, Pengfei Cao, Zhuoran Jin, Huanxuan Liao, Yubo Chen, Kang Liu, Jun Zhao | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Personalized tool utilization is essential for aligning large language models (LLMs) with user preference in interaction scenarios with various tools. However, most of the current benchmarks primarily focus on either personalization of text generation or direct tool-utilizing, without considering bo</span>
            
            <span class="abstract-full" style="display: none;">Personalized tool utilization is essential for aligning large language models (LLMs) with user preference in interaction scenarios with various tools. However, most of the current benchmarks primarily focus on either personalization of text generation or direct tool-utilizing, without considering both. In this work, we introduce a novel benchmark ETAPP for evaluating personalized tool invocation, establishing a sandbox environment, and a comprehensive dataset of 800 testing cases covering diverse user profiles. To improve the accuracy of our evaluation, we propose a key-point-based LLM evaluation method, mitigating biases in the LLM-as-a-judge system by manually annotating key points for each test case and providing them to LLM as the reference. Additionally, we evaluate the excellent LLMs and provide an in-depth analysis. Furthermore, we investigate the impact of different tool-invoking strategies on LLMs' personalization performance and the effects of fine-tuning in our task. The effectiveness of our preference-setting and key-point-based evaluation method is also validated. Our findings offer insights into improving personalized LLM agents. Our Code is available at https://github.com/hypasd-art/ETAPP.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.5 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4698
            </span>
            <a href="https://arxiv.org/abs/2306.03894" target="_blank" rel="noopener noreferrer">Fractals from Regular Behaviours</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Todd Schmid, Victoria Noquez, Lawrence S. Moss | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We forge connections between the theory of fractal sets obtained as attractors of iterated function systems and process calculi. To this end, we reinterpret Milner's expressions for processes as contraction operators on a complete metric space. When the space is, for example, the plane, the denotati</span>
            
            <span class="abstract-full" style="display: none;">We forge connections between the theory of fractal sets obtained as attractors of iterated function systems and process calculi. To this end, we reinterpret Milner's expressions for processes as contraction operators on a complete metric space. When the space is, for example, the plane, the denotations of fixed point terms correspond to familiar fractal sets. We give a sound and complete axiomatization of fractal equivalence, the congruence on terms consisting of pairs that construct identical self-similar sets in all interpretations. We further make connections to labelled Markov chains and to invariant measures. In all of this work, we use important results from process calculi. For example, we use Rabinovich's completeness theorem for trace equivalence in our own completeness theorem. In addition to our results, we also raise many questions related to both fractals and process calculi.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4823
            </span>
            <a href="https://arxiv.org/abs/2504.08633" target="_blank" rel="noopener noreferrer">Transformer-Based Interfaces for Mechanical Assembly Design: A Gear Train Case Study</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mohammadmehdi Ataei, Hyunmin Cheong, Jiwon Jun, Justin Matejka, Alexander Tessier, George Fitzmaurice | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative artificial intelligence (AI), particularly transformer-based models, presents new opportunities for automating and augmenting engineering design workflows. However, effectively integrating these models into interactive tools requires careful interface design that leverages their unique ca</span>
            
            <span class="abstract-full" style="display: none;">Generative artificial intelligence (AI), particularly transformer-based models, presents new opportunities for automating and augmenting engineering design workflows. However, effectively integrating these models into interactive tools requires careful interface design that leverages their unique capabilities. This paper introduces a transformer model tailored for gear train assembly design, paired with two novel interaction modes: Explore and Copilot. Explore Mode uses probabilistic sampling to generate and evaluate diverse design alternatives, while Copilot Mode utilizes autoregressive prediction to support iterative, context-aware refinement. These modes emphasize key transformer properties (sequence-based generation and probabilistic exploration) to facilitate intuitive and efficient human-AI collaboration. Through a case study, we demonstrate how well-designed interfaces can enhance engineers' ability to balance automation with domain expertise. A user study shows that Explore Mode supports rapid exploration and problem redefinition, while Copilot Mode provides greater control and fosters deeper engagement. Our results suggest that hybrid workflows combining both modes can effectively support complex, creative engineering design processes.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4844
            </span>
            <a href="https://arxiv.org/abs/2504.09820" target="_blank" rel="noopener noreferrer">Finite-Precision Conjugate Gradient Method for Massive MIMO Detection</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yiming Fang, Li Chen, Changsheng You, Dingzhu Wen, Pengcheng Zhu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The implementation of the conjugate gradient (CG) method for massive MIMO detection is computationally challenging, especially for a large number of users and correlated channels. In this paper, we propose a low computational complexity CG detection from a finite-precision perspective. First, we dev</span>
            
            <span class="abstract-full" style="display: none;">The implementation of the conjugate gradient (CG) method for massive MIMO detection is computationally challenging, especially for a large number of users and correlated channels. In this paper, we propose a low computational complexity CG detection from a finite-precision perspective. First, we develop a finite-precision CG (FP-CG) detection to mitigate the computational bottleneck of each CG iteration and provide the attainable accuracy, convergence, and computational complexity analysis to reveal the impact of finite-precision arithmetic. A practical heuristic is presented to select suitable precisions. Then, to further reduce the number of iterations, we propose a joint finite-precision and block-Jacobi preconditioned CG (FP-BJ-CG) detection. The corresponding performance analysis is also provided. Finally, simulation results validate the theoretical insights and demonstrate the superiority of the proposed detection.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.4 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4862
            </span>
            <a href="https://arxiv.org/abs/2412.06845" target="_blank" rel="noopener noreferrer">7B Fully Open Source Moxin-LLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkabl</span>
            
            <span class="abstract-full" style="display: none;">Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed, adhering to principles of open science, open source, open data, and open access. We release the pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. After pre-training and obtaining the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our Instruct model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization (GRPO), an efficient and effective reinforcement learning algorithm following DeepSeek R1, to finetune our model, leading to the Moxin Reasoning model. Experiments show that our models achieve superior performance in various evaluations such as zero-shot evaluation, few-shot evaluation, and CoT evaluation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4941
            </span>
            <a href="https://arxiv.org/abs/2504.08520" target="_blank" rel="noopener noreferrer">Joint Transmit Waveform and Receive Filter Design for ISAC System with Jamming</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yuan Shu, Chenhao Qi, Shiwen Mao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, to suppress jamming in the complex electromagnetic environment, we propose a joint transmit waveform and receive filter design framework for integrated sensing and communications (ISAC). By jointly optimizing the transmit waveform and receive filters, we aim at minimizing the multiuse</span>
            
            <span class="abstract-full" style="display: none;">In this paper, to suppress jamming in the complex electromagnetic environment, we propose a joint transmit waveform and receive filter design framework for integrated sensing and communications (ISAC). By jointly optimizing the transmit waveform and receive filters, we aim at minimizing the multiuser interference (MUI), subject to the constraints of the target mainlobe, jamming mainlobe and peak sidelobe level of the receive filter output as well as the transmit power of the ISAC base station. We propose two schemes to solve the problem, including joint transmit waveform and matched filter design (JTMD) and joint transmit waveform and mismatched filter design (JTMMD) schemes. For both schemes, we adopt the alternating direction method of multipliers to iteratively optimize the transmit waveform and receive filters, where the number of targets as well as the range and angles of each target can also be estimated. Simulation results show that both the JTMD and JTMMD schemes achieve superior performance in terms of communication MUI and radar detection performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.0 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.505
            </span>
            <a href="https://arxiv.org/abs/2504.08712" target="_blank" rel="noopener noreferrer">Beyond Black-Box Predictions: Identifying Marginal Feature Effects in Tabular Transformer Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anton Thielmann, Arik Reuter, Benjamin Saefken | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent years, deep neural networks have showcased their predictive power across a variety of tasks. Beyond natural language processing, the transformer architecture has proven efficient in addressing tabular data problems and challenges the previously dominant gradient-based decision trees in the</span>
            
            <span class="abstract-full" style="display: none;">In recent years, deep neural networks have showcased their predictive power across a variety of tasks. Beyond natural language processing, the transformer architecture has proven efficient in addressing tabular data problems and challenges the previously dominant gradient-based decision trees in these areas. However, this predictive power comes at the cost of intelligibility: Marginal feature effects are almost completely lost in the black-box nature of deep tabular transformer networks. Alternative architectures that use the additivity constraints of classical statistical regression models can maintain intelligible marginal feature effects, but often fall short in predictive power compared to their more complex counterparts. To bridge the gap between intelligibility and performance, we propose an adaptation of tabular transformer networks designed to identify marginal feature effects. We provide theoretical justifications that marginal feature effects can be accurately identified, and our ablation study demonstrates that the proposed model efficiently detects these effects, even amidst complex feature interactions. To demonstrate the model's predictive capabilities, we compare it to several interpretable as well as black-box models and find that it can match black-box performances while maintaining intelligibility. The source code is available at https://github.com/OpenTabular/NAMpy.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.506
            </span>
            <a href="https://arxiv.org/abs/2501.18563" target="_blank" rel="noopener noreferrer">No Equations Needed: Learning System Dynamics Without Relying on Closed-Form ODEs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Krzysztof Kacprzyk, Mihaela van der Schaar | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Data-driven modeling of dynamical systems is a crucial area of machine learning. In many scenarios, a thorough understanding of the model's behavior becomes essential for practical applications. For instance, understanding the behavior of a pharmacokinetic model, constructed as part of drug developm</span>
            
            <span class="abstract-full" style="display: none;">Data-driven modeling of dynamical systems is a crucial area of machine learning. In many scenarios, a thorough understanding of the model's behavior becomes essential for practical applications. For instance, understanding the behavior of a pharmacokinetic model, constructed as part of drug development, may allow us to both verify its biological plausibility (e.g., the drug concentration curve is non-negative and decays to zero) and to design dosing guidelines. Discovery of closed-form ordinary differential equations (ODEs) can be employed to obtain such insights by finding a compact mathematical equation and then analyzing it (a two-step approach). However, its widespread use is currently hindered because the analysis process may be time-consuming, requiring substantial mathematical expertise, or even impossible if the equation is too complex. Moreover, if the found equation's behavior does not satisfy the requirements, editing it or influencing the discovery algorithms to rectify it is challenging as the link between the symbolic form of an ODE and its behavior can be elusive. This paper proposes a conceptual shift to modeling low-dimensional dynamical systems by departing from the traditional two-step modeling process. Instead of first discovering a closed-form equation and then analyzing it, our approach, direct semantic modeling, predicts the semantic representation of the dynamical system (i.e., description of its behavior) directly from data, bypassing the need for complex post-hoc analysis. This direct approach also allows the incorporation of intuitive inductive biases into the optimization algorithm and editing the model's behavior directly, ensuring that the model meets the desired specifications. Our approach not only simplifies the modeling pipeline but also enhances the transparency and flexibility of the resulting models compared to traditional closed-form ODEs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5159
            </span>
            <a href="https://arxiv.org/abs/2504.09510" target="_blank" rel="noopener noreferrer">Towards Intuitive Drone Operation Using a Handheld Motion Controller</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Daria Trinitatova, Sofia Shevelo, Dzmitry Tsetserukou | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present an intuitive human-drone interaction system that utilizes a gesture-based motion controller to enhance the drone operation experience in real and simulated environments. The handheld motion controller enables natural control of the drone through the movements of the operator's hand, thumb</span>
            
            <span class="abstract-full" style="display: none;">We present an intuitive human-drone interaction system that utilizes a gesture-based motion controller to enhance the drone operation experience in real and simulated environments. The handheld motion controller enables natural control of the drone through the movements of the operator's hand, thumb, and index finger: the trigger press manages the throttle, the tilt of the hand adjusts pitch and roll, and the thumbstick controls yaw rotation. Communication with drones is facilitated via the ExpressLRS radio protocol, ensuring robust connectivity across various frequencies. The user evaluation of the flight experience with the designed drone controller using the UEQ-S survey showed high scores for both Pragmatic (mean=2.2, SD = 0.8) and Hedonic (mean=2.3, SD = 0.9) Qualities. This versatile control interface supports applications such as research, drone racing, and training programs in real and simulated environments, thereby contributing to advances in the field of human-drone interaction.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.3 -->
                
            <!-- Medicine: 6.6 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Robotics: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5175
            </span>
            <a href="https://arxiv.org/abs/2504.10353" target="_blank" rel="noopener noreferrer">Patch and Shuffle: A Preprocessing Technique for Texture Classification in Autonomous Cementitious Fabrication</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jeremiah Giordani | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Autonomous fabrication systems are transforming construction and manufacturing, yet they remain vulnerable to print errors. Texture classification is a key component of computer vision systems that enable real-time monitoring and adjustment during cementitious fabrication. Traditional classification</span>
            
            <span class="abstract-full" style="display: none;">Autonomous fabrication systems are transforming construction and manufacturing, yet they remain vulnerable to print errors. Texture classification is a key component of computer vision systems that enable real-time monitoring and adjustment during cementitious fabrication. Traditional classification methods often rely on global image features, which can bias the model toward semantic content rather than low-level textures. In this paper, we introduce a novel preprocessing technique called "patch and shuffle," which segments input images into smaller patches, shuffles them, and reconstructs a jumbled image before classification. This transformation removes semantic context, forcing the classifier to rely on local texture features.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.5 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5194
            </span>
            <a href="https://arxiv.org/abs/2504.07709" target="_blank" rel="noopener noreferrer">Integrated Sensing and Communications for Pinching-Antenna Systems (PASS)</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zheng Zhang, Yuanwei Liu, Bingtao He, Jian Chen | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">An integrated sensing and communication (ISAC) design for pinching antenna systems (PASS) is proposed, where the pinching antennas are deployed for establishing reliable line-of-sight communication and sensing links. More particularly, a separated ISAC design is proposed for the two-waveguide PASS, </span>
            
            <span class="abstract-full" style="display: none;">An integrated sensing and communication (ISAC) design for pinching antenna systems (PASS) is proposed, where the pinching antennas are deployed for establishing reliable line-of-sight communication and sensing links. More particularly, a separated ISAC design is proposed for the two-waveguide PASS, where one waveguide is used to emit the joint communication and sensing signals while the other waveguide is used to receive the reflected echo signals. Based on this framework, a penalty-based alternating optimization algorithm is proposed to maximize the illumination power as well as ensure the communication quality-of-service requirement. Numerical results demonstrate that 1) the proposed PASS-ISAC scheme outperforms the other baseline schemes, and 2) the considered equal power allocation model achieves an upper bound performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.0 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 3.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5332
            </span>
            <a href="https://arxiv.org/abs/2504.08202" target="_blank" rel="noopener noreferrer">Harnessing the Unseen: The Hidden Influence of Intrinsic Knowledge in Long-Context Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yu Fu, Haz Sameen Shahgir, Hui Liu, Xianfeng Tang, Qi He, Yue Dong | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent advances in long-context models (LCMs), designed to handle extremely long input contexts, primarily focus on utilizing external contextual information, often leaving the influence of large language models' intrinsic knowledge underexplored. In this work, we investigate how this intrinsic know</span>
            
            <span class="abstract-full" style="display: none;">Recent advances in long-context models (LCMs), designed to handle extremely long input contexts, primarily focus on utilizing external contextual information, often leaving the influence of large language models' intrinsic knowledge underexplored. In this work, we investigate how this intrinsic knowledge affects content generation and demonstrate that its impact becomes increasingly pronounced as context length extends. Furthermore, we show that the model's ability to utilize intrinsic knowledge, which we call intrinsic retrieval ability, does not improve simultaneously with its ability to leverage contextual knowledge through extrinsic retrieval ability. Moreover, better extrinsic retrieval can interfere with the model's ability to use its own knowledge effectively, limiting its full potential. To bridge this gap, we design a simple yet effective Hybrid Needle-in-a-Haystack test that evaluates models based on their capabilities across both retrieval abilities, rather than solely emphasizing extrinsic retrieval ability. Our experimental results reveal that Qwen-2.5 models significantly outperform Llama-3.1 models, demonstrating superior intrinsic retrieval ability. Moreover, even the more powerful Llama-3.1-70B-Instruct model fails to exhibit better performance under LCM conditions, highlighting the importance of evaluating models from a dual-retrieval perspective.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.5 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5527
            </span>
            <a href="https://arxiv.org/abs/2504.09983" target="_blank" rel="noopener noreferrer">DeepCompile: A Compiler-Driven Approach to Optimizing Distributed Deep Learning Training</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Masahiro Tanaka, Du Li, Umesh Chand, Ali Zafar, Haiying Shen, Olatunji Ruwase | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The increasing scale of deep learning models has led to the development of various parallelization strategies for distributed training across accelerators. For example, fully sharded approaches like DeepSpeed ZeRO-3 and FSDP partition the parameters of each layer across multiple GPUs and gather them</span>
            
            <span class="abstract-full" style="display: none;">The increasing scale of deep learning models has led to the development of various parallelization strategies for distributed training across accelerators. For example, fully sharded approaches like DeepSpeed ZeRO-3 and FSDP partition the parameters of each layer across multiple GPUs and gather them through communication when needed. These methods rely on optimizations such as prefetching, which initiates communication early to overlap it with computation and reduce communication overhead, and unsharding, which retains as many parameters in their unsharded form as possible to reduce communication volume. Although the timing of prefetching should be adjusted in response to dynamic memory usage during execution, these systems lack the flexibility to control it, which limits the benefits of prefetching. Moreover, they cannot anticipate how memory usage will change after prefetching is applied, making it difficult to combine it effectively with other optimizations such as unsharding. We present DeepCompile, which compiles user-defined models into computation graphs and applies a sequence of profiling-guided optimization passes for distributed training. Taking dynamic memory usage into account, these passes flexibly insert, reorder, or remove operations to improve communication-computation overlap, reduce memory pressure, and coordinate multiple optimizations in a unified manner. To evaluate the effectiveness of this design, we implemented a fully sharded approach like ZeRO-3 and FSDP on top of DeepCompile, along with three optimizations: proactive prefetching, selective unsharding, and adaptive offloading. We evaluate DeepCompile on the training of Llama 3 70B and Mixtral 8x7B MoE models. DeepCompile achieves up to 1.28x and 1.54x performance improvements over ZeRO-3 and FSDP baselines, respectively, and up to a 7.01x throughput increase with limited GPU resources, using offloading.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.5 -->
                
            <!-- LLMs: 8.7 -->
                
            <!-- Quantum Computing: 4.8 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5559
            </span>
            <a href="https://arxiv.org/abs/2504.09415" target="_blank" rel="noopener noreferrer">Nash Equilibrium Between Consumer Electronic Devices and DoS Attacker for Distributed IoT-enabled RSE Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Gengcan Chen, Donghong Cai, Zahid Khan, Jawad Ahmad, Wadii Boulila | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between cons</span>
            
            <span class="abstract-full" style="display: none;">In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between consumer electronic devices and DoS attackers is critical. This paper focuses on the adversarial strategy between consumer electronic devices and DoS attackers in IoT-enabled RSE Systems. We first propose a remote joint estimation model for distributed measurements to effectively reduce consumer electronic device workload and minimize data leakage risks. The Kalman filter is deployed on the remote estimator, and the DoS attacks with open-loop as well as closed-loop are considered. We further introduce advanced reinforcement learning techniques, including centralized and distributed Minimax-DQN, to address high-dimensional decision-making challenges in both open-loop and closed-loop scenarios. Especially, the Q-network instead of the Q-table is used in the proposed approaches, which effectively solves the challenge of Q-learning. Moreover, the proposed distributed Minimax-DQN reduces the action space to expedite the search for Nash Equilibrium (NE). The experimental results validate that the proposed model can expeditiously restore the RSE error covariance to a stable state in the presence of DoS attacks, exhibiting notable attack robustness. The proposed centralized and distributed Minimax-DQN effectively resolves the NE in both open and closed-loop case, showcasing remarkable performance in terms of convergence. It reveals that substantial advantages in both efficiency and stability are achieved compared with the state-of-the-art methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.5 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5633
            </span>
            <a href="https://arxiv.org/abs/2412.19079" target="_blank" rel="noopener noreferrer">Efficient cell-centered nodal integral method for multi-dimensional Burgers equations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Nadeem Ahmed, Ram Prakash Bharti, Suneet Singh | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">An efficient coarse-mesh nodal integral method (NIM), based on cell-centered variables and termed the cell-centered NIM (CCNIM), is developed and applied to solve multi-dimensional, time-dependent, nonlinear Burgers equations, extending the applicability of CCNIM to nonlinear problems. To overcome t</span>
            
            <span class="abstract-full" style="display: none;">An efficient coarse-mesh nodal integral method (NIM), based on cell-centered variables and termed the cell-centered NIM (CCNIM), is developed and applied to solve multi-dimensional, time-dependent, nonlinear Burgers equations, extending the applicability of CCNIM to nonlinear problems. To overcome the existing limitation of CCNIM to linear problems, the convective velocity in the nonlinear convection term is approximated using two different approaches, both demonstrating accuracy comparable to or better than traditional NIM for nonlinear Burgers problems. Unlike traditional NIM, which utilizes surface-averaged variables as discrete unknowns, this innovative approach formulates the final expression of the numerical scheme using discrete unknowns represented by cell-centered (or node-averaged) variables. Using these cell centroids, the proposed CCNIM approach presents several advantages compared to traditional NIM. These include a simplified implementation process in terms of local coordinate systems, enhanced flexibility regarding the higher order of accuracy in time, straightforward formulation for higher-degree temporal derivatives, and offering a viable option for coupling with other physics. The multi-dimensional time-dependent Burgers problems (propagating shock, propagation, and diffusion of an initial sinusoidal wave, shock-like formation) with known analytical solutions are solved in order to validate the developed scheme. Furthermore, a detailed comparison between the proposed CCNIM approach and other traditional NIM schemes is conducted to demonstrate its effectiveness. The proposed approach has shown quadratic convergence in both space and time, i.e., O[$(\Delta x)^2, (\Delta t)^2$], for the considered test problems. The simplicity and robustness of the approach provide a strong foundation for its seamless extension to more complex fluid flow problems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.4 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5841
            </span>
            <a href="https://arxiv.org/abs/2504.10325" target="_blank" rel="noopener noreferrer">Cumulative-Time Signal Temporal Logic</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hongkai Chen, Zeyu Zhang, Shouvik Roy, Ezio Bartocci, Scott A. Smolka, Scott D. Stoller, Shan Lin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Signal Temporal Logic (STL) is a widely adopted specification language in cyber-physical systems for expressing critical temporal requirements, such as safety conditions and response time. However, STL's expressivity is not sufficient to capture the cumulative duration during which a property holds </span>
            
            <span class="abstract-full" style="display: none;">Signal Temporal Logic (STL) is a widely adopted specification language in cyber-physical systems for expressing critical temporal requirements, such as safety conditions and response time. However, STL's expressivity is not sufficient to capture the cumulative duration during which a property holds within an interval of time. To overcome this limitation, we introduce Cumulative-Time Signal Temporal Logic (CT-STL) that operates over discrete-time signals and extends STL with a new cumulative-time operator. This operator compares the sum of all time steps for which its nested formula is true with a threshold. We present both a qualitative and a quantitative (robustness) semantics for CT-STL and prove both their soundness and completeness properties. We provide an efficient online monitoring algorithm for both semantics. Finally, we show the applicability of CT-STL in two case studies: specifying and monitoring cumulative temporal requirements for a microgrid and an artificial pancreas.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5961
            </span>
            <a href="https://arxiv.org/abs/2504.10372" target="_blank" rel="noopener noreferrer">Simple physical systems as a reference for multivariate information dynamics</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alberto Liardi, Madalina I. Sas, George Blackburne, William J. Knottenbelt, Pedro A. M. Mediano, Henrik Jeldtoft Jensen | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Understanding a complex system entails capturing the non-trivial collective phenomena that arise from interactions between its different parts. Information theory is a flexible and robust framework to study such behaviours, with several measures designed to quantify and characterise the interdepende</span>
            
            <span class="abstract-full" style="display: none;">Understanding a complex system entails capturing the non-trivial collective phenomena that arise from interactions between its different parts. Information theory is a flexible and robust framework to study such behaviours, with several measures designed to quantify and characterise the interdependencies among the system's components. However, since these estimators rely on the statistical distributions of observed quantities, it is crucial to examine the relationships between information-theoretic measures and the system's underlying mechanistic structure. To this end, here we present an information-theoretic analytical investigation of an elementary system of interactive random walkers subject to Gaussian noise. Focusing on partial information decomposition, causal emergence, and integrated information, our results help us develop some intuitions on their relationship with the physical parameters of the system. For instance, we observe that uncoupled systems can exhibit emergent properties, in a way that we suggest may be better described as ''statistically autonomous''. Overall, we observe that in this simple scenario information measures align more reliably with the system's mechanistic properties when calculated at the level of microscopic components, rather than their coarse-grained counterparts, and over timescales comparable with the system's intrinsic dynamics. Moreover, we show that approaches that separate the contributions of the system's dynamics and steady-state distribution (e.g. via causal perturbations) may help strengthen the interpretation of information-theoretic analyses.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.4 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.613
            </span>
            <a href="https://arxiv.org/abs/2504.10399" target="_blank" rel="noopener noreferrer">Unique Decoding of Reed-Solomon and Related Codes for Semi-Adversarial Errors</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Joshua Brakensiek, Yeyuan Chen, Manik Dhar, Zihan Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">For over a quarter century, the Guruswami-Sudan algorithm has served as the state-of-the-art for list-decoding Reed-Solomon (RS) codes up to the Johnson bound against adversarial errors. However, some recent structural results on the combinatorial list decoding of randomly punctured Reed-Solomon cod</span>
            
            <span class="abstract-full" style="display: none;">For over a quarter century, the Guruswami-Sudan algorithm has served as the state-of-the-art for list-decoding Reed-Solomon (RS) codes up to the Johnson bound against adversarial errors. However, some recent structural results on the combinatorial list decoding of randomly punctured Reed-Solomon codes suggest that Johnson bound can likely be broken for some subclasses of RS codes. Motivated by these results, we seek to make traction on understanding adversarial decoding by considering a new model: semi-adversarial errors. This error model bridges between fully random errors and fully adversarial errors by allowing some symbols of a message to be corrupted by an adversary while others are replaced with uniformly random symbols.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6138
            </span>
            <a href="https://arxiv.org/abs/2504.08769" target="_blank" rel="noopener noreferrer">High-order expansion of Neural Ordinary Differential Equations flows</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dario Izzo, Sebastien Origer, Giacomo Acciarini, Francesco Biscani | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Artificial neural networks, widely recognised for their role in machine learning, are now transforming the study of ordinary differential equations (ODEs), bridging data-driven modelling with classical dynamical systems and enabling the development of infinitely deep neural models. However, the prac</span>
            
            <span class="abstract-full" style="display: none;">Artificial neural networks, widely recognised for their role in machine learning, are now transforming the study of ordinary differential equations (ODEs), bridging data-driven modelling with classical dynamical systems and enabling the development of infinitely deep neural models. However, the practical applicability of these models remains constrained by the opacity of their learned dynamics, which operate as black-box systems with limited explainability, thereby hindering trust in their deployment. Existing approaches for the analysis of these dynamical systems are predominantly restricted to first-order gradient information due to computational constraints, thereby limiting the depth of achievable insight. Here, we introduce Event Transition Tensors, a framework based on high-order differentials that provides a rigorous mathematical description of neural ODE dynamics on event manifolds. We demonstrate its versatility across diverse applications: characterising uncertainties in a data-driven prey-predator control model, analysing neural optimal feedback dynamics, and mapping landing trajectories in a three-body neural Hamiltonian system. In all cases, our method enhances the interpretability and rigour of neural ODEs by expressing their behaviour through explicit mathematical structures. Our findings contribute to a deeper theoretical foundation for event-triggered neural differential equations and provide a mathematical construct for explaining complex system dynamics.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.5 -->
                
            <!-- LLMs: 8.3 -->
                
            <!-- Quantum Computing: 5.3 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6201
            </span>
            <a href="https://arxiv.org/abs/2504.08122" target="_blank" rel="noopener noreferrer">Threading the Needle: Test and Evaluation of Early Stage UAS Capabilities to Autonomously Navigate GPS-Denied Environments in the DARPA Fast Lightweight Autonomy (FLA) Program</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Adam Norton, Holly Yanco | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The DARPA Fast Lightweight Autonomy (FLA) program (2015 - 2018) served as a significant milestone in the development of UAS, particularly for autonomous navigation through unknown GPS-denied environments. Three performing teams developed UAS using a common hardware platform, focusing their contribut</span>
            
            <span class="abstract-full" style="display: none;">The DARPA Fast Lightweight Autonomy (FLA) program (2015 - 2018) served as a significant milestone in the development of UAS, particularly for autonomous navigation through unknown GPS-denied environments. Three performing teams developed UAS using a common hardware platform, focusing their contributions on autonomy algorithms and sensing. Several experiments were conducted that spanned indoor and outdoor environments, increasing in complexity over time. This paper reviews the testing methodology developed in order to benchmark and compare the performance of each team, each of the FLA Phase 1 experiments that were conducted, and a summary of the Phase 1 results.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.3 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6376
            </span>
            <a href="https://arxiv.org/abs/2504.09835" target="_blank" rel="noopener noreferrer">Laugh at Your Own Pace: Basic Performance Evaluation of Language Learning Assistance by Adjustment of Video Playback Speeds Based on Laughter Detection</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Naoto Nishida, Hinako Nozaki, Buntarou Shizuki | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Among various methods to learn a second language (L2), such as listening and shadowing, Extensive Viewing involves learning L2 by watching many videos. However, it is difficult for many L2 learners to smoothly and effortlessly comprehend video contents made for native speakers at the original speed.</span>
            
            <span class="abstract-full" style="display: none;">Among various methods to learn a second language (L2), such as listening and shadowing, Extensive Viewing involves learning L2 by watching many videos. However, it is difficult for many L2 learners to smoothly and effortlessly comprehend video contents made for native speakers at the original speed. Therefore, we developed a language learning assistance system that automatically adjusts the playback speed according to the learner's comprehension. Our system judges that learners understand the contents if they laugh at the punchlines of comedy dramas, and vice versa. Experimental results show that this system supports learners with relatively low L2 ability (under 700 in TOEIC Score in the experimental condition) to understand video contents. Our system can widen learners' possible options of native speakers' videos as Extensive Viewing material.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6574
            </span>
            <a href="https://arxiv.org/abs/2407.06165" target="_blank" rel="noopener noreferrer">Tumor likelihood estimation on MRI prostate data by utilizing k-Space information</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: M. Rempe, F. H\"orst, C. Seibold, B. Hadaschik, M. Schlimbach, J. Egger, K. Kr\"oninger, F. Breuer, M. Blaimer, J. Kleesiek | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a novel preprocessing and prediction pipeline for the classification of magnetic resonance imaging (MRI) that takes advantage of the information rich complex valued k-Space. Using a publicly available MRI raw dataset with 312 subject and a total of 9508 slices, we show the advantage of ut</span>
            
            <span class="abstract-full" style="display: none;">We present a novel preprocessing and prediction pipeline for the classification of magnetic resonance imaging (MRI) that takes advantage of the information rich complex valued k-Space. Using a publicly available MRI raw dataset with 312 subject and a total of 9508 slices, we show the advantage of utilizing the k-Space for better prostate cancer likelihood estimation in comparison to just using the magnitudinal information in the image domain, with an AUROC of $86.1\%\pm1.8\%$. Additionally, by using high undersampling rates and a simple principal component analysis (PCA) for coil compression, we reduce the time needed for reconstruction by avoiding the time intensive GRAPPA reconstruction algorithm. By using digital undersampling for our experiments, we show that scanning and reconstruction time could be reduced. Even with an undersampling factor of 16, our approach achieves meaningful results, with an AUROC of $71.4\%\pm2.9\%$, using the PCA coil combination and taking into account the k-Space information. With this study, we were able to show the feasibility of preserving phase and k-Space information, with consistent results. Besides preserving valuable information for further diagnostics, this approach can work without the time intensive ADC and reconstruction calculations, greatly reducing the post processing, as well as potential scanning time, increasing patient comfort and allowing a close to real-time prediction.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.3 -->
                
            <!-- LLMs: 8.0 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6848
            </span>
            <a href="https://arxiv.org/abs/2504.08186" target="_blank" rel="noopener noreferrer">Comparative Analysis of Different Methods for Classifying Polychromatic Sketches</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Fahd Baba, Devon Mack | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Image classification is a significant challenge in computer vision, particularly in domains humans are not accustomed to. As machine learning and artificial intelligence become more prominent, it is crucial these algorithms develop a sense of sight that is on par with or exceeds human ability. For t</span>
            
            <span class="abstract-full" style="display: none;">Image classification is a significant challenge in computer vision, particularly in domains humans are not accustomed to. As machine learning and artificial intelligence become more prominent, it is crucial these algorithms develop a sense of sight that is on par with or exceeds human ability. For this reason, we have collected, cleaned, and parsed a large dataset of hand-drawn doodles and compared multiple machine learning solutions to classify these images into 170 distinct categories. The best model we found achieved a Top-1 accuracy of 47.5%, significantly surpassing human performance on the dataset, which stands at 41%.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.5 -->
                
            <!-- LLMs: 8.8 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.712
            </span>
            <a href="https://arxiv.org/abs/2502.05424" target="_blank" rel="noopener noreferrer">SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xingtong Yu, Zechuan Gong, Chang Zhou, Yuan Fang, Hui Zhang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Graphs are able to model interconnected entities in many online services, supporting a wide range of applications on the Web. This raises an important question: How can we train a graph foundational model on multiple source domains and adapt to an unseen target domain? A major obstacle is that graph</span>
            
            <span class="abstract-full" style="display: none;">Graphs are able to model interconnected entities in many online services, supporting a wide range of applications on the Web. This raises an important question: How can we train a graph foundational model on multiple source domains and adapt to an unseen target domain? A major obstacle is that graphs from different domains often exhibit divergent characteristics. Some studies leverage large language models to align multiple domains based on textual descriptions associated with the graphs, limiting their applicability to text-attributed graphs. For text-free graphs, a few recent works attempt to align different feature distributions across domains, while generally neglecting structural differences. In this work, we propose a novel Structure Alignment framework for text-free Multi-domain Graph Pre-Training and cross-domain adaptation (SAMGPT). It is designed to learn multi-domain knowledge from graphs originating in multiple source domains, which can then be adapted to address applications in an unseen target domain. Specifically, we introduce a set of structure tokens to harmonize structure-based aggregation across source domains during the pre-training phase. Next, for cross-domain adaptation, we design dual prompts, namely, holistic prompts and specific prompts, which adapt unified multi-domain structural knowledge and fine-grained, domain-specific information, respectively, to a target domain. Finally, we conduct comprehensive experiments on seven public datasets to evaluate and analyze the effectiveness of SAMGPT.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.9 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7206
            </span>
            <a href="https://arxiv.org/abs/2406.13073" target="_blank" rel="noopener noreferrer">Let the Noise Speak: Harnessing Noise for a Unified Defense Against Adversarial and Backdoor Attacks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Md Hasan Shahriar, Ning Wang, Naren Ramakrishnan, Y. Thomas Hou, Wenjing Lou | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The exponential adoption of machine learning (ML) is propelling the world into a future of distributed and intelligent automation and data-driven solutions. However, the proliferation of malicious data manipulation attacks against ML, namely adversarial and backdoor attacks, jeopardizes its reliabil</span>
            
            <span class="abstract-full" style="display: none;">The exponential adoption of machine learning (ML) is propelling the world into a future of distributed and intelligent automation and data-driven solutions. However, the proliferation of malicious data manipulation attacks against ML, namely adversarial and backdoor attacks, jeopardizes its reliability in safety-critical applications. The existing detection methods are attack-specific and built upon some strong assumptions, limiting them in diverse practical scenarios. Thus, motivated by the need for a more robust, unified, and attack-agnostic defense mechanism, we first investigate the shared traits of adversarial and backdoor attacks. Based on our observation, we propose NoiSec, a reconstruction-based intrusion detection system that brings a novel perspective by shifting focus from the reconstructed input to the reconstruction noise itself, which is the foundational root cause of such malicious data alterations. NoiSec disentangles the noise from the test input, extracts the underlying features from the noise, and leverages them to recognize systematic malicious manipulation. Our comprehensive evaluation of NoiSec demonstrates its high effectiveness across various datasets, including basic objects, natural scenes, traffic signs, medical images, spectrogram-based audio data, and wireless sensing against five state-of-the-art adversarial attacks and three backdoor attacks under challenging evaluation conditions. NoiSec demonstrates strong detection performance in both white-box and black-box adversarial attack scenarios, significantly outperforming the closest baseline models, particularly in an adaptive attack setting. We will provide the code for future baseline comparison. Our code and artifacts are publicly available at https://github.com/shahriar0651/NoiSec.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.3 -->
                
            <!-- LLMs: 7.6 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7239
            </span>
            <a href="https://arxiv.org/abs/2504.08632" target="_blank" rel="noopener noreferrer">Deep Learning Methods for Detecting Thermal Runaway Events in Battery Production Lines</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Athanasios Athanasopoulos, Mat\'u\v{s} Mihal\'ak, Marcin Pietrasik | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">One of the key safety considerations of battery manufacturing is thermal runaway, the uncontrolled increase in temperature which can lead to fires, explosions, and emissions of toxic gasses. As such, development of automated systems capable of detecting such events is of considerable importance in b</span>
            
            <span class="abstract-full" style="display: none;">One of the key safety considerations of battery manufacturing is thermal runaway, the uncontrolled increase in temperature which can lead to fires, explosions, and emissions of toxic gasses. As such, development of automated systems capable of detecting such events is of considerable importance in both academic and industrial contexts. In this work, we investigate the use of deep learning for detecting thermal runaway in the battery production line of VDL Nedcar, a Dutch automobile manufacturer. Specifically, we collect data from the production line to represent both baseline (non thermal runaway) and thermal runaway conditions. Thermal runaway was simulated through the use of external heat and smoke sources. The data consisted of both optical and thermal images which were then preprocessed and fused before serving as input to our models. In this regard, we evaluated three deep-learning models widely used in computer vision including shallow convolutional neural networks, residual neural networks, and vision transformers on two performance metrics. Furthermore, we evaluated these models using explainability methods to gain insight into their ability to capture the relevant feature information from their inputs. The obtained results indicate that the use of deep learning is a viable approach to thermal runaway detection in battery production lines.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 9.6 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7273
            </span>
            <a href="https://arxiv.org/abs/2502.09525" target="_blank" rel="noopener noreferrer">Robust Learning of Multi-index Models via Iterative Subspace Approximation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, Nikos Zarifis | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We study the task of learning Multi-Index Models (MIMs) with label noise under the Gaussian distribution. A $K$-MIM is any function $f$ that only depends on a $K$-dimensional subspace. We focus on well-behaved MIMs with finite ranges that satisfy certain regularity properties. Our main contribution </span>
            
            <span class="abstract-full" style="display: none;">We study the task of learning Multi-Index Models (MIMs) with label noise under the Gaussian distribution. A $K$-MIM is any function $f$ that only depends on a $K$-dimensional subspace. We focus on well-behaved MIMs with finite ranges that satisfy certain regularity properties. Our main contribution is a general robust learner that is qualitatively optimal in the Statistical Query (SQ) model. Our algorithm iteratively constructs better approximations to the defining subspace by computing low-degree moments conditional on the projection to the subspace computed thus far, and adding directions with relatively large empirical moments. This procedure efficiently finds a subspace $V$ so that $f(\mathbf{x})$ is close to a function of the projection of $\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional moments do not help, we prove an SQ lower bound suggesting that no efficient learner exists. As applications, we provide faster robust learners for the following concept classes:</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7391
            </span>
            <a href="https://arxiv.org/abs/2504.10351" target="_blank" rel="noopener noreferrer">Multimodal Representation Learning Techniques for Comprehensive Facial State Analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kaiwen Zheng, Xuri Ge, Junchen Fu, Jun Peng, Joemon M. Jose | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Multimodal foundation models have significantly improved feature representation by integrating information from multiple modalities, making them highly suitable for a broader set of applications. However, the exploration of multimodal facial representation for understanding perception has been limit</span>
            
            <span class="abstract-full" style="display: none;">Multimodal foundation models have significantly improved feature representation by integrating information from multiple modalities, making them highly suitable for a broader set of applications. However, the exploration of multimodal facial representation for understanding perception has been limited. Understanding and analyzing facial states, such as Action Units (AUs) and emotions, require a comprehensive and robust framework that bridges visual and linguistic modalities. In this paper, we present a comprehensive pipeline for multimodal facial state analysis. First, we compile a new Multimodal Face Dataset (MFA) by generating detailed multilevel language descriptions of face, incorporating Action Unit (AU) and emotion descriptions, by leveraging GPT-4o. Second, we introduce a novel Multilevel Multimodal Face Foundation model (MF^2) tailored for Action Unit (AU) and emotion recognition. Our model incorporates comprehensive visual feature modeling at both local and global levels of face image, enhancing its ability to represent detailed facial appearances. This design aligns visual representations with structured AU and emotion descriptions, ensuring effective cross-modal integration. Third, we develop a Decoupled Fine-Tuning Network (DFN) that efficiently adapts MF^2 across various tasks and datasets. This approach not only reduces computational overhead but also broadens the applicability of the foundation model to diverse scenarios. Experimentation show superior performance for AU and emotion detection tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.9 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7453
            </span>
            <a href="https://arxiv.org/abs/2504.09984" target="_blank" rel="noopener noreferrer">On Precomputation and Caching in Information Retrieval Experiments with Pipeline Architectures</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sean MacAvaney, Craig Macdonald | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Modern information retrieval systems often rely on multiple components executed in a pipeline. In a research setting, this can lead to substantial redundant computations (e.g., retrieving the same query multiple times for evaluating different downstream rerankers). To overcome this, researchers take</span>
            
            <span class="abstract-full" style="display: none;">Modern information retrieval systems often rely on multiple components executed in a pipeline. In a research setting, this can lead to substantial redundant computations (e.g., retrieving the same query multiple times for evaluating different downstream rerankers). To overcome this, researchers take cached "result" files as inputs, which represent the output of another pipeline. However, these result files can be brittle and can cause a disconnect between the conceptual design of the pipeline and its logical implementation. To overcome both the redundancy problem (when executing complete pipelines) and the disconnect problem (when relying on intermediate result files), we describe our recent efforts to improve the caching capabilities in the open-source PyTerrier IR platform. We focus on two main directions: (1) automatic implicit caching of common pipeline prefixes when comparing systems and (2) explicit caching of operations through a new extension package, pyterrier-caching. These approaches allow for the best of both worlds: pipelines can be fully expressed end-to-end, while also avoiding redundant computations between pipelines.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.0 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7521
            </span>
            <a href="https://arxiv.org/abs/2504.09851" target="_blank" rel="noopener noreferrer">Carbon-Efficient 3D DNN Acceleration: Optimizing Performance and Sustainability</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Aikaterini Maria Panteleaki, Konstantinos Balaskas, Georgios Zervakis, Hussam Amrouch, Iraklis Anagnostopoulos | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As Deep Neural Networks (DNNs) continue to drive advancements in artificial intelligence, the design of hardware accelerators faces growing concerns over embodied carbon footprint due to complex fabrication processes. 3D integration improves performance but introduces sustainability challenges, maki</span>
            
            <span class="abstract-full" style="display: none;">As Deep Neural Networks (DNNs) continue to drive advancements in artificial intelligence, the design of hardware accelerators faces growing concerns over embodied carbon footprint due to complex fabrication processes. 3D integration improves performance but introduces sustainability challenges, making carbon-aware optimization essential. In this work, we propose a carbon-efficient design methodology for 3D DNN accelerators, leveraging approximate computing and genetic algorithm-based design space exploration to optimize Carbon Delay Product (CDP). By integrating area-efficient approximate multipliers into Multiply-Accumulate (MAC) units, our approach effectively reduces silicon area and fabrication overhead while maintaining high computational accuracy. Experimental evaluations across three technology nodes (45nm, 14nm, and 7nm) show that our method reduces embodied carbon by up to 30% with negligible accuracy drop.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.8 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7527
            </span>
            <a href="https://arxiv.org/abs/2504.08802" target="_blank" rel="noopener noreferrer">InfoGain Wavelets: Furthering the Design of Diffusion Wavelets for Graph-Structured Data</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: David R. Johnson, Smita Krishnaswamy, Michael Perlmutter | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Diffusion wavelets extract information from graph signals at different scales of resolution by utilizing graph diffusion operators raised to various powers, known as diffusion scales. Traditionally, the diffusion scales are chosen to be dyadic integers, $\mathbf{2^j}$. Here, we propose a novel, unsu</span>
            
            <span class="abstract-full" style="display: none;">Diffusion wavelets extract information from graph signals at different scales of resolution by utilizing graph diffusion operators raised to various powers, known as diffusion scales. Traditionally, the diffusion scales are chosen to be dyadic integers, $\mathbf{2^j}$. Here, we propose a novel, unsupervised method for selecting the diffusion scales based on ideas from information theory. We then show that our method can be incorporated into wavelet-based GNNs via graph classification experiments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.4 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.756
            </span>
            <a href="https://arxiv.org/abs/2409.20052" target="_blank" rel="noopener noreferrer">Mitigating Propensity Bias of Large Language Models for Recommender Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Guixian Zhang, Guan Yuan, Debo Cheng, Lin Liu, Jiuyong Li, Shichao Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The rapid development of Large Language Models (LLMs) creates new opportunities for recommender systems, especially by exploiting the side information (e.g., descriptions and analyses of items) generated by these models. However, aligning this side information with collaborative information from his</span>
            
            <span class="abstract-full" style="display: none;">The rapid development of Large Language Models (LLMs) creates new opportunities for recommender systems, especially by exploiting the side information (e.g., descriptions and analyses of items) generated by these models. However, aligning this side information with collaborative information from historical interactions poses significant challenges. The inherent biases within LLMs can skew recommendations, resulting in distorted and potentially unfair user experiences. On the other hand, propensity bias causes side information to be aligned in such a way that it often tends to represent all inputs in a low-dimensional subspace, leading to a phenomenon known as dimensional collapse, which severely restricts the recommender system's ability to capture user preferences and behaviours. To address these issues, we introduce a novel framework named Counterfactual LLM Recommendation (CLLMR). Specifically, we propose a spectrum-based side information encoder that implicitly embeds structural information from historical interactions into the side information representation, thereby circumventing the risk of dimension collapse. Furthermore, our CLLMR approach explores the causal relationships inherent in LLM-based recommender systems. By leveraging counterfactual inference, we counteract the biases introduced by LLMs. Extensive experiments demonstrate that our CLLMR approach consistently enhances the performance of various recommender models.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.9 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7616
            </span>
            <a href="https://arxiv.org/abs/2504.09250" target="_blank" rel="noopener noreferrer">Adiabatic Encoding of Pre-trained MPS Classifiers into Quantum Circuits</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Keisuke Murota | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Although Quantum Neural Networks (QNNs) offer powerful methods for classification tasks, the training of QNNs faces two major training obstacles: barren plateaus and local minima. A promising solution is to first train a tensor-network (TN) model classically and then embed it into a QNN.\ However, e</span>
            
            <span class="abstract-full" style="display: none;">Although Quantum Neural Networks (QNNs) offer powerful methods for classification tasks, the training of QNNs faces two major training obstacles: barren plateaus and local minima. A promising solution is to first train a tensor-network (TN) model classically and then embed it into a QNN.\ However, embedding TN-classifiers into quantum circuits generally requires postselection whose success probability may decay exponentially with the system size. We propose an \emph{adiabatic encoding} framework that encodes pre-trained MPS-classifiers into quantum MPS (qMPS) circuits with postselection, and gradually removes the postselection while retaining performance. We prove that training qMPS-classifiers from scratch on a certain artificial dataset is exponentially hard due to barren plateaus, but our adiabatic encoding circumvents this issue. Additional numerical experiments on binary MNIST also confirm its robustness.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.763
            </span>
            <a href="https://arxiv.org/abs/2504.10326" target="_blank" rel="noopener noreferrer">AlayaDB: The Data Foundation for Efficient and Effective Long-context LLM Inference</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yangshen Deng, Zhengxin You, Long Xiang, Qilong Li, Peiqi Yuan, Zhaoyang Hong, Yitao Zheng, Wanting Li, Runzhong Li, Haotian Liu, Kyriakos Mouratidis, Man Lung Yiu, Huan Li, Qiaomu Shen, Rui Mao, Bo Tang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">AlayaDB is a cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a nove</span>
            
            <span class="abstract-full" style="display: none;">AlayaDB is a cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a novel vector database system. For the Model as a Service providers (MaaS), AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when comparing with the existing alternative solutions (e.g., KV cache disaggregation, retrieval-based sparse attention). The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into a query processing procedure, and optimizes the performance via a native query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via (i) three use cases from our industry partners, and (ii) extensive experimental results on LLM inference benchmarks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.4 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.765
            </span>
            <a href="https://arxiv.org/abs/2504.08344" target="_blank" rel="noopener noreferrer">EasyGenNet: An Efficient Framework for Audio-Driven Gesture Video Generation Based on Diffusion Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Renda Li, Xiaohua Qi, Qiang Ling, Jun Yu, Ziyi Chen, Peng Chang, Mei HanJing Xiao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Audio-driven cospeech video generation typically involves two stages: speech-to-gesture and gesture-to-video. While significant advances have been made in speech-to-gesture generation, synthesizing natural expressions and gestures remains challenging in gesture-to-video systems. In order to improve </span>
            
            <span class="abstract-full" style="display: none;">Audio-driven cospeech video generation typically involves two stages: speech-to-gesture and gesture-to-video. While significant advances have been made in speech-to-gesture generation, synthesizing natural expressions and gestures remains challenging in gesture-to-video systems. In order to improve the generation effect, previous works adopted complex input and training strategies and required a large amount of data sets for pre-training, which brought inconvenience to practical applications. We propose a simple one-stage training method and a temporal inference method based on a diffusion model to synthesize realistic and continuous gesture videos without the need for additional training of temporal modules.The entire model makes use of existing pre-trained weights, and only a few thousand frames of data are needed for each character at a time to complete fine-tuning. Built upon the video generator, we introduce a new audio-to-video pipeline to synthesize co-speech videos, using 2D human skeleton as the intermediate motion representation. Our experiments show that our method outperforms existing GAN-based and diffusion-based methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.4 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7674
            </span>
            <a href="https://arxiv.org/abs/2504.10265" target="_blank" rel="noopener noreferrer">When Technologies Are Not Enough: Understanding How Domestic Workers Employ (and Avoid) Online Technologies in Their Work Practices</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mariana Fernandez-Espinosa, Mariana Gonzalez-Bejar, Jacobo Wiesner, Diego Gomez-Zara | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Although domestic work is often viewed as manual labor, it involves significant interaction with online technologies. However, the detailed exploration of how domestic workers use these technologies remains limited. This study examines the impact of online technologies on domestic workers' work prac</span>
            
            <span class="abstract-full" style="display: none;">Although domestic work is often viewed as manual labor, it involves significant interaction with online technologies. However, the detailed exploration of how domestic workers use these technologies remains limited. This study examines the impact of online technologies on domestic workers' work practices, perceptions, and relationships with customers and employers. We interviewed 30 domestic workers residing in the United States, who provided examples that highlight the insufficient transformative role of current online technologies in their work. By conducting a thematic analysis, we characterize how they approach and avoid these digital tools at different stages of their work. Through these findings, we investigate the limitations of technology and identify challenges and opportunities that could inform the design of more suitable tools to improve the conditions of this marginalized group.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.0 -->
                
            <!-- LLMs: 8.5 -->
                
            <!-- Quantum Computing: 5.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7778
            </span>
            <a href="https://arxiv.org/abs/2504.09396" target="_blank" rel="noopener noreferrer">Adaptive Insurance Reserving with CVaR-Constrained Reinforcement Learning under Macroeconomic Regimes</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Stella C. Dong, James R. Finlay | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper proposes a reinforcement learning (RL) framework for insurance reserving that integrates tail-risk sensitivity, macroeconomic regime modeling, and regulatory compliance. The reserving problem is formulated as a finite-horizon Markov Decision Process (MDP), in which reserve adjustments are</span>
            
            <span class="abstract-full" style="display: none;">This paper proposes a reinforcement learning (RL) framework for insurance reserving that integrates tail-risk sensitivity, macroeconomic regime modeling, and regulatory compliance. The reserving problem is formulated as a finite-horizon Markov Decision Process (MDP), in which reserve adjustments are optimized using Proximal Policy Optimization (PPO) subject to Conditional Value-at-Risk (CVaR) constraints. To enhance policy robustness across varying economic conditions, the agent is trained using a regime-aware curriculum that progressively increases volatility exposure.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7893
            </span>
            <a href="https://arxiv.org/abs/2504.08840" target="_blank" rel="noopener noreferrer">Adaptive Shrinkage Estimation For Personalized Deep Kernel Regression In Modeling Brain Trajectories</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vasiliki Tassopoulou, Haochang Shou, Christos Davatzikos | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Longitudinal biomedical studies monitor individuals over time to capture dynamics in brain development, disease progression, and treatment effects. However, estimating trajectories of brain biomarkers is challenging due to biological variability, inconsistencies in measurement protocols (e.g., diffe</span>
            
            <span class="abstract-full" style="display: none;">Longitudinal biomedical studies monitor individuals over time to capture dynamics in brain development, disease progression, and treatment effects. However, estimating trajectories of brain biomarkers is challenging due to biological variability, inconsistencies in measurement protocols (e.g., differences in MRI scanners), scarcity, and irregularity in longitudinal measurements. Herein, we introduce a novel personalized deep kernel regression framework for forecasting brain biomarkers, with application to regional volumetric measurements. Our approach integrates two key components: a population model that captures brain trajectories from a large and diverse cohort, and a subject-specific model that captures individual trajectories. To optimally combine these, we propose Adaptive Shrinkage Estimation, which effectively balances population and subject-specific models. We assess our model's performance through predictive accuracy metrics, uncertainty quantification, and validation against external clinical studies. Benchmarking against state-of-the-art statistical and machine learning models -- including linear mixed effects models, generalized additive models, and deep learning methods -- demonstrates the superior predictive performance of our approach. Additionally, we apply our method to predict trajectories of composite neuroimaging biomarkers, which highlights the versatility of our approach in modeling the progression of longitudinal neuroimaging biomarkers. Furthermore, validation on three external neuroimaging studies confirms the robustness of our method across different clinical contexts. We make the code available at https://github.com/vatass/AdaptiveShrinkageDKGP.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 9.8 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7928
            </span>
            <a href="https://arxiv.org/abs/2504.09509" target="_blank" rel="noopener noreferrer">Optimal sparse phase retrieval via a quasi-Bayesian approach</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: The Tien Mai | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper addresses the problem of sparse phase retrieval, a fundamental inverse problem in applied mathematics, physics, and engineering, where a signal need to be reconstructed using only the magnitude of its transformation while phase information remains inaccessible. Leveraging the inherent spa</span>
            
            <span class="abstract-full" style="display: none;">This paper addresses the problem of sparse phase retrieval, a fundamental inverse problem in applied mathematics, physics, and engineering, where a signal need to be reconstructed using only the magnitude of its transformation while phase information remains inaccessible. Leveraging the inherent sparsity of many real-world signals, we introduce a novel sparse quasi-Bayesian approach and provide the first theoretical guarantees for such an approach. Specifically, we employ a scaled Student distribution as a continuous shrinkage prior to enforce sparsity and analyze the method using the PAC-Bayesian inequality framework. Our results establish that the proposed Bayesian estimator achieves minimax-optimal convergence rates under sub-exponential noise, matching those of state-of-the-art frequentist methods. To ensure computational feasibility, we develop an efficient Langevin Monte Carlo sampling algorithm. Through numerical experiments, we demonstrate that our method performs comparably to existing frequentist techniques, highlighting its potential as a principled alternative for sparse phase retrieval in noisy settings.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.796
            </span>
            <a href="https://arxiv.org/abs/2504.08184" target="_blank" rel="noopener noreferrer">Leveraging Passive Compliance of Soft Robotics for Physical Human-Robot Collaborative Manipulation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dallin L. Cordon, Shaden Moss, Marc Killpack, John L. Salmon | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work represents an initial benchmark of a large-scale soft robot performing physical, collaborative manipulation of a long, extended object with a human partner. The robot consists of a pneumatically-actuated, three-link continuum soft manipulator mounted to an omni-directional mobile base. The</span>
            
            <span class="abstract-full" style="display: none;">This work represents an initial benchmark of a large-scale soft robot performing physical, collaborative manipulation of a long, extended object with a human partner. The robot consists of a pneumatically-actuated, three-link continuum soft manipulator mounted to an omni-directional mobile base. The system level configuration of the robot and design of the collaborative manipulation (co-manipulation) study are presented. The initial results, both quantitative and qualitative, are directly compared to previous similar human-human co-manipulation studies. These initial results show promise in the ability for large-scale soft robots to perform comparably to human partners acting as non-visual followers in a co-manipulation task. Furthermore, these results challenge traditional soft robot strength limitations and indicate potential for applications requiring strength and adaptability.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8019
            </span>
            <a href="https://arxiv.org/abs/2409.05045" target="_blank" rel="noopener noreferrer">Using Large Language Models for Template Detection from Security Event Logs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Risto Vaarandi, Hayretdin Bahsi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In modern IT systems and computer networks, real-time and offline event log analysis is a crucial part of cyber security monitoring. In particular, event log analysis techniques are essential for the timely detection of cyber attacks and for assisting security experts with the analysis of past secur</span>
            
            <span class="abstract-full" style="display: none;">In modern IT systems and computer networks, real-time and offline event log analysis is a crucial part of cyber security monitoring. In particular, event log analysis techniques are essential for the timely detection of cyber attacks and for assisting security experts with the analysis of past security incidents. The detection of line patterns or templates from unstructured textual event logs has been identified as an important task of event log analysis since detected templates represent event types in the event log and prepare the logs for downstream online or offline security monitoring tasks. During the last two decades, a number of template mining algorithms have been proposed. However, many proposed algorithms rely on traditional data mining techniques, and the usage of Large Language Models (LLMs) has received less attention so far. Also, most approaches that harness LLMs are supervised, and unsupervised LLM-based template mining remains an understudied area. The current paper addresses this research gap and investigates the application of LLMs for unsupervised detection of templates from unstructured security event logs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.6 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8055
            </span>
            <a href="https://arxiv.org/abs/2308.09549" target="_blank" rel="noopener noreferrer">Quantum and Probabilistic Computers Rigorously Powerful than Traditional Computers, and Derandomization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tianrong Lin | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we extend the techniques used in our previous work to show that there exists a probabilistic Turing machine running within time $O(n^k)$ for all $k\in\mathbb{N}_1$ accepting a language $L_d$ which is different from any language in $\mathcal{P}$, and then further to prove that $L_d\in\</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we extend the techniques used in our previous work to show that there exists a probabilistic Turing machine running within time $O(n^k)$ for all $k\in\mathbb{N}_1$ accepting a language $L_d$ which is different from any language in $\mathcal{P}$, and then further to prove that $L_d\in\mathcal{BPP}$, thus separating the complexity class $\mathcal{BPP}$ from the class $\mathcal{P}$ (i.e., $\mathcal{P}\subsetneq\mathcal{BPP}$).</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.6 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8126
            </span>
            <a href="https://arxiv.org/abs/2504.09882" target="_blank" rel="noopener noreferrer">SIO-Mapper: A Framework for Lane-Level HD Map Construction Using Satellite Images and OpenStreetMap with No On-Site Visits</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Younghun Cho, Jee-Hwan Ryu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">High-definition (HD) maps, particularly those containing lane-level information regarded as ground truth, are crucial for vehicle localization research. Traditionally, constructing HD maps requires highly accurate sensor measurements collection from the target area, followed by manual annotation to </span>
            
            <span class="abstract-full" style="display: none;">High-definition (HD) maps, particularly those containing lane-level information regarded as ground truth, are crucial for vehicle localization research. Traditionally, constructing HD maps requires highly accurate sensor measurements collection from the target area, followed by manual annotation to assign semantic information. Consequently, HD maps are limited in terms of geographic coverage. To tackle this problem, in this paper, we propose SIO-Mapper, a novel lane-level HD map construction framework that constructs city-scale maps without physical site visits by utilizing satellite images and OpenStreetmap data. One of the key contributions of SIO-Mapper is its ability to extract lane information more accurately by introducing SIO-Net, a novel deep learning network that integrates features from satellite image and OpenStreetmap using both Transformer-based and convolution-based encoders. Furthermore, to overcome challenges in merging lanes over large areas, we introduce a novel lane integration methodology that combines cluster-based and graph-based approaches. This algorithm ensures the seamless aggregation of lane segments with high accuracy and coverage, even in complex road environments. We validated SIO-Mapper on the Naver Labs Open Dataset and NuScenes dataset, demonstrating better performance in various environments including Korea, the United States, and Singapore compared to the state-of-the-art lane-level HD mapconstruction methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8176
            </span>
            <a href="https://arxiv.org/abs/2312.01508" target="_blank" rel="noopener noreferrer">CityGen: Infinite and Controllable City Layout Generation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jie Deng, Wenhao Chai, Jianshu Guo, Qixuan Huang, Junsheng Huang, Wenhao Hu, Shengyu Hao, Jenq-Neng Hwang, Gaoang Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The recent surge in interest in city layout generation underscores its significance in urban planning and smart city development. The task involves procedurally or automatically generating spatial arrangements for urban elements such as roads, buildings, water, and vegetation. Previous methods, whet</span>
            
            <span class="abstract-full" style="display: none;">The recent surge in interest in city layout generation underscores its significance in urban planning and smart city development. The task involves procedurally or automatically generating spatial arrangements for urban elements such as roads, buildings, water, and vegetation. Previous methods, whether procedural modeling or deep learning-based approaches like VAEs and GANs, rely on complex priors, expert guidance, or initial layouts, and often lack diversity and interactivity. In this paper, we present CityGen, an end-to-end framework for infinite, diverse, and controllable city layout generation. Our framework introduces an infinite expansion module to extend local layouts to city-scale layouts and a multi-scale refinement module to upsample and refine them. We also designed a user-friendly control scheme, allowing users to guide generation through simple sketching. Additionally, we convert the 2D layout to 3D by synthesizing a height field, facilitating downstream applications. Extensive experiments demonstrate CityGen's state-of-the-art performance across various metrics, making it suitable for a wide range of downstream applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.8 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Math: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8294
            </span>
            <a href="https://arxiv.org/abs/2504.08112" target="_blank" rel="noopener noreferrer">Scaling Laws of Graph Neural Networks for Atomistic Materials Modeling</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chaojian Li, Zhifan Ye, Massimiliano Lupo Pasini, Jong Youl Choi, Cheng Wan, Yingyan Celine Lin, Prasanna Balaprakash | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Atomistic materials modeling is a critical task with wide-ranging applications, from drug discovery to materials science, where accurate predictions of the target material property can lead to significant advancements in scientific discovery. Graph Neural Networks (GNNs) represent the state-of-the-a</span>
            
            <span class="abstract-full" style="display: none;">Atomistic materials modeling is a critical task with wide-ranging applications, from drug discovery to materials science, where accurate predictions of the target material property can lead to significant advancements in scientific discovery. Graph Neural Networks (GNNs) represent the state-of-the-art approach for modeling atomistic material data thanks to their capacity to capture complex relational structures. While machine learning performance has historically improved with larger models and datasets, GNNs for atomistic materials modeling remain relatively small compared to large language models (LLMs), which leverage billions of parameters and terabyte-scale datasets to achieve remarkable performance in their respective domains. To address this gap, we explore the scaling limits of GNNs for atomistic materials modeling by developing a foundational model with billions of parameters, trained on extensive datasets in terabyte-scale. Our approach incorporates techniques from LLM libraries to efficiently manage large-scale data and models, enabling both effective training and deployment of these large-scale GNN models. This work addresses three fundamental questions in scaling GNNs: the potential for scaling GNN model architectures, the effect of dataset size on model accuracy, and the applicability of LLM-inspired techniques to GNN architectures. Specifically, the outcomes of this study include (1) insights into the scaling laws for GNNs, highlighting the relationship between model size, dataset volume, and accuracy, (2) a foundational GNN model optimized for atomistic materials modeling, and (3) a GNN codebase enhanced with advanced LLM-based training techniques. Our findings lay the groundwork for large-scale GNNs with billions of parameters and terabyte-scale datasets, establishing a scalable pathway for future advancements in atomistic materials modeling.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8658
            </span>
            <a href="https://arxiv.org/abs/2412.11423" target="_blank" rel="noopener noreferrer">Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Namhyuk Ahn, KiYoon Yoo, Wonhyuk Ahn, Daesik Kim, Seung-Hun Nam | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent advancements in diffusion models revolutionize image generation but pose risks of misuse, such as replicating artworks or generating deepfakes. Existing image protection methods, though effective, struggle to balance protection efficacy, invisibility, and latency, thus limiting practical use.</span>
            
            <span class="abstract-full" style="display: none;">Recent advancements in diffusion models revolutionize image generation but pose risks of misuse, such as replicating artworks or generating deepfakes. Existing image protection methods, though effective, struggle to balance protection efficacy, invisibility, and latency, thus limiting practical use. We introduce perturbation pre-training to reduce latency and propose a mixture-of-perturbations approach that dynamically adapts to input images to minimize performance degradation. Our novel training strategy computes protection loss across multiple VAE feature spaces, while adaptive targeted protection at inference enhances robustness and invisibility. Experiments show comparable protection performance with improved invisibility and drastically reduced inference time. The code and demo are available at https://webtoon.github.io/impasto</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8682
            </span>
            <a href="https://arxiv.org/abs/2504.10232" target="_blank" rel="noopener noreferrer">Fairness and Efficiency in Two-Sided Matching Markets</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pallavi Jain, Palash Jha, Shubham Solanki | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose a new fairness notion, motivated by the practical challenge of allocating teaching assistants (TAs) to courses in a department. Each course requires a certain number of TAs and each TA has preferences over the courses they want to assist. Similarly, each course instructor has preferences </span>
            
            <span class="abstract-full" style="display: none;">We propose a new fairness notion, motivated by the practical challenge of allocating teaching assistants (TAs) to courses in a department. Each course requires a certain number of TAs and each TA has preferences over the courses they want to assist. Similarly, each course instructor has preferences over the TAs who applied for their course. We demand fairness and efficiency for both sides separately, giving rise to the following criteria: (i) every course gets the required number of TAs and the average utility of the assigned TAs meets a threshold; (ii) the allocation of courses to TAs is envy-free, where a TA envies another TA if the former prefers the latter's course and has a higher or equal grade in that course. Note that the definition of envy-freeness here differs from the one in the literature, and we call it merit-based envy-freeness.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.9 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.878
            </span>
            <a href="https://arxiv.org/abs/2503.22958" target="_blank" rel="noopener noreferrer">Late Breaking Results: Breaking Symmetry- Unconventional Placement of Analog Circuits using Multi-Level Multi-Agent Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Supriyo Maji, Linran Zhao, Souradip Poddar, David Z. Pan | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Layout-dependent effects (LDEs) significantly impact analog circuit performance. Traditionally, designers have relied on symmetric placement of circuit components to mitigate variations caused by LDEs. However, due to non-linear nature of these effects, conventional methods often fall short. We prop</span>
            
            <span class="abstract-full" style="display: none;">Layout-dependent effects (LDEs) significantly impact analog circuit performance. Traditionally, designers have relied on symmetric placement of circuit components to mitigate variations caused by LDEs. However, due to non-linear nature of these effects, conventional methods often fall short. We propose an objective-driven, multi-level, multi-agent Q-learning framework to explore unconventional design space of analog layout, opening new avenues for optimizing analog circuit performance. Our approach achieves better variation performance than the state-of-the-art layout techniques. Notably, this is the first application of multi-agent RL in analog layout automation. The proposed approach is compared with non-ML approach based on simulated annealing.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.9 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8794
            </span>
            <a href="https://arxiv.org/abs/2504.08782" target="_blank" rel="noopener noreferrer">Embedding Hidden Adversarial Capabilities in Pre-Trained Diffusion Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lucas Beerens, Desmond J. Higham | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We introduce a new attack paradigm that embeds hidden adversarial capabilities directly into diffusion models via fine-tuning, without altering their observable behavior or requiring modifications during inference. Unlike prior approaches that target specific images or adjust the generation process </span>
            
            <span class="abstract-full" style="display: none;">We introduce a new attack paradigm that embeds hidden adversarial capabilities directly into diffusion models via fine-tuning, without altering their observable behavior or requiring modifications during inference. Unlike prior approaches that target specific images or adjust the generation process to produce adversarial outputs, our method integrates adversarial functionality into the model itself. The resulting tampered model generates high-quality images indistinguishable from those of the original, yet these images cause misclassification in downstream classifiers at a high rate. The misclassification can be targeted to specific output classes. Users can employ this compromised model unaware of its embedded adversarial nature, as it functions identically to a standard diffusion model. We demonstrate the effectiveness and stealthiness of our approach, uncovering a covert attack vector that raises new security concerns. These findings expose a risk arising from the use of externally-supplied models and highlight the urgent need for robust model verification and defense mechanisms against hidden threats in generative models. The code is available at https://github.com/LucasBeerens/CRAFTed-Diffusion .</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8811
            </span>
            <a href="https://arxiv.org/abs/2504.09301" target="_blank" rel="noopener noreferrer">Continuum-Interaction-Driven Intelligence: Human-Aligned Neural Architecture via Crystallized Reasoning and Fluid Generation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pengcheng Zhou, Zhiqiang Nie, Haochen Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Current AI systems based on probabilistic neural networks, such as large language models (LLMs), have demonstrated remarkable generative capabilities yet face critical challenges including hallucination, unpredictability, and misalignment with human decision-making. These issues fundamentally stem f</span>
            
            <span class="abstract-full" style="display: none;">Current AI systems based on probabilistic neural networks, such as large language models (LLMs), have demonstrated remarkable generative capabilities yet face critical challenges including hallucination, unpredictability, and misalignment with human decision-making. These issues fundamentally stem from the over-reliance on randomized (probabilistic) neural networks-oversimplified models of biological neural networks-while neglecting the role of procedural reasoning (chain-of-thought) in trustworthy decision-making. Inspired by the human cognitive duality of fluid intelligence (flexible generation) and crystallized intelligence (structured knowledge), this study proposes a dual-channel intelligent architecture that integrates probabilistic generation (LLMs) with white-box procedural reasoning (chain-of-thought) to construct interpretable, continuously learnable, and human-aligned AI systems. Concretely, this work: (1) redefines chain-of-thought as a programmable crystallized intelligence carrier, enabling dynamic knowledge evolution and decision verification through multi-turn interaction frameworks; (2) introduces a task-driven modular network design that explicitly demarcates the functional boundaries between randomized generation and procedural control to address trustworthiness in vertical-domain applications; (3) demonstrates that multi-turn interaction is a necessary condition for intelligence emergence, with dialogue depth positively correlating with the system's human-alignment degree. This research not only establishes a new paradigm for trustworthy AI deployment but also provides theoretical foundations for next-generation human-AI collaborative systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8929
            </span>
            <a href="https://arxiv.org/abs/2504.09644" target="_blank" rel="noopener noreferrer">SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kaiyu Li, Zepeng Xin, Li Pang, Chao Pang, Yupeng Deng, Jing Yao, Guisong Xia, Deyu Meng, Zhi Wang, Xiangyong Cao | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Remote sensing has become critical for understanding environmental dynamics, urban planning, and disaster management. However, traditional remote sensing workflows often rely on explicit segmentation or detection methods, which struggle to handle complex, implicit queries that require reasoning over</span>
            
            <span class="abstract-full" style="display: none;">Remote sensing has become critical for understanding environmental dynamics, urban planning, and disaster management. However, traditional remote sensing workflows often rely on explicit segmentation or detection methods, which struggle to handle complex, implicit queries that require reasoning over spatial context, domain knowledge, and implicit user intent. Motivated by this, we introduce a new task, \ie, geospatial pixel reasoning, which allows implicit querying and reasoning and generates the mask of the target region. To advance this task, we construct and release the first large-scale benchmark dataset called EarthReason, which comprises 5,434 manually annotated image masks with over 30,000 implicit question-answer pairs. Moreover, we propose SegEarth-R1, a simple yet effective language-guided segmentation baseline that integrates a hierarchical visual encoder, a large language model (LLM) for instruction parsing, and a tailored mask generator for spatial correlation. The design of SegEarth-R1 incorporates domain-specific adaptations, including aggressive visual token compression to handle ultra-high-resolution remote sensing images, a description projection module to fuse language and multi-scale features, and a streamlined mask prediction pipeline that directly queries description embeddings. Extensive experiments demonstrate that SegEarth-R1 achieves state-of-the-art performance on both reasoning and referring segmentation tasks, significantly outperforming traditional and LLM-based segmentation methods. Our data and code will be released at https://github.com/earth-insights/SegEarth-R1.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8938
            </span>
            <a href="https://arxiv.org/abs/1901.00175" target="_blank" rel="noopener noreferrer">Online Monitoring of Metric Temporal Logic using Sequential Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dogan Ulus | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Metric Temporal Logic (MTL) is a popular formalism to specify temporal patterns with timing constraints over the behavior of cyber-physical systems with application areas ranging in property-based testing, robotics, optimization, and learning. This paper focuses on the unified construction of sequen</span>
            
            <span class="abstract-full" style="display: none;">Metric Temporal Logic (MTL) is a popular formalism to specify temporal patterns with timing constraints over the behavior of cyber-physical systems with application areas ranging in property-based testing, robotics, optimization, and learning. This paper focuses on the unified construction of sequential networks from MTL specifications over discrete and dense time behaviors to provide an efficient and scalable online monitoring framework. Our core technique, future temporal marking, utilizes interval-based symbolic representations of future discrete and dense timelines. Building upon this, we develop efficient update and output functions for sequential network nodes for timed temporal operations. Finally, we extensively test and compare our proposed technique with existing approaches and runtime verification tools. Results highlight the performance and scalability advantages of our monitoring approach and sequential networks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.9 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8977
            </span>
            <a href="https://arxiv.org/abs/2504.08914" target="_blank" rel="noopener noreferrer">Circuits and Formulas for Datalog over Semirings</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Austen Z. Fan, Paraschos Koutris, Sudeepa Roy | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we study circuits and formulas for provenance polynomials of Datalog programs. We ask the following question: given an absorptive semiring and a fact of a Datalog program, what is the optimal depth and size of a circuit/formula that computes its provenance polynomial? We focus on abso</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we study circuits and formulas for provenance polynomials of Datalog programs. We ask the following question: given an absorptive semiring and a fact of a Datalog program, what is the optimal depth and size of a circuit/formula that computes its provenance polynomial? We focus on absorptive semirings as these guarantee the existence of a polynomial-size circuit. Our main result is a dichotomy for several classes of Datalog programs on whether they admit a formula of polynomial size or not. We achieve this result by showing that for these Datalog programs the optimal circuit depth is either $\Theta(\log m)$ or $\Theta(\log^2 m)$, where $m$ is the input size. We also show that for Datalog programs with the polynomial fringe property, we can always construct low-depth circuits of size $O(\log^2 m)$. Finally, we give characterizations of when Datalog programs are bounded over more general semirings.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.7 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9
            </span>
            <a href="https://arxiv.org/abs/2504.09227" target="_blank" rel="noopener noreferrer">SceneScout: Towards AI Agent-driven Access to Street View Imagery for Blind Users</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Gaurav Jain, Leah Findlater, Cole Gleason | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">People who are blind or have low vision (BLV) may hesitate to travel independently in unfamiliar environments due to uncertainty about the physical landscape. While most tools focus on in-situ navigation, those exploring pre-travel assistance typically provide only landmarks and turn-by-turn instruc</span>
            
            <span class="abstract-full" style="display: none;">People who are blind or have low vision (BLV) may hesitate to travel independently in unfamiliar environments due to uncertainty about the physical landscape. While most tools focus on in-situ navigation, those exploring pre-travel assistance typically provide only landmarks and turn-by-turn instructions, lacking detailed visual context. Street view imagery, which contains rich visual information and has the potential to reveal numerous environmental details, remains inaccessible to BLV people. In this work, we introduce SceneScout, a multimodal large language model (MLLM)-driven AI agent that enables accessible interactions with street view imagery. SceneScout supports two modes: (1) Route Preview, enabling users to familiarize themselves with visual details along a route, and (2) Virtual Exploration, enabling free movement within street view imagery. Our user study (N=10) demonstrates that SceneScout helps BLV users uncover visual information otherwise unavailable through existing means. A technical evaluation shows that most descriptions are accurate (72%) and describe stable visual elements (95%) even in older imagery, though occasional subtle and plausible errors make them difficult to verify without sight. We discuss future opportunities and challenges of using street view imagery to enhance navigation experiences.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9113
            </span>
            <a href="https://arxiv.org/abs/2504.09000" target="_blank" rel="noopener noreferrer">CL-CoTNav: Closed-Loop Hierarchical Chain-of-Thought for Zero-Shot Object-Goal Navigation with Vision-Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yuxin Cai, Xiangkun He, Maonan Wang, Hongliang Guo, Wei-Yun Yau, Chen Lv | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Visual Object Goal Navigation (ObjectNav) requires a robot to locate a target object in an unseen environment using egocentric observations. However, decision-making policies often struggle to transfer to unseen environments and novel target objects, which is the core generalization problem. Traditi</span>
            
            <span class="abstract-full" style="display: none;">Visual Object Goal Navigation (ObjectNav) requires a robot to locate a target object in an unseen environment using egocentric observations. However, decision-making policies often struggle to transfer to unseen environments and novel target objects, which is the core generalization problem. Traditional end-to-end learning methods exacerbate this issue, as they rely on memorizing spatial patterns rather than employing structured reasoning, limiting their ability to generalize effectively. In this letter, we introduce Closed-Loop Hierarchical Chain-of-Thought Navigation (CL-CoTNav), a vision-language model (VLM)-driven ObjectNav framework that integrates structured reasoning and closed-loop feedback into navigation decision-making. To enhance generalization, we fine-tune a VLM using multi-turn question-answering (QA) data derived from human demonstration trajectories. This structured dataset enables hierarchical Chain-of-Thought (H-CoT) prompting, systematically extracting compositional knowledge to refine perception and decision-making, inspired by the human cognitive process of locating a target object through iterative reasoning steps. Additionally, we propose a Closed-Loop H-CoT mechanism that incorporates detection and reasoning confidence scores into training. This adaptive weighting strategy guides the model to prioritize high-confidence data pairs, mitigating the impact of noisy inputs and enhancing robustness against hallucinated or incorrect reasoning. Extensive experiments in the AI Habitat environment demonstrate CL-CoTNav's superior generalization to unseen scenes and novel object categories. Our method consistently outperforms state-of-the-art approaches in navigation success rate (SR) and success weighted by path length (SPL) by 22.4\%. We release our datasets, models, and supplementary videos on our project page.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9154
            </span>
            <a href="https://arxiv.org/abs/2504.09781" target="_blank" rel="noopener noreferrer">Reasoning Court: Combining Reasoning, Action, and Judgment for Multi-Hop Reasoning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jingtian Wu, Claire Cardie | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">While large language models (LLMs) have demonstrated strong capabilities in tasks like question answering and fact verification, they continue to suffer from hallucinations and reasoning errors, especially in multi-hop tasks that require integration of multiple information sources. Current methods a</span>
            
            <span class="abstract-full" style="display: none;">While large language models (LLMs) have demonstrated strong capabilities in tasks like question answering and fact verification, they continue to suffer from hallucinations and reasoning errors, especially in multi-hop tasks that require integration of multiple information sources. Current methods address these issues through retrieval-based techniques (grounding reasoning in external evidence), reasoning-based approaches (enhancing coherence via improved prompting), or hybrid strategies combining both elements. One prominent hybrid method, ReAct, has outperformed purely retrieval-based or reasoning-based approaches; however, it lacks internal verification of intermediate reasoning steps, allowing potential errors to propagate through complex reasoning tasks. In this paper, we introduce Reasoning Court (RC), a novel framework that extends iterative reasoning-and-retrieval methods, such as ReAct, with a dedicated LLM judge. Unlike ReAct, RC employs this judge to independently evaluate multiple candidate answers and their associated reasoning generated by separate LLM agents. The judge is asked to select the answer that it considers the most factually grounded and logically coherent based on the presented reasoning and evidence, or synthesizes a new answer using available evidence and its pre-trained knowledge if all candidates are inadequate, flawed, or invalid. Evaluations on multi-hop benchmarks (HotpotQA, MuSiQue) and fact-verification (FEVER) demonstrate that RC consistently outperforms state-of-the-art few-shot prompting methods without task-specific fine-tuning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.9 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9204
            </span>
            <a href="https://arxiv.org/abs/2504.09587" target="_blank" rel="noopener noreferrer">GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Haotian Xu, Yue Hu, Chen Gao, Zhengqiu Zhu, Yong Zhao, Yong Li, Quanjun Yin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic amb</span>
            
            <span class="abstract-full" style="display: none;">Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic ambiguity among objects, and lack of structured spatial reasoning. In this work, we propose GeoNav, a geospatially aware multimodal agent to enable long-range navigation. GeoNav operates in three phases-landmark navigation, target search, and precise localization-mimicking human coarse-to-fine spatial strategies. To support such reasoning, it dynamically builds two different types of spatial memory. The first is a global but schematic cognitive map, which fuses prior textual geographic knowledge and embodied visual cues into a top-down, annotated form for fast navigation to the landmark region. The second is a local but delicate scene graph representing hierarchical spatial relationships between blocks, landmarks, and objects, which is used for definite target localization. On top of this structured representation, GeoNav employs a spatially aware, multimodal chain-of-thought prompting mechanism to enable multimodal large language models with efficient and interpretable decision-making across stages. On the CityNav urban navigation benchmark, GeoNav surpasses the current state-of-the-art by up to 12.53% in success rate and significantly improves navigation efficiency, even in hard-level tasks. Ablation studies highlight the importance of each module, showcasing how geospatial representations and coarse-to-fine reasoning enhance UAV navigation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.2 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9246
            </span>
            <a href="https://arxiv.org/abs/2504.09956" target="_blank" rel="noopener noreferrer">Semantic Depth Matters: Explaining Errors of Deep Vision Networks through Perceived Class Similarities</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Katarzyna Filus, Micha{\l} Romaszewski, Mateusz \.Zarski | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Understanding deep neural network (DNN) behavior requires more than evaluating classification accuracy alone; analyzing errors and their predictability is equally crucial. Current evaluation methodologies lack transparency, particularly in explaining the underlying causes of network misclassificatio</span>
            
            <span class="abstract-full" style="display: none;">Understanding deep neural network (DNN) behavior requires more than evaluating classification accuracy alone; analyzing errors and their predictability is equally crucial. Current evaluation methodologies lack transparency, particularly in explaining the underlying causes of network misclassifications. To address this, we introduce a novel framework that investigates the relationship between the semantic hierarchy depth perceived by a network and its real-data misclassification patterns. Central to our framework is the Similarity Depth (SD) metric, which quantifies the semantic hierarchy depth perceived by a network along with a method of evaluation of how closely the network's errors align with its internally perceived similarity structure. We also propose a graph-based visualization of model semantic relationships and misperceptions. A key advantage of our approach is that leveraging class templates -- representations derived from classifier layer weights -- is applicable to already trained networks without requiring additional data or experiments. Our approach reveals that deep vision networks encode specific semantic hierarchies and that high semantic depth improves the compliance between perceived class similarities and actual errors.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9279
            </span>
            <a href="https://arxiv.org/abs/2306.00037" target="_blank" rel="noopener noreferrer">BotArtist: Generic approach for bot detection in Twitter via semi-automatic machine learning pipeline</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alexander Shevtsov, Despoina Antonakaki, Ioannis Lamprou, Polyvios Pratikakis, Sotiris Ioannidis | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Twitter, as one of the most popular social networks, provides a platform for communication and online discourse. Unfortunately, it has also become a target for bots and fake accounts, resulting in the spread of false information and manipulation. This paper introduces a semi-automatic machine learni</span>
            
            <span class="abstract-full" style="display: none;">Twitter, as one of the most popular social networks, provides a platform for communication and online discourse. Unfortunately, it has also become a target for bots and fake accounts, resulting in the spread of false information and manipulation. This paper introduces a semi-automatic machine learning pipeline (SAMLP) designed to address the challenges associated with machine learning model development. Through this pipeline, we develop a comprehensive bot detection model named BotArtist, based on user profile features. SAMLP leverages nine distinct publicly available datasets to train the BotArtist model. To assess BotArtist's performance against current state-of-the-art solutions, we evaluate 35 existing Twitter bot detection methods, each utilizing a diverse range of features. Our comparative evaluation of BotArtist and these existing methods, conducted across nine public datasets under standardized conditions, reveals that the proposed model outperforms existing solutions by almost 10% in terms of F1-score, achieving an average score of 83.19% and 68.5% over specific and general approaches, respectively. As a result of this research, we provide one of the largest labeled Twitter bot datasets. The dataset contains extracted features combined with BotArtist predictions for 10,929,533 Twitter user profiles, collected via Twitter API during the 2022 Russo-Ukrainian War over a 16-month period. This dataset was created based on [Shevtsov et al., 2022a] where the original authors share anonymized tweets discussing the Russo-Ukrainian war, totaling 127,275,386 tweets. The combination of the existing textual dataset and the provided labeled bot and human profiles will enable future development of more advanced bot detection large language models in the post-Twitter API era.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9301
            </span>
            <a href="https://arxiv.org/abs/2411.00264" target="_blank" rel="noopener noreferrer">TurtleBench: A Visual Programming Benchmark in Turtle Geometry</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sina Rismanchian, Yasaman Razeghi, Sameer Singh, Shayan Doroudi | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Humans have the ability to reason about geometric patterns in images and scenes from a young age. However, developing large multimodal models (LMMs) capable of similar reasoning remains a challenge, highlighting the need for robust evaluation methods to assess these capabilities. We introduce \Turtl</span>
            
            <span class="abstract-full" style="display: none;">Humans have the ability to reason about geometric patterns in images and scenes from a young age. However, developing large multimodal models (LMMs) capable of similar reasoning remains a challenge, highlighting the need for robust evaluation methods to assess these capabilities. We introduce \Turtle, a benchmark designed to evaluate LMMs' capacity to interpret geometric patterns -- given visual examples, textual instructions, or both -- and generate precise code outputs. Inspired by turtle geometry, a notion used to teach children foundational coding and geometric concepts, TurtleBench features tasks with patterned shapes that have underlying algorithmic logic. Our evaluation reveals that leading LMMs struggle significantly with these tasks, with GPT-4o achieving only 19\% accuracy on the simplest tasks and few-shot prompting only marginally improves their performance ($<2\%$). \Turtle highlights the gap between human and AI performance in intuitive and visual geometrical understanding, setting the stage for future research in this area. \Turtle stands as one of the few benchmarks to evaluate the integration of visual understanding and code generation capabilities in LMMs, setting the stage for future research. Code and Dataset for this paper is provided here: \href{https://github.com/sinaris76/TurtleBench}{https://github.com/sinaris76/TurtleBench}</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9331
            </span>
            <a href="https://arxiv.org/abs/2504.08211" target="_blank" rel="noopener noreferrer">LLM for Comparative Narrative Analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Leo Kampen, Carlos Rabat Villarreal, Louis Yu, Santu Karmaker, Dongji Feng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we conducted a Multi-Perspective Comparative Narrative Analysis (CNA) on three prominent LLMs: GPT-3.5, PaLM2, and Llama2. We applied identical prompts and evaluated their outputs on specific tasks, ensuring an equitable and unbiased comparison between various LLMs. Our study revealed</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we conducted a Multi-Perspective Comparative Narrative Analysis (CNA) on three prominent LLMs: GPT-3.5, PaLM2, and Llama2. We applied identical prompts and evaluated their outputs on specific tasks, ensuring an equitable and unbiased comparison between various LLMs. Our study revealed that the three LLMs generated divergent responses to the same prompt, indicating notable discrepancies in their ability to comprehend and analyze the given task. Human evaluation was used as the gold standard, evaluating four perspectives to analyze differences in LLM performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.7 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9365
            </span>
            <a href="https://arxiv.org/abs/2504.08777" target="_blank" rel="noopener noreferrer">The Lyme Disease Controversy: An AI-Driven Discourse Analysis of a Quarter Century of Academic Debate and Divides</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Teo Susnjak, Cole Palffy, Tatiana Zimina, Nazgul Altynbekova, Kunal Garg, Leona Gilbert | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The scientific discourse surrounding Chronic Lyme Disease (CLD) and Post-Treatment Lyme Disease Syndrome (PTLDS) has evolved over the past twenty-five years into a complex and polarised debate, shaped by shifting research priorities, institutional influences, and competing explanatory models. This s</span>
            
            <span class="abstract-full" style="display: none;">The scientific discourse surrounding Chronic Lyme Disease (CLD) and Post-Treatment Lyme Disease Syndrome (PTLDS) has evolved over the past twenty-five years into a complex and polarised debate, shaped by shifting research priorities, institutional influences, and competing explanatory models. This study presents the first large-scale, systematic examination of this discourse using an innovative hybrid AI-driven methodology, combining large language models with structured human validation to analyse thousands of scholarly abstracts spanning 25 years. By integrating Large Language Models (LLMs) with expert oversight, we developed a quantitative framework for tracking epistemic shifts in contested medical fields, with applications to other content analysis domains. Our analysis revealed a progressive transition from infection-based models of Lyme disease to immune-mediated explanations for persistent symptoms. This study offers new empirical insights into the structural and epistemic forces shaping Lyme disease research, providing a scalable and replicable methodology for analysing discourse, while underscoring the value of AI-assisted methodologies in social science and medical research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.8 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9442
            </span>
            <a href="https://arxiv.org/abs/2504.07996" target="_blank" rel="noopener noreferrer">Fusing Global and Local: Transformer-CNN Synergy for Next-Gen Current Estimation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Junlang Huang, Hao Chen, Li Luo, Yong Cai, Lexin Zhang, Tianhao Ma, Yitian Zhang, Zhong Guan | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper presents a hybrid model combining Transformer and CNN for predicting the current waveform in signal lines. Unlike traditional approaches such as current source models, driver linear representations, waveform functional fitting, or equivalent load capacitance methods, our model does not re</span>
            
            <span class="abstract-full" style="display: none;">This paper presents a hybrid model combining Transformer and CNN for predicting the current waveform in signal lines. Unlike traditional approaches such as current source models, driver linear representations, waveform functional fitting, or equivalent load capacitance methods, our model does not rely on fixed simplified models of standard-cell drivers or RC loads. Instead, it replaces the complex Newton iteration process used in traditional SPICE simulations, leveraging the powerful sequence modeling capabilities of the Transformer framework to directly predict current responses without iterative solving steps. The hybrid architecture effectively integrates the global feature-capturing ability of Transformers with the local feature extraction advantages of CNNs, significantly improving the accuracy of current waveform predictions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9455
            </span>
            <a href="https://arxiv.org/abs/2503.18982" target="_blank" rel="noopener noreferrer">Generative Data Imputation for Sparse Learner Performance Data Using Generative Adversarial Imputation Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Liang Zhang, Jionghao Lin, John Sabatini, Diego Zapata-Rivera, Carol Forsyth, Yang Jiang, John Hollander, Xiangen Hu, Arthur C. Graesser | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Learner performance data collected by Intelligent Tutoring Systems (ITSs), such as responses to questions, is essential for modeling and predicting learners' knowledge states. However, missing responses due to skips or incomplete attempts create data sparsity, challenging accurate assessment and per</span>
            
            <span class="abstract-full" style="display: none;">Learner performance data collected by Intelligent Tutoring Systems (ITSs), such as responses to questions, is essential for modeling and predicting learners' knowledge states. However, missing responses due to skips or incomplete attempts create data sparsity, challenging accurate assessment and personalized instruction. To address this, we propose a generative imputation approach using Generative Adversarial Imputation Networks (GAIN). Our method features a three-dimensional (3D) framework (learners, questions, and attempts), flexibly accommodating various sparsity levels. Enhanced by convolutional neural networks and optimized with a least squares loss function, the GAIN-based method aligns input and output dimensions to question-attempt matrices along the learners' dimension. Extensive experiments using datasets from AutoTutor Adult Reading Comprehension (ARC), ASSISTments, and MATHia demonstrate that our approach significantly outperforms tensor factorization and alternative GAN methods in imputation accuracy across different attempt scenarios. Bayesian Knowledge Tracing (BKT) further validates the effectiveness of the imputed data by estimating learning parameters: initial knowledge (P(L0)), learning rate (P(T)), guess rate (P(G)), and slip rate (P(S)). Results indicate the imputed data enhances model fit and closely mirrors original distributions, capturing underlying learning behaviors reliably. Kullback-Leibler (KL) divergence assessments confirm minimal divergence, showing the imputed data preserves essential learning characteristics effectively. These findings underscore GAIN's capability as a robust imputation tool in ITSs, alleviating data sparsity and supporting adaptive, individualized instruction, ultimately leading to more precise and responsive learner assessments and improved educational outcomes.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9494
            </span>
            <a href="https://arxiv.org/abs/2504.08000" target="_blank" rel="noopener noreferrer">Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiahua Lan, Sen Zhang, Haixia Pan, Ruijun Liu, Li Shen, Dacheng Tao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In contrast to the human ability to continuously acquire knowledge, agents struggle with the stability-plasticity dilemma in deep reinforcement learning (DRL), which refers to the trade-off between retaining existing skills (stability) and learning new knowledge (plasticity). Current methods focus o</span>
            
            <span class="abstract-full" style="display: none;">In contrast to the human ability to continuously acquire knowledge, agents struggle with the stability-plasticity dilemma in deep reinforcement learning (DRL), which refers to the trade-off between retaining existing skills (stability) and learning new knowledge (plasticity). Current methods focus on balancing these two aspects at the network level, lacking sufficient differentiation and fine-grained control of individual neurons. To overcome this limitation, we propose Neuron-level Balance between Stability and Plasticity (NBSP) method, by taking inspiration from the observation that specific neurons are strongly relevant to task-relevant skills. Specifically, NBSP first (1) defines and identifies RL skill neurons that are crucial for knowledge retention through a goal-oriented method, and then (2) introduces a framework by employing gradient masking and experience replay techniques targeting these neurons to preserve the encoded existing skills while enabling adaptation to new tasks. Numerous experimental results on the Meta-World and Atari benchmarks demonstrate that NBSP significantly outperforms existing approaches in balancing stability and plasticity.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9517
            </span>
            <a href="https://arxiv.org/abs/2504.09828" target="_blank" rel="noopener noreferrer">FATE: A Prompt-Tuning-Based Semi-Supervised Learning Framework for Extremely Limited Labeled Data</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hezhao Liu, Yang Lu, Mengke Li, Yiqun Zhang, Shreyank N Gowda, Chen Gong, Hanzi Wang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Semi-supervised learning (SSL) has achieved significant progress by leveraging both labeled data and unlabeled data. Existing SSL methods overlook a common real-world scenario when labeled data is extremely scarce, potentially as limited as a single labeled sample in the dataset. General SSL approac</span>
            
            <span class="abstract-full" style="display: none;">Semi-supervised learning (SSL) has achieved significant progress by leveraging both labeled data and unlabeled data. Existing SSL methods overlook a common real-world scenario when labeled data is extremely scarce, potentially as limited as a single labeled sample in the dataset. General SSL approaches struggle to train effectively from scratch under such constraints, while methods utilizing pre-trained models often fail to find an optimal balance between leveraging limited labeled data and abundant unlabeled data. To address this challenge, we propose Firstly Adapt, Then catEgorize (FATE), a novel SSL framework tailored for scenarios with extremely limited labeled data. At its core, the two-stage prompt tuning paradigm FATE exploits unlabeled data to compensate for scarce supervision signals, then transfers to downstream tasks. Concretely, FATE first adapts a pre-trained model to the feature distribution of downstream data using volumes of unlabeled samples in an unsupervised manner. It then applies an SSL method specifically designed for pre-trained models to complete the final classification task. FATE is designed to be compatible with both vision and vision-language pre-trained models. Extensive experiments demonstrate that FATE effectively mitigates challenges arising from the scarcity of labeled samples in SSL, achieving an average performance improvement of 33.74% across seven benchmarks compared to state-of-the-art SSL methods. Code is available at https://anonymous.4open.science/r/Semi-supervised-learning-BA72.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.9 -->
                
            <!-- Medicine: 8.8 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9574
            </span>
            <a href="https://arxiv.org/abs/2504.09149" target="_blank" rel="noopener noreferrer">MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Changhao Li, Yu Xin, Xiaowei Zhou, Ariel Shamir, Hao Zhang, Ligang Liu, Ruizhen Hu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view and parametrized representation of 3D shapes. Inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3D shapes, MASH represents a 3D shape as a collection of observable</span>
            
            <span class="abstract-full" style="display: none;">We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view and parametrized representation of 3D shapes. Inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3D shapes, MASH represents a 3D shape as a collection of observable local surface patches, each defined by a spherical distance function emanating from an anchor point. We further leverage the compactness of spherical harmonics to encode the MASH functions, combined with a generalized view cone with a parameterized base that masks the spatial extent of the spherical function to attain locality. We develop a differentiable optimization algorithm capable of converting any point cloud into a MASH representation accurately approximating ground-truth surfaces with arbitrary geometry and topology. Extensive experiments demonstrate that MASH is versatile for multiple applications including surface reconstruction, shape generation, completion, and blending, achieving superior performance thanks to its unique representation encompassing both implicit and explicit features.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.6 -->
                
            <!-- LLMs: 7.9 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9579
            </span>
            <a href="https://arxiv.org/abs/2410.19631" target="_blank" rel="noopener noreferrer">Efficient Biological Data Acquisition through Inference Set Design</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ihor Neporozhnii, Julien Roy, Emmanuel Bengio, Jason Hartford | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so one might hope to reduce their cost by only experimenting on a subset of the compounds, and predicting the outcomes of the</span>
            
            <span class="abstract-full" style="display: none;">In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so one might hope to reduce their cost by only experimenting on a subset of the compounds, and predicting the outcomes of the remaining experiments. In this work, we model this scenario as a sequential subset selection problem: we aim to select the smallest set of candidates in order to achieve some desired level of accuracy for the system as a whole. Our key observation is that, if there is heterogeneity in the difficulty of the prediction problem across the input space, selectively obtaining the labels for the hardest examples in the acquisition pool will leave only the relatively easy examples to remain in the inference set, leading to better overall system performance. We call this mechanism inference set design, and propose the use of a confidence-based active learning solution to prune out these challenging examples. Our algorithm includes an explicit stopping criterion that interrupts the acquisition loop when it is sufficiently confident that the system has reached the target performance. Our empirical studies on image and molecular datasets, as well as a real-world large-scale biological assay, show that active learning for inference set design leads to significant reduction in experimental cost while retaining high system performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.9 -->
                
            <!-- LLMs: 5.9 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.6 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9649
            </span>
            <a href="https://arxiv.org/abs/2504.10105" target="_blank" rel="noopener noreferrer">Global and Local Mamba Network for Multi-Modality Medical Image Super-Resolution</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zexin Ji, Beiji Zou, Xiaoyan Kui, Sebastien Thureau, Su Ruan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Convolutional neural networks and Transformer have made significant progresses in multi-modality medical image super-resolution. However, these methods either have a fixed receptive field for local learning or significant computational burdens for global learning, limiting the super-resolution perfo</span>
            
            <span class="abstract-full" style="display: none;">Convolutional neural networks and Transformer have made significant progresses in multi-modality medical image super-resolution. However, these methods either have a fixed receptive field for local learning or significant computational burdens for global learning, limiting the super-resolution performance. To solve this problem, State Space Models, notably Mamba, is introduced to efficiently model long-range dependencies in images with linear computational complexity. Relying on the Mamba and the fact that low-resolution images rely on global information to compensate for missing details, while high-resolution reference images need to provide more local details for accurate super-resolution, we propose a global and local Mamba network (GLMamba) for multi-modality medical image super-resolution. To be specific, our GLMamba is a two-branch network equipped with a global Mamba branch and a local Mamba branch. The global Mamba branch captures long-range relationships in low-resolution inputs, and the local Mamba branch focuses more on short-range details in high-resolution reference images. We also use the deform block to adaptively extract features of both branches to enhance the representation ability. A modulator is designed to further enhance deformable features in both global and local Mamba blocks. To fully integrate the reference image for low-resolution image super-resolution, we further develop a multi-modality feature fusion block to adaptively fuse features by considering similarities, differences, and complementary aspects between modalities. In addition, a contrastive edge loss (CELoss) is developed for sufficient enhancement of edge textures and contrast in medical images.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.7 -->
                
            <!-- Medicine: 9.4 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9698
            </span>
            <a href="https://arxiv.org/abs/2504.08336" target="_blank" rel="noopener noreferrer">Research as Resistance: Recognizing and Reconsidering HCI's Role in Technology Hype Cycles</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zefan Sramek, Koji Yatani | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The history of information technology development has been characterized by consecutive waves of boom and bust, as new technologies come to market, fuel surges of investment, and then stabilize towards maturity. However, in recent decades, the acceleration of such technology hype cycles has resulted</span>
            
            <span class="abstract-full" style="display: none;">The history of information technology development has been characterized by consecutive waves of boom and bust, as new technologies come to market, fuel surges of investment, and then stabilize towards maturity. However, in recent decades, the acceleration of such technology hype cycles has resulted in the prioritization of massive capital generation at the expense of longterm sustainability, resulting in a cascade of negative social, political, and environmental consequences. Despite the negative impacts of this pattern, academic research, and in particular HCI research, is not immune from such hype cycles, often contributing substantial amounts of literature to the discourse surrounding a wave of hype. In this paper, we discuss the relationship between technology and capital, offer a critique of the technology hype cycle using generative AI as an example, and finally suggest an approach and a set of strategies for how we can counteract such cycles through research as resistance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.4 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9789
            </span>
            <a href="https://arxiv.org/abs/2502.00757" target="_blank" rel="noopener noreferrer">AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds via Self-Improvement</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: J Rosser, Jakob Nicolaus Foerster | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We ev</span>
            
            <span class="abstract-full" style="display: none;">Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We evaluate discovered scaffolds on widely recognized reasoning, mathematics, and safety benchmarks and compare them with popular baselines. In 'blue' mode, we see a 79.4% average uplift in safety benchmark performance while maintaining or improving capability scores. In 'red' mode, we find adversarially weak scaffolds emerging concurrently with capability optimization. Our work demonstrates the risks of multi-agent scaffolding and provides a framework for mitigating them. Code is available at https://github.com/J-Rosser-UK/AgentBreeder.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.3 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9895
            </span>
            <a href="https://arxiv.org/abs/2504.09601" target="_blank" rel="noopener noreferrer">Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework to Prompt SAM for Generalizable Medical Segmentation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jia Wei, Xiaoqi Zhao, Jonghye Woo, Jinsong Ouyang, Georges El Fakhri, Qingyu Chen, Xiaofeng Liu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Single domain generalization (SDG) has recently attracted growing attention in medical image segmentation. One promising strategy for SDG is to leverage consistent semantic shape priors across different imaging protocols, scanner vendors, and clinical sites. However, existing dictionary learning met</span>
            
            <span class="abstract-full" style="display: none;">Single domain generalization (SDG) has recently attracted growing attention in medical image segmentation. One promising strategy for SDG is to leverage consistent semantic shape priors across different imaging protocols, scanner vendors, and clinical sites. However, existing dictionary learning methods that encode shape priors often suffer from limited representational power with a small set of offline computed shape elements, or overfitting when the dictionary size grows. Moreover, they are not readily compatible with large foundation models such as the Segment Anything Model (SAM). In this paper, we propose a novel Mixture-of-Shape-Experts (MoSE) framework that seamlessly integrates the idea of mixture-of-experts (MoE) training into dictionary learning to efficiently capture diverse and robust shape priors. Our method conceptualizes each dictionary atom as a shape expert, which specializes in encoding distinct semantic shape information. A gating network dynamically fuses these shape experts into a robust shape map, with sparse activation guided by SAM encoding to prevent overfitting. We further provide this shape map as a prompt to SAM, utilizing the powerful generalization capability of SAM through bidirectional integration. All modules, including the shape dictionary, are trained in an end-to-end manner. Extensive experiments on multiple public datasets demonstrate its effectiveness.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.6 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9922
            </span>
            <a href="https://arxiv.org/abs/2504.09169" target="_blank" rel="noopener noreferrer">UX Remix: Improving Measurement Item Design Process Using Large Language Models and Prior Literature</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hyeonggeun Yun, Jinkyu Jang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Researchers often struggle to develop measurement items and lack a standardized process. To support the design process, we present UX Remix, a system to help researchers develop constructs and measurement items using large language models (LLMs). UX Remix leverages a database of constructs and assoc</span>
            
            <span class="abstract-full" style="display: none;">Researchers often struggle to develop measurement items and lack a standardized process. To support the design process, we present UX Remix, a system to help researchers develop constructs and measurement items using large language models (LLMs). UX Remix leverages a database of constructs and associated measurement items from previous papers. Based on the data, UX Remix recommends constructs relevant to the research context. The researchers then select appropriate constructs based on the recommendations. Afterward, selected constructs are used to generate a custom construct, and UX Remix recommends measurement items. UX Remix streamlines the process of selecting constructs, developing measurement items, and adapting them to research contexts, addressing challenges in the selection and reuse of measurement items. This paper describes the implementation of the system, the potential benefits, and future directions to improve the rigor and efficiency of measurement design in human-computer interaction (HCI) research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0089
            </span>
            <a href="https://arxiv.org/abs/2504.08054" target="_blank" rel="noopener noreferrer">Multi-Task Learning with Multi-Annotation Triplet Loss for Improved Object Detection</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Meilun Zhou, Aditya Dutt, Alina Zare | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Triplet loss traditionally relies only on class labels and does not use all available information in multi-task scenarios where multiple types of annotations are available. This paper introduces a Multi-Annotation Triplet Loss (MATL) framework that extends triplet loss by incorporating additional an</span>
            
            <span class="abstract-full" style="display: none;">Triplet loss traditionally relies only on class labels and does not use all available information in multi-task scenarios where multiple types of annotations are available. This paper introduces a Multi-Annotation Triplet Loss (MATL) framework that extends triplet loss by incorporating additional annotations, such as bounding box information, alongside class labels in the loss formulation. By using these complementary annotations, MATL improves multi-task learning for tasks requiring both classification and localization. Experiments on an aerial wildlife imagery dataset demonstrate that MATL outperforms conventional triplet loss in both classification and localization. These findings highlight the benefit of using all available annotations for triplet loss in multi-task learning frameworks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.8 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0127
            </span>
            <a href="https://arxiv.org/abs/2504.10147" target="_blank" rel="noopener noreferrer">A Survey of Personalization: From RAG to Agent</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xiaopeng Li, Pengyue Jia, Derong Xu, Yi Wen, Yingyi Zhang, Wenlin Zhang, Wanyu Wang, Yichao Wang, Zhaocheng Du, Xiangyang Li, Yong Liu, Huifeng Guo, Ruiming Tang, Xiangyu Zhao | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Personalization has become an essential capability in modern AI systems, enabling customized interactions that align with individual user preferences, contexts, and goals. Recent research has increasingly concentrated on Retrieval-Augmented Generation (RAG) frameworks and their evolution into more a</span>
            
            <span class="abstract-full" style="display: none;">Personalization has become an essential capability in modern AI systems, enabling customized interactions that align with individual user preferences, contexts, and goals. Recent research has increasingly concentrated on Retrieval-Augmented Generation (RAG) frameworks and their evolution into more advanced agent-based architectures within personalized settings to enhance user satisfaction. Building on this foundation, this survey systematically examines personalization across the three core stages of RAG: pre-retrieval, retrieval, and generation. Beyond RAG, we further extend its capabilities into the realm of Personalized LLM-based Agents, which enhance traditional RAG systems with agentic functionalities, including user understanding, personalized planning and execution, and dynamic generation. For both personalization in RAG and agent-based personalization, we provide formal definitions, conduct a comprehensive review of recent literature, and summarize key datasets and evaluation metrics. Additionally, we discuss fundamental challenges, limitations, and promising research directions in this evolving field. Relevant papers and resources are continuously updated at https://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0143
            </span>
            <a href="https://arxiv.org/abs/2504.09289" target="_blank" rel="noopener noreferrer">Sparse Hybrid Linear-Morphological Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Konstantinos Fotopoulos, Christos Garoufis, Petros Maragos | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We investigate hybrid linear-morphological networks. Recent studies highlight the inherent affinity of morphological layers to pruning, but also their difficulty in training. We propose a hybrid network structure, wherein morphological layers are inserted between the linear layers of the network, in</span>
            
            <span class="abstract-full" style="display: none;">We investigate hybrid linear-morphological networks. Recent studies highlight the inherent affinity of morphological layers to pruning, but also their difficulty in training. We propose a hybrid network structure, wherein morphological layers are inserted between the linear layers of the network, in place of activation functions. We experiment with the following morphological layers: 1) maxout pooling layers (as a special case of a morphological layer), 2) fully connected dense morphological layers, and 3) a novel, sparsely initialized variant of (2). We conduct experiments on the Magna-Tag-A-Tune (music auto-tagging) and CIFAR-10 (image classification) datasets, replacing the linear classification heads of state-of-the-art convolutional network architectures with our proposed network structure for the various morphological layers. We demonstrate that these networks induce sparsity to their linear layers, making them more prunable under L1 unstructured pruning. We also show that on MTAT our proposed sparsely initialized layer achieves slightly better performance than ReLU, maxout, and densely initialized max-plus layers, and exhibits faster initial convergence.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0206
            </span>
            <a href="https://arxiv.org/abs/2412.06931" target="_blank" rel="noopener noreferrer">Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based Planning and Manoeuvrability-Driven Controls</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hoi-Yin Lee, Peng Zhou, Anqing Duan, Wanyu Ma, Chenguang Yang, David Navarro-Alarcon | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Being able to use tools is a widely recognised indicator of intelligence across species. Humans, for instance, have demonstrated mastery of tool use for over two million years. The ability to use tools is invaluable as it extends an organism's reach and enhances its capacity to interact with objects</span>
            
            <span class="abstract-full" style="display: none;">Being able to use tools is a widely recognised indicator of intelligence across species. Humans, for instance, have demonstrated mastery of tool use for over two million years. The ability to use tools is invaluable as it extends an organism's reach and enhances its capacity to interact with objects and the environment. Being able to understand the geometric-mechanical relations between the tools-objects-environments allows certain species (e.g., apes and crows) to reach food in narrow constrained spaces. The same principles of physical augmentation and its associated non-prehensile manipulation capabilities also apply to robotic systems. For example, by instrumenting them with different types of end-effectors, robots can (in principle) dexterously interact (e.g., push and flip) with objects of various shapes and masses akin to its biological counterpart. However, developing this type of manipulation skill is still an open research problem. Furthermore, the complexity of planning tool-object manipulation tasks, particularly in coordinating the actions of dual-arm robots, presents significant challenges. To address these complexities, we propose integrating Large Language Models (LLMs) to assist in planning and executing these intricate manipulations, thereby enhancing the robot's ability to perform in diverse scenarios.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.8 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0214
            </span>
            <a href="https://arxiv.org/abs/2504.08411" target="_blank" rel="noopener noreferrer">A Knowledge-guided Adversarial Defense for Resisting Malicious Visual Manipulation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dawei Zhou, Suzhi Gang, Decheng Liu, Tongliang Liu, Nannan Wang, Xinbo Gao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Malicious applications of visual manipulation have raised serious threats to the security and reputation of users in many fields. To alleviate these issues, adversarial noise-based defenses have been enthusiastically studied in recent years. However, ``data-only" methods tend to distort fake samples</span>
            
            <span class="abstract-full" style="display: none;">Malicious applications of visual manipulation have raised serious threats to the security and reputation of users in many fields. To alleviate these issues, adversarial noise-based defenses have been enthusiastically studied in recent years. However, ``data-only" methods tend to distort fake samples in the low-level feature space rather than the high-level semantic space, leading to limitations in resisting malicious manipulation. Frontier research has shown that integrating knowledge in deep learning can produce reliable and generalizable solutions. Inspired by these, we propose a knowledge-guided adversarial defense (KGAD) to actively force malicious manipulation models to output semantically confusing samples. Specifically, in the process of generating adversarial noise, we focus on constructing significant semantic confusions at the domain-specific knowledge level, and exploit a metric closely related to visual perception to replace the general pixel-wise metrics. The generated adversarial noise can actively interfere with the malicious manipulation model by triggering knowledge-guided and perception-related disruptions in the fake samples. To validate the effectiveness of the proposed method, we conduct qualitative and quantitative experiments on human perception and visual quality assessment. The results on two different tasks both show that our defense provides better protection compared to state-of-the-art methods and achieves great generalizability.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0325
            </span>
            <a href="https://arxiv.org/abs/2504.10360" target="_blank" rel="noopener noreferrer">Reactive power flow optimization in AC drive systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sanjay Chandrasekaran, Catalin Arghir, Pieder Joerg, Florian Doerfler, Silvia Mastellone | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper explores a limit avoidance approach in the case of input (modulation) and output (current) constraints with the aim of enhancing system availability of AC drives. Drawing on the observation that, in a certain range of reactive power, there exists a trade-off between current and modulation</span>
            
            <span class="abstract-full" style="display: none;">This paper explores a limit avoidance approach in the case of input (modulation) and output (current) constraints with the aim of enhancing system availability of AC drives. Drawing on the observation that, in a certain range of reactive power, there exists a trade-off between current and modulation magnitude, we exploit this freedom and define a constrained optimization problem. We propose two approaches, one in the form of an activation-function which drives the reactive power set-point towards safety, and an approach which uses online feedback optimization to set the reactive power dynamically. Both methods compromise reactive power tracking accuracy for increased system robustness. Through a high fidelity simulation, we compare the benefits of the two methods, highlighting their effectiveness in industrial applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Robotics: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0398
            </span>
            <a href="https://arxiv.org/abs/2504.10102" target="_blank" rel="noopener noreferrer">A Human-Sensitive Controller: Adapting to Human Ergonomics and Physical Constraints via Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vitor Martins (Center for MicroElectroMechanical Systems), Sara M. Cerqueira (Center for MicroElectroMechanical Systems), Mercedes Balcells (IMES, Massachusetts Institute of Technology, Cambridge, MA, USA, GEVAB, IQS School of Engineering, Barcelona, Spain), Elazer R Edelman (IMES, Massachusetts Institute of Technology, Cambridge, MA, USA, Brigham and Women's Hospital, Harvard Medical School Boston, MA, USA), Cristina P. Santos (Center for MicroElectroMechanical Systems, LABBELS-Associate Laboratory, University of Minho, Guimar\~aes, Portugal) | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Work-Related Musculoskeletal Disorders continue to be a major challenge in industrial environments, leading to reduced workforce participation, increased healthcare costs, and long-term disability. This study introduces a human-sensitive robotic system aimed at reintegrating individuals with a histo</span>
            
            <span class="abstract-full" style="display: none;">Work-Related Musculoskeletal Disorders continue to be a major challenge in industrial environments, leading to reduced workforce participation, increased healthcare costs, and long-term disability. This study introduces a human-sensitive robotic system aimed at reintegrating individuals with a history of musculoskeletal disorders into standard job roles, while simultaneously optimizing ergonomic conditions for the broader workforce. This research leverages reinforcement learning to develop a human-aware control strategy for collaborative robots, focusing on optimizing ergonomic conditions and preventing pain during task execution. Two RL approaches, Q-Learning and Deep Q-Network (DQN), were implemented and tested to personalize control strategies based on individual user characteristics. Although experimental results revealed a simulation-to-real gap, a fine-tuning phase successfully adapted the policies to real-world conditions. DQN outperformed Q-Learning by completing tasks faster while maintaining zero pain risk and safe ergonomic levels. The structured testing protocol confirmed the system's adaptability to diverse human anthropometries, underscoring the potential of RL-driven cobots to enable safer, more inclusive workplaces.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.5 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0401
            </span>
            <a href="https://arxiv.org/abs/2504.10409" target="_blank" rel="noopener noreferrer">GPS: Distilling Compact Memories via Grid-based Patch Sampling for Efficient Online Class-Incremental Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mingchuan Ma, Yuhao Zhou, Jindi Lv, Yuxin Tian, Dan Si, Shujian Li, Qing Ye, Jiancheng Lv | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Online class-incremental learning aims to enable models to continuously adapt to new classes with limited access to past data, while mitigating catastrophic forgetting. Replay-based methods address this by maintaining a small memory buffer of previous samples, achieving competitive performance. For </span>
            
            <span class="abstract-full" style="display: none;">Online class-incremental learning aims to enable models to continuously adapt to new classes with limited access to past data, while mitigating catastrophic forgetting. Replay-based methods address this by maintaining a small memory buffer of previous samples, achieving competitive performance. For effective replay under constrained storage, recent approaches leverage distilled data to enhance the informativeness of memory. However, such approaches often involve significant computational overhead due to the use of bi-level optimization. Motivated by these limitations, we introduce Grid-based Patch Sampling (GPS), a lightweight and effective strategy for distilling informative memory samples without relying on a trainable model. GPS generates informative samples by sampling a subset of pixels from the original image, yielding compact low-resolution representations that preserve both semantic content and structural information. During replay, these representations are reassembled to support training and evaluation. Experiments on extensive benchmarks demonstrate that GRS can be seamlessly integrated into existing replay frameworks, leading to 3%-4% improvements in average end accuracy under memory-constrained settings, with limited computational overhead.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0405
            </span>
            <a href="https://arxiv.org/abs/2504.09759" target="_blank" rel="noopener noreferrer">Enhancing Classifier Evaluation: A Fairer Benchmarking Strategy Based on Ability and Robustness</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lucas Cardoso, Vitor Santos, Jos\'e Ribeiro, Regiane Kawasaki, Ricardo Prud\^encio, Ronnie Alves | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Benchmarking is a fundamental practice in machine learning (ML) for comparing the performance of classification algorithms. However, traditional evaluation methods often overlook a critical aspect: the joint consideration of dataset complexity and an algorithm's ability to generalize. Without this d</span>
            
            <span class="abstract-full" style="display: none;">Benchmarking is a fundamental practice in machine learning (ML) for comparing the performance of classification algorithms. However, traditional evaluation methods often overlook a critical aspect: the joint consideration of dataset complexity and an algorithm's ability to generalize. Without this dual perspective, assessments may favor models that perform well on easy instances while failing to capture their true robustness. To address this limitation, this study introduces a novel evaluation methodology that combines Item Response Theory (IRT) with the Glicko-2 rating system, originally developed to measure player strength in competitive games. IRT assesses classifier ability based on performance over difficult instances, while Glicko-2 updates performance metrics - such as rating, deviation, and volatility - via simulated tournaments between classifiers. This combined approach provides a fairer and more nuanced measure of algorithm capability. A case study using the OpenML-CC18 benchmark showed that only 15% of the datasets are truly challenging and that a reduced subset with 50% of the original datasets offers comparable evaluation power. Among the algorithms tested, Random Forest achieved the highest ability score. The results highlight the importance of improving benchmark design by focusing on dataset quality and adopting evaluation strategies that reflect both difficulty and classifier proficiency.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.8 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 2.0 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0421
            </span>
            <a href="https://arxiv.org/abs/2504.09493" target="_blank" rel="noopener noreferrer">Federated Prototype Graph Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhengyu Wu, Xunkai Li, Yinlin Zhu, Rong-Hua Li, Guoren Wang, Chenghu Zhou | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent years, Federated Graph Learning (FGL) has gained significant attention for its distributed training capabilities in graph-based machine intelligence applications, mitigating data silos while offering a new perspective for privacy-preserve large-scale graph learning. However, multi-level FG</span>
            
            <span class="abstract-full" style="display: none;">In recent years, Federated Graph Learning (FGL) has gained significant attention for its distributed training capabilities in graph-based machine intelligence applications, mitigating data silos while offering a new perspective for privacy-preserve large-scale graph learning. However, multi-level FGL heterogeneity presents various client-server collaboration challenges: (1) Model-level: The variation in clients for expected performance and scalability necessitates the deployment of heterogeneous models. Unfortunately, most FGL methods rigidly demand identical client models due to the direct model weight aggregation on the server. (2) Data-level: The intricate nature of graphs, marked by the entanglement of node profiles and topology, poses an optimization dilemma. This implies that models obtained by federated training struggle to achieve superior performance. (3) Communication-level: Some FGL methods attempt to increase message sharing among clients or between clients and the server to improve training, which inevitably leads to high communication costs. In this paper, we propose FedPG as a general prototype-guided optimization method for the above multi-level FGL heterogeneity. Specifically, on the client side, we integrate multi-level topology-aware prototypes to capture local graph semantics. Subsequently, on the server side, leveraging the uploaded prototypes, we employ topology-guided contrastive learning and personalized technology to tailor global prototypes for each client, broadcasting them to improve local training. Experiments demonstrate that FedPG outperforms SOTA baselines by an average of 3.57\% in accuracy while reducing communication costs by 168x.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0635
            </span>
            <a href="https://arxiv.org/abs/2504.08791" target="_blank" rel="noopener noreferrer">PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zonghang Li, Tao Li, Wenjiao Feng, Mohsen Guizani, Hongfang Yu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and</span>
            
            <span class="abstract-full" style="display: none;">Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.6 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0728
            </span>
            <a href="https://arxiv.org/abs/2504.09249" target="_blank" rel="noopener noreferrer">NoTeS-Bank: Benchmarking Neural Transcription and Search for Scientific Notes Understanding</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Aniket Pal, Sanket Biswas, Alloy Das, Ayush Lodh, Priyanka Banerjee, Soumitri Chattopadhyay, Dimosthenis Karatzas, Josep Llados, C. V. Jawahar | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Understanding and reasoning over academic handwritten notes remains a challenge in document AI, particularly for mathematical equations, diagrams, and scientific notations. Existing visual question answering (VQA) benchmarks focus on printed or structured handwritten text, limiting generalization to</span>
            
            <span class="abstract-full" style="display: none;">Understanding and reasoning over academic handwritten notes remains a challenge in document AI, particularly for mathematical equations, diagrams, and scientific notations. Existing visual question answering (VQA) benchmarks focus on printed or structured handwritten text, limiting generalization to real-world note-taking. To address this, we introduce NoTeS-Bank, an evaluation benchmark for Neural Transcription and Search in note-based question answering. NoTeS-Bank comprises complex notes across multiple domains, requiring models to process unstructured and multimodal content. The benchmark defines two tasks: (1) Evidence-Based VQA, where models retrieve localized answers with bounding-box evidence, and (2) Open-Domain VQA, where models classify the domain before retrieving relevant documents and answers. Unlike classical Document VQA datasets relying on optical character recognition (OCR) and structured data, NoTeS-BANK demands vision-language fusion, retrieval, and multimodal reasoning. We benchmark state-of-the-art Vision-Language Models (VLMs) and retrieval frameworks, exposing structured transcription and reasoning limitations. NoTeS-Bank provides a rigorous evaluation with NDCG@5, MRR, Recall@K, IoU, and ANLS, establishing a new standard for visual document understanding and reasoning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.1 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0811
            </span>
            <a href="https://arxiv.org/abs/2501.00569" target="_blank" rel="noopener noreferrer">Probing Visual Language Priors in VLMs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tiange Luo, Ang Cao, Gunhee Lee, Justin Johnson, Honglak Lee | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Despite recent advances in Vision-Language Models (VLMs), they may over-rely on visual language priors existing in their training data rather than true visual reasoning. To investigate this, we introduce ViLP, a benchmark featuring deliberately out-of-distribution images synthesized via image genera</span>
            
            <span class="abstract-full" style="display: none;">Despite recent advances in Vision-Language Models (VLMs), they may over-rely on visual language priors existing in their training data rather than true visual reasoning. To investigate this, we introduce ViLP, a benchmark featuring deliberately out-of-distribution images synthesized via image generation models and out-of-distribution Q&amp;A pairs. Each question in ViLP is coupled with three potential answers and three corresponding images: one that can be resolved by text priors alone and two that demand visual reasoning. Although, humans achieve near-perfect accuracy, modern VLMs falter; for instance, GPT-4 achieves only 66.17% on ViLP. To alleviate this, we propose a self-improving framework in which models generate new VQA data, then apply pixel-level and semantic corruptions to form "good-bad" image pairs for self-training. Our training objectives compel VLMs to focus more on the actual visual inputs, and we demonstrate their effectiveness in boosting the performance of open-source VLMs, including LLaVA-v1.5 and Cambrian.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.9 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0815
            </span>
            <a href="https://arxiv.org/abs/2504.08257" target="_blank" rel="noopener noreferrer">Bayesian Reasoning Enabled by Spin-Orbit Torque Magnetic Tunnel Junctions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yingqian Xu, Xiaohan Li, Caihua Wan, Ran Zhang, Bin He, Shiqiang Liu, Jihao Xia, Dehao Kong, Shilong Xiong, Guoqiang Yu, Xiufeng Han | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Bayesian networks play an increasingly important role in data mining, inference, and reasoning with the rapid development of artificial intelligence. In this paper, we present proof-of-concept experiments demonstrating the use of spin-orbit torque magnetic tunnel junctions (SOT-MTJs) in Bayesian net</span>
            
            <span class="abstract-full" style="display: none;">Bayesian networks play an increasingly important role in data mining, inference, and reasoning with the rapid development of artificial intelligence. In this paper, we present proof-of-concept experiments demonstrating the use of spin-orbit torque magnetic tunnel junctions (SOT-MTJs) in Bayesian network reasoning. Not only can the target probability distribution function (PDF) of a Bayesian network be precisely formulated by a conditional probability table as usual but also quantitatively parameterized by a probabilistic forward propagating neuron network. Moreover, the parameters of the network can also approach the optimum through a simple point-by point training algorithm, by leveraging which we do not need to memorize all historical data nor statistically summarize conditional probabilities behind them, significantly improving storage efficiency and economizing data pretreatment. Furthermore, we developed a simple medical diagnostic system using the SOT-MTJ as a random number generator and sampler, showcasing the application of SOT-MTJ-based Bayesian reasoning. This SOT-MTJ-based Bayesian reasoning shows great promise in the field of artificial probabilistic neural network, broadening the scope of spintronic device applications and providing an efficient and low-storage solution for complex reasoning tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 4.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0919
            </span>
            <a href="https://arxiv.org/abs/2504.08401" target="_blank" rel="noopener noreferrer">Graph Reduction with Unsupervised Learning in Column Generation: A Routing Application</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Abdo Abouelrous, Laurens Bliea, Adriana F. Gabor, Yaoxin Wu, Yingqian Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Column Generation (CG) is a popular method dedicated to enhancing computational efficiency in large scale Combinatorial Optimization (CO) problems. It reduces the number of decision variables in a problem by solving a pricing problem. For many CO problems, the pricing problem is an Elementary Shorte</span>
            
            <span class="abstract-full" style="display: none;">Column Generation (CG) is a popular method dedicated to enhancing computational efficiency in large scale Combinatorial Optimization (CO) problems. It reduces the number of decision variables in a problem by solving a pricing problem. For many CO problems, the pricing problem is an Elementary Shortest Path Problem with Resource Constraints (ESPPRC). Large ESPPRC instances are difficult to solve to near-optimality. Consequently, we use a Graph neural Network (GNN) to reduces the size of the ESPPRC such that it becomes computationally tractable with standard solving techniques. Our GNN is trained by Unsupervised Learning and outputs a distribution for the arcs to be retained in the reduced PP. The reduced PP is solved by a local search that finds columns with large reduced costs and speeds up convergence. We apply our method on a set of Capacitated Vehicle Routing Problems with Time Windows and show significant improvements in convergence compared to simple reduction techniques from the literature. For a fixed computational budget, we improve the objective values by over 9\% for larger instances. We also analyze the performance of our CG algorithm and test the generalization of our method to different classes of instances than the training data.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.6 -->
                
            <!-- LLMs: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- 3D: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0919
            </span>
            <a href="https://arxiv.org/abs/2504.09896" target="_blank" rel="noopener noreferrer">TWSSenti: A Novel Hybrid Framework for Topic-Wise Sentiment Analysis on Social Media Using Transformer Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Aish Albladi, Md Kaosar Uddin, Minarul Islam, Cheryl Seals | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Sentiment analysis is a crucial task in natural language processing (NLP) that enables the extraction of meaningful insights from textual data, particularly from dynamic platforms like Twitter and IMDB. This study explores a hybrid framework combining transformer-based models, specifically BERT, GPT</span>
            
            <span class="abstract-full" style="display: none;">Sentiment analysis is a crucial task in natural language processing (NLP) that enables the extraction of meaningful insights from textual data, particularly from dynamic platforms like Twitter and IMDB. This study explores a hybrid framework combining transformer-based models, specifically BERT, GPT-2, RoBERTa, XLNet, and DistilBERT, to improve sentiment classification accuracy and robustness. The framework addresses challenges such as noisy data, contextual ambiguity, and generalization across diverse datasets by leveraging the unique strengths of these models. BERT captures bidirectional context, GPT-2 enhances generative capabilities, RoBERTa optimizes contextual understanding with larger corpora and dynamic masking, XLNet models dependency through permutation-based learning, and DistilBERT offers efficiency with reduced computational overhead while maintaining high accuracy. We demonstrate text cleaning, tokenization, and feature extraction using Term Frequency Inverse Document Frequency (TF-IDF) and Bag of Words (BoW), ensure high-quality input data for the models. The hybrid approach was evaluated on benchmark datasets Sentiment140 and IMDB, achieving superior accuracy rates of 94\% and 95\%, respectively, outperforming standalone models. The results validate the effectiveness of combining multiple transformer models in ensemble-like setups to address the limitations of individual architectures. This research highlights its applicability to real-world tasks such as social media monitoring, customer sentiment analysis, and public opinion tracking which offers a pathway for future advancements in hybrid NLP frameworks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.7 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0936
            </span>
            <a href="https://arxiv.org/abs/2504.09873" target="_blank" rel="noopener noreferrer">Truncated Matrix Completion - An Empirical Study</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rishhabh Naik, Nisarg Trivedi, Davoud Ataee Tarzanagh, Laura Balzano | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Low-rank Matrix Completion (LRMC) describes the problem where we wish to recover missing entries of partially observed low-rank matrix. Most existing matrix completion work deals with sampling procedures that are independent of the underlying data values. While this assumption allows the derivation </span>
            
            <span class="abstract-full" style="display: none;">Low-rank Matrix Completion (LRMC) describes the problem where we wish to recover missing entries of partially observed low-rank matrix. Most existing matrix completion work deals with sampling procedures that are independent of the underlying data values. While this assumption allows the derivation of nice theoretical guarantees, it seldom holds in real-world applications. In this paper, we consider various settings where the sampling mask is dependent on the underlying data values, motivated by applications in sensing, sequential decision-making, and recommender systems. Through a series of experiments, we study and compare the performance of various LRMC algorithms that were originally successful for data-independent sampling patterns.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0973
            </span>
            <a href="https://arxiv.org/abs/2504.09723" target="_blank" rel="noopener noreferrer">AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dakuo Wang, Ting-Yao Hsu, Yuxuan Lu, Limeng Cui, Yaochen Xie, William Headean, Bingsheng Yao, Akash Veeragouni, Jiapeng Liu, Sreyashi Nag, Jessie Wang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Throu</span>
            
            <span class="abstract-full" style="display: none;">A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.105
            </span>
            <a href="https://arxiv.org/abs/2409.09386" target="_blank" rel="noopener noreferrer">AMBER -- Advanced SegFormer for Multi-Band Image Segmentation: an application to Hyperspectral Imaging</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Andrea Dosi, Massimo Brescia, Stefano Cavuoti, Mariarca D'Aniello, Michele Delli Veneri, Carlo Donadio, Adriano Ettari, Giuseppe Longo, Alvi Rownok, Luca Sannino, Maria Zampella | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Deep learning has revolutionized the field of hyperspectral image (HSI) analysis, enabling the extraction of complex spectral and spatial features. While convolutional neural networks (CNNs) have been the backbone of HSI classification, their limitations in capturing global contextual features have </span>
            
            <span class="abstract-full" style="display: none;">Deep learning has revolutionized the field of hyperspectral image (HSI) analysis, enabling the extraction of complex spectral and spatial features. While convolutional neural networks (CNNs) have been the backbone of HSI classification, their limitations in capturing global contextual features have led to the exploration of Vision Transformers (ViTs). This paper introduces AMBER, an advanced SegFormer specifically designed for multi-band image segmentation. AMBER enhances the original SegFormer by incorporating three-dimensional convolutions, custom kernel sizes, and a Funnelizer layer. This architecture enables processing hyperspectral data directly, without requiring spectral dimensionality reduction during preprocessing. Our experiments, conducted on three benchmark datasets (Salinas, Indian Pines, and Pavia University) and on a dataset from the PRISMA satellite, show that AMBER outperforms traditional CNN-based methods in terms of Overall Accuracy, Kappa coefficient, and Average Accuracy on the first three datasets, and achieves state-of-the-art performance on the PRISMA dataset. These findings highlight AMBER's robustness, adaptability to both airborne and spaceborne data, and its potential as a powerful solution for remote sensing and other domains requiring advanced analysis of high-dimensional data.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.2 -->
                
            <!-- LLMs: 8.1 -->
                
            <!-- Quantum Computing: 4.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1182
            </span>
            <a href="https://arxiv.org/abs/2504.01648" target="_blank" rel="noopener noreferrer">ProtoGuard-guided PROPEL: Class-Aware Prototype Enhancement and Progressive Labeling for Incremental 3D Point Cloud Segmentation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Haosheng Li, Yuecong Xu, Junjie Chen, Kemi Ding | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">3D point cloud semantic segmentation technology has been widely used. However, in real-world scenarios, the environment is evolving. Thus, offline-trained segmentation models may lead to catastrophic forgetting of previously seen classes. Class-incremental learning (CIL) is designed to address the p</span>
            
            <span class="abstract-full" style="display: none;">3D point cloud semantic segmentation technology has been widely used. However, in real-world scenarios, the environment is evolving. Thus, offline-trained segmentation models may lead to catastrophic forgetting of previously seen classes. Class-incremental learning (CIL) is designed to address the problem of catastrophic forgetting. While point clouds are common, we observe high similarity and unclear boundaries between different classes. Meanwhile, they are known to be imbalanced in class distribution. These lead to issues including misclassification between similar classes and the long-tail problem, which have not been adequately addressed in previous CIL methods. We thus propose ProtoGuard and PROPEL (Progressive Refinement Of PsEudo-Labels). In the base-class training phase, ProtoGuard maintains geometric and semantic prototypes for each class, which are combined into prototype features using an attention mechanism. In the novel-class training phase, PROPEL inherits the base feature extractor and classifier, guiding pseudo-label propagation and updates based on density distribution and semantic similarity. Extensive experiments show that our approach achieves remarkable results on both the S3DIS and ScanNet datasets, improving the mIoU of 3D point cloud segmentation by a maximum of 20.39% under the 5-step CIL scenario on S3DIS.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1259
            </span>
            <a href="https://arxiv.org/abs/2504.08219" target="_blank" rel="noopener noreferrer">VL-UR: Vision-Language-guided Universal Restoration of Images Degraded by Adverse Weather Conditions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ziyan Liu, Yuxu Lu, Huashan Yu, Dong yang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Image restoration is critical for improving the quality of degraded images, which is vital for applications like autonomous driving, security surveillance, and digital content enhancement. However, existing methods are often tailored to specific degradation scenarios, limiting their adaptability to </span>
            
            <span class="abstract-full" style="display: none;">Image restoration is critical for improving the quality of degraded images, which is vital for applications like autonomous driving, security surveillance, and digital content enhancement. However, existing methods are often tailored to specific degradation scenarios, limiting their adaptability to the diverse and complex challenges in real-world environments. Moreover, real-world degradations are typically non-uniform, highlighting the need for adaptive and intelligent solutions. To address these issues, we propose a novel vision-language-guided universal restoration (VL-UR) framework. VL-UR leverages a zero-shot contrastive language-image pre-training (CLIP) model to enhance image restoration by integrating visual and semantic information. A scene classifier is introduced to adapt CLIP, generating high-quality language embeddings aligned with degraded images while predicting degraded types for complex scenarios. Extensive experiments across eleven diverse degradation settings demonstrate VL-UR's state-of-the-art performance, robustness, and adaptability. This positions VL-UR as a transformative solution for modern image restoration challenges in dynamic, real-world environments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1421
            </span>
            <a href="https://arxiv.org/abs/2504.08624" target="_blank" rel="noopener noreferrer">TorchFX: A modern approach to Audio DSP with PyTorch and GPU acceleration</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Matteo Spanio, Antonio Rod\`a | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The burgeoning complexity and real-time processing demands of audio signals necessitate optimized algorithms that harness the computational prowess of Graphics Processing Units (GPUs). Existing Digital Signal Processing (DSP) libraries often fall short in delivering the requisite efficiency and flex</span>
            
            <span class="abstract-full" style="display: none;">The burgeoning complexity and real-time processing demands of audio signals necessitate optimized algorithms that harness the computational prowess of Graphics Processing Units (GPUs). Existing Digital Signal Processing (DSP) libraries often fall short in delivering the requisite efficiency and flexibility, particularly in integrating Artificial Intelligence (AI) models. In response, we introduce TorchFX: a GPU-accelerated Python library for DSP, specifically engineered to facilitate sophisticated audio signal processing. Built atop the PyTorch framework, TorchFX offers an Object-Oriented interface that emulates the usability of torchaudio, enhancing functionality with a novel pipe operator for intuitive filter chaining. This library provides a comprehensive suite of Finite Impulse Response (FIR) and Infinite Impulse Response (IIR) filters, with a focus on multichannel audio files, thus facilitating the integration of DSP and AI-based approaches. Our benchmarking results demonstrate significant efficiency gains over traditional libraries like SciPy, particularly in multichannel contexts. Despite current limitations in GPU compatibility, ongoing developments promise broader support and real-time processing capabilities. TorchFX aims to become a useful tool for the community, contributing to innovation and progress in DSP with GPU acceleration. TorchFX is publicly available on GitHub at https://github.com/matteospanio/torchfx.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1439
            </span>
            <a href="https://arxiv.org/abs/2504.09909" target="_blank" rel="noopener noreferrer">Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Farha Nausheen, Khandakar Ahmed, M Imad Khan | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent developments, deep learning methodologies applied to Natural Language Processing (NLP) have revealed a paradox: They improve performance but demand considerable data and resources for their training. Alternatively, quantum computing exploits the principles of quantum mechanics to overcome </span>
            
            <span class="abstract-full" style="display: none;">In recent developments, deep learning methodologies applied to Natural Language Processing (NLP) have revealed a paradox: They improve performance but demand considerable data and resources for their training. Alternatively, quantum computing exploits the principles of quantum mechanics to overcome the computational limitations of current methodologies, thereby establishing an emerging field known as quantum natural language processing (QNLP). This domain holds the potential to attain a quantum advantage in the processing of linguistic structures, surpassing classical models in both efficiency and accuracy. In this paper, it is proposed to categorise QNLP models based on quantum computing principles, architecture, and computational approaches. This paper attempts to provide a survey on how quantum meets language by mapping state-of-the-art in this area, embracing quantum encoding techniques for classical data, QNLP models for prevalent NLP tasks, and quantum optimisation techniques for hyper parameter tuning. The landscape of quantum computing approaches applied to various NLP tasks is summarised by showcasing the specific QNLP methods used, and the popularity of these methods is indicated by their count. From the findings, it is observed that QNLP approaches are still limited to small data sets, with only a few models explored extensively, and there is increasing interest in the application of quantum computing to natural language processing tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.4 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 5.4 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1473
            </span>
            <a href="https://arxiv.org/abs/2504.09426" target="_blank" rel="noopener noreferrer">BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shengao Wang, Arjun Chandra, Aoming Liu, Venkatesh Saligrama, Boqing Gong | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Human infants rapidly develop visual reasoning skills from minimal input, suggesting that developmentally inspired pretraining could significantly enhance the efficiency of vision-language models (VLMs). Although recent efforts have leveraged infant-inspired datasets like SAYCam, existing evaluation</span>
            
            <span class="abstract-full" style="display: none;">Human infants rapidly develop visual reasoning skills from minimal input, suggesting that developmentally inspired pretraining could significantly enhance the efficiency of vision-language models (VLMs). Although recent efforts have leveraged infant-inspired datasets like SAYCam, existing evaluation benchmarks remain misaligned--they are either too simplistic, narrowly scoped, or tailored for large-scale pretrained models. Additionally, training exclusively on infant data overlooks the broader, diverse input from which infants naturally learn. To address these limitations, we propose BabyVLM, a novel framework comprising comprehensive in-domain evaluation benchmarks and a synthetic training dataset created via child-directed transformations of existing datasets. We demonstrate that VLMs trained with our synthetic dataset achieve superior performance on BabyVLM tasks compared to models trained solely on SAYCam or general-purpose data of the SAYCam size. BabyVLM thus provides a robust, developmentally aligned evaluation tool and illustrates how compact models trained on carefully curated data can generalize effectively, opening pathways toward data-efficient vision-language learning paradigms.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1477
            </span>
            <a href="https://arxiv.org/abs/2504.09164" target="_blank" rel="noopener noreferrer">Can postgraduate translation students identify machine-generated text?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Michael Farrell | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Given the growing use of generative artificial intelligence as a tool for creating multilingual content and bypassing both machine and traditional translation methods, this study explores the ability of linguistically trained individuals to discern machine-generated output from human-written text (H</span>
            
            <span class="abstract-full" style="display: none;">Given the growing use of generative artificial intelligence as a tool for creating multilingual content and bypassing both machine and traditional translation methods, this study explores the ability of linguistically trained individuals to discern machine-generated output from human-written text (HT). After brief training sessions on the textual anomalies typically found in synthetic text (ST), twenty-three postgraduate translation students analysed excerpts of Italian prose and assigned likelihood scores to indicate whether they believed they were human-written or AI-generated (ChatGPT-4o). The results show that, on average, the students struggled to distinguish between HT and ST, with only two participants achieving notable accuracy. Closer analysis revealed that the students often identified the same textual anomalies in both HT and ST, although features such as low burstiness and self-contradiction were more frequently associated with ST. These findings suggest the need for improvements in the preparatory training. Moreover, the study raises questions about the necessity of editing synthetic text to make it sound more human-like and recommends further research to determine whether AI-generated text is already sufficiently natural-sounding not to require further refinement.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1486
            </span>
            <a href="https://arxiv.org/abs/2504.10174" target="_blank" rel="noopener noreferrer">LLaVA-ReID: Selective Multi-image Questioner for Interactive Person Re-Identification</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yiding Lu, Mouxing Yang, Dezhong Peng, Peng Hu, Yijie Lin, Xi Peng | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Traditional text-based person ReID assumes that person descriptions from witnesses are complete and provided at once. However, in real-world scenarios, such descriptions are often partial or vague. To address this limitation, we introduce a new task called interactive person re-identification (Inter</span>
            
            <span class="abstract-full" style="display: none;">Traditional text-based person ReID assumes that person descriptions from witnesses are complete and provided at once. However, in real-world scenarios, such descriptions are often partial or vague. To address this limitation, we introduce a new task called interactive person re-identification (Inter-ReID). Inter-ReID is a dialogue-based retrieval task that iteratively refines initial descriptions through ongoing interactions with the witnesses. To facilitate the study of this new task, we construct a dialogue dataset that incorporates multiple types of questions by decomposing fine-grained attributes of individuals. We further propose LLaVA-ReID, a question model that generates targeted questions based on visual and textual contexts to elicit additional details about the target person. Leveraging a looking-forward strategy, we prioritize the most informative questions as supervision during training. Experimental results on both Inter-ReID and text-based ReID benchmarks demonstrate that LLaVA-ReID significantly outperforms baselines.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1661
            </span>
            <a href="https://arxiv.org/abs/2504.09761" target="_blank" rel="noopener noreferrer">Dynamical symmetries in the fluctuation-driven regime: an application of Noether's theorem to noisy dynamical systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: John J. Vastola | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Noether's theorem provides a powerful link between continuous symmetries and conserved quantities for systems governed by some variational principle. Perhaps unfortunately, most dynamical systems of interest in neuroscience and artificial intelligence cannot be described by any such principle. On th</span>
            
            <span class="abstract-full" style="display: none;">Noether's theorem provides a powerful link between continuous symmetries and conserved quantities for systems governed by some variational principle. Perhaps unfortunately, most dynamical systems of interest in neuroscience and artificial intelligence cannot be described by any such principle. On the other hand, nonequilibrium physics provides a variational principle that describes how fairly generic noisy dynamical systems are most likely to transition between two states; in this work, we exploit this principle to apply Noether's theorem, and hence learn about how the continuous symmetries of dynamical systems constrain their most likely trajectories. We identify analogues of the conservation of energy, momentum, and angular momentum, and briefly discuss examples of each in the context of models of decision-making, recurrent neural networks, and diffusion generative models.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.7 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1696
            </span>
            <a href="https://arxiv.org/abs/2406.10244" target="_blank" rel="noopener noreferrer">GLINT-RU: Gated Lightweight Intelligent Recurrent Units for Sequential Recommender Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sheng Zhang, Maolin Wang, Wanyu Wang, Jingtong Gao, Xiangyu Zhao, Yu Yang, Xuetao Wei, Zitao Liu, Tong Xu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Transformer-based models have gained significant traction in sequential recommender systems (SRSs) for their ability to capture user-item interactions effectively. However, these models often suffer from high computational costs and slow inference. Meanwhile, existing efficient SRS approaches strugg</span>
            
            <span class="abstract-full" style="display: none;">Transformer-based models have gained significant traction in sequential recommender systems (SRSs) for their ability to capture user-item interactions effectively. However, these models often suffer from high computational costs and slow inference. Meanwhile, existing efficient SRS approaches struggle to embed high-quality semantic and positional information into latent representations. To tackle these challenges, this paper introduces GLINT-RU, a lightweight and efficient SRS leveraging a single-layer dense selective Gated Recurrent Units (GRU) module to accelerate inference. By incorporating a dense selective gate, GLINT-RU adaptively captures temporal dependencies and fine-grained positional information, generating high-quality latent representations. Additionally, a parallel mixing block infuses fine-grained positional features into user-item interactions, enhancing both recommendation quality and efficiency. Extensive experiments on three datasets demonstrate that GLINT-RU achieves superior prediction accuracy and inference speed, outperforming baselines based on RNNs, Transformers, MLPs, and SSMs. These results establish GLINT-RU as a powerful and efficient solution for SRSs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1701
            </span>
            <a href="https://arxiv.org/abs/2504.06106" target="_blank" rel="noopener noreferrer">A ROS2-based software library for inverse dynamics computation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vincenzo Petrone, Enrico Ferrentino, Pasquale Chiacchio | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Inverse dynamics computation is a critical component in robot control, planning and simulation, enabling the calculation of joint torques required to achieve a desired motion. This paper presents a ROS2-based software library designed to solve the inverse dynamics problem for robotic systems. The li</span>
            
            <span class="abstract-full" style="display: none;">Inverse dynamics computation is a critical component in robot control, planning and simulation, enabling the calculation of joint torques required to achieve a desired motion. This paper presents a ROS2-based software library designed to solve the inverse dynamics problem for robotic systems. The library is built around an abstract class with three concrete implementations: one for simulated robots and two for real UR10 and Franka robots. This contribution aims to provide a flexible, extensible, robot-agnostic solution to inverse dynamics, suitable for both simulation and real-world scenarios involving planning and control applications. The related software is available at https://github.com/unisa-acg/inverse-dynamics-solver/tree/rap.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1825
            </span>
            <a href="https://arxiv.org/abs/2504.10159" target="_blank" rel="noopener noreferrer">Monadic type-and-effect soundness</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Francesco Dagnino, Paola Giannini, Elena Zucca | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We introduce the abstract notions of "monadic operational semantics", a small-step semantics where computational effects are modularly modeled by a monad, and "type-and-effect system", including "effect types" whose interpretation lifts well-typedness to its monadic version. In this meta-theory, as </span>
            
            <span class="abstract-full" style="display: none;">We introduce the abstract notions of "monadic operational semantics", a small-step semantics where computational effects are modularly modeled by a monad, and "type-and-effect system", including "effect types" whose interpretation lifts well-typedness to its monadic version. In this meta-theory, as usually done in the non-monadic case, we can express progress and subject reduction properties and provide a proof, given once and for all, that they imply soundness. The approach is illustrated on a lambda calculus with generic effects. We equip the calculus with an expressive type-and-effect system, and provide proofs of progress and subject reduction which are parametric on the interpretation of effect types. In this way, we obtain as instances many significant examples, such as checking exceptions, preventing/limiting non-determinism, constraining order/fairness of outputs on different locations. We also provide an extension with constructs to raise and handle computational effects, which can be instantiated to model different policies.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1887
            </span>
            <a href="https://arxiv.org/abs/2504.09776" target="_blank" rel="noopener noreferrer">An Investigation of Large Language Models and Their Vulnerabilities in Spam Detection</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Qiyao Tang, Xiangyang Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Spam messages continue to present significant challenges to digital users, cluttering inboxes and posing security risks. Traditional spam detection methods, including rules-based, collaborative, and machine learning approaches, struggle to keep up with the rapidly evolving tactics employed by spamme</span>
            
            <span class="abstract-full" style="display: none;">Spam messages continue to present significant challenges to digital users, cluttering inboxes and posing security risks. Traditional spam detection methods, including rules-based, collaborative, and machine learning approaches, struggle to keep up with the rapidly evolving tactics employed by spammers. This project studies new spam detection systems that leverage Large Language Models (LLMs) fine-tuned with spam datasets. More importantly, we want to understand how LLM-based spam detection systems perform under adversarial attacks that purposefully modify spam emails and data poisoning attacks that exploit the differences between the training data and the massages in detection, to which traditional machine learning models are shown to be vulnerable. This experimentation employs two LLM models of GPT2 and BERT and three spam datasets of Enron, LingSpam, and SMSspamCollection for extensive training and testing tasks. The results show that, while they can function as effective spam filters, the LLM models are susceptible to the adversarial and data poisoning attacks. This research provides very useful insights for future applications of LLM models for information security.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.9 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1919
            </span>
            <a href="https://arxiv.org/abs/2501.01383" target="_blank" rel="noopener noreferrer">Electrical networks and data analysis in phylogenetics</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: V. Gorbounov, A. Kazakov | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A classic problem in data analysis is studying the systems of subsets defined by either a similarity or a dissimilarity function on $X$ which is either observed directly or derived from a data set. For an electrical network there are two functions on the set of the nodes defined by the resistance ma</span>
            
            <span class="abstract-full" style="display: none;">A classic problem in data analysis is studying the systems of subsets defined by either a similarity or a dissimilarity function on $X$ which is either observed directly or derived from a data set. For an electrical network there are two functions on the set of the nodes defined by the resistance matrix and the response matrix either of which defines the network completely. We argue that these functions should be viewed as a similarity and a dissimilarity function on the set of the nodes moreover they are related via the covariance mapping also known as the Farris transform or the Gromov product. We will explore the properties of electrical networks from this point of view. It has been known for a while that the resistance matrix defines a metric on the nodes of the electrical networks. Moreover for a circular electrical network this metric obeys the Kalmanson property as it was shown recently. We will call such a metric an electrical Kalmanson metric. The main results of this paper is a complete description of the electrical Kalmanson metrics in the set of all Kalmanson metrics in terms of the geometry of the positive Isotropic Grassmannian whose connection to the theory of electrical networks was discovered earlier. One important area of applications where Kalmanson metrics are actively used is the theory of phylogenetic networks which are a generalization of phylogenetic trees. Our results allow us to use in phylogenetics the powerful methods of reconstruction of the minimal graphs of electrical networks and possibly open the door into data analysis for the methods of the theory of cluster algebras.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 5.1 -->
                
            <!-- LLMs: 4.7 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Math: 3.9 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Pathfinding: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- SpikingNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.213
            </span>
            <a href="https://arxiv.org/abs/2504.09879" target="_blank" rel="noopener noreferrer">Revisiting the attacker's knowledge in inference attacks against Searchable Symmetric Encryption</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Marc Damie, Jean-Benoist Leger, Florian Hahn, Andreas Peter | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Encrypted search schemes have been proposed to address growing privacy concerns. However, several leakage-abuse attacks have highlighted some security vulnerabilities. Recent attacks assumed an attacker's knowledge containing data ``similar'' to the indexed data. However, this vague assumption is ba</span>
            
            <span class="abstract-full" style="display: none;">Encrypted search schemes have been proposed to address growing privacy concerns. However, several leakage-abuse attacks have highlighted some security vulnerabilities. Recent attacks assumed an attacker's knowledge containing data ``similar'' to the indexed data. However, this vague assumption is barely discussed in literature: how likely is it for an attacker to obtain a "similar enough" data?</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2416
            </span>
            <a href="https://arxiv.org/abs/2504.09685" target="_blank" rel="noopener noreferrer">Can LLMs Revolutionize the Design of Explainable and Efficient TinyML Models?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Christophe El Zeinaty, Wassim Hamidouche, Glenn Herrou, Daniel Menard, Merouane Debbah | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper introduces a novel framework for designing efficient neural network architectures specifically tailored to tiny machine learning (TinyML) platforms. By leveraging large language models (LLMs) for neural architecture search (NAS), a vision transformer (ViT)-based knowledge distillation (KD</span>
            
            <span class="abstract-full" style="display: none;">This paper introduces a novel framework for designing efficient neural network architectures specifically tailored to tiny machine learning (TinyML) platforms. By leveraging large language models (LLMs) for neural architecture search (NAS), a vision transformer (ViT)-based knowledge distillation (KD) strategy, and an explainability module, the approach strikes an optimal balance between accuracy, computational efficiency, and memory usage. The LLM-guided search explores a hierarchical search space, refining candidate architectures through Pareto optimization based on accuracy, multiply-accumulate operations (MACs), and memory metrics. The best-performing architectures are further fine-tuned using logits-based KD with a pre-trained ViT-B/16 model, which enhances generalization without increasing model size. Evaluated on the CIFAR-100 dataset and deployed on an STM32H7 microcontroller (MCU), the three proposed models, LMaNet-Elite, LMaNet-Core, and QwNet-Core, achieve accuracy scores of 74.50%, 74.20% and 73.00%, respectively. All three models surpass current state-of-the-art (SOTA) models, such as MCUNet-in3/in4 (69.62% / 72.86%) and XiNet (72.27%), while maintaining a low computational cost of less than 100 million MACs and adhering to the stringent 320 KB static random-access memory (SRAM) constraint. These results demonstrate the efficiency and performance of the proposed framework for TinyML platforms, underscoring the potential of combining LLM-driven search, Pareto optimization, KD, and explainability to develop accurate, efficient, and interpretable models. This approach opens new possibilities in NAS, enabling the design of efficient architectures specifically suited for TinyML.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2485
            </span>
            <a href="https://arxiv.org/abs/2406.00004" target="_blank" rel="noopener noreferrer">Navigating the Future of Federated Recommendation Systems with Foundation Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhiwei Li, Guodong Long, Chunxu Zhang, Honglei Zhang, Jing Jiang, Chengqi Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Federated Recommendation Systems (FRSs) offer a privacy-preserving alternative to traditional centralized approaches by decentralizing data storage. However, they face persistent challenges such as data sparsity and heterogeneity, largely due to isolated client environments. Recent advances in Found</span>
            
            <span class="abstract-full" style="display: none;">Federated Recommendation Systems (FRSs) offer a privacy-preserving alternative to traditional centralized approaches by decentralizing data storage. However, they face persistent challenges such as data sparsity and heterogeneity, largely due to isolated client environments. Recent advances in Foundation Models (FMs), particularly large language models like ChatGPT, present an opportunity to surmount these issues through powerful, cross-task knowledge transfer. In this position paper, we systematically examine the convergence of FRSs and FMs, illustrating how FM-enhanced frameworks can substantially improve client-side personalization, communication efficiency, and server-side aggregation. We also delve into pivotal challenges introduced by this integration, including privacy-security trade-offs, non-IID data, and resource constraints in federated setups, and propose prospective research directions in areas such as multimodal recommendation, real-time FM adaptation, and explainable federated reasoning. By unifying FRSs with FMs, our position paper provides a forward-looking roadmap for advancing privacy-preserving, high-performance recommendation systems that fully leverage large-scale pre-trained knowledge to enhance local performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2528
            </span>
            <a href="https://arxiv.org/abs/2412.14566" target="_blank" rel="noopener noreferrer">AIArena: A Blockchain-Based Decentralized AI Training Platform</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhipeng Wang, Rui Sun, Elizabeth Lui, Tuo Zhou, Yizhe Wen, Jiahao Sun | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The rapid advancement of AI has underscored critical challenges in its development and implementation, largely due to centralized control by a few major corporations. This concentration of power intensifies biases within AI models, resulting from inadequate governance and oversight mechanisms. Addit</span>
            
            <span class="abstract-full" style="display: none;">The rapid advancement of AI has underscored critical challenges in its development and implementation, largely due to centralized control by a few major corporations. This concentration of power intensifies biases within AI models, resulting from inadequate governance and oversight mechanisms. Additionally, it limits public involvement and heightens concerns about the integrity of model generation. Such monopolistic control over data and AI outputs threatens both innovation and fair data usage, as users inadvertently contribute data that primarily benefits these corporations. In this work, we propose AIArena, a blockchain-based decentralized AI training platform designed to democratize AI development and alignment through on-chain incentive mechanisms. AIArena fosters an open and collaborative environment where participants can contribute models and computing resources. Its on-chain consensus mechanism ensures fair rewards for participants based on their contributions. We instantiate and implement AIArena on the public Base blockchain Sepolia testnet, and the evaluation results demonstrate the feasibility of AIArena in real-world applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.9 -->
                
            <!-- Medicine: 9.0 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2678
            </span>
            <a href="https://arxiv.org/abs/2504.09074" target="_blank" rel="noopener noreferrer">A Case for Kolmogorov-Arnold Networks in Prefetching: Towards Low-Latency, Generalizable ML-Based Prefetchers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dhruv Kulkarni, Bharat Bhammar, Henil Thaker, Pranav Dhobi, R. P. Gohil, Sai Manoj Pudukotai Dinkarrao | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The memory wall problem arises due to the disparity between fast processors and slower memory, causing significant delays in data access, even more so on edge devices. Data prefetching is a key strategy to address this, with traditional methods evolving to incorporate Machine Learning (ML) for impro</span>
            
            <span class="abstract-full" style="display: none;">The memory wall problem arises due to the disparity between fast processors and slower memory, causing significant delays in data access, even more so on edge devices. Data prefetching is a key strategy to address this, with traditional methods evolving to incorporate Machine Learning (ML) for improved accuracy. Modern prefetchers must balance high accuracy with low latency to further practicality. We explore the applicability of utilizing Kolmogorov-Arnold Networks (KAN) with learnable activation functions,a prefetcher we implemented called KANBoost, to further this aim. KANs are a novel, state-of-the-art model that work on breaking down continuous, bounded multi-variate functions into functions of their constituent variables, and use these constitutent functions as activations on each individual neuron. KANBoost predicts the next memory access by modeling deltas between consecutive addresses, offering a balance of accuracy and efficiency to mitigate the memory wall problem with minimal overhead, instead of relying on address-correlation prefetching. Initial results indicate that KAN-based prefetching reduces inference latency (18X lower than state-of-the-art ML prefetchers) while achieving moderate IPC improvements (2.5\% over no-prefetching). While KANs still face challenges in capturing long-term dependencies, we propose that future research should explore hybrid models that combine KAN efficiency with stronger sequence modeling techniques, paving the way for practical ML-based prefetching in edge devices and beyond.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2714
            </span>
            <a href="https://arxiv.org/abs/2503.20737" target="_blank" rel="noopener noreferrer">Ontology-based Semantic Similarity Measures for Clustering Medical Concepts in Drug Safety</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jeffery L Painter, Fran\c{c}ois Haguinet, Gregory E Powell, Andrew Bate | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Semantic similarity measures (SSMs) are widely used in biomedical research but remain underutilized in pharmacovigilance. This study evaluates six ontology-based SSMs for clustering MedDRA Preferred Terms (PTs) in drug safety data. Using the Unified Medical Language System (UMLS), we assess each met</span>
            
            <span class="abstract-full" style="display: none;">Semantic similarity measures (SSMs) are widely used in biomedical research but remain underutilized in pharmacovigilance. This study evaluates six ontology-based SSMs for clustering MedDRA Preferred Terms (PTs) in drug safety data. Using the Unified Medical Language System (UMLS), we assess each method's ability to group PTs around medically meaningful centroids. A high-throughput framework was developed with a Java API and Python and R interfaces support large-scale similarity computations. Results show that while path-based methods perform moderately with F1 scores of 0.36 for WUPALMER and 0.28 for LCH, intrinsic information content (IC)-based measures, especially INTRINSIC-LIN and SOKAL, consistently yield better clustering accuracy (F1 score of 0.403). Validated against expert review and standard MedDRA queries (SMQs), our findings highlight the promise of IC-based SSMs in enhancing pharmacovigilance workflows by improving early signal detection and reducing manual review.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2888
            </span>
            <a href="https://arxiv.org/abs/2412.18417" target="_blank" rel="noopener noreferrer">Ultra-Low Complexity On-Orbit Compression for Remote Sensing Imagery via Block Modulated Imaging</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhibin Wang, Yanxin Cai, Jiayi Zhou, Yangming Zhang, Tianyu Li, Wei Li, Xun Liu, Guoqing Wang, Yang Yang | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The growing field of remote sensing faces a challenge: the ever-increasing size and volume of imagery data are exceeding the storage and transmission capabilities of satellite platforms. Efficient compression of remote sensing imagery is a critical solution to alleviate these burdens on satellites. </span>
            
            <span class="abstract-full" style="display: none;">The growing field of remote sensing faces a challenge: the ever-increasing size and volume of imagery data are exceeding the storage and transmission capabilities of satellite platforms. Efficient compression of remote sensing imagery is a critical solution to alleviate these burdens on satellites. However, existing compression methods are often too computationally expensive for satellites. With the continued advancement of compressed sensing theory, single-pixel imaging emerges as a powerful tool that brings new possibilities for on-orbit image compression. However, it still suffers from prolonged imaging times and the inability to perform high-resolution imaging, hindering its practical application. This paper advances the study of compressed sensing in remote sensing image compression, proposing Block Modulated Imaging (BMI). By requiring only a single exposure, BMI significantly enhances imaging acquisition speeds. Additionally, BMI obviates the need for digital micromirror devices and surpasses limitations in image resolution. Furthermore, we propose a novel decoding network specifically designed to reconstruct images compressed under the BMI framework. Leveraging the gated 3D convolutions and promoting efficient information flow across stages through a Two-Way Cross-Attention module, our decoding network exhibits demonstrably superior reconstruction performance. Extensive experiments conducted on multiple renowned remote sensing datasets unequivocally demonstrate the efficacy of our proposed method. To further validate its practical applicability, we developed and tested a prototype of the BMI-based camera, which has shown promising potential for on-orbit image compression. The code is available at https://github.com/Johnathan218/BMNet.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.0 -->
                
            <!-- LLMs: 8.9 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2895
            </span>
            <a href="https://arxiv.org/abs/2504.05172" target="_blank" rel="noopener noreferrer">Attention-Based Multiscale Temporal Fusion Network for Uncertain-Mode Fault Diagnosis in Multimode Processes</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Guangqiang Li, M. Amine Atoui, Xiangshun Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Fault diagnosis in multimode processes plays a critical role in ensuring the safe operation of industrial systems across multiple modes. It faces a great challenge yet to be addressed - that is, the significant distributional differences among monitoring data from multiple modes make it difficult fo</span>
            
            <span class="abstract-full" style="display: none;">Fault diagnosis in multimode processes plays a critical role in ensuring the safe operation of industrial systems across multiple modes. It faces a great challenge yet to be addressed - that is, the significant distributional differences among monitoring data from multiple modes make it difficult for the models to extract shared feature representations related to system health conditions. In response to this problem, this paper introduces a novel method called attention-based multiscale temporal fusion network. The multiscale depthwise convolution and gated recurrent unit are employed to extract multiscale contextual local features and long-short-term features. Instance normalization is applied to suppress mode-specific information. Furthermore, a temporal attention mechanism is designed to focus on critical time points with higher cross-mode shared information, thereby enhancing the accuracy of fault diagnosis. The proposed model is applied to Tennessee Eastman process dataset and three-phase flow facility dataset. The experiments demonstrate that the proposed model achieves superior diagnostic performance and maintains a small model size. The source code will be available on GitHub at https://github.com/GuangqiangLi/AMTFNet.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2937
            </span>
            <a href="https://arxiv.org/abs/2504.10203" target="_blank" rel="noopener noreferrer">A moving horizon estimator for aquifer thermal energy storages</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Johannes van Randenborgh, Moritz Schulze Darup | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Aquifer thermal energy storages (ATES) represent groundwater saturated aquifers that store thermal energy in the form of heated or cooled groundwater. Combining two ATES, one can harness excess thermal energy from summer (heat) and winter (cold) to support the building's heating, ventilation, and ai</span>
            
            <span class="abstract-full" style="display: none;">Aquifer thermal energy storages (ATES) represent groundwater saturated aquifers that store thermal energy in the form of heated or cooled groundwater. Combining two ATES, one can harness excess thermal energy from summer (heat) and winter (cold) to support the building's heating, ventilation, and air conditioning (HVAC) technology. In general, a dynamic operation of ATES throughout the year is beneficial to avoid using fossil fuel-based HVAC technology and maximize the ``green use'' of ATES. Model predictive control (MPC) with an appropriate system model may become a crucial control approach for ATES systems. Consequently, the MPC model should reflect spatial temperature profiles around ATES' boreholes to predict extracted groundwater temperatures accurately. However, meaningful predictions require the estimation of the current state of the system, as measurements are usually only at the borehole of the ATES. In control, this is often realized by model-based observers. Still, observing the state of an ATES system is non-trivial, since the model is typically hybrid. We show how to exploit the specific structure of the hybrid ATES model and design an easy-to-solve moving horizon estimator based on a quadratic program.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.0 -->
                
            <!-- LLMs: 6.7 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2969
            </span>
            <a href="https://arxiv.org/abs/2504.08544" target="_blank" rel="noopener noreferrer">Slicing the Gaussian Mixture Wasserstein Distance</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Moritz Piening, Robert Beinert | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Gaussian mixture models (GMMs) are widely used in machine learning for tasks such as clustering, classification, image reconstruction, and generative modeling. A key challenge in working with GMMs is defining a computationally efficient and geometrically meaningful metric. The mixture Wasserstein (M</span>
            
            <span class="abstract-full" style="display: none;">Gaussian mixture models (GMMs) are widely used in machine learning for tasks such as clustering, classification, image reconstruction, and generative modeling. A key challenge in working with GMMs is defining a computationally efficient and geometrically meaningful metric. The mixture Wasserstein (MW) distance adapts the Wasserstein metric to GMMs and has been applied in various domains, including domain adaptation, dataset comparison, and reinforcement learning. However, its high computational cost -- arising from repeated Wasserstein distance computations involving matrix square root estimations and an expensive linear program -- limits its scalability to high-dimensional and large-scale problems. To address this, we propose multiple novel slicing-based approximations to the MW distance that significantly reduce computational complexity while preserving key optimal transport properties. From a theoretical viewpoint, we establish several weak and strong equivalences between the introduced metrics, and show the relations to the original MW distance and the well-established sliced Wasserstein distance. Furthermore, we validate the effectiveness of our approach through numerical experiments, demonstrating computational efficiency and applications in clustering, perceptual image comparison, and GMM minimization</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3017
            </span>
            <a href="https://arxiv.org/abs/2409.01990" target="_blank" rel="noopener noreferrer">Designing Large Foundation Models for Efficient Training and Inference: A Survey</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dong Liu, Yanxuan Yu, Yite Wang, Jing Wu, Zhongwei Wan, Sina Alinejad, Benjamin Lengerich, Ying Nian Wu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper focuses on modern efficient training and inference technologies on foundation models and illustrates them from two perspectives: model and system design. Model and System Design optimize LLM training and inference from different aspects to save computational resources, making LLMs more ef</span>
            
            <span class="abstract-full" style="display: none;">This paper focuses on modern efficient training and inference technologies on foundation models and illustrates them from two perspectives: model and system design. Model and System Design optimize LLM training and inference from different aspects to save computational resources, making LLMs more efficient, affordable, and more accessible. The paper list repository is available at https://github.com/NoakLiu/Efficient-Foundation-Models-Survey.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.8 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3102
            </span>
            <a href="https://arxiv.org/abs/2504.09963" target="_blank" rel="noopener noreferrer">Towards Unbiased Federated Graph Learning: Label and Topology Perspectives</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhengyu Wu, Boyang Pang, Xunkai Li, Yinlin Zhu, Daohan Su, Bowen Fan, Rong-Hua Li, Guoren Wang, Chenghu Zhou | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Federated Graph Learning (FGL) enables privacy-preserving, distributed training of graph neural networks without sharing raw data. Among its approaches, subgraph-FL has become the dominant paradigm, with most work focused on improving overall node classification accuracy. However, these methods ofte</span>
            
            <span class="abstract-full" style="display: none;">Federated Graph Learning (FGL) enables privacy-preserving, distributed training of graph neural networks without sharing raw data. Among its approaches, subgraph-FL has become the dominant paradigm, with most work focused on improving overall node classification accuracy. However, these methods often overlook fairness due to the complexity of node features, labels, and graph structures. In particular, they perform poorly on nodes with disadvantaged properties, such as being in the minority class within subgraphs or having heterophilous connections (neighbors with dissimilar labels or misleading features). This reveals a critical issue: high accuracy can mask degraded performance on structurally or semantically marginalized nodes. To address this, we advocate for two fairness goals: (1) improving representation of minority class nodes for class-wise fairness and (2) mitigating topological bias from heterophilous connections for topology-aware fairness. We propose FairFGL, a novel framework that enhances fairness through fine-grained graph mining and collaborative learning. On the client side, the History-Preserving Module prevents overfitting to dominant local classes, while the Majority Alignment Module refines representations of heterophilous majority-class nodes. The Gradient Modification Module transfers minority-class knowledge from structurally favorable clients to improve fairness. On the server side, FairFGL uploads only the most influenced subset of parameters to reduce communication costs and better reflect local distributions. A cluster-based aggregation strategy reconciles conflicting updates and curbs global majority dominance . Extensive evaluations on eight benchmarks show FairFGL significantly improves minority-group performance , achieving up to a 22.62 percent Macro-F1 gain while enhancing convergence over state-of-the-art baselines.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.312
            </span>
            <a href="https://arxiv.org/abs/2504.10003" target="_blank" rel="noopener noreferrer">NaviDiffusor: Cost-Guided Diffusion Model for Visual Navigation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yiming Zeng, Hao Ren, Shuhang Wang, Junlong Huang, Hui Cheng | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Visual navigation, a fundamental challenge in mobile robotics, demands versatile policies to handle diverse environments. Classical methods leverage geometric solutions to minimize specific costs, offering adaptability to new scenarios but are prone to system errors due to their multi-modular design</span>
            
            <span class="abstract-full" style="display: none;">Visual navigation, a fundamental challenge in mobile robotics, demands versatile policies to handle diverse environments. Classical methods leverage geometric solutions to minimize specific costs, offering adaptability to new scenarios but are prone to system errors due to their multi-modular design and reliance on hand-crafted rules. Learning-based methods, while achieving high planning success rates, face difficulties in generalizing to unseen environments beyond the training data and often require extensive training. To address these limitations, we propose a hybrid approach that combines the strengths of learning-based methods and classical approaches for RGB-only visual navigation. Our method first trains a conditional diffusion model on diverse path-RGB observation pairs. During inference, it integrates the gradients of differentiable scene-specific and task-level costs, guiding the diffusion model to generate valid paths that meet the constraints. This approach alleviates the need for retraining, offering a plug-and-play solution. Extensive experiments in both indoor and outdoor settings, across simulated and real-world scenarios, demonstrate zero-shot transfer capability of our approach, achieving higher success rates and fewer collisions compared to baseline methods. Code will be released at https://github.com/SYSU-RoboticsLab/NaviD.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 8.9 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3121
            </span>
            <a href="https://arxiv.org/abs/2504.08779" target="_blank" rel="noopener noreferrer">Can AI Master Construction Management (CM)? Benchmarking State-of-the-Art Large Language Models on CM Certification Exams</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ruoxin Xiong, Yanyu Wang, Suat Gunhan, Yimin Zhu, Charles Berryman | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The growing complexity of construction management (CM) projects, coupled with challenges such as strict regulatory requirements and labor shortages, requires specialized analytical tools that streamline project workflow and enhance performance. Although large language models (LLMs) have demonstrated</span>
            
            <span class="abstract-full" style="display: none;">The growing complexity of construction management (CM) projects, coupled with challenges such as strict regulatory requirements and labor shortages, requires specialized analytical tools that streamline project workflow and enhance performance. Although large language models (LLMs) have demonstrated exceptional performance in general reasoning tasks, their effectiveness in tackling CM-specific challenges, such as precise quantitative analysis and regulatory interpretation, remains inadequately explored. To bridge this gap, this study introduces CMExamSet, a comprehensive benchmarking dataset comprising 689 authentic multiple-choice questions sourced from four nationally accredited CM certification exams. Our zero-shot evaluation assesses overall accuracy, subject areas (e.g., construction safety), reasoning complexity (single-step and multi-step), and question formats (text-only, figure-referenced, and table-referenced). The results indicate that GPT-4o and Claude 3.7 surpass typical human pass thresholds (70%), with average accuracies of 82% and 83%, respectively. Additionally, both models performed better on single-step tasks, with accuracies of 85.7% (GPT-4o) and 86.7% (Claude 3.7). Multi-step tasks were more challenging, reducing performance to 76.5% and 77.6%, respectively. Furthermore, both LLMs show significant limitations on figure-referenced questions, with accuracies dropping to approximately 40%. Our error pattern analysis further reveals that conceptual misunderstandings are the most common (44.4% and 47.9%), underscoring the need for enhanced domain-specific reasoning models. These findings underscore the potential of LLMs as valuable supplementary analytical tools in CM, while highlighting the need for domain-specific refinements and sustained human oversight in complex decision making.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.1 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.314
            </span>
            <a href="https://arxiv.org/abs/2504.08418" target="_blank" rel="noopener noreferrer">seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yilin Ning, Yian Ma, Mingxuan Liu, Xin Li, Nan Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Fairness in artificial intelligence (AI) prediction models is increasingly emphasized to support responsible adoption in high-stakes domains such as health care and criminal justice. Guidelines and implementation frameworks highlight the importance of both predictive accuracy and equitable outcomes.</span>
            
            <span class="abstract-full" style="display: none;">Fairness in artificial intelligence (AI) prediction models is increasingly emphasized to support responsible adoption in high-stakes domains such as health care and criminal justice. Guidelines and implementation frameworks highlight the importance of both predictive accuracy and equitable outcomes. However, current fairness toolkits often evaluate classification performance disparities in isolation, with limited attention to other critical aspects such as calibration. To address these gaps, we present seeBias, an R package for comprehensive evaluation of model fairness and predictive performance. seeBias offers an integrated evaluation across classification, calibration, and other performance domains, providing a more complete view of model behavior. It includes customizable visualizations to support transparent reporting and responsible AI implementation. Using public datasets from criminal justice and healthcare, we demonstrate how seeBias supports fairness evaluations, and uncovers disparities that conventional fairness metrics may overlook. The R package is available on GitHub, and a Python version is under development.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.4 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3192
            </span>
            <a href="https://arxiv.org/abs/2501.07015" target="_blank" rel="noopener noreferrer">SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yue Hu, Rong Liu, Meida Chen, Peter Beerel, Andrew Feng | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Achieving high-fidelity 3D reconstruction from monocular video remains challenging due to the inherent limitations of traditional methods like Structure-from-Motion (SfM) and monocular SLAM in accurately capturing scene details. While differentiable rendering techniques such as Neural Radiance Field</span>
            
            <span class="abstract-full" style="display: none;">Achieving high-fidelity 3D reconstruction from monocular video remains challenging due to the inherent limitations of traditional methods like Structure-from-Motion (SfM) and monocular SLAM in accurately capturing scene details. While differentiable rendering techniques such as Neural Radiance Fields (NeRF) address some of these challenges, their high computational costs make them unsuitable for real-time applications. Additionally, existing 3D Gaussian Splatting (3DGS) methods often focus on photometric consistency, neglecting geometric accuracy and failing to exploit SLAM's dynamic depth and pose updates for scene refinement. We propose a framework integrating dense SLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach introduces SLAM-Informed Adaptive Densification, which dynamically updates and densifies the Gaussian model by leveraging dense point clouds from SLAM. Additionally, we incorporate Geometry-Guided Optimization, which combines edge-aware geometric constraints and photometric consistency to jointly optimize the appearance and geometry of the 3DGS scene representation, enabling detailed and accurate SLAM mapping reconstruction. Experiments on the Replica and TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving state-of-the-art results among monocular systems. Specifically, our method achieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica, representing improvements of 10.7%, 6.4%, and 49.4%, respectively, over the previous SOTA. On TUM-RGBD, our method outperforms the closest baseline by 10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the potential of our framework in bridging the gap between photometric and geometric dense 3D scene representations, paving the way for practical and efficient monocular dense reconstruction.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.5 -->
                
            <!-- LLMs: 8.4 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.33
            </span>
            <a href="https://arxiv.org/abs/2404.19656" target="_blank" rel="noopener noreferrer">Towards Scenario- and Capability-Driven Dataset Development and Evaluation: An Approach in the Context of Mapless Automated Driving</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Felix Gr\"un, Marcus Nolte, Markus Maurer | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The foundational role of datasets in defining the capabilities of deep learning models has led to their rapid proliferation. At the same time, published research focusing on the process of dataset development for environment perception in automated driving has been scarce, thereby reducing the appli</span>
            
            <span class="abstract-full" style="display: none;">The foundational role of datasets in defining the capabilities of deep learning models has led to their rapid proliferation. At the same time, published research focusing on the process of dataset development for environment perception in automated driving has been scarce, thereby reducing the applicability of openly available datasets and impeding the development of effective environment perception systems. Sensor-based, mapless automated driving is one of the contexts where this limitation is evident. While leveraging real-time sensor data, instead of pre-defined HD maps promises enhanced adaptability and safety by effectively navigating unexpected environmental changes, it also increases the demands on the scope and complexity of the information provided by the perception system.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.5 -->
                
            <!-- LLMs: 6.4 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Math: 2.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3318
            </span>
            <a href="https://arxiv.org/abs/2504.09258" target="_blank" rel="noopener noreferrer">PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jianyu Wu, Hao Yang, Xinhua Zeng, Guibing He, Zhiyu Chen, Zihui Li, Xiaochuan Zhang, Yangyang Ma, Run Fang, Yang Liu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The diagnosis of pathological images is often limited by expert availability and regional disparities, highlighting the importance of automated diagnosis using Vision-Language Models (VLMs). Traditional multimodal models typically emphasize outcomes over the reasoning process, compromising the relia</span>
            
            <span class="abstract-full" style="display: none;">The diagnosis of pathological images is often limited by expert availability and regional disparities, highlighting the importance of automated diagnosis using Vision-Language Models (VLMs). Traditional multimodal models typically emphasize outcomes over the reasoning process, compromising the reliability of clinical decisions. To address the weak reasoning abilities and lack of supervised processes in pathological VLMs, we have innovatively proposed PathVLM-R1, a visual language model designed specifically for pathological images. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its performance for pathological tasks through meticulously designed post-training strategies. Firstly, we conduct supervised fine-tuning guided by pathological data to imbue the model with foundational pathological knowledge, forming a new pathological base model. Subsequently, we introduce Group Relative Policy Optimization (GRPO) and propose a dual reward-driven reinforcement learning optimization, ensuring strict constraint on logical supervision of the reasoning process and accuracy of results via cross-modal process reward and outcome accuracy reward. In the pathological image question-answering tasks, the testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy compared to baseline methods, and it demonstrated superior performance compared to the Qwen2.5-VL-32B version despite having a significantly smaller parameter size. Furthermore, in out-domain data evaluation involving four medical imaging modalities: Computed Tomography (CT), dermoscopy, fundus photography, and Optical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance improved by an average of 17.3% compared to traditional SFT methods. These results clearly indicate that PathVLM-R1 not only enhances accuracy but also possesses broad applicability and expansion potential.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.0 -->
                
            <!-- LLMs: 8.7 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3336
            </span>
            <a href="https://arxiv.org/abs/2504.05045" target="_blank" rel="noopener noreferrer">Attention-Augmented Inverse Reinforcement Learning with Graph Convolutions for Multi-Agent Task Allocation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Huilin Yin, Zhikun Yang, Linchuan Zhang, Daniel Watzenig | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3769
            </span>
            <a href="https://arxiv.org/abs/2403.18174" target="_blank" rel="noopener noreferrer">First-order (coarse) correlated equilibria in non-concave games</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mete \c{S}eref Ahunbay | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We investigate first-order notions of correlated equilibria; distributions of actions for smooth, potentially non-concave games such that players do not incur any regret against small modifications to their strategies along a set of continuous vector fields. We define two such notions, based on loca</span>
            
            <span class="abstract-full" style="display: none;">We investigate first-order notions of correlated equilibria; distributions of actions for smooth, potentially non-concave games such that players do not incur any regret against small modifications to their strategies along a set of continuous vector fields. We define two such notions, based on local deviations and on stationarity of the distribution, and identify the notion of coarseness as the setting where the strategy modifications are prescribed by gradient fields. For coarse equilibria, we prove that online (projected) gradient decent has a universal approximation property for both variants of equilibrium. In the non-coarse setting, we inspect the problem of computing first-order correlated equilibria through the lens of both recent work based on the framework of ``fixed points in expectation'', and also via the classical framework of Lagrangian hedging, with the goal of identifying tractable instances with additional convergence guarantees. Finally, we study the primal-dual framework to our notion of first-order equilibria. For coarse equilibria defined by a family of functions, we find that a dual bound on the worst-case expectation of a performance metric takes the form of a generalised Lyapunov function for the dynamics of the game. Specifically, usual primal-dual price of anarchy analysis for coarse correlated equilibria as well as the smoothness framework of Roughgarden are both equivalent to a problem of general Lyapunov function estimation. For non-coarse equilibria, we instead observe a vector field fit problem for the gradient dynamics of the game. These follow from containment results in normal form games, and our work overall provides insights on how to tighten equilibrium analysis for gradient-based learning dynamics, as well as delineating notions of first-order equilibrium that may rule out cycling behaviour versus those that do not.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.4 -->
                
            <!-- LLMs: 6.2 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Math: 3.0 -->
                
            <!-- Reinforcement Learning: 2.4 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.7 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3844
            </span>
            <a href="https://arxiv.org/abs/2504.09506" target="_blank" rel="noopener noreferrer">Pillar-Voxel Fusion Network for 3D Object Detection in Airborne Hyperspectral Point Clouds</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yanze Jiang, Yanfeng Gu, Xian Li | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Hyperspectral point clouds (HPCs) can simultaneously characterize 3D spatial and spectral information of ground objects, offering excellent 3D perception and target recognition capabilities. Current approaches for generating HPCs often involve fusion techniques with hyperspectral images and LiDAR po</span>
            
            <span class="abstract-full" style="display: none;">Hyperspectral point clouds (HPCs) can simultaneously characterize 3D spatial and spectral information of ground objects, offering excellent 3D perception and target recognition capabilities. Current approaches for generating HPCs often involve fusion techniques with hyperspectral images and LiDAR point clouds, which inevitably lead to geometric-spectral distortions due to fusion errors and obstacle occlusions. These adverse effects limit their performance in downstream fine-grained tasks across multiple scenarios, particularly in airborne applications. To address these issues, we propose PiV-AHPC, a 3D object detection network for airborne HPCs. To the best of our knowledge, this is the first attempt at this HPCs task. Specifically, we first develop a pillar-voxel dual-branch encoder, where the former captures spectral and vertical structural features from HPCs to overcome spectral distortion, while the latter emphasizes extracting accurate 3D spatial features from point clouds. A multi-level feature fusion mechanism is devised to enhance information interaction between the two branches, achieving neighborhood feature alignment and channel-adaptive selection, thereby organically integrating heterogeneous features and mitigating geometric distortion. Extensive experiments on two airborne HPCs datasets demonstrate that PiV-AHPC possesses state-of-the-art detection performance and high generalization capability.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3981
            </span>
            <a href="https://arxiv.org/abs/2409.16561" target="_blank" rel="noopener noreferrer">Supporting Co-Adaptive Machine Teaching through Human Concept Learning and Cognitive Theories</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Simret Araya Gebreegziabher, Yukun Yang, Elena L. Glassman, Toby Jia-Jun Li | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">An important challenge in interactive machine learning, particularly in subjective or ambiguous domains, is fostering bi-directional alignment between humans and models. Users teach models their concept definition through data labeling, while refining their own understandings throughout the process.</span>
            
            <span class="abstract-full" style="display: none;">An important challenge in interactive machine learning, particularly in subjective or ambiguous domains, is fostering bi-directional alignment between humans and models. Users teach models their concept definition through data labeling, while refining their own understandings throughout the process. To facilitate this, we introduce MOCHA, an interactive machine learning tool informed by two theories of human concept learning and cognition. First, it utilizes a neuro-symbolic pipeline to support Variation Theory-based counterfactual data generation. By asking users to annotate counterexamples that are syntactically and semantically similar to already-annotated data but predicted to have different labels, the system can learn more effectively while helping users understand the model and reflect on their own label definitions. Second, MOCHA uses Structural Alignment Theory to present groups of counterexamples, helping users comprehend alignable differences between data items and annotate them in batch. We validated MOCHA's effectiveness and usability through a lab study with 18 participants.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4253
            </span>
            <a href="https://arxiv.org/abs/2501.02406" target="_blank" rel="noopener noreferrer">Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tara Radvand, Mojtaba Abdolmaleki, Mohamed Mostagir, Ambuj Tewari | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Verifying the provenance of content is crucial to the function of many organizations, e.g., educational institutions, social media platforms, firms, etc. This problem is becoming increasingly challenging as text generated by Large Language Models (LLMs) becomes almost indistinguishable from human-ge</span>
            
            <span class="abstract-full" style="display: none;">Verifying the provenance of content is crucial to the function of many organizations, e.g., educational institutions, social media platforms, firms, etc. This problem is becoming increasingly challenging as text generated by Large Language Models (LLMs) becomes almost indistinguishable from human-generated content. In addition, many institutions utilize in-house LLMs and want to ensure that external, non-sanctioned LLMs do not produce content within the institution. We answer the following question: Given a piece of text, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can be a human)? We model LLM-generated text as a sequential stochastic process with complete dependence on history and design zero-shot statistical tests to distinguish between (i) the text generated by two different sets of LLMs $A$ (in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and human-generated texts. We prove that our tests' type I and type II errors decrease exponentially as text length increases. For designing our tests for a given string, we demonstrate that if the string is generated by the evaluator model $A$, the log-perplexity of the string under $A$ converges to the average entropy of the string under $A$, except with an exponentially small probability in the string length. We also show that if $B$ generates the text, except with an exponentially small probability in string length, the log-perplexity of the string under $A$ converges to the average cross-entropy of $B$ and $A$. For our experiments: First, we present experiments using open-source LLMs to support our theoretical results, and then we provide experiments in a black-box setting with adversarial attacks. Practically, our work enables guaranteed finding of the origin of harmful or false LLM-generated text, which can be useful for combating misinformation and compliance with emerging AI regulations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.3 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Math: 2.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.435
            </span>
            <a href="https://arxiv.org/abs/2504.08335" target="_blank" rel="noopener noreferrer">Entropic bounds for conditionally Gaussian vectors and applications to neural networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lucia Celli, Giovanni Peccati | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Using entropic inequalities from information theory, we provide new bounds on the total variation and 2-Wasserstein distances between a conditionally Gaussian law and a Gaussian law with invertible covariance matrix. We apply our results to quantify the speed of convergence to Gaussian of a randomly</span>
            
            <span class="abstract-full" style="display: none;">Using entropic inequalities from information theory, we provide new bounds on the total variation and 2-Wasserstein distances between a conditionally Gaussian law and a Gaussian law with invertible covariance matrix. We apply our results to quantify the speed of convergence to Gaussian of a randomly initialized fully connected neural network and its derivatives - evaluated in a finite number of inputs - when the initialization is Gaussian and the sizes of the inner layers diverge to infinity. Our results require mild assumptions on the activation function, and allow one to recover optimal rates of convergence in a variety of distances, thus improving and extending the findings of Basteri and Trevisan (2023), Favaro et al. (2023), Trevisan (2024) and Apollonio et al. (2024). One of our main tools are the quantitative cumulant estimates established in Hanin (2024). As an illustration, we apply our results to bound the total variation distance between the Bayesian posterior law of the neural network and its derivatives, and the posterior law of the corresponding Gaussian limit: this yields quantitative versions of a posterior CLT by Hron et al. (2022), and extends several estimates by Trevisan (2024) to the total variation metric.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.1 -->
                
            <!-- LLMs: 6.4 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4469
            </span>
            <a href="https://arxiv.org/abs/2504.09769" target="_blank" rel="noopener noreferrer">Identification of Community Structures in Networks Employing a Modified Divisive Algorithm</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ghazal Ghajari, Hooshang Jazayeri-Rad, Mashalla Abbasi Dezfooli | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In numerous networks, it is vital to identify communities consisting of closely joined groups of individuals. Such communities often reveal the role of the networks or primary properties of the individuals. In this perspective, Newman and Girvan proposed a modularity score (Q) for quantifying the po</span>
            
            <span class="abstract-full" style="display: none;">In numerous networks, it is vital to identify communities consisting of closely joined groups of individuals. Such communities often reveal the role of the networks or primary properties of the individuals. In this perspective, Newman and Girvan proposed a modularity score (Q) for quantifying the power of community structure and measuring the appropriateness of a division. The Q function has newly become a significant standard. In this paper, the strengths of the Q score and another technique known as the divisive algorithm are combined to enhance the efficiently of the identification of communities from a network. To achieve that goal, we have developed a new algorithm. The simulation results indicated that our algorithm achieved a division with a slightly higher Q score against some conventional methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.6 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- GNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4676
            </span>
            <a href="https://arxiv.org/abs/2504.08713" target="_blank" rel="noopener noreferrer">ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sahil Sethi, David Chen, Thomas Statchen, Michael C. Burkhart, Nipun Bhandari, Bashar Ramadan, Brett Beaulieu-Jones | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Deep learning-based electrocardiogram (ECG) classification has shown impressive performance but clinical adoption has been slowed by the lack of transparent and faithful explanations. Post hoc methods such as saliency maps may fail to reflect a model's true decision process. Prototype-based reasonin</span>
            
            <span class="abstract-full" style="display: none;">Deep learning-based electrocardiogram (ECG) classification has shown impressive performance but clinical adoption has been slowed by the lack of transparent and faithful explanations. Post hoc methods such as saliency maps may fail to reflect a model's true decision process. Prototype-based reasoning offers a more transparent alternative by grounding decisions in similarity to learned representations of real ECG segments, enabling faithful, case-based explanations. We introduce ProtoECGNet, a prototype-based deep learning model for interpretable, multi-label ECG classification. ProtoECGNet employs a structured, multi-branch architecture that reflects clinical interpretation workflows: it integrates a 1D CNN with global prototypes for rhythm classification, a 2D CNN with time-localized prototypes for morphology-based reasoning, and a 2D CNN with global prototypes for diffuse abnormalities. Each branch is trained with a prototype loss designed for multi-label learning, combining clustering, separation, diversity, and a novel contrastive loss that encourages appropriate separation between prototypes of unrelated classes while allowing clustering for frequently co-occurring diagnoses. We evaluate ProtoECGNet on all 71 diagnostic labels from the PTB-XL dataset, demonstrating competitive performance relative to state-of-the-art black-box models while providing structured, case-based explanations. To assess prototype quality, we conduct a structured clinician review of the final model's projected prototypes, finding that they are rated as representative and clear. ProtoECGNet shows that prototype learning can be effectively scaled to complex, multi-label time-series classification, offering a practical path toward transparent and trustworthy deep learning models for clinical decision support.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.9 -->
                
            <!-- LLMs: 9.3 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4743
            </span>
            <a href="https://arxiv.org/abs/2504.08192" target="_blank" rel="noopener noreferrer">SAEs $\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Aashiq Muhamed, Jacopo Bonato, Mona Diab, Virginia Smith | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability </span>
            
            <span class="abstract-full" style="display: none;">Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce $\textbf{Dynamic DAE Guardrails}$ (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5419
            </span>
            <a href="https://arxiv.org/abs/2504.10370" target="_blank" rel="noopener noreferrer">Further Comments on Yablo's Construction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Karl Schlechta | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We continue our analysis of Yablo's coding of the liar paradox by infinite acyclic graphs. The present notes are based on and continue the author's previous results on the problem. In particular, our approach is often more systematic than before.</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.5 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5499
            </span>
            <a href="https://arxiv.org/abs/2209.03440" target="_blank" rel="noopener noreferrer">Deep Learning-Based Automatic Diagnosis System for Developmental Dysplasia of the Hip</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yang Li, Leo Yan Li-Han, Hua Tian | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Objective: The clinical diagnosis of developmental dysplasia of the hip (DDH) typically involves manually measuring key radiological angles -- Center-Edge (CE), Tonnis, and Sharp angles -- from pelvic radiographs, a process that is time-consuming and susceptible to variability. This study aims to de</span>
            
            <span class="abstract-full" style="display: none;">Objective: The clinical diagnosis of developmental dysplasia of the hip (DDH) typically involves manually measuring key radiological angles -- Center-Edge (CE), Tonnis, and Sharp angles -- from pelvic radiographs, a process that is time-consuming and susceptible to variability. This study aims to develop an automated system that integrates these measurements to enhance the accuracy and consistency of DDH diagnosis.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5739
            </span>
            <a href="https://arxiv.org/abs/2407.19546" target="_blank" rel="noopener noreferrer">MMCLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Biao Wu, Yutong Xie, Zeyu Zhang, Minh Hieu Phan, Qi Chen, Ling Chen, Qi Wu | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Vision-and-language pretraining (VLP) in the medical field utilizes contrastive learning on image-text pairs to achieve effective transfer across tasks. Yet, current VLP approaches with the masked modeling strategy face two challenges when applied to the medical domain. First, current models struggl</span>
            
            <span class="abstract-full" style="display: none;">Vision-and-language pretraining (VLP) in the medical field utilizes contrastive learning on image-text pairs to achieve effective transfer across tasks. Yet, current VLP approaches with the masked modeling strategy face two challenges when applied to the medical domain. First, current models struggle to accurately reconstruct key pathological features due to the scarcity of medical data. Second, most methods only adopt either paired image-text or image-only data, failing to exploit the combination of both paired and unpaired data. To this end, this paper proposes the MMCLIP (Masked Medical Contrastive Language-Image Pre-Training) framework to enhance pathological learning and feature learning via unpaired data. First, we introduce the attention-masked image modeling (AttMIM) and entity-driven masked language modeling module (EntMLM), which learns to reconstruct pathological visual and textual tokens via multi-modal feature interaction, thus improving medical-enhanced features. The AttMIM module masks a portion of the image features that are highly responsive to textual features. This allows MMCLIP to improve the reconstruction of highly similar image data in medicine efficiency. Second, our MMCLIP capitalizes unpaired data to enhance multimodal learning by introducing disease-kind prompts. The experimental results show that MMCLIP achieves SOTA for zero-shot and fine-tuning classification performance on five datasets. Our code will be available at https://github.com/White65534/MMCLIP.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.7 -->
                
            <!-- LLMs: 8.6 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.575
            </span>
            <a href="https://arxiv.org/abs/2504.08645" target="_blank" rel="noopener noreferrer">Title block detection and information extraction for enhanced building drawings search</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alessio Lombardi (Buro Happold, London), Li Duan (Birmingham City University), Ahmed Elnagar (Buro Happold, London), Ahmed Zaalouk (Birmingham City University), Khalid Ismail (Birmingham City University), Edlira Vakaj (Birmingham City University) | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The architecture, engineering, and construction (AEC) industry still heavily relies on information stored in drawings for building construction, maintenance, compliance and error checks. However, information extraction (IE) from building drawings is often time-consuming and costly, especially when d</span>
            
            <span class="abstract-full" style="display: none;">The architecture, engineering, and construction (AEC) industry still heavily relies on information stored in drawings for building construction, maintenance, compliance and error checks. However, information extraction (IE) from building drawings is often time-consuming and costly, especially when dealing with historical buildings. Drawing search can be simplified by leveraging the information stored in the title block portion of the drawing, which can be seen as drawing metadata. However, title block IE can be complex especially when dealing with historical drawings which do not follow existing standards for uniformity. This work performs a comparison of existing methods for this kind of IE task, and then proposes a novel title block detection and IE pipeline which outperforms existing methods, in particular when dealing with complex, noisy historical drawings. The pipeline is obtained by combining a lightweight Convolutional Neural Network and GPT-4o, the proposed inference pipeline detects building engineering title blocks with high accuracy, and then extract structured drawing metadata from the title blocks, which can be used for drawing search, filtering and grouping. The work demonstrates high accuracy and efficiency in IE for both vector (CAD) and hand-drawn (historical) drawings. A user interface (UI) that leverages the extracted metadata for drawing search is established and deployed on real projects, which demonstrates significant time savings. Additionally, an extensible domain-expert-annotated dataset for title block detection is developed, via an efficient AEC-friendly annotation workflow that lays the foundation for future work.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.7 -->
                
            <!-- LLMs: 8.9 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.58
            </span>
            <a href="https://arxiv.org/abs/2504.09468" target="_blank" rel="noopener noreferrer">Incubation and Beyond: A Comparative Analysis of ASF Projects Sustainability Impacts on Software Quality</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Adam Alami, Steffan Klockmann, Lasse Rehder S{\o}rensen, Ra\'ul Pardo, Johan Lin\r{a}ker | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Free and Open Source Software (FOSS) communities' sustainability, meaning to remain operational without signs of weakening or interruptions to its development, is fundamental for the resilience and continuity of society's digital infrastructure. Many digital services and products either leverage or </span>
            
            <span class="abstract-full" style="display: none;">Free and Open Source Software (FOSS) communities' sustainability, meaning to remain operational without signs of weakening or interruptions to its development, is fundamental for the resilience and continuity of society's digital infrastructure. Many digital services and products either leverage or entirely rely on FOSS in their software stack. FOSS sustainability is a multifaceted concept, and the impact of its decline on community products is less known. In this study, we sought to understand how the different aspects of FOSS sustainability impact software quality from a life-cycle perspective. Specifically, we investigate whether and how support and incubation of FOSS projects or bypassing incubation correlate with software quality outcomes. We selected 342 FOSS projects from the Apache Software Foundation that have either graduated, retired, or bypassed their incubator program. We used 16 sustainability metrics to examine their impact on eight software quality metrics. Using Bayesian data analysis, we found that our selected sustainability metrics exhibit distinct relationships with software quality across different project trajectories. Graduated projects showed the strongest sustainability-software quality (SWQ) relationship, both during and post-incubation. In contrast, retired projects showed weaker relationships, despite receiving similar governance support. Bypassed projects, while not outperforming graduated ones, showed comparable sustainability-SWQ relationships. While structured incubation strengthens sustainability and SWQ in graduated projects, retired projects struggle to maintain strong sustainability-SWQ relationships, indicating that additional factors internal and specific to projects influence sustainability. This effect was evident among bypassed projects; their self-reliant sustainability practices yielded stronger sustainability-SWQ compared to the retired ones.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6246
            </span>
            <a href="https://arxiv.org/abs/2502.08576" target="_blank" rel="noopener noreferrer">Mapping the Landscape of Generative AI in Network Monitoring and Management</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Giampaolo Bovenzi, Francesco Cerasuolo, Domenico Ciuonzo, Davide Di Monda, Idio Guarino, Antonio Montieri, Valerio Persico, Antonio Pescap\`e | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as</span>
            
            <span class="abstract-full" style="display: none;">Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as well as challenges and opportunities. We discuss how network traffic generation and classification, network intrusion detection, networked system log analysis, and network digital assistance can benefit from the use of GenAI models. Additionally, we provide an overview of the available GenAI models, datasets for large-scale training phases, and platforms for the development of such models. Finally, we discuss research directions that potentially mitigate the roadblocks to the adoption of GenAI for network monitoring and management. Our investigation aims to map the current landscape and pave the way for future research in leveraging GenAI for network monitoring and management.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.7042
            </span>
            <a href="https://arxiv.org/abs/2504.09714" target="_blank" rel="noopener noreferrer">Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ay\c{s}e Aysu Cengiz, Ahmet Kaan Sever, Elif Ecem \"Um\"utl\"u, Naime \c{S}eyma Erdem, Burak Aytan, B\"u\c{s}ra Tufan, Abdullah Topraksoy, Esra Dar{\i}c{\i}, Cagri Toraman | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The reliance on translated or adapted datasets from English or multilingual resources introduces challenges regarding linguistic and cultural suitability. This study addresses the need for robust and culturally appropriate benchmarks by evaluating the quality of 17 commonly used Turkish benchmark da</span>
            
            <span class="abstract-full" style="display: none;">The reliance on translated or adapted datasets from English or multilingual resources introduces challenges regarding linguistic and cultural suitability. This study addresses the need for robust and culturally appropriate benchmarks by evaluating the quality of 17 commonly used Turkish benchmark datasets. Using a comprehensive framework that assesses six criteria, both human and LLM-judge annotators provide detailed evaluations to identify dataset strengths and shortcomings.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 9.2 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.7333
            </span>
            <a href="https://arxiv.org/abs/2504.08310" target="_blank" rel="noopener noreferrer">DeQompile: quantum circuit decompilation using genetic programming for explainable quantum architecture search</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shubing Xie, Aritra Sarkar, Sebastian Feld | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Demonstrating quantum advantage using conventional quantum algorithms remains challenging on current noisy gate-based quantum computers. Automated quantum circuit synthesis via quantum machine learning has emerged as a promising solution, employing trainable parametric quantum circuits to alleviate </span>
            
            <span class="abstract-full" style="display: none;">Demonstrating quantum advantage using conventional quantum algorithms remains challenging on current noisy gate-based quantum computers. Automated quantum circuit synthesis via quantum machine learning has emerged as a promising solution, employing trainable parametric quantum circuits to alleviate this. The circuit ansatz in these solutions is often designed through reinforcement learning-based quantum architecture search when the domain knowledge of the problem and hardware are not effective. However, the interpretability of these synthesized circuits remains a significant bottleneck, limiting their scalability and applicability across diverse problem domains.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.8 -->
                
            <!-- LLMs: 7.1 -->
                
            <!-- Quantum Computing: 6.6 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.1069
            </span>
            <a href="https://arxiv.org/abs/2504.10151" target="_blank" rel="noopener noreferrer">Continual learning for rotating machinery fault diagnosis with cross-domain environmental and operational variations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Diogo Risca, Afonso Louren\c{c}o, Goreti Marreiros | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Although numerous machine learning models exist to detect issues like rolling bearing strain and deformation, typically caused by improper mounting, overloading, or poor lubrication, these models often struggle to isolate faults from the noise of real-world operational and environmental variability.</span>
            
            <span class="abstract-full" style="display: none;">Although numerous machine learning models exist to detect issues like rolling bearing strain and deformation, typically caused by improper mounting, overloading, or poor lubrication, these models often struggle to isolate faults from the noise of real-world operational and environmental variability. Conditions such as variable loads, high temperatures, stress, and rotational speeds can mask early signs of failure, making reliable detection challenging. To address these limitations, this work proposes a continual deep learning approach capable of learning across domains that share underlying structure over time. This approach goes beyond traditional accuracy metrics by addressing four second-order challenges: catastrophic forgetting (where new learning overwrites past knowledge), lack of plasticity (where models fail to adapt to new data), forward transfer (using past knowledge to improve future learning), and backward transfer (refining past knowledge with insights from new domains). The method comprises a feature generator and domain-specific classifiers, allowing capacity to grow as new domains emerge with minimal interference, while an experience replay mechanism selectively revisits prior domains to mitigate forgetting. Moreover, nonlinear dependencies across domains are exploited by prioritizing replay from those with the highest prior errors, refining models based on most informative past experiences. Experiments show high average domain accuracy (up to 88.96%), with forgetting measures as low as .0027 across non-stationary class-incremental environments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.1098
            </span>
            <a href="https://arxiv.org/abs/2504.07967" target="_blank" rel="noopener noreferrer">Double Directional Wireless Channel Generation: A Statistics-Informed Generative Approach</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Md-Ferdous Pervej, Patel Pratik, Koushik Manjunatha, Prasad Shamain, Andreas F. Molisch | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Channel models that represent various operating conditions a communication system might experience are important for design and standardization of any communication system. While statistical channel models have long dominated this space, machine learning (ML) is becoming a popular alternative approa</span>
            
            <span class="abstract-full" style="display: none;">Channel models that represent various operating conditions a communication system might experience are important for design and standardization of any communication system. While statistical channel models have long dominated this space, machine learning (ML) is becoming a popular alternative approach. However, existing approaches have mostly focused on predictive solutions to match instantaneous channel realizations. Other solutions have focused on pathloss modeling, while double-directional (DD) channel representation is needed for a complete description. Motivated by this, we (a) develop a generative solution that uses a hybrid Transformer (hTransformer) model with a low-rank projected attention calculation mechanism and a bi-directional long short-term memory (BiLSTM) layer to generate complete DD channel information and (b) design a domain-knowledge-informed training method to match the generated and true channel realizations' statistics. Our extensive simulation results validate that the generated samples' statistics closely align with the true statistics while mostly outperforming the performance of existing predictive approaches.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.0 -->
                
            <!-- Medicine: 9.0 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.1564
            </span>
            <a href="https://arxiv.org/abs/2504.09657" target="_blank" rel="noopener noreferrer">Nonlinear Online Optimization for Vehicle-Home-Grid Integration including Household Load Prediction and Battery Degradation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Francesco Popolizio, Torsten Wik, Chih Feng Lee, Changfu Zou | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper investigates the economic impact of vehicle-home-grid integration, by proposing an online energy management algorithm that optimizes energy flows between an electric vehicle (EV), a household, and the electrical grid. The algorithm leverages vehicle-to-home (V2H) for self-consumption and </span>
            
            <span class="abstract-full" style="display: none;">This paper investigates the economic impact of vehicle-home-grid integration, by proposing an online energy management algorithm that optimizes energy flows between an electric vehicle (EV), a household, and the electrical grid. The algorithm leverages vehicle-to-home (V2H) for self-consumption and vehicle-to-grid (V2G) for energy trading, adapting to real-time conditions through a hybrid long short-term memory (LSTM) neural network for accurate household load prediction, alongside a comprehensive nonlinear battery degradation model accounting for both cycle and calendar aging. Simulation results reveal significant economic advantages: compared to smart unidirectional charging, the proposed method yields an annual economic benefit of up to EUR 3046.81, despite a modest 1.96% increase in battery degradation. Even under unfavorable market conditions, where V2G energy selling generates no revenue, V2H alone ensures yearly savings of EUR 425.48. A systematic sensitivity analysis investigates how variations in battery capacity, household load, and price ratios affect economic outcomes, confirming the consistent benefits of bidirectional energy exchange. These findings highlight the potential of EVs as active energy nodes, enabling sustainable energy management and cost-effective battery usage in real-world conditions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.1856
            </span>
            <a href="https://arxiv.org/abs/2504.07995" target="_blank" rel="noopener noreferrer">SafeChat: A Framework for Building Trustworthy Collaborative Assistants and a Case Study of its Usefulness</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Biplav Srivastava, Kausik Lakkaraju, Nitin Gupta, Vansh Nagpal, Bharath C. Muppasani, Sara E. Jones | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Collaborative assistants, or chatbots, are data-driven decision support systems that enable natural interaction for task completion. While they can meet critical needs in modern society, concerns about their reliability and trustworthiness persist. In particular, Large Language Model (LLM)-based cha</span>
            
            <span class="abstract-full" style="display: none;">Collaborative assistants, or chatbots, are data-driven decision support systems that enable natural interaction for task completion. While they can meet critical needs in modern society, concerns about their reliability and trustworthiness persist. In particular, Large Language Model (LLM)-based chatbots like ChatGPT, Gemini, and DeepSeek are becoming more accessible. However, such chatbots have limitations, including their inability to explain response generation, the risk of generating problematic content, the lack of standardized testing for reliability, and the need for deep AI expertise and extended development times. These issues make chatbots unsuitable for trust-sensitive applications like elections or healthcare. To address these concerns, we introduce SafeChat, a general architecture for building safe and trustworthy chatbots, with a focus on information retrieval use cases. Key features of SafeChat include: (a) safety, with a domain-agnostic design where responses are grounded and traceable to approved sources (provenance), and 'do-not-respond' strategies to prevent harmful answers; (b) usability, with automatic extractive summarization of long responses, traceable to their sources, and automated trust assessments to communicate expected chatbot behavior, such as sentiment; and (c) fast, scalable development, including a CSV-driven workflow, automated testing, and integration with various devices. We implemented SafeChat in an executable framework using the open-source chatbot platform Rasa. A case study demonstrates its application in building ElectionBot-SC, a chatbot designed to safely disseminate official election information. SafeChat is being used in many domains, validating its potential, and is available at: https://github.com/ai4society/trustworthy-chatbot.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 9.5 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.2751
            </span>
            <a href="https://arxiv.org/abs/2504.10249" target="_blank" rel="noopener noreferrer">Struggle First, Prompt Later: How Task Complexity Shapes Learning with GenAI-Assisted Pretesting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mahir Akgun, Sacip Toker | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This study examines the role of AI-assisted pretesting in enhancing learning outcomes, particularly when integrated with generative AI tools like ChatGPT. Pretesting, a learning strategy in which students attempt to answer questions or solve problems before receiving instruction, has been shown to i</span>
            
            <span class="abstract-full" style="display: none;">This study examines the role of AI-assisted pretesting in enhancing learning outcomes, particularly when integrated with generative AI tools like ChatGPT. Pretesting, a learning strategy in which students attempt to answer questions or solve problems before receiving instruction, has been shown to improve retention by activating prior knowledge. The adaptability and interactivity of AI-assisted pretesting introduce new opportunities for optimizing learning in digital environments. Across three experimental studies, we explored how pretesting strategies, task characteristics, and student motivation influence learning. Findings suggest that AI-assisted pretesting enhances learning outcomes, particularly for tasks requiring higher-order thinking. While adaptive AI-driven pretesting increased engagement, its benefits were most pronounced in complex, exploratory tasks rather than straightforward computational problems. These results highlight the importance of aligning pretesting strategies with task demands, demonstrating that AI can optimize learning when applied to tasks requiring deeper cognitive engagement. This research provides insights into how AI-assisted pretesting can be effectively integrated with generative AI tools to enhance both cognitive and motivational outcomes in learning environments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.3737
            </span>
            <a href="https://arxiv.org/abs/2504.09211" target="_blank" rel="noopener noreferrer">Accurate Diagnosis of Respiratory Viruses Using an Explainable Machine Learning with Mid-Infrared Biomolecular Fingerprinting of Nasopharyngeal Secretions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wenwen Zhang, Zhouzhuo Tang, Yingmei Feng, Xia Yu, Qi Jie Wang, Zhiping Lin | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Accurate identification of respiratory viruses (RVs) is critical for outbreak control and public health. This study presents a diagnostic system that combines Attenuated Total Reflectance Fourier Transform Infrared Spectroscopy (ATR-FTIR) from nasopharyngeal secretions with an explainable Rotary Pos</span>
            
            <span class="abstract-full" style="display: none;">Accurate identification of respiratory viruses (RVs) is critical for outbreak control and public health. This study presents a diagnostic system that combines Attenuated Total Reflectance Fourier Transform Infrared Spectroscopy (ATR-FTIR) from nasopharyngeal secretions with an explainable Rotary Position Embedding-Sparse Attention Transformer (RoPE-SAT) model to accurately identify multiple RVs within 10 minutes. Spectral data (4000-00 cm-1) were collected, and the bio-fingerprint region (1800-900 cm-1) was employed for analysis. Standard normal variate (SNV) normalization and second-order derivation were applied to reduce scattering and baseline drift. Gradient-weighted class activation mapping (Grad-CAM) was employed to generate saliency maps, highlighting spectral regions most relevant to classification and enhancing the interpretability of model outputs. Two independent cohorts from Beijing Youan Hospital, processed with different viral transport media (VTMs) and drying methods, were evaluated, with one including influenza B, SARS-CoV-2, and healthy controls, and the other including mycoplasma, SARS-CoV-2, and healthy controls. The model achieved sensitivity and specificity above 94.40% across both cohorts. By correlating model-selected infrared regions with known biomolecular signatures, we verified that the system effectively recognizes virus-specific spectral fingerprints, including lipids, Amide I, Amide II, Amide III, nucleic acids, and carbohydrates, and leverages their weighted contributions for accurate classification.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.7 -->
                
            <!-- LLMs: 9.6 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.3856
            </span>
            <a href="https://arxiv.org/abs/2504.09004" target="_blank" rel="noopener noreferrer">Exploring Families' Use and Mediation of Generative AI: A Multi-User Perspective</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shirley Zhang, Bengisu Cagiltay, Jennica Li, Dakota Sullivan, Bilge Mutlu, Heather Kirkorian, Kassem Fawaz | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Applications of Generative AI (GenAI), such as ChatGPT, have gained popularity among the public due to their ease of access, use, and support of educational and creative activities. Despite these benefits, GenAI poses unique risks for families, such as lacking sufficient safeguards tailored to prote</span>
            
            <span class="abstract-full" style="display: none;">Applications of Generative AI (GenAI), such as ChatGPT, have gained popularity among the public due to their ease of access, use, and support of educational and creative activities. Despite these benefits, GenAI poses unique risks for families, such as lacking sufficient safeguards tailored to protect children under 16 years of age and not offering parental control features. This study explores families' use and co-use of GenAI, the perceived risks and opportunities of ChatGPT, and how parents mediate their children's use of GenAI. Through semi-structured interviews with 12 families, we identified ways families used and mediated GenAI and factors that influenced parents' GenAI mediation strategies. We contextualize our findings with a modified model of family mediation strategies, drawing from previous family media and mediation frameworks. We provide insights for future research on family-GenAI interactions and highlight the need for more robust protective measures on GenAI platforms for families.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 8.9 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.5181
            </span>
            <a href="https://arxiv.org/abs/2504.08660" target="_blank" rel="noopener noreferrer">Channel Estimation by Infinite Width Convolutional Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mohammed Mallik, Guillaume Villemaud | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In wireless communications, estimation of channels in OFDM systems spans frequency and time, which relies on sparse collections of pilot data, posing an ill-posed inverse problem. Moreover, deep learning estimators require large amounts of training data, computational resources, and true channels to</span>
            
            <span class="abstract-full" style="display: none;">In wireless communications, estimation of channels in OFDM systems spans frequency and time, which relies on sparse collections of pilot data, posing an ill-posed inverse problem. Moreover, deep learning estimators require large amounts of training data, computational resources, and true channels to produce accurate channel estimates, which are not realistic. To address this, a convolutional neural tangent kernel (CNTK) is derived from an infinitely wide convolutional network whose training dynamics can be expressed by a closed-form equation. This CNTK is used to impute the target matrix and estimate the missing channel response using only the known values available at pilot locations. This is a promising solution for channel estimation that does not require a large training set. Numerical results on realistic channel datasets demonstrate that our strategy accurately estimates the channels without a large dataset and significantly outperforms deep learning methods in terms of speed, accuracy, and computational resources.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.6 -->
                
            <!-- LLMs: 8.9 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -10.902
            </span>
            <a href="https://arxiv.org/abs/2504.08410" target="_blank" rel="noopener noreferrer">PMNI: Pose-free Multi-view Normal Integration for Reflective and Textureless Surface Reconstruction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mingzhi Pei, Xu Cao, Xiangyi Wang, Heng Guo, Zhanyu Ma | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Reflective and textureless surfaces remain a challenge in multi-view 3D reconstruction.Both camera pose calibration and shape reconstruction often fail due to insufficient or unreliable cross-view visual features. To address these issues, we present PMNI (Pose-free Multi-view Normal Integration), a </span>
            
            <span class="abstract-full" style="display: none;">Reflective and textureless surfaces remain a challenge in multi-view 3D reconstruction.Both camera pose calibration and shape reconstruction often fail due to insufficient or unreliable cross-view visual features. To address these issues, we present PMNI (Pose-free Multi-view Normal Integration), a neural surface reconstruction method that incorporates rich geometric information by leveraging surface normal maps instead of RGB images. By enforcing geometric constraints from surface normals and multi-view shape consistency within a neural signed distance function (SDF) optimization framework, PMNI simultaneously recovers accurate camera poses and high-fidelity surface geometry. Experimental results on synthetic and real-world datasets show that our method achieves state-of-the-art performance in the reconstruction of reflective surfaces, even without reliable initial camera poses.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #76aa96" title="Confidence: 75.6%">
                        3D
                    </span>
            <!-- LLMs: 19.8 -->
                
            <!-- GNN: 3.9 -->
                
            <!-- Robotics: 3.0 -->
                
            <!-- T2I: 2.3 -->
                
            <!-- RAG: 1.9 -->
                
            <!-- Medicine: 1.4 -->
                
            <!-- Bayesian Optimization: 1.3 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Attention: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -10.9163
            </span>
            <a href="https://arxiv.org/abs/2503.16681" target="_blank" rel="noopener noreferrer">GauRast: Enhancing GPU Triangle Rasterizers to Accelerate 3D Gaussian Splatting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sixu Li, Ben Keller, Yingyan Celine Lin, Brucek Khailany | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">3D intelligence leverages rich 3D features and stands as a promising frontier in AI, with 3D rendering fundamental to many downstream applications. 3D Gaussian Splatting (3DGS), an emerging high-quality 3D rendering method, requires significant computation, making real-time execution on existing GPU</span>
            
            <span class="abstract-full" style="display: none;">3D intelligence leverages rich 3D features and stands as a promising frontier in AI, with 3D rendering fundamental to many downstream applications. 3D Gaussian Splatting (3DGS), an emerging high-quality 3D rendering method, requires significant computation, making real-time execution on existing GPU-equipped edge devices infeasible. Previous efforts to accelerate 3DGS rely on dedicated accelerators that require substantial integration overhead and hardware costs. This work proposes an acceleration strategy that leverages the similarities between the 3DGS pipeline and the highly optimized conventional graphics pipeline in modern GPUs. Instead of developing a dedicated accelerator, we enhance existing GPU rasterizer hardware to efficiently support 3DGS operations. Our results demonstrate a 23$\times$ increase in processing speed and a 24$\times$ reduction in energy consumption, with improvements yielding 6$\times$ faster end-to-end runtime for the original 3DGS algorithm and 4$\times$ for the latest efficiency-improved pipeline, achieving 24 FPS and 46 FPS respectively. These enhancements incur only a minimal area overhead of 0.2\% relative to the entire SoC chip area, underscoring the practicality and efficiency of our approach for enabling 3DGS rendering on resource-constrained platforms.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #76aa96" title="Confidence: 75.8%">
                        3D
                    </span>
            <!-- LLMs: 17.1 -->
                
            <!-- GNN: 4.0 -->
                
            <!-- Robotics: 3.1 -->
                
            <!-- T2I: 2.4 -->
                
            <!-- RAG: 1.9 -->
                
            <!-- Medicine: 1.3 -->
                
            <!-- Bayesian Optimization: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Attention: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -10.9426
            </span>
            <a href="https://arxiv.org/abs/2409.16938" target="_blank" rel="noopener noreferrer">Generative Object Insertion in Gaussian Splatting with a Multi-View Diffusion Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hongliang Zhong, Can Wang, Jingbo Zhang, Jing Liao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generating and inserting new objects into 3D content is a compelling approach for achieving versatile scene recreation. Existing methods, which rely on SDS optimization or single-view inpainting, often struggle to produce high-quality results. To address this, we propose a novel method for object in</span>
            
            <span class="abstract-full" style="display: none;">Generating and inserting new objects into 3D content is a compelling approach for achieving versatile scene recreation. Existing methods, which rely on SDS optimization or single-view inpainting, often struggle to produce high-quality results. To address this, we propose a novel method for object insertion in 3D content represented by Gaussian Splatting. Our approach introduces a multi-view diffusion model, dubbed MVInpainter, which is built upon a pre-trained stable video diffusion model to facilitate view-consistent object inpainting. Within MVInpainter, we incorporate a ControlNet-based conditional injection module to enable controlled and more predictable multi-view generation. After generating the multi-view inpainted results, we further propose a mask-aware 3D reconstruction technique to refine Gaussian Splatting reconstruction from these sparse inpainted views. By leveraging these fabricate techniques, our approach yields diverse results, ensures view-consistent and harmonious insertions, and produces better object quality. Extensive experiments demonstrate that our approach outperforms existing methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #76aa96" title="Confidence: 76.4%">
                        3D
                    </span>
            <!-- LLMs: 18.1 -->
                
            <!-- GNN: 3.9 -->
                
            <!-- Robotics: 3.0 -->
                
            <!-- T2I: 2.7 -->
                
            <!-- RAG: 2.0 -->
                
            <!-- Bayesian Optimization: 1.3 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Attention: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Medicine: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -11.03
            </span>
            <a href="https://arxiv.org/abs/2401.12452" target="_blank" rel="noopener noreferrer">Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural Calibration</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yifan Zhang, Siyu Ren, Junhui Hou, Jinjian Wu, Yixuan Yuan, Guangming Shi | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper introduces a novel self-supervised learning framework for enhancing 3D perception in autonomous driving scenes. Specifically, our approach, namely NCLR, focuses on 2D-3D neural calibration, a novel pretext task that estimates the rigid pose aligning camera and LiDAR coordinate systems. Fi</span>
            
            <span class="abstract-full" style="display: none;">This paper introduces a novel self-supervised learning framework for enhancing 3D perception in autonomous driving scenes. Specifically, our approach, namely NCLR, focuses on 2D-3D neural calibration, a novel pretext task that estimates the rigid pose aligning camera and LiDAR coordinate systems. First, we propose the learnable transformation alignment to bridge the domain gap between image and point cloud data, converting features into a unified representation space for effective comparison and matching. Second, we identify the overlapping area between the image and point cloud with the fused features. Third, we establish dense 2D-3D correspondences to estimate the rigid pose. The framework not only learns fine-grained matching from points to pixels but also achieves alignment of the image and point cloud at a holistic level, understanding their relative pose. We demonstrate the efficacy of NCLR by applying the pre-trained backbone to downstream tasks, such as LiDAR-based 3D semantic segmentation, object detection, and panoptic segmentation. Comprehensive experiments on various datasets illustrate the superiority of NCLR over existing self-supervised methods. The results confirm that joint learning from different modalities significantly enhances the network's understanding abilities and effectiveness of learned representation. The code is publicly available at https://github.com/Eaphan/NCLR.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- 3D: 41.4 -->
                
            <!-- LLMs: 8.6 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Medicine: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -16.8908
            </span>
            <a href="https://arxiv.org/abs/2411.03976" target="_blank" rel="noopener noreferrer">HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion Segmentation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ziyuan Ding, Yixiong Liang, Shichao Kan, Qing Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">High resolution is crucial for precise segmentation in fundus images, yet handling high-resolution inputs incurs considerable GPU memory costs, with diminishing performance gains as overhead increases. To address this issue while tackling the challenge of segmenting tiny objects, recent studies have</span>
            
            <span class="abstract-full" style="display: none;">High resolution is crucial for precise segmentation in fundus images, yet handling high-resolution inputs incurs considerable GPU memory costs, with diminishing performance gains as overhead increases. To address this issue while tackling the challenge of segmenting tiny objects, recent studies have explored local-global fusion methods. These methods preserve fine details using local regions and capture long-range context information from downscaled global images. However, the necessity of multiple forward passes inevitably incurs significant computational overhead, adversely affecting inference speed. In this paper, we propose HRDecoder, a simple High-Resolution Decoder network for fundus lesion segmentation. It integrates a high-resolution representation learning module to capture fine-grained local features and a high-resolution fusion module to fuse multi-scale predictions. Our method effectively improves the overall segmentation accuracy of fundus lesions while consuming reasonable memory and computational overhead, and maintaining satisfying inference speed. Experimental results on the IDRiD and DDR datasets demonstrate the effectiveness of our method. Code is available at https://github.com/CVIU-CSU/HRDecoder.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 67.3%">
                        Medicine
                    </span>
            <!-- LLMs: 7.8 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -18.2289
            </span>
            <a href="https://arxiv.org/abs/2504.08659" target="_blank" rel="noopener noreferrer">BowelRCNN: Region-based Convolutional Neural Network System for Bowel Sound Auscultation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Igor Matynia, Robert Nowak | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Sound events representing intestinal activity detection is a diagnostic tool with potential to identify gastrointestinal conditions. This article introduces BowelRCNN, a novel bowel sound detection system that uses audio recording, spectrogram analysys and region-based convolutional neural network (</span>
            
            <span class="abstract-full" style="display: none;">Sound events representing intestinal activity detection is a diagnostic tool with potential to identify gastrointestinal conditions. This article introduces BowelRCNN, a novel bowel sound detection system that uses audio recording, spectrogram analysys and region-based convolutional neural network (RCNN) architecture. The system was trained and validated on a real recording dataset gathered from 19 patients, comprising 60 minutes of prepared and annotated audio data. BowelRCNN achieved a classification accuracy of 96% and an F1 score of 71%. This research highlights the feasibility of using CNN architectures for bowel sound auscultation, achieving results comparable to those of recurrent-convolutional methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 66.8%">
                        Medicine
                    </span>
            <!-- LLMs: 6.8 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -18.5355
            </span>
            <a href="https://arxiv.org/abs/2504.08329" target="_blank" rel="noopener noreferrer">MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Junmo Kim, Namkyeong Lee, Jiwon Kim, Kwangsoo Kim | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generali</span>
            
            <span class="abstract-full" style="display: none;">Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generality of EHR foundation models and the integration of models trained with different vocabularies. To deal with this problem, we propose MedRep for EHR foundation models based on the observational medical outcome partnership (OMOP) common data model (CDM), providing the integrated medical concept representations and the basic data augmentation strategy for patient trajectories. For concept representation learning, we enrich the information of each concept with a minimal definition through large language model (LLM) prompts and enhance the text-based representations through graph ontology of OMOP vocabulary. Trajectory augmentation randomly replaces selected concepts with other similar concepts that have closely related representations to let the model practice with the concepts out-of-vocabulary. Finally, we demonstrate that EHR foundation models trained with MedRep better maintain the prediction performance in external datasets. Our code implementation is publicly available at https://github.com/kicarussays/MedRep.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 68.9%">
                        Medicine
                    </span>
            <!-- LLMs: 7.3 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- GNN: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -19.1154
            </span>
            <a href="https://arxiv.org/abs/2504.08481" target="_blank" rel="noopener noreferrer">A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Medical Image Classification</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kerol Djoumessi, Samuel Ofosu Mensah, Philipp Berens | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In many medical imaging tasks, convolutional neural networks (CNNs) efficiently extract local features hierarchically. More recently, vision transformers (ViTs) have gained popularity, using self-attention mechanisms to capture global dependencies, but lacking the inherent spatial localization of co</span>
            
            <span class="abstract-full" style="display: none;">In many medical imaging tasks, convolutional neural networks (CNNs) efficiently extract local features hierarchically. More recently, vision transformers (ViTs) have gained popularity, using self-attention mechanisms to capture global dependencies, but lacking the inherent spatial localization of convolutions. Therefore, hybrid models combining CNNs and ViTs have been developed to combine the strengths of both architectures. However, such hybrid CNN-ViT models are difficult to interpret, which hinders their application in medical imaging. In this work, we introduce an interpretable-by-design hybrid fully convolutional CNN-Transformer architecture for medical image classification. Unlike widely used post-hoc saliency methods for ViTs, our approach generates faithful and localized evidence maps that directly reflect the model's decision process. We evaluated our method on two medical image classification tasks using color fundus images. Our model not only achieves state-of-the-art predictive performance compared to both black-box and interpretable models but also provides class-specific sparse evidence maps in a single forward pass. The code is available at: https://anonymous.4open.science/r/Expl-CNN-Transformer/.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 62.6%">
                        Medicine
                    </span>
            <!-- LLMs: 9.2 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -20.3415
            </span>
            <a href="https://arxiv.org/abs/2503.14049" target="_blank" rel="noopener noreferrer">A Modular Edge Device Network for Surgery Digitalization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vincent Schorp, Fr\'ed\'eric Giraud, Gianluca Parg\"atzi, Michael W\"aspe, Lorenzo von Ritter-Zahony, Marcel Wegmann, Nicola A. Cavalcanti, John Garcia Henao, Nicholas B\"unger, Dominique Cachin, Sebastiano Caprara, Philipp F\"urnstahl, Fabio Carrillo | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Future surgical care demands real-time, integrated data to drive informed decision-making and improve patient outcomes. The pressing need for seamless and efficient data capture in the OR motivates our development of a modular solution that bridges the gap between emerging machine learning technique</span>
            
            <span class="abstract-full" style="display: none;">Future surgical care demands real-time, integrated data to drive informed decision-making and improve patient outcomes. The pressing need for seamless and efficient data capture in the OR motivates our development of a modular solution that bridges the gap between emerging machine learning techniques and interventional medicine. We introduce a network of edge devices, called Data Hubs (DHs), that interconnect diverse medical sensors, imaging systems, and robotic tools via optical fiber and a centralized network switch. Built on the NVIDIA Jetson Orin NX, each DH supports multiple interfaces (HDMI, USB-C, Ethernet) and encapsulates device-specific drivers within Docker containers using the Isaac ROS framework and ROS2. A centralized user interface enables straightforward configuration and real-time monitoring, while an Nvidia DGX computer provides state-of-the-art data processing and storage. We validate our approach through an ultrasound-based 3D anatomical reconstruction experiment that combines medical imaging, pose tracking, and RGB-D data acquisition.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 61.4%">
                        Medicine
                    </span>
            <!-- LLMs: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -20.5423
            </span>
            <a href="https://arxiv.org/abs/2504.08675" target="_blank" rel="noopener noreferrer">X2BR: High-Fidelity 3D Bone Reconstruction from a Planar X-Ray Image with Hybrid Neural Implicit Methods</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Gokce Guven, H. Fatih Ugurdag, Hasan F. Ates | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Accurate 3D bone reconstruction from a single planar X-ray remains a challenge due to anatomical complexity and limited input data. We propose X2BR, a hybrid neural implicit framework that combines continuous volumetric reconstruction with template-guided non-rigid registration. The core network, X2</span>
            
            <span class="abstract-full" style="display: none;">Accurate 3D bone reconstruction from a single planar X-ray remains a challenge due to anatomical complexity and limited input data. We propose X2BR, a hybrid neural implicit framework that combines continuous volumetric reconstruction with template-guided non-rigid registration. The core network, X2B, employs a ConvNeXt-based encoder to extract spatial features from X-rays and predict high-fidelity 3D bone occupancy fields without relying on statistical shape models. To further refine anatomical accuracy, X2BR integrates a patient-specific template mesh, constructed using YOLOv9-based detection and the SKEL biomechanical skeleton model. The coarse reconstruction is aligned to the template using geodesic-based coherent point drift, enabling anatomically consistent 3D bone volumes. Experimental results on a clinical dataset show that X2B achieves the highest numerical accuracy, with an IoU of 0.952 and Chamfer-L1 distance of 0.005, outperforming recent baselines including X2V and D2IM-Net. Building on this, X2BR incorporates anatomical priors via YOLOv9-based bone detection and biomechanical template alignment, leading to reconstructions that, while slightly lower in IoU (0.875), offer superior anatomical realism, especially in rib curvature and vertebral alignment. This numerical accuracy vs. visual consistency trade-off between X2B and X2BR highlights the value of hybrid frameworks for clinically relevant 3D reconstructions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 64.3%">
                        Medicine
                    </span>
            <!-- 3D: 40.2 -->
                
            <!-- LLMs: 10.9 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -20.5694
            </span>
            <a href="https://arxiv.org/abs/2504.08469" target="_blank" rel="noopener noreferrer">Artifact detection and localization in single-channel mobile EEG for sleep research using deep learning and attention mechanisms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Khrystyna Semkiv, Jia Zhang, Maria Laura Ferster, Walter Karlen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Artifacts in the electroencephalogram (EEG) degrade signal quality and impact the analysis of brain activity. Current methods for detecting artifacts in sleep EEG rely on simple threshold-based algorithms that require manual intervention, which is time-consuming and impractical due to the vast volum</span>
            
            <span class="abstract-full" style="display: none;">Artifacts in the electroencephalogram (EEG) degrade signal quality and impact the analysis of brain activity. Current methods for detecting artifacts in sleep EEG rely on simple threshold-based algorithms that require manual intervention, which is time-consuming and impractical due to the vast volume of data that novel mobile recording systems generate. We propose a convolutional neural network (CNN) model incorporating a convolutional block attention module (CNN-CBAM) to detect and identify the location of artifacts in the sleep EEG with attention maps. We benchmarked this model against six other machine learning and signal processing approaches. We trained/tuned all models on 72 manually annotated EEG recordings obtained during home-based monitoring from 18 healthy participants with a mean (SD) age of 68.05 y ($\pm$5.02). We tested them on 26 separate recordings from 6 healthy participants with a mean (SD) age of 68.33 y ($\pm$4.08), with contained artifacts in 4\% of epochs. CNN-CBAM achieved the highest area under the receiver operating characteristic curve (0.88), sensitivity (0.81), and specificity (0.86) when compared to the other approaches. The attention maps from CNN-CBAM localized artifacts within the epoch with a sensitivity of 0.71 and specificity of 0.67. This work demonstrates the feasibility of automating the detection and localization of artifacts in wearable sleep EEG.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 72.3%">
                        Medicine
                    </span>
            <!-- Quantum Computing: 4.7 -->
                
            <!-- LLMs: 4.3 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -35.9688
            </span>
            <a href="https://arxiv.org/abs/2412.09404" target="_blank" rel="noopener noreferrer">Opinion de-polarization of social networks with GNNs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Konstantinos Mylonas, Thrasyvoulos Spyropoulos | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Nowadays, social media is the ground for political debate and exchange of opinions. There is a significant amount of research that suggests that social media are highly polarized. A phenomenon that is commonly observed is the echo chamber structure, where users are organized in polarized communities</span>
            
            <span class="abstract-full" style="display: none;">Nowadays, social media is the ground for political debate and exchange of opinions. There is a significant amount of research that suggests that social media are highly polarized. A phenomenon that is commonly observed is the echo chamber structure, where users are organized in polarized communities and form connections only with similar-minded individuals, limiting themselves to consume specific content. In this paper we explore a way to decrease the polarization of networks with two echo chambers. Particularly, we observe that if some users adopt a moderate opinion about a topic, the polarization of the network decreases. Based on this observation, we propose an efficient algorithm to identify a good set of K users, such that if they adopt a moderate stance around a topic, the polarization is minimized. Our algorithm employs a Graph Neural Network and thus it can handle large graphs more effectively than other approaches</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #b243cd" title="Confidence: 76.8%">
                        GNN
                    </span>
            <!-- LLMs: 15.7 -->
                
            <!-- Robotics: 3.0 -->
                
            <!-- 3D: 2.7 -->
                
            <!-- T2I: 2.5 -->
                
            <!-- RAG: 1.7 -->
                
            <!-- Attention: 1.4 -->
                
            <!-- Bayesian Optimization: 1.4 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Medicine: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -40.6141
            </span>
            <a href="https://arxiv.org/abs/2503.10790" target="_blank" rel="noopener noreferrer">Quantum Error Detection For Early Term Fault-Tolerant Quantum Algorithms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tom Ginsberg, Vyom Patel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Quantum error detection (QED) offers a promising pathway to fault tolerance in near-term quantum devices by balancing error suppression with minimal resource overhead. However, its practical utility hinges on optimizing design parameters-such as syndrome measurement frequency-to avoid diminishing re</span>
            
            <span class="abstract-full" style="display: none;">Quantum error detection (QED) offers a promising pathway to fault tolerance in near-term quantum devices by balancing error suppression with minimal resource overhead. However, its practical utility hinges on optimizing design parameters-such as syndrome measurement frequency-to avoid diminishing returns from detection overhead. In this work, we present a comprehensive framework for fault-tolerant compilation and simulation of quantum algorithms using [[n, n-2, 2]] codes, which enable low-qubit-overhead error detection and a simple nearly fault-tolerant universal set of operations. We demonstrate and analyze our pipeline with a purely statistical interpretation and through the implementation of Grover's search algorithm. Our results are used to answer the question is quantum error detection a worthwhile avenue for early-term fault tolerance, and if so how can we get the most out of it? Simulations under the circuit-level noise model reveal that finding optimal syndrome schedules improves algorithm success probabilities by an average of 6.7x but eventual statistical limits from post-selection in noisy/resource-limited regimes constrain scalability. Furthermore, we propose a simple data-driven approach to predict fault tolerant compilation parameters, such as optimal syndrome schedules, and expected fault tolerant performance gains based on circuit and noise features. These results provide actionable guidelines for implementing QED in early-term quantum experiments and underscore its role as a pragmatic, constant-overhead error mitigation layer for shallow algorithms. To aid in further research, we release all simulation data computed for this work and provide an experimental QED compiler at https://codeqraft.xyz/qed.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 59.9%">
                        Quantum Computing
                    </span>
            <!-- LLMs: 8.3 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -48.4493
            </span>
            <a href="https://arxiv.org/abs/2504.06951" target="_blank" rel="noopener noreferrer">GLT hidden structures in mean-field quantum spin systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Christiaan J. F. van de Ven, Muhammad Faisal Khan, S. Serra-Capizzano | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work explores structured matrix sequences arising in mean-field quantum spin systems. We express these sequences within the framework of generalized locally Toeplitz (GLT) $*$-algebras, leveraging the fact that each GLT matrix sequence has a unique GLT symbol. This symbol characterizes both the</span>
            
            <span class="abstract-full" style="display: none;">This work explores structured matrix sequences arising in mean-field quantum spin systems. We express these sequences within the framework of generalized locally Toeplitz (GLT) $*$-algebras, leveraging the fact that each GLT matrix sequence has a unique GLT symbol. This symbol characterizes both the asymptotic singular value distribution and, for Hermitian or quasi-Hermitian sequences, the asymptotic spectral distribution. Specifically, we analyze two cases of real symmetric matrix sequences stemming from mean-field quantum spin systems and determine their associated distributions using GLT theory. Our study concludes with visualizations and numerical tests that validate the theoretical findings, followed by a discussion of open problems and future directions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 68.3%">
                        Quantum Computing
                    </span>
            <!-- Medicine: 8.2 -->
                
            <!-- LLMs: 6.0 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Hardware: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -49.8057
            </span>
            <a href="https://arxiv.org/abs/2405.08190" target="_blank" rel="noopener noreferrer">Barren plateaus are amplified by the dimension of qudits</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lucas Friedrich, Tiago de Souza Farias, Jonas Maziero | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for attaining quantum advantage in diverse scientific and technological domains, notably within Quantum Neural Networks. However, despite their potential, VQAs encounter significant obstacles, chief among them being the vanishi</span>
            
            <span class="abstract-full" style="display: none;">Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for attaining quantum advantage in diverse scientific and technological domains, notably within Quantum Neural Networks. However, despite their potential, VQAs encounter significant obstacles, chief among them being the vanishing gradient problem, commonly referred to as barren plateaus. In this article, through meticulous analysis, we demonstrate that existing literature implicitly suggests the intrinsic influence of qudit dimensionality on barren plateaus. To instantiate these findings, we present numerical results that exemplify the impact of qudit dimensionality on barren plateaus. Therefore, despite the proposition of various error mitigation techniques, our results call for further scrutiny about their efficacy in the context of VQAs with qudits.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 70.2%">
                        Quantum Computing
                    </span>
            <!-- Medicine: 8.4 -->
                
            <!-- LLMs: 6.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -52.1041
            </span>
            <a href="https://arxiv.org/abs/2504.07048" target="_blank" rel="noopener noreferrer">Context Switching for Secure Multi-programming of Near-Term Quantum Computers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Avinash Kumar, Meng Wang, Chenxu Liu, Ang Li, Prashant J. Nair, Poulami Das | Date: 2025-04-15
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Multi-programming quantum computers improve device utilization and throughput. However, crosstalk from concurrent two-qubit CNOT gates poses security risks, compromising the fidelity and output of co-running victim programs. We design Zero Knowledge Tampering Attacks (ZKTAs), using which attackers c</span>
            
            <span class="abstract-full" style="display: none;">Multi-programming quantum computers improve device utilization and throughput. However, crosstalk from concurrent two-qubit CNOT gates poses security risks, compromising the fidelity and output of co-running victim programs. We design Zero Knowledge Tampering Attacks (ZKTAs), using which attackers can exploit crosstalk without knowledge of the hardware error profile. ZKTAs can alter victim program outputs in 40% of cases on commercial systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #d37d97" title="Confidence: 78.7%">
                        Quantum Computing
                    </span>
            <!-- Medicine: 10.3 -->
                
            <!-- Networks: 3.4 -->
                
            <!-- Reinforcement Learning: 2.7 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Blockchain: 1.7 -->
                
            <!-- LLMs: 1.7 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Hardware: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.3 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- SpikingNN: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -71.1251
            </span>
            <a href="https://arxiv.org/abs/2504.07589" target="_blank" rel="noopener noreferrer">Copy-and-Paste? Identifying EVM-Inequivalent Code Smells in Multi-chain Reuse Contracts</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zexu Wang, Jiachi Chen, Tao Zhang, Yu Zhang, Weizhe Zhang, Yuming Feng, Zibin Zheng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As the development of Solidity contracts on Ethereum, more developers are reusing them on other compatible blockchains. However, developers may overlook the differences between the designs of the blockchain system, such as the Gas Mechanism and Consensus Protocol, leading to the same contracts on di</span>
            
            <span class="abstract-full" style="display: none;">As the development of Solidity contracts on Ethereum, more developers are reusing them on other compatible blockchains. However, developers may overlook the differences between the designs of the blockchain system, such as the Gas Mechanism and Consensus Protocol, leading to the same contracts on different blockchains not being able to achieve consistent execution as on Ethereum. This inconsistency reveals design flaws in reused contracts, exposing code smells that hinder code reusability, and we define this inconsistency as EVM-Inequivalent Code Smells. In this paper, we conducted the first empirical study to reveal the causes and characteristics of EVM-Inequivalent Code Smells. To ensure the identified smells reflect real developer concerns, we collected and analyzed 1,379 security audit reports and 326 Stack Overflow posts related to reused contracts on EVM-compatible blockchains, such as Binance Smart Chain (BSC) and Polygon. Using the open card sorting method, we defined six types of EVM-Inequivalent Code Smells. For automated detection, we developed a tool named EquivGuard. It employs static taint analysis to identify key paths from different patterns and uses symbolic execution to verify path reachability. Our analysis of 905,948 contracts across six major blockchains shows that EVM-Inequivalent Code Smells are widespread, with an average prevalence of 17.70%. While contracts with code smells do not necessarily lead to financial loss and attacks, their high prevalence and significant asset management underscore the potential threats of reusing these smelly Ethereum contracts. Thus, developers are advised to abandon Copy-and-Paste programming practices and detect EVM-Inequivalent Code Smells before reusing Ethereum contracts.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #3cc377" title="Confidence: 62.8%">
                        Blockchain
                    </span>
            <!-- Medicine: 7.4 -->
                
            <!-- LLMs: 6.8 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    
    <div id="jsonPopup" class="json-popup">
        <pre id="jsonContent"></pre>
        <button onclick="copyJson()">Copy to Clipboard</button>
        <button onclick="closePopup()">Close</button>
    </div>

    <script>
        function extractPaperData(paperElement) {
            const titleElement = paperElement.querySelector('.paper-title a');
            const metaElement = paperElement.querySelector('.paper-meta');
            const abstractElement = paperElement.querySelector('.paper-abstract');
            const tagsElement = paperElement.querySelector('.paper-tags');
            
            const authorsText = metaElement.textContent.split('|')[0].replace('Authors:', '').trim();
            const dateText = metaElement.textContent.split('|')[1].replace('Date:', '').trim();
            
            const paperData = {
                title: titleElement.textContent,
                url: titleElement.href,
                authors: authorsText.split(',').map(author => author.trim()),
                created: dateText,
                abstract: abstractElement.querySelector('.abstract-full').textContent
            };
            
            return paperData;
        }

        function showJson(paperElement) {
            const popup = document.getElementById('jsonPopup');
            const content = document.getElementById('jsonContent');
            const paperData = extractPaperData(paperElement);
            content.textContent = JSON.stringify(paperData, null, null);
            popup.style.display = 'block';
            document.addEventListener('click', function closePopupOnClick(event) {
                if (!popup.contains(event.target)) {
                    popup.style.display = 'none';
                    document.removeEventListener('click', closePopupOnClick);
                }
            });
        }
        function toggleAbstract(element) {
            const abstract = element.parentElement;
            const short = abstract.querySelector('.abstract-short');
            const full = abstract.querySelector('.abstract-full');
            const lowConfidenceTags = abstract.parentElement.querySelectorAll('.tag-badge.low-confidence');
            
            if (element.textContent === '... more') {
                short.style.display = 'none';
                full.style.display = 'inline';
                element.textContent = ' less';
                lowConfidenceTags.forEach(tag => tag.style.display = 'inline-block');
            } else {
                short.style.display = 'inline';
                full.style.display = 'none';
                element.textContent = '... more';
                lowConfidenceTags.forEach(tag => tag.style.display = 'none');
            }
        }

        function closePopup() {
            document.getElementById('jsonPopup').style.display = 'none';
        }

        function copyJson() {
            const content = document.getElementById('jsonContent').textContent;
            navigator.clipboard.writeText(content).catch(() => {
                // If clipboard API is not available, just show the popup
                alert('Could not copy to clipboard. JSON is displayed in the popup.');
            });
        }

        // Close popup when clicking outside
        window.onclick = function(event) {
            const popup = document.getElementById('jsonPopup');
            if (event.target === popup) {
                popup.style.display = 'none';
            }
        }
    </script>
</body>
</html> 